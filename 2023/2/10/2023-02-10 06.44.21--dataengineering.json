{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.\n\n\n\nNobody would know \n\n\u2014 time spent to find optimal parallelism\n\n\n\u2014 time spent for solving weird dq issues\n\n\n\u2014 time spent to make the pipeline dynamically handle schema change\n\n\n\u2014 time spent for threading it all together in orchestrator\n\n\nAfter all these years still hurts on how thankless this job is.\n\nOnly lesson : Thank your plumber next time you meet him.", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thanking my plumber every time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ybytx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 68, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675990984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.&lt;/p&gt;\n\n&lt;p&gt;Nobody would know &lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent to find optimal parallelism&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent for solving weird dq issues&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent to make the pipeline dynamically handle schema change&lt;/p&gt;\n\n&lt;p&gt;\u2014 time spent for threading it all together in orchestrator&lt;/p&gt;\n\n&lt;p&gt;After all these years still hurts on how thankless this job is.&lt;/p&gt;\n\n&lt;p&gt;Only lesson : Thank your plumber next time you meet him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ybytx", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/", "subreddit_subscribers": 89051, "created_utc": 1675990984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I became a data engineer, most of the data engineers were coming from BI/ETL, usually after having spent a few years doing BI.\n\nHow do people get to engineering now? I saw many get to it via data science for a couple of years before they specialised\n\n[View Poll](https://www.reddit.com/poll/10xus1c)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New data engineers (0-3y in the job): How did you get into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xus1c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675949395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I became a data engineer, most of the data engineers were coming from BI/ETL, usually after having spent a few years doing BI.&lt;/p&gt;\n\n&lt;p&gt;How do people get to engineering now? I saw many get to it via data science for a couple of years before they specialised&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10xus1c\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10xus1c", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1676208595272, "options": [{"text": "I started as a data scientist and later moved to DE", "id": "21513133"}, {"text": "I started as a software developer and later moved to DE", "id": "21513134"}, {"text": "I started as a BI/DWH/business analyst and moved to DE", "id": "21513135"}, {"text": "I started my career directly as DE", "id": "21513136"}, {"text": "I have more than 3y experience, so I am out of scope of the question.", "id": "21513137"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 1505, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xus1c/new_data_engineers_03y_in_the_job_how_did_you_get/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10xus1c/new_data_engineers_03y_in_the_job_how_did_you_get/", "subreddit_subscribers": 89051, "created_utc": 1675949395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI need to build an Azure data infrastructure, which ingests data from different on-prem source systems (but mainly from relational databases and from text files), loads the data to some sort of data lake (it could be something else too, like delta lake, all that matters is to have a platform from which various departments can access the data for various reasons), makes it possible to do transformations on this data (we are possibly talking about big amounts), then load the data to some kind of SQL Database / DWH System. This final platform needs to be something that provides seamless integration with cloud Dynamics 365 and Power BI.\n\nMy question for you is what kind of architecture would you use in this situation?\n\nSo far, I have explored the following options:\n\n- Data Factory / Azure Synapse Pipelines for copying data from on-prem to the data lake, and copying data from data lake to SQL Database. Also a possible option for starting scheduled Databricks jobs\n- Azure Data Lake Gen 2 as a data lake -&gt; this would be where raw, ingested data is stored (mainly text files, parquet files)\n- Azure Databricks  -&gt; For doing transformations on the ingested data. I have also explored the possibility of using Azure Delta Lake options. I am not sure what I the best practice in this situation. Is it worth to maintain a Data Lake AND a Delta Lake?\n- Azure Synapse Serverless / Dedicated SQL Pool OR SQL database / managed instance -&gt; this would be the platform which is the backend of the Dynamics 365 system. I'm really not sure if I should use Synapse for this whole process. It might be overkill for my needs.\n\nI understand this is probably a bit chaotic, Azure architecture is new to me. I'd appreciate any advice, any tips on best practices which help me decide between my options. \n\nTL;DR: Need to set up an Azure infrastructure for integrating data from on-prem DB and text files, storing raw data available for multiple use cases, enabling scheduled transformations, and providing some sort of SQL DB which can easily be integrated with Dynamics 365.", "author_fullname": "t2_vun99h9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for bringing data to Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvxpw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675952523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I need to build an Azure data infrastructure, which ingests data from different on-prem source systems (but mainly from relational databases and from text files), loads the data to some sort of data lake (it could be something else too, like delta lake, all that matters is to have a platform from which various departments can access the data for various reasons), makes it possible to do transformations on this data (we are possibly talking about big amounts), then load the data to some kind of SQL Database / DWH System. This final platform needs to be something that provides seamless integration with cloud Dynamics 365 and Power BI.&lt;/p&gt;\n\n&lt;p&gt;My question for you is what kind of architecture would you use in this situation?&lt;/p&gt;\n\n&lt;p&gt;So far, I have explored the following options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Factory / Azure Synapse Pipelines for copying data from on-prem to the data lake, and copying data from data lake to SQL Database. Also a possible option for starting scheduled Databricks jobs&lt;/li&gt;\n&lt;li&gt;Azure Data Lake Gen 2 as a data lake -&amp;gt; this would be where raw, ingested data is stored (mainly text files, parquet files)&lt;/li&gt;\n&lt;li&gt;Azure Databricks  -&amp;gt; For doing transformations on the ingested data. I have also explored the possibility of using Azure Delta Lake options. I am not sure what I the best practice in this situation. Is it worth to maintain a Data Lake AND a Delta Lake?&lt;/li&gt;\n&lt;li&gt;Azure Synapse Serverless / Dedicated SQL Pool OR SQL database / managed instance -&amp;gt; this would be the platform which is the backend of the Dynamics 365 system. I&amp;#39;m really not sure if I should use Synapse for this whole process. It might be overkill for my needs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I understand this is probably a bit chaotic, Azure architecture is new to me. I&amp;#39;d appreciate any advice, any tips on best practices which help me decide between my options. &lt;/p&gt;\n\n&lt;p&gt;TL;DR: Need to set up an Azure infrastructure for integrating data from on-prem DB and text files, storing raw data available for multiple use cases, enabling scheduled transformations, and providing some sort of SQL DB which can easily be integrated with Dynamics 365.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xvxpw", "is_robot_indexable": true, "report_reasons": null, "author": "justadataengineer", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvxpw/best_practices_for_bringing_data_to_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvxpw/best_practices_for_bringing_data_to_azure/", "subreddit_subscribers": 89051, "created_utc": 1675952523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m a manager at Big 4 consulting and I\u2019m trying to find resources which I can show my engineers to help them understand best practices &amp; common patterns in developing production ready applications / code bases. \n\nSome things I can think of that I\u2019ve had to teach them:\n\n1. Git\n2. Environment specific parameters\n3. Writing Unittests, mocking cloud services\n4. What is CI/CD", "author_fullname": "t2_ay99iuoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do we learn SWE best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ye0ji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675996673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m a manager at Big 4 consulting and I\u2019m trying to find resources which I can show my engineers to help them understand best practices &amp;amp; common patterns in developing production ready applications / code bases. &lt;/p&gt;\n\n&lt;p&gt;Some things I can think of that I\u2019ve had to teach them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Git&lt;/li&gt;\n&lt;li&gt;Environment specific parameters&lt;/li&gt;\n&lt;li&gt;Writing Unittests, mocking cloud services&lt;/li&gt;\n&lt;li&gt;What is CI/CD&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ye0ji", "is_robot_indexable": true, "report_reasons": null, "author": "Rich_Repair", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ye0ji/how_do_we_learn_swe_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ye0ji/how_do_we_learn_swe_best_practices/", "subreddit_subscribers": 89051, "created_utc": 1675996673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working on a private Python package with some useful stuff in it that I use across various bits of our pipeline. \n\nI'm trying to figure out how best to host and use it. I had this idea that I could just install directly from the package GitHub repo using a pip install in the Docker images built in CICD. However, I've recently read a bit about hosting packages on AWS Codeartifact and wondered if I should actually be doing a proper package build and storing it there? I'm not sure what benefits I'd get from that versus installing direct from source on GitHub, but instinctively I like it more. Annoyingly (and surprisingly) GitHub Packages doesn't support Python.\n\nIf anyone has done something similar I'd really appreciate some advice!", "author_fullname": "t2_1znkakv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Private Python packages in pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xttw4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675946578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a private Python package with some useful stuff in it that I use across various bits of our pipeline. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out how best to host and use it. I had this idea that I could just install directly from the package GitHub repo using a pip install in the Docker images built in CICD. However, I&amp;#39;ve recently read a bit about hosting packages on AWS Codeartifact and wondered if I should actually be doing a proper package build and storing it there? I&amp;#39;m not sure what benefits I&amp;#39;d get from that versus installing direct from source on GitHub, but instinctively I like it more. Annoyingly (and surprisingly) GitHub Packages doesn&amp;#39;t support Python.&lt;/p&gt;\n\n&lt;p&gt;If anyone has done something similar I&amp;#39;d really appreciate some advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xttw4", "is_robot_indexable": true, "report_reasons": null, "author": "Psychological-Suit-5", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xttw4/private_python_packages_in_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xttw4/private_python_packages_in_pipelines/", "subreddit_subscribers": 89051, "created_utc": 1675946578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey people,\n\n&amp;#x200B;\n\nok so, \"data as code\" is a simple idea: *put all your data (especially all that is newly created) into code, and treat it like any other software component. Put it in version control (in SOME way), test it before \"deploying it\" to production, run CD over it.*\n\n&amp;#x200B;\n\nI've been writing about this for some time and technology has moved quite a bit to make data as code real easy to do... So I'm still confused that it hasn't caught on.\n\n&amp;#x200B;\n\nThe benefit of having data in VC is pretty simple, it allows a level of reproducibility that is insane, and we don't have at all right now. It is also more secure, allows for more robust data stacks, and more...\n\n**So what is your take on \"data as code\"?** \n\n&amp;#x200B;\n\n**Notes:**\n\n\\- \"having it in version control\" for data obviously doesn't mean having huge amounts of data in VC. Instead, all reasonable solutions like lakeFS, DVC use metadata to make it fast and easy to switch between different \"versions\". \n\n\\- A simple implementation for reporting could look like this:\n\n   1. Your CI Pipeline (e.g. GitHub Actions or GitLab) triggers your Data Ingestion. The data gets ingested  and gets stored inbetween inside S3 with a new $COMMITHASH identifier. \n\n   2. Your CI Pipeline then (best in parallel!) pushes the data into your snowflake schema snowflake schema called \"cicd\\_$COMMITHASH\"; \n\n   3. Your CI Pipeline then triggers the dbt model runs, on \"cicd\\_$COMMITHASH\".\n\n4. You then run all your tests over your snowflake schema. if you're happy, you do a \"swap\" operation inside snowflake replacing the production schema with your new data.\n\n5. should something break, you trigger a \"roll-back to last version\" button inside your CI Pipeline that simply does a \"swap back\"\n\n6. Should you need to reproduce way older results, you simply trigger your CI Pipeline starting at step 2 with a different DVC data input set. \n\n\\- data as code works just fine for all the major kinds of data. For analytical data (reporting, dashboarding), for machine learning system and even for operational data (although this is way down the line in terms of importance).\n\n\\- yes in all applications (as you can see in the implementation) you do have to be careful to always use metadata like data movement operations (that are super fast) as not to slow down your pipelines, but there's enough technology available now to make that possible.", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data as code - why doesn't it catch on? What's your take?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xob67", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675926523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey people,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ok so, &amp;quot;data as code&amp;quot; is a simple idea: &lt;em&gt;put all your data (especially all that is newly created) into code, and treat it like any other software component. Put it in version control (in SOME way), test it before &amp;quot;deploying it&amp;quot; to production, run CD over it.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been writing about this for some time and technology has moved quite a bit to make data as code real easy to do... So I&amp;#39;m still confused that it hasn&amp;#39;t caught on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The benefit of having data in VC is pretty simple, it allows a level of reproducibility that is insane, and we don&amp;#39;t have at all right now. It is also more secure, allows for more robust data stacks, and more...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what is your take on &amp;quot;data as code&amp;quot;?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;having it in version control&amp;quot; for data obviously doesn&amp;#39;t mean having huge amounts of data in VC. Instead, all reasonable solutions like lakeFS, DVC use metadata to make it fast and easy to switch between different &amp;quot;versions&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;- A simple implementation for reporting could look like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Your CI Pipeline (e.g. GitHub Actions or GitLab) triggers your Data Ingestion. The data gets ingested  and gets stored inbetween inside S3 with a new $COMMITHASH identifier. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your CI Pipeline then (best in parallel!) pushes the data into your snowflake schema snowflake schema called &amp;quot;cicd_$COMMITHASH&amp;quot;; &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your CI Pipeline then triggers the dbt model runs, on &amp;quot;cicd_$COMMITHASH&amp;quot;.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You then run all your tests over your snowflake schema. if you&amp;#39;re happy, you do a &amp;quot;swap&amp;quot; operation inside snowflake replacing the production schema with your new data.&lt;/li&gt;\n&lt;li&gt;should something break, you trigger a &amp;quot;roll-back to last version&amp;quot; button inside your CI Pipeline that simply does a &amp;quot;swap back&amp;quot;&lt;/li&gt;\n&lt;li&gt;Should you need to reproduce way older results, you simply trigger your CI Pipeline starting at step 2 with a different DVC data input set. &lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- data as code works just fine for all the major kinds of data. For analytical data (reporting, dashboarding), for machine learning system and even for operational data (although this is way down the line in terms of importance).&lt;/p&gt;\n\n&lt;p&gt;- yes in all applications (as you can see in the implementation) you do have to be careful to always use metadata like data movement operations (that are super fast) as not to slow down your pipelines, but there&amp;#39;s enough technology available now to make that possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xob67", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xob67/data_as_code_why_doesnt_it_catch_on_whats_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xob67/data_as_code_why_doesnt_it_catch_on_whats_your/", "subreddit_subscribers": 89051, "created_utc": 1675926523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like there was a lot of noise and they showed some impressive benchmarks a while back. But from people I've spoken to and heard from at conferences I'm not aware of anyone using it. Is the project waning or just still super early? It's barely mentioned on this sub.", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use Redpanda?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xn9l4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675923013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like there was a lot of noise and they showed some impressive benchmarks a while back. But from people I&amp;#39;ve spoken to and heard from at conferences I&amp;#39;m not aware of anyone using it. Is the project waning or just still super early? It&amp;#39;s barely mentioned on this sub.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xn9l4", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xn9l4/anyone_use_redpanda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xn9l4/anyone_use_redpanda/", "subreddit_subscribers": 89051, "created_utc": 1675923013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zus64vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data imputation in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_10y9rwp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/upxsp65nnsAGtTc-gAoMDIllQnv_ZRo1odB1CqnmtpQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675985209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hubs.li", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hubs.li/Q01BW4NT0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?auto=webp&amp;v=enabled&amp;s=7e35df19320e1aad03aadd059f628000aeead487", "width": 800, "height": 526}, "resolutions": [{"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6ca36cd4a5a93b0c47d6e70a1c198501108da83", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=416c8300fc41b7723d9ebe3786248688a509aeb1", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e656629a9830f76ea20131566c7d5cb268153ca", "width": 320, "height": 210}, {"url": "https://external-preview.redd.it/-vmmwYqhrWGPbN3V0o0gS0905RPlMVcXDT7EXXZUmhw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f68309c60f31f46fdfc36020d446292538d58788", "width": 640, "height": 420}], "variants": {}, "id": "nGnUWldmoOF3Mysh8RqBH_abXJv3jq3i_pyrYZ3w6oc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10y9rwp", "is_robot_indexable": true, "report_reasons": null, "author": "oli_k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y9rwp/data_imputation_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hubs.li/Q01BW4NT0", "subreddit_subscribers": 89051, "created_utc": 1675985209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Azure data factory doesn't seem to provide direct copy activity from SAP to Databricks. One way I found was to get it in adls staging and auto create the table in databricks then send to Databricks. Are there better ways to do it.", "author_fullname": "t2_m1qiwmwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to migrate data from SAP to Databricks using ADF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xug50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675948412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Azure data factory doesn&amp;#39;t seem to provide direct copy activity from SAP to Databricks. One way I found was to get it in adls staging and auto create the table in databricks then send to Databricks. Are there better ways to do it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xug50", "is_robot_indexable": true, "report_reasons": null, "author": "uncertainBoi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xug50/what_is_the_best_way_to_migrate_data_from_sap_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xug50/what_is_the_best_way_to_migrate_data_from_sap_to/", "subreddit_subscribers": 89051, "created_utc": 1675948412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I am going through the understanding of  end to end data engineering process. I understand how things work, however, I have not found one resource to understand how the data is saved in such cloud storage by the enterprises?\n\nI mean, I understand data can come from anywhere like, website, mobile application or any app that is storing users details like facebooks user details. Now how does these data like, connected into (injested) into cloud storage? How are they mounted? A few real world examples or cases can help to understand. \n\nFor example: I like a post, or create a user account in Facebook, this data is saved in some database which is mounted to cloud storage? is this is it? and we go on with next from there?", "author_fullname": "t2_lngymjsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the idle scenario where a data in stored in azure blog or any cloud storage like S3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yb413", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675988622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am going through the understanding of  end to end data engineering process. I understand how things work, however, I have not found one resource to understand how the data is saved in such cloud storage by the enterprises?&lt;/p&gt;\n\n&lt;p&gt;I mean, I understand data can come from anywhere like, website, mobile application or any app that is storing users details like facebooks user details. Now how does these data like, connected into (injested) into cloud storage? How are they mounted? A few real world examples or cases can help to understand. &lt;/p&gt;\n\n&lt;p&gt;For example: I like a post, or create a user account in Facebook, this data is saved in some database which is mounted to cloud storage? is this is it? and we go on with next from there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10yb413", "is_robot_indexable": true, "report_reasons": null, "author": "iamdhage", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yb413/what_is_the_idle_scenario_where_a_data_in_stored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yb413/what_is_the_idle_scenario_where_a_data_in_stored/", "subreddit_subscribers": 89051, "created_utc": 1675988622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everybody! \n\nI am analyzing the data infrastructure and reliability for data collection in e-commerce marketplace where you can book appointments for services both from website and app. \n\nThe structure is as follows: \n\nSources: \n\n1. Google Tag Manager: in which there are tags for GA3 (UA), GA4 and Advertising Platforms (let's say acquisitions via Meta ads, Google and so on). Focusing solely on GA4, this data collection flow is considered **not reliable**. Consider that the company operates in Europe so I guess that the tracking via GA is highly biased by GDPR/Privacy problems.\n2. Snowplow.io : collects behavioral data, **reliable**.\n3. App collecting CRM data, considered **reliable.**\n\nhttps://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555\n\nMoreover, the data from:\n\n1. Advertising Platforms \n2. Snowplow events \n3. Product (CRM) data from the App \n\nare joined and used directly into Looker (apparently, without an intermediate data warehouse!)\n\nMy questions are: \n\nA. How would you run an assessment of this data structure? I would imagine to start with understanding why GA4 transactional data are wrongly collected and why they are \\*not\\* used into the Looker data. There might be consistent differences between the data in the adv APIs vs. data isn GA4.\n\n...Maybe looking in a server-side tracking solution could make this sources more reliable (for sure)?\n\n&amp;#x200B;\n\nB. Knowing that one can only trust behavioral data / CRM (so snowplow events and CRM data) how can you establish a unified source of truth?\n\nIn this case for me the idea would be to combine these data sources in a dwh/datalake first. \n\n&amp;#x200B;\n\nThanks for any hint in this analysis!", "author_fullname": "t2_bwe0zlu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evaluate reliability for a data infrastructure (e-commerce marketplace scenario)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cgk9xbon45ha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ec1f473c85355102d1bac9bd5308abf07941d69"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92a6f8205ef2ba66a37da2d57f5d9a848ab53fd2"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c58949813fc1be1b0423e8c5f01becbbb8fa50"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51835c982a9cb64b254fcee59c43beb7fe07ac3d"}, {"y": 567, "x": 960, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41137a70ba5563891a15b45611bf43414b36144b"}, {"y": 638, "x": 1080, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c96a228d3b4890c7d61249bb725db120dbb7305d"}], "s": {"y": 1136, "x": 1922, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555"}, "id": "cgk9xbon45ha1"}}, "name": "t3_10xrnof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DnpvPe1PyyxdDzwsPVhCV87CeCccw8OpJxT2nTvy4rs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675939097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody! &lt;/p&gt;\n\n&lt;p&gt;I am analyzing the data infrastructure and reliability for data collection in e-commerce marketplace where you can book appointments for services both from website and app. &lt;/p&gt;\n\n&lt;p&gt;The structure is as follows: &lt;/p&gt;\n\n&lt;p&gt;Sources: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Google Tag Manager: in which there are tags for GA3 (UA), GA4 and Advertising Platforms (let&amp;#39;s say acquisitions via Meta ads, Google and so on). Focusing solely on GA4, this data collection flow is considered &lt;strong&gt;not reliable&lt;/strong&gt;. Consider that the company operates in Europe so I guess that the tracking via GA is highly biased by GDPR/Privacy problems.&lt;/li&gt;\n&lt;li&gt;Snowplow.io : collects behavioral data, &lt;strong&gt;reliable&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;App collecting CRM data, considered &lt;strong&gt;reliable.&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555\"&gt;https://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Moreover, the data from:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Advertising Platforms &lt;/li&gt;\n&lt;li&gt;Snowplow events &lt;/li&gt;\n&lt;li&gt;Product (CRM) data from the App &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;are joined and used directly into Looker (apparently, without an intermediate data warehouse!)&lt;/p&gt;\n\n&lt;p&gt;My questions are: &lt;/p&gt;\n\n&lt;p&gt;A. How would you run an assessment of this data structure? I would imagine to start with understanding why GA4 transactional data are wrongly collected and why they are *not* used into the Looker data. There might be consistent differences between the data in the adv APIs vs. data isn GA4.&lt;/p&gt;\n\n&lt;p&gt;...Maybe looking in a server-side tracking solution could make this sources more reliable (for sure)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;B. Knowing that one can only trust behavioral data / CRM (so snowplow events and CRM data) how can you establish a unified source of truth?&lt;/p&gt;\n\n&lt;p&gt;In this case for me the idea would be to combine these data sources in a dwh/datalake first. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for any hint in this analysis!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xrnof", "is_robot_indexable": true, "report_reasons": null, "author": "xvinc666x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xrnof/evaluate_reliability_for_a_data_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xrnof/evaluate_reliability_for_a_data_infrastructure/", "subreddit_subscribers": 89051, "created_utc": 1675939097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The nurse can be called to a specific room , now what is the cardinality between the nurse and the room ? ( There are multiple examination rooms and Nurse)\n\nMy logic is that:\n\n it's m:n  because any nurse can be called to any room \n\nAm I right ?", "author_fullname": "t2_2ah0kkor", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Er diagram question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xp7l7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675929752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The nurse can be called to a specific room , now what is the cardinality between the nurse and the room ? ( There are multiple examination rooms and Nurse)&lt;/p&gt;\n\n&lt;p&gt;My logic is that:&lt;/p&gt;\n\n&lt;p&gt;it&amp;#39;s m:n  because any nurse can be called to any room &lt;/p&gt;\n\n&lt;p&gt;Am I right ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xp7l7", "is_robot_indexable": true, "report_reasons": null, "author": "omidhhh", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xp7l7/er_diagram_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xp7l7/er_diagram_question/", "subreddit_subscribers": 89051, "created_utc": 1675929752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I really love R, and want to use it as much as I can for analysis and visualizations.  (Although I am open to criticism)\n\nMy understanding of the two Synapse pools:\n\n1. Dedicated SQL pool: I think I should be able to fetch data with R using RODBC and the like, to work on the data in RStudio locally, in memory (assuming I pull something down that fits in local memory).  Is this true??\n\n2. Serverless SQL pool (parquet files in data lake):   Serverless natively \u201csupports R\u201d now in Azure with notebooks, but it seems like it\u2019s watered down, and not at all like running R in the RStudio IDE with the plethora of packages and features.  I may be wrong. Someone please tell me I\u2019m dumb and wrong.\n\nCan I point RStudio/RODBC to the Serverless pool?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to use R for data analysis/viz - do I have a choice between Synapse Dedicated SQL pool vs Serverless (parquet)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xndv6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675923383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really love R, and want to use it as much as I can for analysis and visualizations.  (Although I am open to criticism)&lt;/p&gt;\n\n&lt;p&gt;My understanding of the two Synapse pools:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Dedicated SQL pool: I think I should be able to fetch data with R using RODBC and the like, to work on the data in RStudio locally, in memory (assuming I pull something down that fits in local memory).  Is this true??&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Serverless SQL pool (parquet files in data lake):   Serverless natively \u201csupports R\u201d now in Azure with notebooks, but it seems like it\u2019s watered down, and not at all like running R in the RStudio IDE with the plethora of packages and features.  I may be wrong. Someone please tell me I\u2019m dumb and wrong.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can I point RStudio/RODBC to the Serverless pool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xndv6", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xndv6/i_want_to_use_r_for_data_analysisviz_do_i_have_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xndv6/i_want_to_use_r_for_data_analysisviz_do_i_have_a/", "subreddit_subscribers": 89051, "created_utc": 1675923383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently out of a job because I recently moved to a new country and sorting out a work permit.\n\nI'm currently volunteering at an NGO, they do not have a data warehouse. They manually manage data on their records management system. I see some ways I can leverage open-source modern data stack tools to automate the processes.\n\nShould finance be an issue for cloud storage, then I can do with them provisioning me with a server-like machine for the on-prem Postgres warehouse. \n\nMy concern however is, since this NGO is funded by the government, I have concerns about how the project can be scalable and won\u2019t eventually become an irrelevant project in the long run.\n\nJust to save me from \u201cif it ain\u2019t broke, don\u2019t fix it\u201d.\n\nI\u2019ll appreciate related experience and/or advice.", "author_fullname": "t2_c36b59fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Open-Source Data Stack for the NGO I\u2019m volunteering at - it it worth the effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10yhj68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676007903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently out of a job because I recently moved to a new country and sorting out a work permit.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently volunteering at an NGO, they do not have a data warehouse. They manually manage data on their records management system. I see some ways I can leverage open-source modern data stack tools to automate the processes.&lt;/p&gt;\n\n&lt;p&gt;Should finance be an issue for cloud storage, then I can do with them provisioning me with a server-like machine for the on-prem Postgres warehouse. &lt;/p&gt;\n\n&lt;p&gt;My concern however is, since this NGO is funded by the government, I have concerns about how the project can be scalable and won\u2019t eventually become an irrelevant project in the long run.&lt;/p&gt;\n\n&lt;p&gt;Just to save me from \u201cif it ain\u2019t broke, don\u2019t fix it\u201d.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll appreciate related experience and/or advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10yhj68", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-Flow220", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yhj68/building_a_opensource_data_stack_for_the_ngo_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yhj68/building_a_opensource_data_stack_for_the_ngo_im/", "subreddit_subscribers": 89051, "created_utc": 1676007903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What should I prepare for new grad application and interview?", "author_fullname": "t2_cl5t0lrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer New Grad Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yfnqa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676001660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What should I prepare for new grad application and interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10yfnqa", "is_robot_indexable": true, "report_reasons": null, "author": "Unlucky-Difficulty86", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yfnqa/data_engineer_new_grad_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yfnqa/data_engineer_new_grad_interview/", "subreddit_subscribers": 89051, "created_utc": 1676001660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I have been working with DE for about 3 years and my current company after a long discussion decided to settle for SSIS, does anyone have some good courses or source material on it? I know it can\u2019t be that hard to learn for someone with SQL and coding background but any help is appreciated.", "author_fullname": "t2_592klrfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSIS material", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y6hhl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675977451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have been working with DE for about 3 years and my current company after a long discussion decided to settle for SSIS, does anyone have some good courses or source material on it? I know it can\u2019t be that hard to learn for someone with SQL and coding background but any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10y6hhl", "is_robot_indexable": true, "report_reasons": null, "author": "pvic234", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y6hhl/ssis_material/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y6hhl/ssis_material/", "subreddit_subscribers": 89051, "created_utc": 1675977451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been recently asked to look into optimizing some notebooks written using spark sql and those notebooks sometime have 4 levels of nesting in the from clause. Does this effect performance? vs writing the logic in different data frames considering Spark optimizes the query plan anyway. If so how?", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "sub queries in spark - bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1kwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675966154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been recently asked to look into optimizing some notebooks written using spark sql and those notebooks sometime have 4 levels of nesting in the from clause. Does this effect performance? vs writing the logic in different data frames considering Spark optimizes the query plan anyway. If so how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10y1kwq", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y1kwq/sub_queries_in_spark_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y1kwq/sub_queries_in_spark_bad/", "subreddit_subscribers": 89051, "created_utc": 1675966154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I regularly receive files from 3rd parties that I import into our database as temporary tables.  I'm able to join most of the records with queries, matching on people's names and other fields.  But a small percentage need to be done manually.  Is there a tool out there for filtering/searching two data sets and using the mouse to drag and drop to match the remaining records together?  I'm getting tired of copying and pasting keys into an update statement to match these records.\n\nWhen I google this problem, there are lots of fuzzy-logic tools, but I'm looking for human-operated manual matching.", "author_fullname": "t2_71qwor1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a GUI tool for manually matching and setting foreign keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvucj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675955029.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675952277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I regularly receive files from 3rd parties that I import into our database as temporary tables.  I&amp;#39;m able to join most of the records with queries, matching on people&amp;#39;s names and other fields.  But a small percentage need to be done manually.  Is there a tool out there for filtering/searching two data sets and using the mouse to drag and drop to match the remaining records together?  I&amp;#39;m getting tired of copying and pasting keys into an update statement to match these records.&lt;/p&gt;\n\n&lt;p&gt;When I google this problem, there are lots of fuzzy-logic tools, but I&amp;#39;m looking for human-operated manual matching.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xvucj", "is_robot_indexable": true, "report_reasons": null, "author": "Jz7t6ak5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvucj/is_there_a_gui_tool_for_manually_matching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvucj/is_there_a_gui_tool_for_manually_matching_and/", "subreddit_subscribers": 89051, "created_utc": 1675952277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Situation: I have a pyspark job that needs to be deployed on a cluster that has SparkonK8s support. A pipeline is in place that builds an image and pushes an image to azure container registry and now there is Airflow job that refers to that image having a DAG that runs this pyspark job periodically. \nI have secrets that are being utilised by this job that need to be populated to the environment as environment varibales. These secrets are placed in azure ado library as a variable group connected to a keyvault and have been authorised for pipeline usage.\n\nProblem: Now I have an issue populating the environment variables with these secrets in the environment variables.\n\nMy approach:\n1.  I have tried populating the secrets through the pipeline itself using Bash @3 task/Powershell@2 task. I try to print them in the pipeline and they are successfully printed as well. But when i run kubectl apply to my spark config file to check if the environment has variables I don't see them.\n2. Created a shell script that would login through the azure service principal, connect to the key vault and export the secrets using export command in bash(This approach works really well in local). I trigger this script through python file it populates the environment but my job which is in a spark session isn't able to get the environment variables.\n\nQuestions\n1.Where I should be assigning the secrets as an environment variables?\n2. What should the ideal flow.\n3. If I am triggering the python script why is the pyspark job not able to get the secrets?\n\nDue to this issue the entire deployment of jobs is kind of stuck.\nPlease help, any kind of discussions or help is greatly appreciated?\n\n#pyspark #airflow #kubernetes #sparkonk8s #secrets", "author_fullname": "t2_389zs7ih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "But it works in my local\ud83d\ude29.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvk5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675951526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Situation: I have a pyspark job that needs to be deployed on a cluster that has SparkonK8s support. A pipeline is in place that builds an image and pushes an image to azure container registry and now there is Airflow job that refers to that image having a DAG that runs this pyspark job periodically. \nI have secrets that are being utilised by this job that need to be populated to the environment as environment varibales. These secrets are placed in azure ado library as a variable group connected to a keyvault and have been authorised for pipeline usage.&lt;/p&gt;\n\n&lt;p&gt;Problem: Now I have an issue populating the environment variables with these secrets in the environment variables.&lt;/p&gt;\n\n&lt;p&gt;My approach:\n1.  I have tried populating the secrets through the pipeline itself using Bash @3 task/Powershell@2 task. I try to print them in the pipeline and they are successfully printed as well. But when i run kubectl apply to my spark config file to check if the environment has variables I don&amp;#39;t see them.\n2. Created a shell script that would login through the azure service principal, connect to the key vault and export the secrets using export command in bash(This approach works really well in local). I trigger this script through python file it populates the environment but my job which is in a spark session isn&amp;#39;t able to get the environment variables.&lt;/p&gt;\n\n&lt;p&gt;Questions\n1.Where I should be assigning the secrets as an environment variables?\n2. What should the ideal flow.\n3. If I am triggering the python script why is the pyspark job not able to get the secrets?&lt;/p&gt;\n\n&lt;p&gt;Due to this issue the entire deployment of jobs is kind of stuck.\nPlease help, any kind of discussions or help is greatly appreciated?&lt;/p&gt;\n\n&lt;h1&gt;pyspark #airflow #kubernetes #sparkonk8s #secrets&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xvk5g", "is_robot_indexable": true, "report_reasons": null, "author": "shrey2204", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvk5g/but_it_works_in_my_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvk5g/but_it_works_in_my_local/", "subreddit_subscribers": 89051, "created_utc": 1675951526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Big Data is dead. Is the balloon deflated. \n\n[https://motherduck.com/blog/big-data-is-dead/](https://motherduck.com/blog/big-data-is-dead/)", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data is Dead - blog Opinions!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xtpko", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675946199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Big Data is dead. Is the balloon deflated. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://motherduck.com/blog/big-data-is-dead/\"&gt;https://motherduck.com/blog/big-data-is-dead/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?auto=webp&amp;v=enabled&amp;s=422507ff3e1d876f1565f9afa286ac2a3b12b70d", "width": 1024, "height": 535}, "resolutions": [{"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eea8d6f5a4b37bc287fcc1bd5d4b8364089b4589", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84293dc6fd5ff654378e1403be79188e69070a05", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8c1be6a0050d06000aac1c8326148e53643be6b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1af5f5964cf9dc80b6d7c498f369c8505d1a4dc3", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ab1c4a64b03a27503a012ea8235765e4de0220", "width": 960, "height": 501}], "variants": {}, "id": "C3v2JVQcD5hONfZJ0FsXvcBoNEQPQWKeM2eQkWh2PMw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xtpko", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xtpko/big_data_is_dead_blog_opinions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xtpko/big_data_is_dead_blog_opinions/", "subreddit_subscribers": 89051, "created_utc": 1675946199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fb83g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Querying 1 Billion Rows of AWS Cost Data 100X Faster with DuckDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_10xw9f0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-0aQIqmXqp2my4VxPbjhZXmebP2OS68ue_K6fPhCpTc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675953372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "vantage.sh", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.vantage.sh/blog/querying-aws-cost-data-duckdb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?auto=webp&amp;v=enabled&amp;s=9f0238778e1ee72ab50629878190a93a3adca59c", "width": 2540, "height": 1520}, "resolutions": [{"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=786e823d297bfd470e7032bb81c80bb9e519ad49", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81134544132584f87ad324085ed7baab9e82d8fb", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=745ede3c1a390215a1a14e700fd51f25995aa7c1", "width": 320, "height": 191}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d43866963ec48799e7e8a961fc44ee5942742e25", "width": 640, "height": 382}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97c71936c8524fd624420049d7ce1d066da23f0d", "width": 960, "height": 574}, {"url": "https://external-preview.redd.it/pMpfHhXYugqhHSyLlUydTg9-V8Jx-uWKUj3t18sul8A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=24f013a94ee32dbf47ef52e053864a386807b084", "width": 1080, "height": 646}], "variants": {}, "id": "fFFOps9uNyZKaT6C1uHqRiZJAv21eiUxoFEL-ZkhJHM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10xw9f0", "is_robot_indexable": true, "report_reasons": null, "author": "include_stdio_h", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xw9f0/querying_1_billion_rows_of_aws_cost_data_100x/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.vantage.sh/blog/querying-aws-cost-data-duckdb", "subreddit_subscribers": 89051, "created_utc": 1675953372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you ever wished you could ask any question to your data and get a fast, contextual answer?\n\nWe have some exciting news for you. Today we're launching a private beta\u00a0for \u2728 Secoda AI: the first AI suite of search, catalog, lineage, and documentation solutions, to enhance your data workspace.\n\nOur new AI Assistant \ud83e\udd16\u00a0will save you hours of manual labor by:\n\n\u2705\u00a0Surfacing insights through natural language search  \n\u2705\u00a0Automatically generating documentation  \n\u2705\u00a0Turning text into SQL\n\nLearn more about Secoda AI and how to get access to the private beta in the comments blow.\n\nTry it out today and see how it can improve your data discovery experience! \n\nLearn more about Secoda AI: [https://www.secoda.co/blog/learn-about-secoda-ai](https://www.secoda.co/blog/learn-about-secoda-ai)", "author_fullname": "t2_aiinah9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing: Secoda AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1eba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.22, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever wished you could ask any question to your data and get a fast, contextual answer?&lt;/p&gt;\n\n&lt;p&gt;We have some exciting news for you. Today we&amp;#39;re launching a private beta\u00a0for \u2728 Secoda AI: the first AI suite of search, catalog, lineage, and documentation solutions, to enhance your data workspace.&lt;/p&gt;\n\n&lt;p&gt;Our new AI Assistant \ud83e\udd16\u00a0will save you hours of manual labor by:&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Surfacing insights through natural language search&lt;br/&gt;\n\u2705\u00a0Automatically generating documentation&lt;br/&gt;\n\u2705\u00a0Turning text into SQL&lt;/p&gt;\n\n&lt;p&gt;Learn more about Secoda AI and how to get access to the private beta in the comments blow.&lt;/p&gt;\n\n&lt;p&gt;Try it out today and see how it can improve your data discovery experience! &lt;/p&gt;\n\n&lt;p&gt;Learn more about Secoda AI: &lt;a href=\"https://www.secoda.co/blog/learn-about-secoda-ai\"&gt;https://www.secoda.co/blog/learn-about-secoda-ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?auto=webp&amp;v=enabled&amp;s=8c5b8cb57e5526d06edcfa692d557d8de635a1c7", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f609b1dff87c4361f4eb611b4e0516ee8be17d67", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab583607a5d69a1fb66ed01b404ad79fd1cb4ab8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93e1fe995115293cf4c58f9f966aae2e1a97dbf1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b68e67e7d00848f42033d8553d997ae83372e1e6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6fd0043d6d5b3a3c2a7a189aaee5cd2c9291a81", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc0ff6d857747ec0e900d773fbb565ce45012064", "width": 1080, "height": 607}], "variants": {}, "id": "7WY8fl_h7RsynzfO-QX-YTUd7xXR4M4ad5IO3mvCPLI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10y1eba", "is_robot_indexable": true, "report_reasons": null, "author": "secodaHQ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y1eba/introducing_secoda_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y1eba/introducing_secoda_ai/", "subreddit_subscribers": 89051, "created_utc": 1675965732.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}