{"kind": "Listing", "data": {"after": "t3_116kxal", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "(crosspost from r/kiwix but relevant to the Data hoarding crowd I believe)\n\nAs a reminder, Kiwix is an offline reader: once you download your zim file (Wikipedia, StackOverflow or whatever) you can browse it without any further need for internet connectivity. There's much talk that one could fit Wikipedia into 21 Gb, but that would be a text-only, compressed and unformatted (ie not human readable) dump. Kiwix, on the other hand, is ready for consumption and use cases range from preppers to rural schools to Antarctic bases and anything inbetween.\n\nLast update was from May last year, but we've solved quite a number of issues since and so expect to be able to resume our monthly update schedule.\n\nThis new zim file contains 6,608,280 articles, about 97GB's worth of the Sum of All Human Knowledge. Other large wikis (FR, DE, anything &gt; 1M articles really) are also on their way.\n\nThe scrape lasted this time less than a week (5 days and 10 hours exactly). This is a substantial difference from 2022-05, which took approximately 11 days, and 2021-12, with 8 and a half days. \n\nThe download link is here ([http](https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim)) or here ([torrent](https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim.torrent), **recommended**).\n\nKiwix is free, open-source and is run as a non-profit. Thanks to everyone who helped with [fixing bugs](https://github.com/openzim/mwoffliner/issues) and / or [donated to support](https://support.kiwix.org) the project.", "author_fullname": "t2_24n3qggu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latest Wikipedia zim dump (97 GB) is available for download", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1178fz2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 194, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 194, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676904546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(crosspost from &lt;a href=\"/r/kiwix\"&gt;r/kiwix&lt;/a&gt; but relevant to the Data hoarding crowd I believe)&lt;/p&gt;\n\n&lt;p&gt;As a reminder, Kiwix is an offline reader: once you download your zim file (Wikipedia, StackOverflow or whatever) you can browse it without any further need for internet connectivity. There&amp;#39;s much talk that one could fit Wikipedia into 21 Gb, but that would be a text-only, compressed and unformatted (ie not human readable) dump. Kiwix, on the other hand, is ready for consumption and use cases range from preppers to rural schools to Antarctic bases and anything inbetween.&lt;/p&gt;\n\n&lt;p&gt;Last update was from May last year, but we&amp;#39;ve solved quite a number of issues since and so expect to be able to resume our monthly update schedule.&lt;/p&gt;\n\n&lt;p&gt;This new zim file contains 6,608,280 articles, about 97GB&amp;#39;s worth of the Sum of All Human Knowledge. Other large wikis (FR, DE, anything &amp;gt; 1M articles really) are also on their way.&lt;/p&gt;\n\n&lt;p&gt;The scrape lasted this time less than a week (5 days and 10 hours exactly). This is a substantial difference from 2022-05, which took approximately 11 days, and 2021-12, with 8 and a half days. &lt;/p&gt;\n\n&lt;p&gt;The download link is here (&lt;a href=\"https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim\"&gt;http&lt;/a&gt;) or here (&lt;a href=\"https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim.torrent\"&gt;torrent&lt;/a&gt;, &lt;strong&gt;recommended&lt;/strong&gt;).&lt;/p&gt;\n\n&lt;p&gt;Kiwix is free, open-source and is run as a non-profit. Thanks to everyone who helped with &lt;a href=\"https://github.com/openzim/mwoffliner/issues\"&gt;fixing bugs&lt;/a&gt; and / or &lt;a href=\"https://support.kiwix.org\"&gt;donated to support&lt;/a&gt; the project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1178fz2", "is_robot_indexable": true, "report_reasons": null, "author": "The_other_kiwix_guy", "discussion_type": null, "num_comments": 48, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1178fz2/latest_wikipedia_zim_dump_97_gb_is_available_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1178fz2/latest_wikipedia_zim_dump_97_gb_is_available_for/", "subreddit_subscribers": 670580, "created_utc": 1676904546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all I have around 6x 20 TB WD external HDDs. Used for torrenting and Plex server. Just wondering if there\u2019s anything I can do software or practices to prolong the life of the drives. I\u2019m on macOS by the way.", "author_fullname": "t2_vs43uoy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tips on keeping external HDDs healthy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116vxkk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676861873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I have around 6x 20 TB WD external HDDs. Used for torrenting and Plex server. Just wondering if there\u2019s anything I can do software or practices to prolong the life of the drives. I\u2019m on macOS by the way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116vxkk", "is_robot_indexable": true, "report_reasons": null, "author": "CosmicCobbler", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116vxkk/any_tips_on_keeping_external_hdds_healthy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116vxkk/any_tips_on_keeping_external_hdds_healthy/", "subreddit_subscribers": 670580, "created_utc": 1676861873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "DigitalOcean have laid off [Geoff Graham](https://geoffgraham.me/goodbye-css-tricks/), and I'm sceptical of the availability of the site in the long-term, so I'd like to see if there's any way to properly archive the site.\n\n[Article on hn](https://news.ycombinator.com/item?id=34864701).", "author_fullname": "t2_3x9pv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to properly archive css-tricks? It's an invaluable resource", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1170xkb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676879194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DigitalOcean have laid off &lt;a href=\"https://geoffgraham.me/goodbye-css-tricks/\"&gt;Geoff Graham&lt;/a&gt;, and I&amp;#39;m sceptical of the availability of the site in the long-term, so I&amp;#39;d like to see if there&amp;#39;s any way to properly archive the site.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://news.ycombinator.com/item?id=34864701\"&gt;Article on hn&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?auto=webp&amp;v=enabled&amp;s=3ac9af9d3f9bb819c97b3887e3cd3f9df7c90ffd", "width": 538, "height": 348}, "resolutions": [{"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0df5aab32d4f1d33a7b85c824e1867fbfee0c051", "width": 108, "height": 69}, {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6bbea08544072cd4a354472aa0911e6520306ea1", "width": 216, "height": 139}, {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c595b929668178505faa315382be71404c953840", "width": 320, "height": 206}], "variants": {}, "id": "yvtCkFuqQWxfc5Je6a9ovKSdFy3Fh72jg35_lkR_3bU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1170xkb", "is_robot_indexable": true, "report_reasons": null, "author": "EmSixTeen", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1170xkb/is_there_any_way_to_properly_archive_csstricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1170xkb/is_there_any_way_to_properly_archive_csstricks/", "subreddit_subscribers": 670580, "created_utc": 1676879194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hopefully this is a right sub for this!\n\nUnfortunately I have been tasked to manage and backup a deceased person's data. They had a computer, a phone, two cloud-services, an e-mail and messenger accounts and possibly some other services that I would like to keep backed-up. There are numerous duplicates, since for example all of their photos were backed-up to two different cloud services and are also stored locally on their phone. All in all I estimate that there is 1 Tb of data at most, likely a lot less. The data consists of personal photos and files. The photos have obviously strong sentimental value, files not so much. Nothing is objectively valuable or sensitive, since all their possessions will be shared, their identity can no longer be stolen and there are no work related things etc.\n\nI currently have access to all of their devices and services. All paid subscriptions are still active.\n\n&amp;#x200B;\n\nI need advice on how I should proceed from here.\n\n&amp;#x200B;\n\nI was planning on buying a 2 Tb portable SSD. I would transfer all of their files from the computer and the phone, and import everything from cloud, e-mail and other services that they had. I would simply store everything as is and only do some very basic archiving like a different folder for each source of data, essentially just accepting that there will be numerous duplicates (especially photos).\n\nThen I would label the SSD and leave it to their family member, and I would also upload all that data to that family member's cloud under a clearly labelled folder. The SSD would remain as an un-accessed local back-up. I am confident that the cloud service will be managed and kept active indefinitely.\n\n&amp;#x200B;\n\nIs that a good plan, or should I do something differently?\n\n&amp;#x200B;\n\nWhat kind of portable SSD would you recommend? I'm completely out of the loop of portable SSD's, and googling doesn't really help. Initially I wanted to buy one without a separate power supply for convenience, but I realised that if the power supply fails in a couple of years, the drive might be irreparable. On the other hand, if it comes with a separate proprietary power supply, the same problem remains and there is also the added possibility that the power supply may get misplaced and lost. Aside from that, I just want it to last as long as possible, 5 - 10 years at minimum (I don't know what is realistic).\n\nI understand this sub heavily advocates for a 3-2-1 rule, but that is not realistic in this situation. In this case it is important that the data is easily accessible from the cloud service which the family member is already familiar with. The SSD serves as a device that makes it easier for me to first compile everything together and the upload all of it to the cloud, but it would also act as a local backup for now until technical failure. It is extremely unlikely that anyone will even attempt to access it ever again. To reiterate, the cloud would be the main backup.", "author_fullname": "t2_13g02v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage the data of a deceased person?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1174q1b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676893891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully this is a right sub for this!&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I have been tasked to manage and backup a deceased person&amp;#39;s data. They had a computer, a phone, two cloud-services, an e-mail and messenger accounts and possibly some other services that I would like to keep backed-up. There are numerous duplicates, since for example all of their photos were backed-up to two different cloud services and are also stored locally on their phone. All in all I estimate that there is 1 Tb of data at most, likely a lot less. The data consists of personal photos and files. The photos have obviously strong sentimental value, files not so much. Nothing is objectively valuable or sensitive, since all their possessions will be shared, their identity can no longer be stolen and there are no work related things etc.&lt;/p&gt;\n\n&lt;p&gt;I currently have access to all of their devices and services. All paid subscriptions are still active.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need advice on how I should proceed from here.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was planning on buying a 2 Tb portable SSD. I would transfer all of their files from the computer and the phone, and import everything from cloud, e-mail and other services that they had. I would simply store everything as is and only do some very basic archiving like a different folder for each source of data, essentially just accepting that there will be numerous duplicates (especially photos).&lt;/p&gt;\n\n&lt;p&gt;Then I would label the SSD and leave it to their family member, and I would also upload all that data to that family member&amp;#39;s cloud under a clearly labelled folder. The SSD would remain as an un-accessed local back-up. I am confident that the cloud service will be managed and kept active indefinitely.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is that a good plan, or should I do something differently?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What kind of portable SSD would you recommend? I&amp;#39;m completely out of the loop of portable SSD&amp;#39;s, and googling doesn&amp;#39;t really help. Initially I wanted to buy one without a separate power supply for convenience, but I realised that if the power supply fails in a couple of years, the drive might be irreparable. On the other hand, if it comes with a separate proprietary power supply, the same problem remains and there is also the added possibility that the power supply may get misplaced and lost. Aside from that, I just want it to last as long as possible, 5 - 10 years at minimum (I don&amp;#39;t know what is realistic).&lt;/p&gt;\n\n&lt;p&gt;I understand this sub heavily advocates for a 3-2-1 rule, but that is not realistic in this situation. In this case it is important that the data is easily accessible from the cloud service which the family member is already familiar with. The SSD serves as a device that makes it easier for me to first compile everything together and the upload all of it to the cloud, but it would also act as a local backup for now until technical failure. It is extremely unlikely that anyone will even attempt to access it ever again. To reiterate, the cloud would be the main backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1174q1b", "is_robot_indexable": true, "report_reasons": null, "author": "RsSime", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1174q1b/how_to_manage_the_data_of_a_deceased_person/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1174q1b/how_to_manage_the_data_of_a_deceased_person/", "subreddit_subscribers": 670580, "created_utc": 1676893891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a software with which i can select a drive and have it auto convert every file to the least file size with the same destination and file name. Like just give it a drive and have it convert every video file with the best h.265 preset.", "author_fullname": "t2_tylmkfsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "auto handbrake video converter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1176kot", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676899479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a software with which i can select a drive and have it auto convert every file to the least file size with the same destination and file name. Like just give it a drive and have it convert every video file with the best h.265 preset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1176kot", "is_robot_indexable": true, "report_reasons": null, "author": "Fun_Activity7114", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1176kot/auto_handbrake_video_converter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1176kot/auto_handbrake_video_converter/", "subreddit_subscribers": 670580, "created_utc": 1676899479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8ux9h3d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I bought this 12TB MyBook and wanna shuck it, if somebody knows if it contains the Helium filled drive or the Air Cooled drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_117378j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_mS5EaY-ei-VtN6rK4g_Y1REy29FVe0P3II7btA00Ns.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676888222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t1cs9nhyjbja1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?auto=webp&amp;v=enabled&amp;s=c35a9f2369c12fafec8a012089bd195fa5e3b743", "width": 899, "height": 1599}, "resolutions": [{"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d817b4f686782da6dc127f105daa754349ffedfc", "width": 108, "height": 192}, {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8a8e1d74f21b14ea02cc7bf680228678978d361", "width": 216, "height": 384}, {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e73686c543e4c508b4f6af3ca8b60783aa9b8f6d", "width": 320, "height": 569}, {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c7063ec2507b314421f6bb08436c098848e3067", "width": 640, "height": 1138}], "variants": {}, "id": "eqy3h4_1KCV-JbvrQsb572oLzpqik9oGCTZcuz-9PLg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117378j", "is_robot_indexable": true, "report_reasons": null, "author": "petyr420", "discussion_type": null, "num_comments": 24, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117378j/i_bought_this_12tb_mybook_and_wanna_shuck_it_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t1cs9nhyjbja1.jpg", "subreddit_subscribers": 670580, "created_utc": 1676888222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nSo I\u2019ve got a bunch of really old SIM cards from my childhood (nano, micro, and standard), and I\u2019m pretty sure I may have sms messages saved on them. What\u2019s the easiest way of viewing their contents and extracting them?\n\nThanks!", "author_fullname": "t2_u1k87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to extract data from old Sim Cards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116q78n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;So I\u2019ve got a bunch of really old SIM cards from my childhood (nano, micro, and standard), and I\u2019m pretty sure I may have sms messages saved on them. What\u2019s the easiest way of viewing their contents and extracting them?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116q78n", "is_robot_indexable": true, "report_reasons": null, "author": "MurmurOfTheCine", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116q78n/how_to_extract_data_from_old_sim_cards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116q78n/how_to_extract_data_from_old_sim_cards/", "subreddit_subscribers": 670580, "created_utc": 1676846321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "every single hard drive has disappeared and cant be ordered.\n\ni only needed to wait a few more days to order my 20TB Exos :( \n\nnow i dont know what to do.", "author_fullname": "t2_8l9xfrvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what happened on serverpartdeals.com today", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1172lpi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676885843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;every single hard drive has disappeared and cant be ordered.&lt;/p&gt;\n\n&lt;p&gt;i only needed to wait a few more days to order my 20TB Exos :( &lt;/p&gt;\n\n&lt;p&gt;now i dont know what to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1172lpi", "is_robot_indexable": true, "report_reasons": null, "author": "seqvirtualtours", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1172lpi/what_happened_on_serverpartdealscom_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1172lpi/what_happened_on_serverpartdealscom_today/", "subreddit_subscribers": 670580, "created_utc": 1676885843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to move a heap of data from my phone to my backup SSD. Nothing is corrupted or damaged on my phone storage.\n\nWhen I transfer data to my SSD, some of it shows up corrupted and I can't work out why. I've used different cables, and connected the phone to a laptop and then SSD, as well as directly to the SSD.\n\nIt looks like the corruption of data affects video files only - not all of them, but some of them. Maybe 1 in 20. There's 190GB of files, a mix of images and videos, and it's a bit of a mess.\n\nI have a Sandisk 1TB Gen1 SSD that I'm moving data to from my Galaxy S23 1TB phone via USB-C to USB-C.\n\nAnyone know why this could be happening?", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been battling data corruption issues all day.. narrowed it down to video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116k8br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676831576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to move a heap of data from my phone to my backup SSD. Nothing is corrupted or damaged on my phone storage.&lt;/p&gt;\n\n&lt;p&gt;When I transfer data to my SSD, some of it shows up corrupted and I can&amp;#39;t work out why. I&amp;#39;ve used different cables, and connected the phone to a laptop and then SSD, as well as directly to the SSD.&lt;/p&gt;\n\n&lt;p&gt;It looks like the corruption of data affects video files only - not all of them, but some of them. Maybe 1 in 20. There&amp;#39;s 190GB of files, a mix of images and videos, and it&amp;#39;s a bit of a mess.&lt;/p&gt;\n\n&lt;p&gt;I have a Sandisk 1TB Gen1 SSD that I&amp;#39;m moving data to from my Galaxy S23 1TB phone via USB-C to USB-C.&lt;/p&gt;\n\n&lt;p&gt;Anyone know why this could be happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116k8br", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116k8br/been_battling_data_corruption_issues_all_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116k8br/been_battling_data_corruption_issues_all_day/", "subreddit_subscribers": 670580, "created_utc": 1676831576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_byyu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Infinite Storage Glitch: YouTube as cloud storage for ANY files, not just video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_117crzx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/u4dBt_Cyukywdh1o7WCwXwEGIEGyVrjvVzKyuGN7XXc.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676910985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/DvorakDwarf/Infinite-Storage-Glitch", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?auto=webp&amp;v=enabled&amp;s=5d92c3536aa3ba854f1f2972af10f4f8a169461e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e0c091c087c5860358bf1956e7e6713c3f86d4e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b085543f0bd4ec2336c0fed6a9770f2960d2124", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f31f1552b861c17786d70e761a5a5b58a2ee620b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67ec9132580bdd95a6e524068aae587f741a3c6a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68b562d506b8fcf07489199cc9883f93c09fa283", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a7a5fb0dc736063a1f44bf3d54b0bcebfbae029", "width": 1080, "height": 540}], "variants": {}, "id": "Thm20V0Y9NRyUvE0KHYnyzfalmsEiKIh0XJD4MdN1fU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "90TiB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "117crzx", "is_robot_indexable": true, "report_reasons": null, "author": "2bluesc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/117crzx/infinite_storage_glitch_youtube_as_cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/DvorakDwarf/Infinite-Storage-Glitch", "subreddit_subscribers": 670580, "created_utc": 1676910985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've restored images to many single drives before with Macrium being my go-to, but I currently need to image multiple drives at once and can't for the life of me find reliable software to do so. Sadly, Macrium is only one at a time and I'm trying to avoid sitting there to start a new drive myself one after another. Either restoring an image or cloning from an existing drive is fine, either way, I just need it to be able to batch together up to 10 drives plugged in automatically. Balena Etcher can do batch cloning but it's taking forever and seems have trouble actually completing that many drives at a time. Anything else you guys have used for something like this?", "author_fullname": "t2_id2ll0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for imaging to multiple drives at a time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116z62v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676872635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve restored images to many single drives before with Macrium being my go-to, but I currently need to image multiple drives at once and can&amp;#39;t for the life of me find reliable software to do so. Sadly, Macrium is only one at a time and I&amp;#39;m trying to avoid sitting there to start a new drive myself one after another. Either restoring an image or cloning from an existing drive is fine, either way, I just need it to be able to batch together up to 10 drives plugged in automatically. Balena Etcher can do batch cloning but it&amp;#39;s taking forever and seems have trouble actually completing that many drives at a time. Anything else you guys have used for something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116z62v", "is_robot_indexable": true, "report_reasons": null, "author": "TheMaster627", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116z62v/software_for_imaging_to_multiple_drives_at_a_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116z62v/software_for_imaging_to_multiple_drives_at_a_time/", "subreddit_subscribers": 670580, "created_utc": 1676872635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have been processing my use case for a few weeks now and I have realized the following:\n\n1. I really only care for syncing data remotely and streaming short clips (5-15 minutes maximum). I will be syncing quite a lot of data (my main repo is 170GB in size and I expect 3-6 GB in changes weekly) but I will not be streaming very many clips. I don't actually want additional features on top of this since it would probably just serve as bloatware to me.\n\n2. I think a 4-bay NAS would work for my purposes as far as extensibility of storage space goes.\n\n3. I am simply not prepared for building my own NAS as I lack to many competencies. I need a progression/baby step into a homebrew NAS rather than starting out building one from scratch.\n\n4. Aspirationally, I'm looking for something that costs as little idle energy as possible.\n\n\nWhat cues should I look for in retail NAS/es that would indicate that satisfying of these 4 criteria?", "author_fullname": "t2_ahrudp61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything specific I should look for in a NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116yslu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676871275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been processing my use case for a few weeks now and I have realized the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I really only care for syncing data remotely and streaming short clips (5-15 minutes maximum). I will be syncing quite a lot of data (my main repo is 170GB in size and I expect 3-6 GB in changes weekly) but I will not be streaming very many clips. I don&amp;#39;t actually want additional features on top of this since it would probably just serve as bloatware to me.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I think a 4-bay NAS would work for my purposes as far as extensibility of storage space goes.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am simply not prepared for building my own NAS as I lack to many competencies. I need a progression/baby step into a homebrew NAS rather than starting out building one from scratch.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Aspirationally, I&amp;#39;m looking for something that costs as little idle energy as possible.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What cues should I look for in retail NAS/es that would indicate that satisfying of these 4 criteria?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116yslu", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane_Grab_8727", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116yslu/is_there_anything_specific_i_should_look_for_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116yslu/is_there_anything_specific_i_should_look_for_in_a/", "subreddit_subscribers": 670580, "created_utc": 1676871275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a way to checksum files from a cold storage HDD to my current NAS, as the source.\n\nI have an external HDD I keep offsite that I update about once every 6 months. I have a NAS running Unraid that I use as my main storage.\n\nI would like a way to compare checksums of the files, to see if any corruption occured on the cold storage drive while it was unpowered. I only copy over new files, usually with FreeFileSync, so I could have corruption / bitrot on the older files and not know.\n\nDealing with about 5TB of data\n\nIs Teracopy the best way to do this? I have read horror stories of corruption caused by teracopy.\n\nMain OS is Windows.\n\n&amp;#x200B;\n\nThanks everyone!", "author_fullname": "t2_becvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum for cold storage backup, compare to source before backing up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116q5qi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a way to checksum files from a cold storage HDD to my current NAS, as the source.&lt;/p&gt;\n\n&lt;p&gt;I have an external HDD I keep offsite that I update about once every 6 months. I have a NAS running Unraid that I use as my main storage.&lt;/p&gt;\n\n&lt;p&gt;I would like a way to compare checksums of the files, to see if any corruption occured on the cold storage drive while it was unpowered. I only copy over new files, usually with FreeFileSync, so I could have corruption / bitrot on the older files and not know.&lt;/p&gt;\n\n&lt;p&gt;Dealing with about 5TB of data&lt;/p&gt;\n\n&lt;p&gt;Is Teracopy the best way to do this? I have read horror stories of corruption caused by teracopy.&lt;/p&gt;\n\n&lt;p&gt;Main OS is Windows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116q5qi", "is_robot_indexable": true, "report_reasons": null, "author": "technica-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116q5qi/checksum_for_cold_storage_backup_compare_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116q5qi/checksum_for_cold_storage_backup_compare_to/", "subreddit_subscribers": 670580, "created_utc": 1676846215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using TeraCopy to manually sync folders between my Windows laptop and a remote Synology NAS.  Folders are created and populated with files on my laptop just once for each folder, with no further modifications, so syncing manually has thus been pretty simple so far.  The relevant NAS directory is mapped as a network drive, and I must connect via VPN in order to access it remotely.\n\nTo verify the copy operations I always use TeraCopy's \"Verify\" option, which is great, but very slow when I'm connected to the NAS remotely, I believe because it's actually resending all the data as part of the verification process ([https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p](https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p)).\n\nAnyone know of a simple, ideally automated way of doing file verification faster (consuming less throughput) than TeraCopy?\n\nIn the near future I'm thinking of setting up Free File Sync to handle the syncing, but I still need to research and test it out more, and I'm not sure how it handles verification.", "author_fullname": "t2_9i6f7eeq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File verification optimized for remote network file copy (Windows)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116nrde", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676840273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using TeraCopy to manually sync folders between my Windows laptop and a remote Synology NAS.  Folders are created and populated with files on my laptop just once for each folder, with no further modifications, so syncing manually has thus been pretty simple so far.  The relevant NAS directory is mapped as a network drive, and I must connect via VPN in order to access it remotely.&lt;/p&gt;\n\n&lt;p&gt;To verify the copy operations I always use TeraCopy&amp;#39;s &amp;quot;Verify&amp;quot; option, which is great, but very slow when I&amp;#39;m connected to the NAS remotely, I believe because it&amp;#39;s actually resending all the data as part of the verification process (&lt;a href=\"https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p\"&gt;https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;Anyone know of a simple, ideally automated way of doing file verification faster (consuming less throughput) than TeraCopy?&lt;/p&gt;\n\n&lt;p&gt;In the near future I&amp;#39;m thinking of setting up Free File Sync to handle the syncing, but I still need to research and test it out more, and I&amp;#39;m not sure how it handles verification.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?auto=webp&amp;v=enabled&amp;s=5bf61032a6a11077440cf0421aceee21a943a62d", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbd71549aa2d678c4628f02ae8cd5b7ddaf23d73", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=451335daa7d9e8aa739cd3a851acc983948bf130", "width": 216, "height": 216}], "variants": {}, "id": "PJ2AnV34BC-Bc5jLZ5IhQtzJPf-P0PW2LhpwYCeTNyE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116nrde", "is_robot_indexable": true, "report_reasons": null, "author": "VladsBestFriend", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116nrde/file_verification_optimized_for_remote_network/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116nrde/file_verification_optimized_for_remote_network/", "subreddit_subscribers": 670580, "created_utc": 1676840273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tonight I learned about the 604 3.3v rails that need to be taped over. Figured that out. I have 6 new to me drives. 4x604 and 4x6L4.  All four of the 604s work fine. The two 6L4s do not.  Anyone know what I might be missing? The 6l4's spin up but when testing in windows they suffer a I/O error when trying to initialize.  I'm going to be putting all of these in an unraid setup with a backplane when it arrives but that might be a few steps from now.  Is there some \"gotcha\" about these drives I don't know about or are they just bad drives?", "author_fullname": "t2_7olt1k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the difference between the WD WUH721414ALE6L4 and the 604?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ud2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676857132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tonight I learned about the 604 3.3v rails that need to be taped over. Figured that out. I have 6 new to me drives. 4x604 and 4x6L4.  All four of the 604s work fine. The two 6L4s do not.  Anyone know what I might be missing? The 6l4&amp;#39;s spin up but when testing in windows they suffer a I/O error when trying to initialize.  I&amp;#39;m going to be putting all of these in an unraid setup with a backplane when it arrives but that might be a few steps from now.  Is there some &amp;quot;gotcha&amp;quot; about these drives I don&amp;#39;t know about or are they just bad drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116ud2r", "is_robot_indexable": true, "report_reasons": null, "author": "ElBigBad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116ud2r/whats_the_difference_between_the_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116ud2r/whats_the_difference_between_the_wd/", "subreddit_subscribers": 670580, "created_utc": 1676857132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Heya good day!\n\nIt's been ages since I've gone hard drive shopping.\n\nMust have been at least like 5 years ago. around 2016 or something.\n\nI got myself a pair of ST4000DM004 drives. 4 tb each back then.\n\nWhile not the best. And to be fair still universally hated for whatever reason. They worked fine for me.\n\n\nBeen in multiple rigs. One of them even repurposed into multiple servers. And then back to my main rig.\n\nNow one of them has developed bad sectors. So I backed everything up about a month ago. \n\nAnd today when I was going to update the backup. The drive's condition has worsened quickly.\n\n\nso I've been browsing around for hard drives and pricing to replace my tired bois with.\n\n\ni've noticed that the ST4000DM004  is still for sale today here.\n\nand is also currently the lowest Euro to TB. at about 17.16 Euro per TB. 68.65 per 4tb drive.\n\nshould i just grab these drives again. or should i opt for other drives? \n\n\nfor example. \n\nSeagate Skyhawk ST4000VX016  +Rescue 4TB for 18.27 per TB. \n\nand Seagate Skyhawk Surveillance HDD ST4000VX013 4TB for 19.50 per TB.\n\n\ngoogled around a bit. and heard people talk about that starting/stopping them often could lead to increased wear. so might not be the decent option.\n\n\nso after a bit more research i came opun these 2.\n\nThe WD purple WD42PURZ 256mb cache for 20.82 per TB\n\nand Seagate NAS ST4000VN006 Iron wolf for 20.98 per TB\n\n\n\nwhat should I go for?\n\n these will not live in raid. these will just be chucked into my main rig as storage drives.\n \nthese drives wont be used for constant high throughput applications like editing or streaming to multiple devices from multiple sources.\n\n\nthese will just be used for simple data storage. think of hundreds of GB of ISO's. that i have to occasionally pull off to copy to a usb drive.\n\none of them will be used as a steam library drive though. which is currently at a chunky size of 2+ TB. so far these Seagate drives have been perfectly fine running most games from.\n\nhowever most games larger than about 60 gigs i tend to install to my SSD when I play them anyways.\n\n\n\nthank you in avance for your advice!\n\nP.S i know i should get a NAS with zfs and such to have proper 1-2-3 backup solutions. I'm working on that. plan is 8x4 or 8x8TB server when i move in with my partner ^ ^", "author_fullname": "t2_157dz0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been ages since I've gone HDD shopping. I need some help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116jigb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676829844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heya good day!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been ages since I&amp;#39;ve gone hard drive shopping.&lt;/p&gt;\n\n&lt;p&gt;Must have been at least like 5 years ago. around 2016 or something.&lt;/p&gt;\n\n&lt;p&gt;I got myself a pair of ST4000DM004 drives. 4 tb each back then.&lt;/p&gt;\n\n&lt;p&gt;While not the best. And to be fair still universally hated for whatever reason. They worked fine for me.&lt;/p&gt;\n\n&lt;p&gt;Been in multiple rigs. One of them even repurposed into multiple servers. And then back to my main rig.&lt;/p&gt;\n\n&lt;p&gt;Now one of them has developed bad sectors. So I backed everything up about a month ago. &lt;/p&gt;\n\n&lt;p&gt;And today when I was going to update the backup. The drive&amp;#39;s condition has worsened quickly.&lt;/p&gt;\n\n&lt;p&gt;so I&amp;#39;ve been browsing around for hard drives and pricing to replace my tired bois with.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve noticed that the ST4000DM004  is still for sale today here.&lt;/p&gt;\n\n&lt;p&gt;and is also currently the lowest Euro to TB. at about 17.16 Euro per TB. 68.65 per 4tb drive.&lt;/p&gt;\n\n&lt;p&gt;should i just grab these drives again. or should i opt for other drives? &lt;/p&gt;\n\n&lt;p&gt;for example. &lt;/p&gt;\n\n&lt;p&gt;Seagate Skyhawk ST4000VX016  +Rescue 4TB for 18.27 per TB. &lt;/p&gt;\n\n&lt;p&gt;and Seagate Skyhawk Surveillance HDD ST4000VX013 4TB for 19.50 per TB.&lt;/p&gt;\n\n&lt;p&gt;googled around a bit. and heard people talk about that starting/stopping them often could lead to increased wear. so might not be the decent option.&lt;/p&gt;\n\n&lt;p&gt;so after a bit more research i came opun these 2.&lt;/p&gt;\n\n&lt;p&gt;The WD purple WD42PURZ 256mb cache for 20.82 per TB&lt;/p&gt;\n\n&lt;p&gt;and Seagate NAS ST4000VN006 Iron wolf for 20.98 per TB&lt;/p&gt;\n\n&lt;p&gt;what should I go for?&lt;/p&gt;\n\n&lt;p&gt;these will not live in raid. these will just be chucked into my main rig as storage drives.&lt;/p&gt;\n\n&lt;p&gt;these drives wont be used for constant high throughput applications like editing or streaming to multiple devices from multiple sources.&lt;/p&gt;\n\n&lt;p&gt;these will just be used for simple data storage. think of hundreds of GB of ISO&amp;#39;s. that i have to occasionally pull off to copy to a usb drive.&lt;/p&gt;\n\n&lt;p&gt;one of them will be used as a steam library drive though. which is currently at a chunky size of 2+ TB. so far these Seagate drives have been perfectly fine running most games from.&lt;/p&gt;\n\n&lt;p&gt;however most games larger than about 60 gigs i tend to install to my SSD when I play them anyways.&lt;/p&gt;\n\n&lt;p&gt;thank you in avance for your advice!&lt;/p&gt;\n\n&lt;p&gt;P.S i know i should get a NAS with zfs and such to have proper 1-2-3 backup solutions. I&amp;#39;m working on that. plan is 8x4 or 8x8TB server when i move in with my partner ^ ^&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116jigb", "is_robot_indexable": true, "report_reasons": null, "author": "appletechgeek", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116jigb/been_ages_since_ive_gone_hdd_shopping_i_need_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116jigb/been_ages_since_ive_gone_hdd_shopping_i_need_some/", "subreddit_subscribers": 670580, "created_utc": 1676829844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if this is not the place to ask, but I've been looking for the best bang for buck SSD in my budget which is 7.5k INR. I was thinking of getting crucial P3, Samsung 980 (non pro) or WD SN570.", "author_fullname": "t2_kbh8f54", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What 1TB NVME M.2 SSD should i get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_117fplp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676915916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is not the place to ask, but I&amp;#39;ve been looking for the best bang for buck SSD in my budget which is 7.5k INR. I was thinking of getting crucial P3, Samsung 980 (non pro) or WD SN570.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117fplp", "is_robot_indexable": true, "report_reasons": null, "author": "Daniel11200", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117fplp/what_1tb_nvme_m2_ssd_should_i_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117fplp/what_1tb_nvme_m2_ssd_should_i_get/", "subreddit_subscribers": 670580, "created_utc": 1676915916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found this when looking for drives for my Nas\n\n&amp;#x200B;\n\n[https://www.newegg.com/p/1Z4-002P-024F9?Description=8tb%20Exos%207E10&amp;cm\\_re=8tb\\_Exos%207E10-\\_-1Z4-002P-024F9-\\_-Product&amp;quicklink=true](https://www.newegg.com/p/1Z4-002P-024F9?Description=8tb%20Exos%207E10&amp;cm_re=8tb_Exos%207E10-_-1Z4-002P-024F9-_-Product&amp;quicklink=true)\n\n&amp;#x200B;\n\nOnly $119.99 for Exos 8Tb", "author_fullname": "t2_g8f5v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8TB Exos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117c8uj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676910271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this when looking for drives for my Nas&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.newegg.com/p/1Z4-002P-024F9?Description=8tb%20Exos%207E10&amp;amp;cm_re=8tb_Exos%207E10-_-1Z4-002P-024F9-_-Product&amp;amp;quicklink=true\"&gt;https://www.newegg.com/p/1Z4-002P-024F9?Description=8tb%20Exos%207E10&amp;amp;cm_re=8tb_Exos%207E10-_-1Z4-002P-024F9-_-Product&amp;amp;quicklink=true&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Only $119.99 for Exos 8Tb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/93zhzDm47mQv8XA4My0yH5iAYOyMwHQsZXc00l0wwEY.jpg?auto=webp&amp;v=enabled&amp;s=a51b8f6ba83502ce84d5016ddbd2b128a81aa893", "width": 640, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/93zhzDm47mQv8XA4My0yH5iAYOyMwHQsZXc00l0wwEY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4758ea966304ef5c14bc753d42c5bd864614cac7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/93zhzDm47mQv8XA4My0yH5iAYOyMwHQsZXc00l0wwEY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=181c75a0342439a8b52725e061161ea27aa306af", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/93zhzDm47mQv8XA4My0yH5iAYOyMwHQsZXc00l0wwEY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88d34eaa9e8c1a36fc3ed4f3e3fbab527b97cdcd", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/93zhzDm47mQv8XA4My0yH5iAYOyMwHQsZXc00l0wwEY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e046a2c6f26f9dc17227f04237ce5d41362e93b", "width": 640, "height": 480}], "variants": {}, "id": "OnQX-L9UdshR9eHGun1vHWH2kw3c8g70rGC0VNDiTqI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "117c8uj", "is_robot_indexable": true, "report_reasons": null, "author": "cwpc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117c8uj/8tb_exos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117c8uj/8tb_exos/", "subreddit_subscribers": 670580, "created_utc": 1676910271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Nowadays in 2023 a 5tb external hard drive for 102\u20ac is a good deal? 20,4\u20ac a tb.\n\nLast year i bought in amazon a 4tb for 60\u20ac, used, but like new.\nI feel the external hard drives are continuously increasing.", "author_fullname": "t2_u70bh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It is a good deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117c01w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676909930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Nowadays in 2023 a 5tb external hard drive for 102\u20ac is a good deal? 20,4\u20ac a tb.&lt;/p&gt;\n\n&lt;p&gt;Last year i bought in amazon a 4tb for 60\u20ac, used, but like new.\nI feel the external hard drives are continuously increasing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "117c01w", "is_robot_indexable": true, "report_reasons": null, "author": "miguelboas", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117c01w/it_is_a_good_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117c01w/it_is_a_good_deal/", "subreddit_subscribers": 670580, "created_utc": 1676909930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, \n\nI'm not the most knowledgeable when it comes to external hard drives. I'm looking for an external HDD that is capable of storing and playing movies and TV shows in high definition via Plex - do I need to consider read or write speeds of the HDD for this? Or are they all largely the same in capibilities?\n\nI was looking at Western Digital, Toshiba, and Seagate drives, maybe around the 4TB range.\n\nAny advice or recommendations would be appreciated! Thanks.", "author_fullname": "t2_vtwstgfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD noob", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117by5t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676909859.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not the most knowledgeable when it comes to external hard drives. I&amp;#39;m looking for an external HDD that is capable of storing and playing movies and TV shows in high definition via Plex - do I need to consider read or write speeds of the HDD for this? Or are they all largely the same in capibilities?&lt;/p&gt;\n\n&lt;p&gt;I was looking at Western Digital, Toshiba, and Seagate drives, maybe around the 4TB range.&lt;/p&gt;\n\n&lt;p&gt;Any advice or recommendations would be appreciated! Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117by5t", "is_robot_indexable": true, "report_reasons": null, "author": "Different-Effect-732", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117by5t/external_hdd_noob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117by5t/external_hdd_noob/", "subreddit_subscribers": 670580, "created_utc": 1676909859.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can someone help me download all the images from [this mobbin page?](https://mobbin.com/apps/spotify-ios-5c2cb04f-6558-4f39-b347-f0cf97fe787c/f0ae78eb-6736-429a-a199-9fab7813ca62/screens) They're perfectly legal to download and can be downloaded one at a time, but that will be too slow for me since there are many images. Can anyone help me or show me a script that will work well for scraping the images on the site.", "author_fullname": "t2_9h9u20l0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help. Can't download the images one at a time. They're too many. Need a method to download all.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117bp1h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676909529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone help me download all the images from &lt;a href=\"https://mobbin.com/apps/spotify-ios-5c2cb04f-6558-4f39-b347-f0cf97fe787c/f0ae78eb-6736-429a-a199-9fab7813ca62/screens\"&gt;this mobbin page?&lt;/a&gt; They&amp;#39;re perfectly legal to download and can be downloaded one at a time, but that will be too slow for me since there are many images. Can anyone help me or show me a script that will work well for scraping the images on the site.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?auto=webp&amp;v=enabled&amp;s=8597dbe85334dd7add47219b39791d9869bf9eff", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44f7e945fdc8c38e6a8b2bb36815d78727fd1a34", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=157099d2db4c90419ee33d7e5e9a7c951e0de4c9", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff3e834daa888a0b14b77997ef82b2e2889deee3", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9978ae6579e3b664cb2d080d6625e7466f63c677", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d264d506136e09dab5ed158f6d5d39ad676b60c0", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/cwGnuVdMwdXjx5XfjxGYxie7Z77Ym6FHmss9ekbGZG8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e35bed0af379fb8279862957579bad8bca9c36c4", "width": 1080, "height": 564}], "variants": {}, "id": "heR3n48tQ_XkBFY5M3scOSjhqRENOFlP4D19OouSIto"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117bp1h", "is_robot_indexable": true, "report_reasons": null, "author": "Qsand0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117bp1h/help_cant_download_the_images_one_at_a_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117bp1h/help_cant_download_the_images_one_at_a_time/", "subreddit_subscribers": 670580, "created_utc": 1676909529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have 4 disks and will do JBOD. Question is what is the most performant way to connect it?\n\n1. Enclosure via USB\n2. Direct SATA\n3. SAS controller\n4. Or are they all about the same at my level?", "author_fullname": "t2_3njitzry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting JBOD HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117bm2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676909415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 4 disks and will do JBOD. Question is what is the most performant way to connect it?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Enclosure via USB&lt;/li&gt;\n&lt;li&gt;Direct SATA&lt;/li&gt;\n&lt;li&gt;SAS controller&lt;/li&gt;\n&lt;li&gt;Or are they all about the same at my level?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117bm2p", "is_robot_indexable": true, "report_reasons": null, "author": "AirplaneRodeo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117bm2p/connecting_jbod_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117bm2p/connecting_jbod_hdd/", "subreddit_subscribers": 670580, "created_utc": 1676909415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Doing some quick Google searches, it seems that although there is a slight speed advantage with external SSDs, the gap is closing. Secondly, it seems that external SSDs are slightly more expensive than thumb drives.   \n\n\nI'm looking for 2 TB solutions for a 3-2-1 data backup scheme, and I want to keep costs low, but I don't want to sacrifice reliability too much.  \n\n\nThe only downside I see to external SSDs is that the power consumption is a little higher than a USB drive. This means that on some USB Hubs, or KVM switches, you may not get enough power, at times.", "author_fullname": "t2_tbm9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the differences in reliability, speed, and cost between external SSDs and USB drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1178ew9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676904463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing some quick Google searches, it seems that although there is a slight speed advantage with external SSDs, the gap is closing. Secondly, it seems that external SSDs are slightly more expensive than thumb drives.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for 2 TB solutions for a 3-2-1 data backup scheme, and I want to keep costs low, but I don&amp;#39;t want to sacrifice reliability too much.  &lt;/p&gt;\n\n&lt;p&gt;The only downside I see to external SSDs is that the power consumption is a little higher than a USB drive. This means that on some USB Hubs, or KVM switches, you may not get enough power, at times.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1178ew9", "is_robot_indexable": true, "report_reasons": null, "author": "pantshirtshoes", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1178ew9/what_are_the_differences_in_reliability_speed_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1178ew9/what_are_the_differences_in_reliability_speed_and/", "subreddit_subscribers": 670580, "created_utc": 1676904463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nWhat would you guys prefer for external backup?  \nExternal usb disk (WD my passport or Toshiba canvio) in SMR.\n\nOr a CMR disk with a external cabinett?  \nToshiba L200 HDWJ110UZSVA 8MB 1TB  \n[https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-l200/?pdf](https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-l200/?pdf)", "author_fullname": "t2_1addx075", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is best for external backup? SMR usbdisk or a usb cabinett with CMR disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1173amv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676888573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;What would you guys prefer for external backup?&lt;br/&gt;\nExternal usb disk (WD my passport or Toshiba canvio) in SMR.&lt;/p&gt;\n\n&lt;p&gt;Or a CMR disk with a external cabinett?&lt;br/&gt;\nToshiba L200 HDWJ110UZSVA 8MB 1TB&lt;br/&gt;\n&lt;a href=\"https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-l200/?pdf\"&gt;https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-l200/?pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1173amv", "is_robot_indexable": true, "report_reasons": null, "author": "raynoralpha123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1173amv/what_is_best_for_external_backup_smr_usbdisk_or_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1173amv/what_is_best_for_external_backup_smr_usbdisk_or_a/", "subreddit_subscribers": 670580, "created_utc": 1676888573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Edited with solution using a powershell script\n\n~~I don't know if this is the right sub to ask this but I have a Pixel 1st gen that still has the ability to upload unlimited original quality picture through google photos.~~\n\n~~The problem is that I have the 32GB version of the phone and only has 18GB of free space. So that means I have to upload 1.5TB of pictures and videos in \\~18GB intervals. I would like to know if there's a solution to split a 1.5TB folder into multiple folders of approximately 18GB. That way I could just use syncthings or external drive and just transfer each \\~18GB folder everytime the last one uploads. It doesn't have to be exact, it just has to be no more than 18GB.~~\n\n~~I tried googling this but the solution they give is to zip and split the big folder into multiple zipped pieces. Which doesn't really help me since I need to transfer the original jpg or mp4 to the phone in order to upload. They won't upload if they are zipped.~~\n\n~~I also tried chatgpt but it is giving me command/script that has invalid syntax/parameter.~~\n\n`xcopy /s &lt;source folder&gt; &lt;destination folder&gt;\\&lt;prefix&gt; /f /max &lt;maximum file size&gt;`\n\n~~I told it that /max is an invalid parameter and it sent me to powershell to do this~~\n\n`Get-ChildItem -Path \"&lt;source folder&gt;\" -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object (robocopy $_ \"&lt;destination folder&gt;\\$($_.Substring($_. LastIndexOf(\"\\\")+1))\" /mov /sizemax: &lt;maximum file size&gt;}`\n\n~~I told it once again sizemax is not a valid parameter and it just errored out.~~\n\n~~To test both of these scripts/commands I just created a folder called \"my folder\" and another one called \"subdirectories\" under desktop. Also used fsutils to create different sized dummy files inside \"my folder\". I didn't run these scripts with my actual files.~~\n\nedit:for anyone that stumbles upon this in the future here's the powershell script I used to do this.\n\n`$sourcePath = \"Source\"` \n\n`$destinationPath = \"Destination\"` \n\n`$maxSize = 18GB`\n\n`$currentSize = 0 $currentFolder = 1 $destination = Join-Path -Path $destinationPath -ChildPath \"Folder$currentFolder\"`\n\n`if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null }`\n\n`Get-ChildItem -Path $sourcePath -File | ForEach-Object { if ($currentSize + $_.Length -ge $maxSize) { $currentFolder++ $destination = Join-Path -Path $destinationPath -ChildPath \"Folder$currentFolder\" if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null } $currentSize = 0 }`\n\n`$destinationFile = Join-Path -Path $destination -ChildPath $_.Name Copy-Item -Path $_.FullName -Destination $destinationFile -Force`\n\n`$currentSize += $_.Length }`\n\n`Write-Output \"Folders created: $currentFolder\"`\n\nAll you have to do is change the source and destination path to match yours and also the maxSize to whatever you want. No need to change anything else after the first 3 lines. This script will copy files from your source directory into the giving destination and will automatically create a new folder when it reaches the threshold or if and when the next file will put it above the threshold given.\n\n&amp;#x200B;", "author_fullname": "t2_vcp81gxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to split a large folder into multiple smaller folders of no more than a certain size without zipping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116kxal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676870537.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676833260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edited with solution using a powershell script&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I don&amp;#39;t know if this is the right sub to ask this but I have a Pixel 1st gen that still has the ability to upload unlimited original quality picture through google photos.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;The problem is that I have the 32GB version of the phone and only has 18GB of free space. So that means I have to upload 1.5TB of pictures and videos in ~18GB intervals. I would like to know if there&amp;#39;s a solution to split a 1.5TB folder into multiple folders of approximately 18GB. That way I could just use syncthings or external drive and just transfer each ~18GB folder everytime the last one uploads. It doesn&amp;#39;t have to be exact, it just has to be no more than 18GB.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I tried googling this but the solution they give is to zip and split the big folder into multiple zipped pieces. Which doesn&amp;#39;t really help me since I need to transfer the original jpg or mp4 to the phone in order to upload. They won&amp;#39;t upload if they are zipped.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I also tried chatgpt but it is giving me command/script that has invalid syntax/parameter.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;xcopy /s &amp;lt;source folder&amp;gt; &amp;lt;destination folder&amp;gt;\\&amp;lt;prefix&amp;gt; /f /max &amp;lt;maximum file size&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I told it that /max is an invalid parameter and it sent me to powershell to do this&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Get-ChildItem -Path &amp;quot;&amp;lt;source folder&amp;gt;&amp;quot; -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object (robocopy $_ &amp;quot;&amp;lt;destination folder&amp;gt;\\$($_.Substring($_. LastIndexOf(&amp;quot;\\&amp;quot;)+1))&amp;quot; /mov /sizemax: &amp;lt;maximum file size&amp;gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I told it once again sizemax is not a valid parameter and it just errored out.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;To test both of these scripts/commands I just created a folder called &amp;quot;my folder&amp;quot; and another one called &amp;quot;subdirectories&amp;quot; under desktop. Also used fsutils to create different sized dummy files inside &amp;quot;my folder&amp;quot;. I didn&amp;#39;t run these scripts with my actual files.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;edit:for anyone that stumbles upon this in the future here&amp;#39;s the powershell script I used to do this.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$sourcePath = &amp;quot;Source&amp;quot;&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$destinationPath = &amp;quot;Destination&amp;quot;&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$maxSize = 18GB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$currentSize = 0 $currentFolder = 1 $destination = Join-Path -Path $destinationPath -ChildPath &amp;quot;Folder$currentFolder&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null }&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Get-ChildItem -Path $sourcePath -File | ForEach-Object { if ($currentSize + $_.Length -ge $maxSize) { $currentFolder++ $destination = Join-Path -Path $destinationPath -ChildPath &amp;quot;Folder$currentFolder&amp;quot; if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null } $currentSize = 0 }&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$destinationFile = Join-Path -Path $destination -ChildPath $_.Name Copy-Item -Path $_.FullName -Destination $destinationFile -Force&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$currentSize += $_.Length }&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Write-Output &amp;quot;Folders created: $currentFolder&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;All you have to do is change the source and destination path to match yours and also the maxSize to whatever you want. No need to change anything else after the first 3 lines. This script will copy files from your source directory into the giving destination and will automatically create a new folder when it reaches the threshold or if and when the next file will put it above the threshold given.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116kxal", "is_robot_indexable": true, "report_reasons": null, "author": "arcohex", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116kxal/is_there_a_way_to_split_a_large_folder_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116kxal/is_there_a_way_to_split_a_large_folder_into/", "subreddit_subscribers": 670580, "created_utc": 1676833260.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}