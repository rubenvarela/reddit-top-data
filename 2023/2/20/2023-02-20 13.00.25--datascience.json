{"kind": "Listing", "data": {"after": "t3_1170un7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_gbo2p1mi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There are too many charlatans on Linkedin posing as Data Scientist. Gone through his profile, not a single mention of his work. Most of the posts are engagement farming. The awards also seems to be suspicious and paid. My main question is who should you follow for quality content ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_116yrs4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 115, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 115, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mqxGMYzRHijwhhOO_sR84Wp-LKu_-mtumj2eTnNlaBg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676871193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/n5wm8qxr4aja1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/n5wm8qxr4aja1.png?auto=webp&amp;v=enabled&amp;s=8f85af0d9d8cab68f26f815c61d56e4d4aa9b823", "width": 617, "height": 740}, "resolutions": [{"url": "https://preview.redd.it/n5wm8qxr4aja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37222747d05d86d5aead534b4abe58c0d1426aef", "width": 108, "height": 129}, {"url": "https://preview.redd.it/n5wm8qxr4aja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c50e48c9a60cf99f60fedc57ebfbf3cf1117cf98", "width": 216, "height": 259}, {"url": "https://preview.redd.it/n5wm8qxr4aja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf62ebccdd7ec3d118100cb7396f92b4cb28d20b", "width": 320, "height": 383}], "variants": {}, "id": "dBKttO9EuDjVmm1Va3D1V8Vp6Hkjo6lp0FLuUzCiRI4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "116yrs4", "is_robot_indexable": true, "report_reasons": null, "author": "Mental-Leopard8027", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116yrs4/there_are_too_many_charlatans_on_linkedin_posing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/n5wm8qxr4aja1.png", "subreddit_subscribers": 849581, "created_utc": 1676871193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just wondering if should make portfolio if I already have 3 years of experience in the data science field.", "author_fullname": "t2_6xabempy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you need a portfolio if you already have ~3 years of experience on the field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116l932", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 63, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 63, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676834048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if should make portfolio if I already have 3 years of experience in the data science field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116l932", "is_robot_indexable": true, "report_reasons": null, "author": "marjose2", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116l932/do_you_need_a_portfolio_if_you_already_have_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116l932/do_you_need_a_portfolio_if_you_already_have_3/", "subreddit_subscribers": 849581, "created_utc": 1676834048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I often find myself wanting to run a couple SQL commands against a CSV, I have poor Excel skills, and so I made [https://sqlacsv.com/](https://sqlacsv.com/). You can drag-n-drop any CSV, its a completely offline app, and it gives a quick overview of each column's distribution.\n\n**Is this something people might find helpful? Would love to get some feedback on the tool.** \n\nHere some screenshots of what happens after you upload a CSV:\n\n[Simple SQL Editor](https://preview.redd.it/335fjx7cr8ja1.png?width=1886&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a22da852724c61f4846ca08f917111e9b99f980c)\n\n&amp;#x200B;\n\n[Overview of Values per Columns](https://preview.redd.it/qlt46ttdr8ja1.png?width=1873&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9d3e64d2eeac20b69c2d4805fd2e307b04ae6789)\n\nThanks in advanced!", "author_fullname": "t2_ynkil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Website to quickly SQL a CSV: feedback?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qlt46ttdr8ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 51, "x": 108, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06ae57099f905bf7f1f4b3e5095307e272000cd8"}, {"y": 102, "x": 216, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07a3b8e32d4f280bd71cb0a72166dafcc10d9ece"}, {"y": 151, "x": 320, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72afe3f4b2f2bbeab9a1377706cb078b7bb7e91d"}, {"y": 303, "x": 640, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ccb5242268ea20f744164a3cf836b4cf3cf5dec"}, {"y": 454, "x": 960, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9f9e7e8bb6c33dc377b4a8dcd08b79d4a18e8fa"}, {"y": 511, "x": 1080, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f5077f4a6108be495fa9bd2d7c3713b6a158af6"}], "s": {"y": 887, "x": 1873, "u": "https://preview.redd.it/qlt46ttdr8ja1.png?width=1873&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9d3e64d2eeac20b69c2d4805fd2e307b04ae6789"}, "id": "qlt46ttdr8ja1"}, "335fjx7cr8ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fabf8f4c9f5c8081b68564f1e611e26daf15e6a"}, {"y": 97, "x": 216, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6487f140f6e5cc728157a3e43654a092d9746596"}, {"y": 145, "x": 320, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a3f3e60c3300cb9fd2d8405dad2d1ee21fa2edd"}, {"y": 290, "x": 640, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4592dd9d6b785c49da973a78fdcfb6018790445e"}, {"y": 435, "x": 960, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fb376923ab3ad675ccb2b34833b2203faf43082"}, {"y": 489, "x": 1080, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed1ba5c9dbd7103d8fb8a145ee88226085493528"}], "s": {"y": 855, "x": 1886, "u": "https://preview.redd.it/335fjx7cr8ja1.png?width=1886&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a22da852724c61f4846ca08f917111e9b99f980c"}, "id": "335fjx7cr8ja1"}}, "name": "t3_116tgd8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LcP4XMrwO1f_KnyFGINRJlIZ3JLRH3nA3V9u534oMQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676854607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I often find myself wanting to run a couple SQL commands against a CSV, I have poor Excel skills, and so I made &lt;a href=\"https://sqlacsv.com/\"&gt;https://sqlacsv.com/&lt;/a&gt;. You can drag-n-drop any CSV, its a completely offline app, and it gives a quick overview of each column&amp;#39;s distribution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is this something people might find helpful? Would love to get some feedback on the tool.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Here some screenshots of what happens after you upload a CSV:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/335fjx7cr8ja1.png?width=1886&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a22da852724c61f4846ca08f917111e9b99f980c\"&gt;Simple SQL Editor&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qlt46ttdr8ja1.png?width=1873&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9d3e64d2eeac20b69c2d4805fd2e307b04ae6789\"&gt;Overview of Values per Columns&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advanced!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116tgd8", "is_robot_indexable": true, "report_reasons": null, "author": "downvotedragon", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116tgd8/website_to_quickly_sql_a_csv_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116tgd8/website_to_quickly_sql_a_csv_feedback/", "subreddit_subscribers": 849581, "created_utc": 1676854607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, I'm planning to create a dashboard for data visualization, and I think I want to use Python for the project. I've been looking into Dash and Streamlit, but I'm not sure which one would be the best choice for a beginner like me. Do you have any suggestions on which library to use? Also, I'm hoping to find a library that won't have a very steep learning curve.\n\nIf you have any recommendations for other Python-based libraries for data visualization and dashboard creation, I'd love to hear them as well. Thanks in advance for your help!", "author_fullname": "t2_5qka7bxz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Python Libraries for Data Visualization and Dashboard Creation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116us5l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676858394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I&amp;#39;m planning to create a dashboard for data visualization, and I think I want to use Python for the project. I&amp;#39;ve been looking into Dash and Streamlit, but I&amp;#39;m not sure which one would be the best choice for a beginner like me. Do you have any suggestions on which library to use? Also, I&amp;#39;m hoping to find a library that won&amp;#39;t have a very steep learning curve.&lt;/p&gt;\n\n&lt;p&gt;If you have any recommendations for other Python-based libraries for data visualization and dashboard creation, I&amp;#39;d love to hear them as well. Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116us5l", "is_robot_indexable": true, "report_reasons": null, "author": "theflash444123", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116us5l/best_python_libraries_for_data_visualization_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116us5l/best_python_libraries_for_data_visualization_and/", "subreddit_subscribers": 849581, "created_utc": 1676858394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So Basically I have worked on a data science project with a professor in Cannada and we got good results in a medical dataset and got a good AUROC of .85 and we published the paper for it and got selected , but what surprised me was the professor never went through my code and just gave suggestions and tips to do improve the model , but isn't this a lot risky.Everyday I am scared that some person would go through my code and invalidate my entire results based on some simple error which could have been corrected if there were proper code reviews.\n\nFast forward 6 months , I got a job in  a data science company as an intern and he told me to develop self supervised model for their image dataset and I did that and it good kind of okay results and he told me to move on to another project , still there was no code reviews or code checks , do people in data science just blindly trust each others code , I feel managers should at least give a look through to see if we taking the correct data split or if the model is correct or if there is any data leakage.There is a lot of red flags in trusting the AUROC results blindly. Is this a norm or maybe its just for me.\n\nMoreover most of the data scientists in the company dont even write proper documentation and for interns like me its such a pain , it took  me 2 weeks to understand their entire training repo, data science people should talk with the data engineers and take some of their methods and practices.", "author_fullname": "t2_py4qwirz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal in the data Science field to not have that many code checks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11714te", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676880230.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676880002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So Basically I have worked on a data science project with a professor in Cannada and we got good results in a medical dataset and got a good AUROC of .85 and we published the paper for it and got selected , but what surprised me was the professor never went through my code and just gave suggestions and tips to do improve the model , but isn&amp;#39;t this a lot risky.Everyday I am scared that some person would go through my code and invalidate my entire results based on some simple error which could have been corrected if there were proper code reviews.&lt;/p&gt;\n\n&lt;p&gt;Fast forward 6 months , I got a job in  a data science company as an intern and he told me to develop self supervised model for their image dataset and I did that and it good kind of okay results and he told me to move on to another project , still there was no code reviews or code checks , do people in data science just blindly trust each others code , I feel managers should at least give a look through to see if we taking the correct data split or if the model is correct or if there is any data leakage.There is a lot of red flags in trusting the AUROC results blindly. Is this a norm or maybe its just for me.&lt;/p&gt;\n\n&lt;p&gt;Moreover most of the data scientists in the company dont even write proper documentation and for interns like me its such a pain , it took  me 2 weeks to understand their entire training repo, data science people should talk with the data engineers and take some of their methods and practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11714te", "is_robot_indexable": true, "report_reasons": null, "author": "skeletons_of_closet", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11714te/is_it_normal_in_the_data_science_field_to_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11714te/is_it_normal_in_the_data_science_field_to_not/", "subreddit_subscribers": 849581, "created_utc": 1676880002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I am fitting count data for each place. That means, my response variable is the count data and I want to regress them to given explanatory variables. And each row represents each place. But the problem is that the count data are collected over years and I found it has a seasonal pattern. I am sorry to confuse you but I also have another dataset that consists of count data according to time and I used this dataset to visualize the time series plot.\n\nSo, my main goal is to make a regression model of the count data using explanatory in the original dataset (that has no time component) while adjusting for seasonality and my question is how I can deal with this. Do I have to request for the new dataset that contains the time the count data were recorded and fit a regression model? Thank you in advance!", "author_fullname": "t2_4b2tl19h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regression and Time Series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qyz0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676848212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am fitting count data for each place. That means, my response variable is the count data and I want to regress them to given explanatory variables. And each row represents each place. But the problem is that the count data are collected over years and I found it has a seasonal pattern. I am sorry to confuse you but I also have another dataset that consists of count data according to time and I used this dataset to visualize the time series plot.&lt;/p&gt;\n\n&lt;p&gt;So, my main goal is to make a regression model of the count data using explanatory in the original dataset (that has no time component) while adjusting for seasonality and my question is how I can deal with this. Do I have to request for the new dataset that contains the time the count data were recorded and fit a regression model? Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116qyz0", "is_robot_indexable": true, "report_reasons": null, "author": "ZirhoZirk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116qyz0/regression_and_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116qyz0/regression_and_time_series/", "subreddit_subscribers": 849581, "created_utc": 1676848212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Deploy Machine Learning Models with Django\n\nhttps://github.com/R-Mahmoudi/DeployMachineLearningModel", "author_fullname": "t2_8okds99o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploy Machine Learning Models with Django", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qef7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Deploy Machine Learning Models with Django&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/R-Mahmoudi/DeployMachineLearningModel\"&gt;https://github.com/R-Mahmoudi/DeployMachineLearningModel&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?auto=webp&amp;v=enabled&amp;s=54fe2ce57a49bacdfccaa126f6e2b3929d963494", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0dcfccb0b32f4400bfdfe104f26239095176289", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1b585d921be17536437cad1c0f75a5cc5f54051", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ba5f88b0b60b8922164d573e76d11094050a9f4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d56082665ef2d7db8324e0f0f62579a5df6e41cf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f9c70f7f579ccc9a99838f6fd1e5eb53292cac9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZzaaH894sHSinNu5ukml0I7FzUSxYoBd5Wdu9dFgUvY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eeaf1ed5a485c360bf86e694b29e92fefe8da94f", "width": 1080, "height": 540}], "variants": {}, "id": "Buhe-VKKc_DiDzwK1BiE0lE1sSfhx-mcqd7OhXRvT9A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116qef7", "is_robot_indexable": true, "report_reasons": null, "author": "Smooth-Ad1528", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116qef7/deploy_machine_learning_models_with_django/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116qef7/deploy_machine_learning_models_with_django/", "subreddit_subscribers": 849581, "created_utc": 1676846814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently between jobs; my last boss told me he believes that improving my coding level will help me become a better data scientist.\n\nAny ideas how to work on that?\nPython, of course..", "author_fullname": "t2_rzfif4vz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to improve my coding level", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116m6as", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676836308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently between jobs; my last boss told me he believes that improving my coding level will help me become a better data scientist.&lt;/p&gt;\n\n&lt;p&gt;Any ideas how to work on that?\nPython, of course..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116m6as", "is_robot_indexable": true, "report_reasons": null, "author": "NoManner4303", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116m6as/how_to_improve_my_coding_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116m6as/how_to_improve_my_coding_level/", "subreddit_subscribers": 849581, "created_utc": 1676836308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I make a regression model to predict force plate using plantar pressure. I am trying to use CNN model in this case. I have 2 different datasets, dataset A (force plate data) and dataset B (plantar pressure data). then before training the data I normalize the data first by using minmaxscaler(). during training I got good results and low MSE. then I tried to test the model and return the predicted data which was normalized to original data using the inverse function. after I inverted the data and I checked the MSE value again, the MSE value I got after inverting was very high. even though when viewed from the prediction results in my opinion very good. why is this MSE value suddenly so high. \n\n&amp;#x200B;\n\n the data look like below: \n\n&gt;Force Plate Data: \\[ 4.46733, 4.39629, -34.2351 , -4077.23 , -6206.81 , -874.539 \\]  \n&gt;  \n&gt;Force Plate Data Shape : (15000,6)  \n&gt;  \n&gt;SmartInsole Data: \\[ 0. 0. 0. 13. 1. 0. 0. 0. 0. 0. 15. 92. 60. 0. 36. 0. 0. 0. 0. 0. 0. 62. 80. 58. 37. 0. 0. 0. 0. 40. 83. 72. 32. 22. 0. 0. 0. 0. 0. 0. 98. 108. 74. 56. 30. 17. 0. 0. 44. 121. 127. 83. 0. 0. 0. 0. 0. 3. 83. 64. 63. 63. 77. 70. 43. 55. 115. 138. 144. 137. 0. 0. 0. 0. 66. 107. 127. 146. 150. 52. 0. 0. 0. 129. 133. 18. 0. 0. 0.\\]  \n&gt;  \n&gt;SmartInsole Data Shape : (15000,89)\n\n here my model code: \n\n \n\n    ##\u00a0Load\u00a0Data\nInsole\u00a0=\u00a0pd.read_csv('1225_Rwalk10min1_list.txt',\u00a0header=None,\u00a0low_memory=False)\nSIData\u00a0=\u00a0\u00a0np.array(Insole)\n\ndf\u00a0=\u00a0pd.read_csv('1225_Rwalk10min.csv',\u00a0low_memory=False)\ncolumns\u00a0=\u00a0['Fx','Fy','Fz','Mx','My','Mz']\nselected_df\u00a0=\u00a0df[columns]\nFPDatas\u00a0=\u00a0selected_df[:15000]\n\nlabel\u00a0=\u00a0pd.read_csv('label.txt',\u00a0header=None,\u00a0low_memory=False)\nlabelData\u00a0=\u00a0\u00a0np.array(label).astype('float32')\n\nSmartInsole\u00a0=\u00a0np.array(SIData[:15000]).astype('float32')\nFPData\u00a0=\u00a0np.array(FPDatas).astype('float32')\n\nLabel\u00a0=\u00a0np.array(labelData[:15000]).astype('float32')\n\nSIlabeled\u00a0=\u00a0np.concatenate((Label,\u00a0SmartInsole),\u00a0axis=1)\nSIlabeled\u00a0=\u00a0np.array(SIlabeled).astype('float32')\n##\u00a0End\u00a0Load\u00a0Data\n\n#\u00a0Data\u00a0Normalization\nminInsole\u00a0=\u00a0SIlabeled.min()\nmaxInsole\u00a0=\u00a0SIlabeled.max()\nxscale\u00a0=\u00a0(SIlabeled\u00a0-\u00a0minInsole)\u00a0/\u00a0(\u00a0maxInsole\u00a0-\u00a0minInsole\u00a0)\n\nFPmax\u00a0=\u00a0[]\nFPmin\u00a0=\u00a0[]\nyscale\u00a0=\u00a0[]\n\nfor\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0\u00a0\u00a0minFP\u00a0=\u00a0FPData[:,i].min()\n\u00a0\u00a0\u00a0\u00a0maxFP\u00a0=\u00a0FPData[:,i].max()\n\u00a0\u00a0\u00a0\u00a0FPmin.append(minFP)\n\u00a0\u00a0\u00a0\u00a0FPmax.append(maxFP)\n\nFPmin\u00a0=\u00a0np.array(FPmin)\nFPmax\u00a0=\u00a0np.array(FPmax)\n\nfor\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0scale\u00a0=\u00a0(FPData[:,i]\u00a0-\u00a0FPmin[i])\u00a0/\u00a0(\u00a0FPmax[i]\u00a0-\u00a0FPmin[i]\u00a0)\n\u00a0\u00a0yscale.append(scale)\nyscale\u00a0=\u00a0np.array(yscale)\nyscale\u00a0=\u00a0yscale.transpose()\n\n#End\u00a0Data\u00a0Normalization\n\n#\u00a0Spliting\u00a0Data\nsample_size\u00a0=\u00a0xscale.shape[0]\ntime_steps\u00a0\u00a0=\u00a0xscale.shape[1]\ninput_dimension\u00a0=\u00a01\n\ntrain_data_reshaped\u00a0=\u00a0xscale.reshape(sample_size,time_steps,input_dimension)\n\nX_train,\u00a0X_test,\u00a0y_train,\u00a0y_test\u00a0=\u00a0train_test_split(train_data_reshaped,\u00a0yscale,\u00a0test_size=0.20,\u00a0random_state=2)\nprint(X_train.shape,X_test.shape)\nprint(y_train.shape,y_test.shape)\n#End\u00a0Spliting\u00a0Data\n\n#Model\u00a0Structure\nmodel\u00a0=\u00a0Sequential(name=\"model_conv1D\")\n\nn_timesteps\u00a0=\u00a0train_data_reshaped.shape[1\nn_features\u00a0\u00a0=\u00a0train_data_reshaped.shape[2]\n\nmodel.add(Input(shape=(n_timesteps,n_features)))\n\nmodel.add(Conv1D(filters=64,\u00a0kernel_size=3,\u00a0padding='same',\u00a0activation='relu'))\nmodel.add(Conv1D(filters=128,\u00a0kernel_size=3,\u00a0padding='same',\u00a0activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=256,\u00a0kernel_size=3,\u00a0padding='same',\u00a0activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(500,\u00a0activation='relu'))\nmodel.add(Dense(6,\u00a0activation='sigmoid'))\n\nmodel.summary()\nmodel.compile(loss='mse',\u00a0optimizer=Adam(learning_rate=0.002),\u00a0metrics=['mse'])\n\nhistory\u00a0=\u00a0model.fit(X_train,\u00a0y_train,\u00a0batch_size=64,\u00a0epochs=200,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0validation_data=(X_test,\u00a0y_test),\u00a0verbose=2)\n#End\u00a0Model\u00a0Structure\n\n#Evaluate\u00a0Model\nmodel.evaluate(train_data_reshaped,\u00a0yscale)\nypred\u00a0=\u00a0model.predict(train_data_reshaped)\n\nplt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model\u00a0Loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train',\u00a0'validation'],\u00a0loc='upper\u00a0right')\n#\u00a0plt.show()\nplt.savefig('Loss\u00a0Result.png')\n\nprint('MSE:\u00a0',mean_squared_error(yscale,\u00a0ypred))\nprint('RMSE:\u00a0',math.sqrt(mean_squared_error(yscale,\u00a0ypred)))\nprint('Coefficient\u00a0of\u00a0determination\u00a0(r2\u00a0Score):\u00a0',\u00a0r2_score(yscale,\u00a0ypred))\n\n\n    #Inverse\ny_inverse\u00a0=\u00a0[]\ny_pred_inverse\u00a0=\u00a0[]\n\n\n    for\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0Y_inver\u00a0=\u00a0\u00a0yscale[0:15000,\u00a0i]*(\u00a0FPmax[i]\u00a0-\u00a0FPmin[i]\u00a0)+FPmin[i]\n\u00a0\u00a0Pred_inver\u00a0=\u00a0ypred[0:15000,\u00a0i]*(\u00a0FPmax[i]\u00a0-\u00a0FPmin[i]\u00a0)+FPmin[i]\n\u00a0\u00a0y_inverse.append(Y_inver)\n\u00a0\u00a0y_pred_inverse.append(Pred_inver)\ny_inverse\u00a0=\u00a0np.array(y_inverse)\ny_inverse\u00a0=\u00a0y_inverse.transpose()\ny_pred_inverse\u00a0=\u00a0np.array(y_pred_inverse)\ny_pred_inverse\u00a0=\u00a0y_pred_inverse.transpose()\n\n\n    print('MSE:\u00a0',mean_squared_error(y_inverse,\u00a0y_pred_inverse))\nprint('RMSE:\u00a0',math.sqrt(mean_squared_error(y_inverse,\u00a0y_pred_inverse)))\nprint('Coefficient\u00a0of\u00a0determination\u00a0(r2\u00a0Score):\u00a0',\u00a0r2_score(y_inverse,\u00a0y_pred_inverse))\n\nx=[]\ncolors=['red','green','brown','teal','gray','black','maroon','orange','purple']\ncolors2=['green','red','orange','black','maroon','teal','blue','gray','brown']\nx\u00a0=\u00a0np.arange(0,3000)*60/3000 \nfor\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0\u00a0\u00a0plt.figure(figsize=(15,6))\n #\u00a0plt.figure()\n\u00a0\u00a0\u00a0\u00a0plt.plot(x,y_inverse[0:3000,i],color='red')\n\u00a0\u00a0\u00a0\u00a0plt.plot(x,y_pred_inverse[0:3000,i],\u00a0markerfacecolor='none',color='green')\n\u00a0\u00a0\u00a0\u00a0plt.title('CNN\u00a0Regression\u00a0(Training\u00a0Data)')\n if\u00a0i\u00a0&lt;\u00a03:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plt.ylabel('Force/'+columns[i])\n else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plt.ylabel('Moment/'+columns[i])\n\u00a0\u00a0\u00a0\u00a0plt.xlabel('Time(s)')\n\u00a0\u00a0\u00a0\u00a0plt.legend(['Real\u00a0value',\u00a0'Predicted\u00a0Value'],\u00a0loc='best')\n\u00a0\u00a0\u00a0\u00a0plt.show()\n#End\u00a0Evaluate\u00a0Model\n\n the model loss: \n\n&amp;#x200B;\n\nhttps://preview.redd.it/gtd2cvrfebja1.png?width=405&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1a6d5f5f6f34d6f2ea961d2ca2280fde4846d9a1\n\n the mse use the normalize data: \n\n    MSE:  0.00033982666 \n    RMSE:  0.018434387873554003 \n    Coefficient of determination (r2 Score):  0.9934412267882915\n\n the mse after inverse data:\n\n    MSE:  711726.3 \n    RMSE:  843.6387334042931 \n    Coefficient of determination (r2 Score):  0.9934412272949391 \n\n the prediction result samples: \n\n&amp;#x200B;\n\nhttps://preview.redd.it/ucd7jk2oebja1.png?width=885&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=09e0ee47f4d5bee51edfa5813aa4d56bde15d061\n\nhttps://preview.redd.it/mfheq5uoebja1.png?width=918&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e3dda3c70ffecd5172fcf5e065a82cc7f62ae1b\n\n    print(\"Real Value : \",y_inverse[51]) \n    print(\"prediction Value : \",y_pred_inverse[51]) \n    Real Value :  [    4.46733 4.39629    -34.235107 -4077.2305   -6206.8125   -874.53906 ] prediction Value :  [    6.6143274 5.6351166   -31.929413  -3412.164     -6177.2344  -2047.6455   ]\n\n  how to make MSE value not change when the data is inverted?", "author_fullname": "t2_end0qlqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why MSE will be high when I inverse data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mfheq5uoebja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 45, "x": 108, "u": "https://preview.redd.it/mfheq5uoebja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8cc98b2bdfc46fe8054f4d53f2db9b32139f0227"}, {"y": 91, "x": 216, "u": "https://preview.redd.it/mfheq5uoebja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=472988cf082da154f21c14faad42ea771976f142"}, {"y": 134, "x": 320, "u": "https://preview.redd.it/mfheq5uoebja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ef453debbf13dd18e826dfbfef3afacc1d4c4b5"}, {"y": 269, "x": 640, "u": "https://preview.redd.it/mfheq5uoebja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e153dc049ab575094d4bddea6a1698584ac7890"}], "s": {"y": 387, "x": 918, "u": "https://preview.redd.it/mfheq5uoebja1.png?width=918&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5e3dda3c70ffecd5172fcf5e065a82cc7f62ae1b"}, "id": "mfheq5uoebja1"}, "gtd2cvrfebja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/gtd2cvrfebja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ad1b876d8da82981c3ec9335cae1a4b4a4bf9c"}, {"y": 148, "x": 216, "u": "https://preview.redd.it/gtd2cvrfebja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a329c7e96647e94b340c729e5d548a5bb1eab301"}, {"y": 219, "x": 320, "u": "https://preview.redd.it/gtd2cvrfebja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef7b3c6cbf647a8b7f44080f320046425de6a31c"}], "s": {"y": 278, "x": 405, "u": "https://preview.redd.it/gtd2cvrfebja1.png?width=405&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1a6d5f5f6f34d6f2ea961d2ca2280fde4846d9a1"}, "id": "gtd2cvrfebja1"}, "ucd7jk2oebja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/ucd7jk2oebja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96fb662363b3ef98e89062403609565ea992d7e0"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/ucd7jk2oebja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7e5057c4e211cb2e952f98da344ef43d489e0cd"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/ucd7jk2oebja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c2c3cf6cd66361e6f861962a85dcc15ec7a28ec"}, {"y": 279, "x": 640, "u": "https://preview.redd.it/ucd7jk2oebja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c980bc64462467990887608c0d393e418d5b16a"}], "s": {"y": 387, "x": 885, "u": "https://preview.redd.it/ucd7jk2oebja1.png?width=885&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=09e0ee47f4d5bee51edfa5813aa4d56bde15d061"}, "id": "ucd7jk2oebja1"}}, "name": "t3_1172qzm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/pw5LD9I4SE5MsUqlrMdk9BX94pxdSCqL6wTsQygtkA8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676886456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I make a regression model to predict force plate using plantar pressure. I am trying to use CNN model in this case. I have 2 different datasets, dataset A (force plate data) and dataset B (plantar pressure data). then before training the data I normalize the data first by using minmaxscaler(). during training I got good results and low MSE. then I tried to test the model and return the predicted data which was normalized to original data using the inverse function. after I inverted the data and I checked the MSE value again, the MSE value I got after inverting was very high. even though when viewed from the prediction results in my opinion very good. why is this MSE value suddenly so high. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;the data look like below: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Force Plate Data: [ 4.46733, 4.39629, -34.2351 , -4077.23 , -6206.81 , -874.539 ]  &lt;/p&gt;\n\n&lt;p&gt;Force Plate Data Shape : (15000,6)  &lt;/p&gt;\n\n&lt;p&gt;SmartInsole Data: [ 0. 0. 0. 13. 1. 0. 0. 0. 0. 0. 15. 92. 60. 0. 36. 0. 0. 0. 0. 0. 0. 62. 80. 58. 37. 0. 0. 0. 0. 40. 83. 72. 32. 22. 0. 0. 0. 0. 0. 0. 98. 108. 74. 56. 30. 17. 0. 0. 44. 121. 127. 83. 0. 0. 0. 0. 0. 3. 83. 64. 63. 63. 77. 70. 43. 55. 115. 138. 144. 137. 0. 0. 0. 0. 66. 107. 127. 146. 150. 52. 0. 0. 0. 129. 133. 18. 0. 0. 0.]  &lt;/p&gt;\n\n&lt;p&gt;SmartInsole Data Shape : (15000,89)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;here my model code: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;##\u00a0Load\u00a0Data\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Insole\u00a0=\u00a0pd.read_csv(&amp;#39;1225_Rwalk10min1_list.txt&amp;#39;,\u00a0header=None,\u00a0low_memory=False)\nSIData\u00a0=\u00a0\u00a0np.array(Insole)&lt;/p&gt;\n\n&lt;p&gt;df\u00a0=\u00a0pd.read_csv(&amp;#39;1225_Rwalk10min.csv&amp;#39;,\u00a0low_memory=False)\ncolumns\u00a0=\u00a0[&amp;#39;Fx&amp;#39;,&amp;#39;Fy&amp;#39;,&amp;#39;Fz&amp;#39;,&amp;#39;Mx&amp;#39;,&amp;#39;My&amp;#39;,&amp;#39;Mz&amp;#39;]\nselected_df\u00a0=\u00a0df[columns]\nFPDatas\u00a0=\u00a0selected_df[:15000]&lt;/p&gt;\n\n&lt;p&gt;label\u00a0=\u00a0pd.read_csv(&amp;#39;label.txt&amp;#39;,\u00a0header=None,\u00a0low_memory=False)\nlabelData\u00a0=\u00a0\u00a0np.array(label).astype(&amp;#39;float32&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;SmartInsole\u00a0=\u00a0np.array(SIData[:15000]).astype(&amp;#39;float32&amp;#39;)\nFPData\u00a0=\u00a0np.array(FPDatas).astype(&amp;#39;float32&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;Label\u00a0=\u00a0np.array(labelData[:15000]).astype(&amp;#39;float32&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;SIlabeled\u00a0=\u00a0np.concatenate((Label,\u00a0SmartInsole),\u00a0axis=1)\nSIlabeled\u00a0=\u00a0np.array(SIlabeled).astype(&amp;#39;float32&amp;#39;)&lt;/p&gt;\n\n&lt;h2&gt;\u00a0End\u00a0Load\u00a0Data&lt;/h2&gt;\n\n&lt;h1&gt;\u00a0Data\u00a0Normalization&lt;/h1&gt;\n\n&lt;p&gt;minInsole\u00a0=\u00a0SIlabeled.min()\nmaxInsole\u00a0=\u00a0SIlabeled.max()\nxscale\u00a0=\u00a0(SIlabeled\u00a0-\u00a0minInsole)\u00a0/\u00a0(\u00a0maxInsole\u00a0-\u00a0minInsole\u00a0)&lt;/p&gt;\n\n&lt;p&gt;FPmax\u00a0=\u00a0[]\nFPmin\u00a0=\u00a0[]\nyscale\u00a0=\u00a0[]&lt;/p&gt;\n\n&lt;p&gt;for\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0\u00a0\u00a0minFP\u00a0=\u00a0FPData[:,i].min()\n\u00a0\u00a0\u00a0\u00a0maxFP\u00a0=\u00a0FPData[:,i].max()\n\u00a0\u00a0\u00a0\u00a0FPmin.append(minFP)\n\u00a0\u00a0\u00a0\u00a0FPmax.append(maxFP)&lt;/p&gt;\n\n&lt;p&gt;FPmin\u00a0=\u00a0np.array(FPmin)\nFPmax\u00a0=\u00a0np.array(FPmax)&lt;/p&gt;\n\n&lt;p&gt;for\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0scale\u00a0=\u00a0(FPData[:,i]\u00a0-\u00a0FPmin[i])\u00a0/\u00a0(\u00a0FPmax[i]\u00a0-\u00a0FPmin[i]\u00a0)\n\u00a0\u00a0yscale.append(scale)\nyscale\u00a0=\u00a0np.array(yscale)\nyscale\u00a0=\u00a0yscale.transpose()&lt;/p&gt;\n\n&lt;h1&gt;End\u00a0Data\u00a0Normalization&lt;/h1&gt;\n\n&lt;h1&gt;\u00a0Spliting\u00a0Data&lt;/h1&gt;\n\n&lt;p&gt;sample_size\u00a0=\u00a0xscale.shape[0]\ntime_steps\u00a0\u00a0=\u00a0xscale.shape[1]\ninput_dimension\u00a0=\u00a01&lt;/p&gt;\n\n&lt;p&gt;train_data_reshaped\u00a0=\u00a0xscale.reshape(sample_size,time_steps,input_dimension)&lt;/p&gt;\n\n&lt;p&gt;X_train,\u00a0X_test,\u00a0y_train,\u00a0y_test\u00a0=\u00a0train_test_split(train_data_reshaped,\u00a0yscale,\u00a0test_size=0.20,\u00a0random_state=2)\nprint(X_train.shape,X_test.shape)\nprint(y_train.shape,y_test.shape)&lt;/p&gt;\n\n&lt;h1&gt;End\u00a0Spliting\u00a0Data&lt;/h1&gt;\n\n&lt;h1&gt;Model\u00a0Structure&lt;/h1&gt;\n\n&lt;p&gt;model\u00a0=\u00a0Sequential(name=&amp;quot;model_conv1D&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;n_timesteps\u00a0=\u00a0train_data_reshaped.shape[1\nn_features\u00a0\u00a0=\u00a0train_data_reshaped.shape[2]&lt;/p&gt;\n\n&lt;p&gt;model.add(Input(shape=(n_timesteps,n_features)))&lt;/p&gt;\n\n&lt;p&gt;model.add(Conv1D(filters=64,\u00a0kernel_size=3,\u00a0padding=&amp;#39;same&amp;#39;,\u00a0activation=&amp;#39;relu&amp;#39;))\nmodel.add(Conv1D(filters=128,\u00a0kernel_size=3,\u00a0padding=&amp;#39;same&amp;#39;,\u00a0activation=&amp;#39;relu&amp;#39;))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=256,\u00a0kernel_size=3,\u00a0padding=&amp;#39;same&amp;#39;,\u00a0activation=&amp;#39;relu&amp;#39;))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(500,\u00a0activation=&amp;#39;relu&amp;#39;))\nmodel.add(Dense(6,\u00a0activation=&amp;#39;sigmoid&amp;#39;))&lt;/p&gt;\n\n&lt;p&gt;model.summary()\nmodel.compile(loss=&amp;#39;mse&amp;#39;,\u00a0optimizer=Adam(learning_rate=0.002),\u00a0metrics=[&amp;#39;mse&amp;#39;])&lt;/p&gt;\n\n&lt;p&gt;history\u00a0=\u00a0model.fit(X_train,\u00a0y_train,\u00a0batch_size=64,\u00a0epochs=200,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0validation_data=(X_test,\u00a0y_test),\u00a0verbose=2)&lt;/p&gt;\n\n&lt;h1&gt;End\u00a0Model\u00a0Structure&lt;/h1&gt;\n\n&lt;h1&gt;Evaluate\u00a0Model&lt;/h1&gt;\n\n&lt;p&gt;model.evaluate(train_data_reshaped,\u00a0yscale)\nypred\u00a0=\u00a0model.predict(train_data_reshaped)&lt;/p&gt;\n\n&lt;p&gt;plt.figure()\nplt.plot(history.history[&amp;#39;loss&amp;#39;])\nplt.plot(history.history[&amp;#39;val_loss&amp;#39;])\nplt.title(&amp;#39;Model\u00a0Loss&amp;#39;)\nplt.ylabel(&amp;#39;Loss&amp;#39;)\nplt.xlabel(&amp;#39;epoch&amp;#39;)\nplt.legend([&amp;#39;train&amp;#39;,\u00a0&amp;#39;validation&amp;#39;],\u00a0loc=&amp;#39;upper\u00a0right&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;\u00a0plt.show()&lt;/h1&gt;\n\n&lt;p&gt;plt.savefig(&amp;#39;Loss\u00a0Result.png&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;#39;MSE:\u00a0&amp;#39;,mean_squared_error(yscale,\u00a0ypred))\nprint(&amp;#39;RMSE:\u00a0&amp;#39;,math.sqrt(mean_squared_error(yscale,\u00a0ypred)))\nprint(&amp;#39;Coefficient\u00a0of\u00a0determination\u00a0(r2\u00a0Score):\u00a0&amp;#39;,\u00a0r2_score(yscale,\u00a0ypred))&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#Inverse\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;y_inverse\u00a0=\u00a0[]\ny_pred_inverse\u00a0=\u00a0[]&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for\u00a0i\u00a0in range(0,6):\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;\u00a0\u00a0Y_inver\u00a0=\u00a0\u00a0yscale[0:15000,\u00a0i]&lt;em&gt;(\u00a0FPmax[i]\u00a0-\u00a0FPmin[i]\u00a0)+FPmin[i]\n\u00a0\u00a0Pred_inver\u00a0=\u00a0ypred[0:15000,\u00a0i]&lt;/em&gt;(\u00a0FPmax[i]\u00a0-\u00a0FPmin[i]\u00a0)+FPmin[i]\n\u00a0\u00a0y_inverse.append(Y_inver)\n\u00a0\u00a0y_pred_inverse.append(Pred_inver)\ny_inverse\u00a0=\u00a0np.array(y_inverse)\ny_inverse\u00a0=\u00a0y_inverse.transpose()\ny_pred_inverse\u00a0=\u00a0np.array(y_pred_inverse)\ny_pred_inverse\u00a0=\u00a0y_pred_inverse.transpose()&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;print(&amp;#39;MSE:\u00a0&amp;#39;,mean_squared_error(y_inverse,\u00a0y_pred_inverse))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;print(&amp;#39;RMSE:\u00a0&amp;#39;,math.sqrt(mean_squared_error(y_inverse,\u00a0y_pred_inverse)))\nprint(&amp;#39;Coefficient\u00a0of\u00a0determination\u00a0(r2\u00a0Score):\u00a0&amp;#39;,\u00a0r2_score(y_inverse,\u00a0y_pred_inverse))&lt;/p&gt;\n\n&lt;p&gt;x=[]\ncolors=[&amp;#39;red&amp;#39;,&amp;#39;green&amp;#39;,&amp;#39;brown&amp;#39;,&amp;#39;teal&amp;#39;,&amp;#39;gray&amp;#39;,&amp;#39;black&amp;#39;,&amp;#39;maroon&amp;#39;,&amp;#39;orange&amp;#39;,&amp;#39;purple&amp;#39;]\ncolors2=[&amp;#39;green&amp;#39;,&amp;#39;red&amp;#39;,&amp;#39;orange&amp;#39;,&amp;#39;black&amp;#39;,&amp;#39;maroon&amp;#39;,&amp;#39;teal&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;gray&amp;#39;,&amp;#39;brown&amp;#39;]\nx\u00a0=\u00a0np.arange(0,3000)*60/3000 \nfor\u00a0i\u00a0in range(0,6):\n\u00a0\u00a0\u00a0\u00a0plt.figure(figsize=(15,6))\n #\u00a0plt.figure()\n\u00a0\u00a0\u00a0\u00a0plt.plot(x,y_inverse[0:3000,i],color=&amp;#39;red&amp;#39;)\n\u00a0\u00a0\u00a0\u00a0plt.plot(x,y_pred_inverse[0:3000,i],\u00a0markerfacecolor=&amp;#39;none&amp;#39;,color=&amp;#39;green&amp;#39;)\n\u00a0\u00a0\u00a0\u00a0plt.title(&amp;#39;CNN\u00a0Regression\u00a0(Training\u00a0Data)&amp;#39;)\n if\u00a0i\u00a0&amp;lt;\u00a03:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plt.ylabel(&amp;#39;Force/&amp;#39;+columns[i])\n else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plt.ylabel(&amp;#39;Moment/&amp;#39;+columns[i])\n\u00a0\u00a0\u00a0\u00a0plt.xlabel(&amp;#39;Time(s)&amp;#39;)\n\u00a0\u00a0\u00a0\u00a0plt.legend([&amp;#39;Real\u00a0value&amp;#39;,\u00a0&amp;#39;Predicted\u00a0Value&amp;#39;],\u00a0loc=&amp;#39;best&amp;#39;)\n\u00a0\u00a0\u00a0\u00a0plt.show()&lt;/p&gt;\n\n&lt;h1&gt;End\u00a0Evaluate\u00a0Model&lt;/h1&gt;\n\n&lt;p&gt;the model loss: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gtd2cvrfebja1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1a6d5f5f6f34d6f2ea961d2ca2280fde4846d9a1\"&gt;https://preview.redd.it/gtd2cvrfebja1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1a6d5f5f6f34d6f2ea961d2ca2280fde4846d9a1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;the mse use the normalize data: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;MSE:  0.00033982666 \nRMSE:  0.018434387873554003 \nCoefficient of determination (r2 Score):  0.9934412267882915\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the mse after inverse data:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;MSE:  711726.3 \nRMSE:  843.6387334042931 \nCoefficient of determination (r2 Score):  0.9934412272949391 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the prediction result samples: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ucd7jk2oebja1.png?width=885&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=09e0ee47f4d5bee51edfa5813aa4d56bde15d061\"&gt;https://preview.redd.it/ucd7jk2oebja1.png?width=885&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=09e0ee47f4d5bee51edfa5813aa4d56bde15d061&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mfheq5uoebja1.png?width=918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e3dda3c70ffecd5172fcf5e065a82cc7f62ae1b\"&gt;https://preview.redd.it/mfheq5uoebja1.png?width=918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5e3dda3c70ffecd5172fcf5e065a82cc7f62ae1b&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;print(&amp;quot;Real Value : &amp;quot;,y_inverse[51]) \nprint(&amp;quot;prediction Value : &amp;quot;,y_pred_inverse[51]) \nReal Value :  [    4.46733 4.39629    -34.235107 -4077.2305   -6206.8125   -874.53906 ] prediction Value :  [    6.6143274 5.6351166   -31.929413  -3412.164     -6177.2344  -2047.6455   ]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;how to make MSE value not change when the data is inverted?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1172qzm", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Cranberry29", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1172qzm/why_mse_will_be_high_when_i_inverse_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1172qzm/why_mse_will_be_high_when_i_inverse_data/", "subreddit_subscribers": 849581, "created_utc": 1676886456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nI have recently been offered a data scientist position at my current company, but I was told that I would be working exclusively on the Google Cloud Platform (GCP), and that it won't be necessary to code in Python. I've spent the past year learning about data science, models, and programming, and I'm concerned that all of that knowledge won't be put to use in this new role. I always thought that a data scientist would be in front of their Jupyter notebook, so I'm worried that this is a cheap data scientist position.\n\nMy main question is: is it standard for data scientists to work solely on cloud computing platforms like GCP, and is it common to not code in Python? I'd love to hear your thoughts and experiences on this.\n\nThanks in advance for your help !", "author_fullname": "t2_7td50", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it standard for data scientists to work solely on cloud computing platforms like GCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171lyy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676881854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have recently been offered a data scientist position at my current company, but I was told that I would be working exclusively on the Google Cloud Platform (GCP), and that it won&amp;#39;t be necessary to code in Python. I&amp;#39;ve spent the past year learning about data science, models, and programming, and I&amp;#39;m concerned that all of that knowledge won&amp;#39;t be put to use in this new role. I always thought that a data scientist would be in front of their Jupyter notebook, so I&amp;#39;m worried that this is a cheap data scientist position.&lt;/p&gt;\n\n&lt;p&gt;My main question is: is it standard for data scientists to work solely on cloud computing platforms like GCP, and is it common to not code in Python? I&amp;#39;d love to hear your thoughts and experiences on this.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1171lyy", "is_robot_indexable": true, "report_reasons": null, "author": "asmodee59", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1171lyy/is_it_standard_for_data_scientists_to_work_solely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1171lyy/is_it_standard_for_data_scientists_to_work_solely/", "subreddit_subscribers": 849581, "created_utc": 1676881854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!\n\nI have a situation that I\u2019d very much love your opinion on!\n\nA little bit about myself: \nI have a MS in biomedical engineering, and I\u2019m currently pursuing a MS in analytics. I\u2019m expected to finish the degree by the end of 2023.\n\nI have just over 2 years of experience in a hospital as a research associate. 50% of my job duty is experimental design (mostly A/B testing), analyzing experimental dataset, visualization, etc. The size of the data is usually very small (n=200 or less). And the other 50% is sensors integration and hands on clinical experimental work. I\u2019m also an adjunct professor in university teaching biomedical engineering classes.\n\nI have a couple abstracts in ML published, and one manuscript currently in the work. However,  the journals I submitted to are very health-oriented. The technical findings are not ground breaking, just applying known ML techniques to the experimental data.\n\nInitially I was planning to pursue a PhD in biomed after finishing the analytics MS program, but I find myself swaying away. I\u2019d like to change to a pure DSA career afterwards.\n\nMy question is, should I apply to intern jobs and start over, or is my experience in the hospital valid? My work is 100% Python based, I know a bit of SQL from course project and leetcode.\n\nIt feels like aiming for a mid level job (2year +) may be too far fetched since I technically only have one full year of experience in DS.\n\nSo that may leave me with a new grad/ intern option\n\nWhat do you guys think? \n\nThanks so much", "author_fullname": "t2_t8c8gas5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Go for internship, new grad, or mid level career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ws9x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I have a situation that I\u2019d very much love your opinion on!&lt;/p&gt;\n\n&lt;p&gt;A little bit about myself: \nI have a MS in biomedical engineering, and I\u2019m currently pursuing a MS in analytics. I\u2019m expected to finish the degree by the end of 2023.&lt;/p&gt;\n\n&lt;p&gt;I have just over 2 years of experience in a hospital as a research associate. 50% of my job duty is experimental design (mostly A/B testing), analyzing experimental dataset, visualization, etc. The size of the data is usually very small (n=200 or less). And the other 50% is sensors integration and hands on clinical experimental work. I\u2019m also an adjunct professor in university teaching biomedical engineering classes.&lt;/p&gt;\n\n&lt;p&gt;I have a couple abstracts in ML published, and one manuscript currently in the work. However,  the journals I submitted to are very health-oriented. The technical findings are not ground breaking, just applying known ML techniques to the experimental data.&lt;/p&gt;\n\n&lt;p&gt;Initially I was planning to pursue a PhD in biomed after finishing the analytics MS program, but I find myself swaying away. I\u2019d like to change to a pure DSA career afterwards.&lt;/p&gt;\n\n&lt;p&gt;My question is, should I apply to intern jobs and start over, or is my experience in the hospital valid? My work is 100% Python based, I know a bit of SQL from course project and leetcode.&lt;/p&gt;\n\n&lt;p&gt;It feels like aiming for a mid level job (2year +) may be too far fetched since I technically only have one full year of experience in DS.&lt;/p&gt;\n\n&lt;p&gt;So that may leave me with a new grad/ intern option&lt;/p&gt;\n\n&lt;p&gt;What do you guys think? &lt;/p&gt;\n\n&lt;p&gt;Thanks so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116ws9x", "is_robot_indexable": true, "report_reasons": null, "author": "Fatcatthebestcat", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116ws9x/go_for_internship_new_grad_or_mid_level_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116ws9x/go_for_internship_new_grad_or_mid_level_career/", "subreddit_subscribers": 849581, "created_utc": 1676864560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "It seems like there are countless data providers that offer identity solutions like skip tracing and people search capabilities, but all references to obtaining this information seem very broad and vague. \n\nSo, how are all of these companies aggregating their people data like phone numbers, email addresses, relatives, etc in an efficient way and with large coverage of the US population? \n\nAre there public bulk datasets that include this information so it can be aggregated and sold for marketing purposes without violating GLBA or FCRA regulations?\n\nAs an expert, how would you go about acquiring this information in bulk and qualifying it for accuracy?\n\nCurrently, the big players in the space are IDI, TLO, Delvepoint, and Tracers. They all have access to regulated data from credit bureaus and other third-parties to be sold to specific industries, but they also provide unregulated data for public use cases, so where is that information coming from?\n\nThis one has had me stumped for years.", "author_fullname": "t2_h5y5l2xd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracers, IDI, Enformion, PIPL, etc -- How do these companies obtain their data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116oddc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676841802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like there are countless data providers that offer identity solutions like skip tracing and people search capabilities, but all references to obtaining this information seem very broad and vague. &lt;/p&gt;\n\n&lt;p&gt;So, how are all of these companies aggregating their people data like phone numbers, email addresses, relatives, etc in an efficient way and with large coverage of the US population? &lt;/p&gt;\n\n&lt;p&gt;Are there public bulk datasets that include this information so it can be aggregated and sold for marketing purposes without violating GLBA or FCRA regulations?&lt;/p&gt;\n\n&lt;p&gt;As an expert, how would you go about acquiring this information in bulk and qualifying it for accuracy?&lt;/p&gt;\n\n&lt;p&gt;Currently, the big players in the space are IDI, TLO, Delvepoint, and Tracers. They all have access to regulated data from credit bureaus and other third-parties to be sold to specific industries, but they also provide unregulated data for public use cases, so where is that information coming from?&lt;/p&gt;\n\n&lt;p&gt;This one has had me stumped for years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116oddc", "is_robot_indexable": true, "report_reasons": null, "author": "Helpful-Bus9011", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116oddc/tracers_idi_enformion_pipl_etc_how_do_these/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116oddc/tracers_idi_enformion_pipl_etc_how_do_these/", "subreddit_subscribers": 849581, "created_utc": 1676841802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "https://github.com/R-Mahmoudi/Real-Time-Object-Counting-on-Jetson-Nano", "author_fullname": "t2_8okds99o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-Time-Object-Counting-on-Jetson-Nano", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1174q4x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676893901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/R-Mahmoudi/Real-Time-Object-Counting-on-Jetson-Nano\"&gt;https://github.com/R-Mahmoudi/Real-Time-Object-Counting-on-Jetson-Nano&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?auto=webp&amp;v=enabled&amp;s=44264ddd1301fb6a2553dfbaf25de52e81859762", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0253366b8b56018bb1bc9ca6be804344a1f0ed2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4aa1e7389a4567ac813342b76497e7ca6b7ef666", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b71df4dce96e270510e17377eaff09611ae42978", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d43a79fda5038558c3bcbcb0a1d4aad0c98a9cb2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e2901efc803629163ace0798e8c357b3f5d651d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/shtotNCVBDVhDVdVBbpjsrYDglJTdiIsa5liS9vpBKQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57150797dcf0e1913275c638aec18356ab63810d", "width": 1080, "height": 540}], "variants": {}, "id": "4wKxqSgXRjxXQr3e8zlhq2HaL0BhefQZ4vum0hJ6XFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1174q4x", "is_robot_indexable": true, "report_reasons": null, "author": "Smooth-Ad1528", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1174q4x/realtimeobjectcountingonjetsonnano/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1174q4x/realtimeobjectcountingonjetsonnano/", "subreddit_subscribers": 849581, "created_utc": 1676893901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to apply for data science jobs in the UK, and my CV is getting rejected. Can you please provide me a critical feedback on what changes I can make in order to get my CV shortlisted\n\nhttps://preview.redd.it/h2zah8e1zbja1.jpg?width=1700&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8a93af6d9c6b5d3f3fad282f62b899a3719abc38", "author_fullname": "t2_77fx5mrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you all please review my CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"h2zah8e1zbja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 177, "x": 108, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f7f48ee2f3a1b2a82aad3fe75ada4e37145f8cc"}, {"y": 355, "x": 216, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=056eef1b37d7fd7ff937d0766566111def8d16f7"}, {"y": 527, "x": 320, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83ff08e69a1e83818abd4470bf5bfc233116242c"}, {"y": 1054, "x": 640, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b7c9e7b46c328b9d7e791c9a11bae732849b161"}, {"y": 1581, "x": 960, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39ab0a7c643fc4e62d84e3485b93a86cea98e5ff"}, {"y": 1778, "x": 1080, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=875a1309e6dbd85772264151ec1c317284e4a08a"}], "s": {"y": 2800, "x": 1700, "u": "https://preview.redd.it/h2zah8e1zbja1.jpg?width=1700&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8a93af6d9c6b5d3f3fad282f62b899a3719abc38"}, "id": "h2zah8e1zbja1"}}, "name": "t3_1174jm7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xtAiJ6eyttcmhOX4vuq-8ud54hkXU5iSDoiikAIz2hM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676893255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to apply for data science jobs in the UK, and my CV is getting rejected. Can you please provide me a critical feedback on what changes I can make in order to get my CV shortlisted&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h2zah8e1zbja1.jpg?width=1700&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8a93af6d9c6b5d3f3fad282f62b899a3719abc38\"&gt;https://preview.redd.it/h2zah8e1zbja1.jpg?width=1700&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8a93af6d9c6b5d3f3fad282f62b899a3719abc38&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1174jm7", "is_robot_indexable": true, "report_reasons": null, "author": "grvkapoor15", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1174jm7/can_you_all_please_review_my_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1174jm7/can_you_all_please_review_my_cv/", "subreddit_subscribers": 849581, "created_utc": 1676893255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_c7y94ryn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner book suggestion: I\u2019m total beginner (UG)with zero knowledge in coding (python) or data science pls suggest me a book which covers all this topics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_11749ri", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jfKGp-m49J_Nvhsy8xe03XFCqYxl7itDyJuZ2bxjbuI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676892242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/z12ywrtkddja1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/z12ywrtkddja1.jpg?auto=webp&amp;v=enabled&amp;s=186c3d3e1140d518c8541398b000020dcbce35cd", "width": 1280, "height": 963}, "resolutions": [{"url": "https://preview.redd.it/z12ywrtkddja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91e431f970ea1c7fa3ac8c7339880be9f4f54175", "width": 108, "height": 81}, {"url": "https://preview.redd.it/z12ywrtkddja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3964f717b28ccc2d57ebe680e424ca5517fb4d9b", "width": 216, "height": 162}, {"url": "https://preview.redd.it/z12ywrtkddja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23697c7eb06aee4510d160219d60e6a24a4585ab", "width": 320, "height": 240}, {"url": "https://preview.redd.it/z12ywrtkddja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51eaaa15c1938be066f31a45d961aaf00fcd86a9", "width": 640, "height": 481}, {"url": "https://preview.redd.it/z12ywrtkddja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=760cbc421b5d1d4fb7314a31e2e9abd500bfb632", "width": 960, "height": 722}, {"url": "https://preview.redd.it/z12ywrtkddja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff5d650abccc8ba9bff580394cd930e26fe86a51", "width": 1080, "height": 812}], "variants": {}, "id": "K3Xe5fc0FmVWutDmlYGIgliYcvs8JCcX6nCi4f1Bif8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11749ri", "is_robot_indexable": true, "report_reasons": null, "author": "_The_DogeMaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11749ri/beginner_book_suggestion_im_total_beginner_ugwith/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/z12ywrtkddja1.jpg", "subreddit_subscribers": 849581, "created_utc": 1676892242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI am currently building a simple ARIMA model to predict future monthly bandwith usage. Already have the full code and have been reading some papers online regarding the implementation of ARIMA / SARIMA models and have found some weird contradictions that I hope you guys can help me sort them out: \n\n1) For instance, I understand that we should always split the data before applying any pre processing techniques in order to avoid data leakage problems. Is this still the case with time series analysis? \n\nSince most of the time we will want to apply differentiating to a time series to possibly remove trend and seasonality does it make a difference if we apply before or after the split? I have been seeing many notebooks online, for instance, [https://github.com/BernardoRaimundo/Time-Series-Analysis/blob/main/Author's%20Notes/ARMA%20Modeling.ipynb](https://github.com/BernardoRaimundo/Time-Series-Analysis/blob/main/Author's%20Notes/ARMA%20Modeling.ipynb), that applied first order differentiating and then the data was split to make predictions.\n\n2) When finding the optimal parameters for an ARIMA / SARIMA model using, for instance, the AIC, this running should be performed on the differentiated data correct? Then afterwards we convert to the original data\n\nThank you!", "author_fullname": "t2_fqj7ay6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question regarding time series decomposition and splitting the data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1173k2t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676889586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I am currently building a simple ARIMA model to predict future monthly bandwith usage. Already have the full code and have been reading some papers online regarding the implementation of ARIMA / SARIMA models and have found some weird contradictions that I hope you guys can help me sort them out: &lt;/p&gt;\n\n&lt;p&gt;1) For instance, I understand that we should always split the data before applying any pre processing techniques in order to avoid data leakage problems. Is this still the case with time series analysis? &lt;/p&gt;\n\n&lt;p&gt;Since most of the time we will want to apply differentiating to a time series to possibly remove trend and seasonality does it make a difference if we apply before or after the split? I have been seeing many notebooks online, for instance, &lt;a href=\"https://github.com/BernardoRaimundo/Time-Series-Analysis/blob/main/Author&amp;#x27;s%20Notes/ARMA%20Modeling.ipynb\"&gt;https://github.com/BernardoRaimundo/Time-Series-Analysis/blob/main/Author&amp;#39;s%20Notes/ARMA%20Modeling.ipynb&lt;/a&gt;, that applied first order differentiating and then the data was split to make predictions.&lt;/p&gt;\n\n&lt;p&gt;2) When finding the optimal parameters for an ARIMA / SARIMA model using, for instance, the AIC, this running should be performed on the differentiated data correct? Then afterwards we convert to the original data&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?auto=webp&amp;v=enabled&amp;s=da8cf088750146925a23b6b5f41e2f210b55b089", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5464926ac45ba3e0c06540cb098db8299a14909d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09b3439b7e3286d864001600f7e7616294f6c45c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adcef846f1052d26c40dc89da4101e6604ab1e9f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3dc76d180971d7883c21653e6400aac0a038cf6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1158447db1321a9bd8a7015bc7833e6472dc155b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RB1gScjH50z8t_WgB8vxnL_RzJJ1HakfFq9bNk68vS8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=596676a9cfb0c672e37c5858f281ef498db224c7", "width": 1080, "height": 540}], "variants": {}, "id": "t-CxMVPtyNTZzYQPh4UM0rmCdU6bZZkp8qtN1Iriuhc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1173k2t", "is_robot_indexable": true, "report_reasons": null, "author": "TugaRuca_20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1173k2t/question_regarding_time_series_decomposition_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1173k2t/question_regarding_time_series_decomposition_and/", "subreddit_subscribers": 849581, "created_utc": 1676889586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!\n\nI want to get some input in regards to the role of requirements engineering for data science \u2013 and who should be (or is) responsible for this in your department.\n\n**Background**: Our department has multiple subteams \u2013 e.g. data engineering, reporting, \u2026 and data science (this is where I work). Each one of those has somebody responsible for creating tickets, maintaining their backlog or simply to communicate with other product owners / managers to organize and plan interaction / bigger projects.\n\nAdditionally, we have one product owner, who is responsible for the oversight of all other product owner/techleads of subteams.\n\nRight now, we are moving into cloud, which is a big challenge for a lot of us (as the team almost exclusively coded in SQL for the last 10+ years, with the exception of our data architects and us data scientists).\n\n**Situation:** As we originated from a business intelligence / data warehousing team, most of us have always been used to getting only prosaic/short user stories as input for our work. This worked out just fine for us as a team, as most of us have a lot of experience as SQL-developers and within our company.\n\nBut now that we had some open positions and mostly just were able to attract juniors, we thought it would be a good idea to provide them with someone who helps them \u2013 which lead to the promotion of a colleague from SQL-developer to requirements engineer. She is very experienced in on-prem-data warehousing and is very structured and organized \u2013 she is doing her job great.\n\n**Challenge:** Our colleague now wants to do requirements engineering for all subteams, but we are still working on the details or how to make it work. For me as a data scientist (and my additional two data scientists), it is sometimes hard to understand the benefit of an additional person taking on requirements.\n\nRight now, I\u2019m talking with our customers and writing a rough sketch of a story with some technical details (if it makes sense to provide them to my colleagues \u2013 but they are already very experienced so they mostly don\u2019t need detailed information). During our weekly/daily meetings I present new stories to them, explain the details/requirements and collect their questions.\n\nIf there are just too many or the topic is bigger, I just invite one or both of my colleagues to meetings with our clients. This worked out great for the last two years (since we were established).\n\nBecause of that, I mainly see disadvantages with introducing our requirements engineer into the process:\n\n&amp;#x200B;\n\n* She has no experience whatsoever about data science, which leads to her asking me to define which algorithm I plan to use in 6 months from now for a specific project - or in general, asking for a template for a solution for every (un)known problem on this planet (*\u201cso I always know what to write in the requirement\u201d*), while most of the stuff we do is pretty new for us ourselves. More often than not, we have to do some research for ourselves at first \u2013 I can\u2019t give her detailed answers to a problem I\u2019ve never worked on, and we don\u2019t simply can\u2019t and want to \u201csolve\u201d everything with xgboost\n* she does not understand that we can\u2019t define which columns we will use at the end of a project (and how to transform it or how to define the dataset for data engineers) before we did our EDA and asked the business lines a big load of questions. she also does not understand why a substantial amount of data science projects should and will be ended after EDA/problem formulation (because of data quality issues etc \u2013 you name and know it) and why we can\u2019t simply work all our problems the same way our data engineers do\n* She has no experience in regards to cloud environment / python, which makes me wonder how she is supposed to fully understand the technical preconditions and problems, as well as formulate a proposal for a solution (these are things she does on-prem for sql, and it works very well there because of her experience / skillset)\n* She expects our customers to know exactly, what their data science use case is, should be able to do, which data they need, what algo they should use, how the result will be looking \u2026. while I see it this way: we are a new product in our company and therefore have to sell our services. We have to explain to our customers what\u2019s possible and most of the time, we have to find out what\u2019s possible is for a few days or even weeks (EDA, prototypes, statistical tests). If we just tell everyone to \u201csuck it up\u201d and rewrite their requirements in a very detailed and technical way, why would we need her \u2013 or myself? I just don\u2019t think expecting it from them will lead to them being knowledgeable in data science\n* Due to her being inexperience in data science / cloud / python / selling project, it leads to a lot of additional meetings in different combinations (business/her/me, business/her/my colleagues, her/me, her/me/my colleagues, \u2026). It feels like I\u2019m still attending every meeting, and when I\u2019m not included in one of them, things go wrong. I still have to do all of the \u201cextra things\u201d besides being a data scientist, and somehow even have to do more explaining and communication on top than before\n* She is just one requirement engineer and there are a lot of subteams and tasks in our department - which means, if she is ill or on vacation, there is no one to write the requirements (in theory, because in reality the senior devs just pick up the prosa-stories like they did the last few years \u2013 and they help our juniors). She is already talking about getting another requirements engineer\n\nNow I realize I ended up in a rant, but I want to underline, how valuable she is as a senior on-prem developer. She is smart, structured, well organized and always offers her help.\n\nMaybe I just feel attacked (because giving her this part of my work, would mean I\u2019d be back just fulfilling detailed tasks without having an influence on the decision makers).\n\n**Questions**: Therefore I\u2019m really interested in your opinion and experiences:\n\n1. Who takes the requirements in your department? Is he/she knowledgeable in data science or has a background in the technologies or business aspects?\n2. How long does it take from speaking with the business to getting a ticket on your board?\n3. How detailed are those tickets \u2013 are there differences in detail/granularity depending on whether a senior or a junior will do that story (most of the time, we already know who will take which ticket)?\n4. Do you follow CRISP-DM or any other process, are you happy with it and it works 100%? Do you use a JIRA/Kanban Board or Scrum, Sprints, \u2026?\n5. How do you explain to management or non-data-science-folks that you can\u2019t already know how exactly to solve a problem or which algorithm you\u2019ll use for a new business problem? Or are you just experienced enough \u201cto know\u201d?\n6. Do you see a place for a specific role for requirements engineering in your department? What could be advantages for you as a data science team having such a role?\n\nThank you so much for your input.\n\nBest regards", "author_fullname": "t2_1bw01s6w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requirements Engineering for data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171y3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676886983.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676883193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I want to get some input in regards to the role of requirements engineering for data science \u2013 and who should be (or is) responsible for this in your department.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: Our department has multiple subteams \u2013 e.g. data engineering, reporting, \u2026 and data science (this is where I work). Each one of those has somebody responsible for creating tickets, maintaining their backlog or simply to communicate with other product owners / managers to organize and plan interaction / bigger projects.&lt;/p&gt;\n\n&lt;p&gt;Additionally, we have one product owner, who is responsible for the oversight of all other product owner/techleads of subteams.&lt;/p&gt;\n\n&lt;p&gt;Right now, we are moving into cloud, which is a big challenge for a lot of us (as the team almost exclusively coded in SQL for the last 10+ years, with the exception of our data architects and us data scientists).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Situation:&lt;/strong&gt; As we originated from a business intelligence / data warehousing team, most of us have always been used to getting only prosaic/short user stories as input for our work. This worked out just fine for us as a team, as most of us have a lot of experience as SQL-developers and within our company.&lt;/p&gt;\n\n&lt;p&gt;But now that we had some open positions and mostly just were able to attract juniors, we thought it would be a good idea to provide them with someone who helps them \u2013 which lead to the promotion of a colleague from SQL-developer to requirements engineer. She is very experienced in on-prem-data warehousing and is very structured and organized \u2013 she is doing her job great.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; Our colleague now wants to do requirements engineering for all subteams, but we are still working on the details or how to make it work. For me as a data scientist (and my additional two data scientists), it is sometimes hard to understand the benefit of an additional person taking on requirements.&lt;/p&gt;\n\n&lt;p&gt;Right now, I\u2019m talking with our customers and writing a rough sketch of a story with some technical details (if it makes sense to provide them to my colleagues \u2013 but they are already very experienced so they mostly don\u2019t need detailed information). During our weekly/daily meetings I present new stories to them, explain the details/requirements and collect their questions.&lt;/p&gt;\n\n&lt;p&gt;If there are just too many or the topic is bigger, I just invite one or both of my colleagues to meetings with our clients. This worked out great for the last two years (since we were established).&lt;/p&gt;\n\n&lt;p&gt;Because of that, I mainly see disadvantages with introducing our requirements engineer into the process:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;She has no experience whatsoever about data science, which leads to her asking me to define which algorithm I plan to use in 6 months from now for a specific project - or in general, asking for a template for a solution for every (un)known problem on this planet (&lt;em&gt;\u201cso I always know what to write in the requirement\u201d&lt;/em&gt;), while most of the stuff we do is pretty new for us ourselves. More often than not, we have to do some research for ourselves at first \u2013 I can\u2019t give her detailed answers to a problem I\u2019ve never worked on, and we don\u2019t simply can\u2019t and want to \u201csolve\u201d everything with xgboost&lt;/li&gt;\n&lt;li&gt;she does not understand that we can\u2019t define which columns we will use at the end of a project (and how to transform it or how to define the dataset for data engineers) before we did our EDA and asked the business lines a big load of questions. she also does not understand why a substantial amount of data science projects should and will be ended after EDA/problem formulation (because of data quality issues etc \u2013 you name and know it) and why we can\u2019t simply work all our problems the same way our data engineers do&lt;/li&gt;\n&lt;li&gt;She has no experience in regards to cloud environment / python, which makes me wonder how she is supposed to fully understand the technical preconditions and problems, as well as formulate a proposal for a solution (these are things she does on-prem for sql, and it works very well there because of her experience / skillset)&lt;/li&gt;\n&lt;li&gt;She expects our customers to know exactly, what their data science use case is, should be able to do, which data they need, what algo they should use, how the result will be looking \u2026. while I see it this way: we are a new product in our company and therefore have to sell our services. We have to explain to our customers what\u2019s possible and most of the time, we have to find out what\u2019s possible is for a few days or even weeks (EDA, prototypes, statistical tests). If we just tell everyone to \u201csuck it up\u201d and rewrite their requirements in a very detailed and technical way, why would we need her \u2013 or myself? I just don\u2019t think expecting it from them will lead to them being knowledgeable in data science&lt;/li&gt;\n&lt;li&gt;Due to her being inexperience in data science / cloud / python / selling project, it leads to a lot of additional meetings in different combinations (business/her/me, business/her/my colleagues, her/me, her/me/my colleagues, \u2026). It feels like I\u2019m still attending every meeting, and when I\u2019m not included in one of them, things go wrong. I still have to do all of the \u201cextra things\u201d besides being a data scientist, and somehow even have to do more explaining and communication on top than before&lt;/li&gt;\n&lt;li&gt;She is just one requirement engineer and there are a lot of subteams and tasks in our department - which means, if she is ill or on vacation, there is no one to write the requirements (in theory, because in reality the senior devs just pick up the prosa-stories like they did the last few years \u2013 and they help our juniors). She is already talking about getting another requirements engineer&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now I realize I ended up in a rant, but I want to underline, how valuable she is as a senior on-prem developer. She is smart, structured, well organized and always offers her help.&lt;/p&gt;\n\n&lt;p&gt;Maybe I just feel attacked (because giving her this part of my work, would mean I\u2019d be back just fulfilling detailed tasks without having an influence on the decision makers).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;: Therefore I\u2019m really interested in your opinion and experiences:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Who takes the requirements in your department? Is he/she knowledgeable in data science or has a background in the technologies or business aspects?&lt;/li&gt;\n&lt;li&gt;How long does it take from speaking with the business to getting a ticket on your board?&lt;/li&gt;\n&lt;li&gt;How detailed are those tickets \u2013 are there differences in detail/granularity depending on whether a senior or a junior will do that story (most of the time, we already know who will take which ticket)?&lt;/li&gt;\n&lt;li&gt;Do you follow CRISP-DM or any other process, are you happy with it and it works 100%? Do you use a JIRA/Kanban Board or Scrum, Sprints, \u2026?&lt;/li&gt;\n&lt;li&gt;How do you explain to management or non-data-science-folks that you can\u2019t already know how exactly to solve a problem or which algorithm you\u2019ll use for a new business problem? Or are you just experienced enough \u201cto know\u201d?&lt;/li&gt;\n&lt;li&gt;Do you see a place for a specific role for requirements engineering in your department? What could be advantages for you as a data science team having such a role?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you so much for your input.&lt;/p&gt;\n\n&lt;p&gt;Best regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1171y3e", "is_robot_indexable": true, "report_reasons": null, "author": "workah0lik", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1171y3e/requirements_engineering_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1171y3e/requirements_engineering_for_data_science/", "subreddit_subscribers": 849581, "created_utc": 1676883193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_hqv1j56y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what evaluation metric do you use for a seq2seq model/ chatbot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116z9dj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676872934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116z9dj", "is_robot_indexable": true, "report_reasons": null, "author": "Early-Quote1725", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116z9dj/what_evaluation_metric_do_you_use_for_a_seq2seq/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116z9dj/what_evaluation_metric_do_you_use_for_a_seq2seq/", "subreddit_subscribers": 849581, "created_utc": 1676872934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 20 Feb, 2023 - 27 Feb, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116y7t5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676869288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116y7t5", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116y7t5/weekly_entering_transitioning_thread_20_feb_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/116y7t5/weekly_entering_transitioning_thread_20_feb_2023/", "subreddit_subscribers": 849581, "created_utc": 1676869288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I apologize I don't really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don't fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I'm really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.", "author_fullname": "t2_4yh9ceg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would community-owned data centers help with the sovereignty of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wql0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize I don&amp;#39;t really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don&amp;#39;t fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I&amp;#39;m really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116wql0", "is_robot_indexable": true, "report_reasons": null, "author": "proudmisfit", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116wql0/would_communityowned_data_centers_help_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116wql0/would_communityowned_data_centers_help_with_the/", "subreddit_subscribers": 849581, "created_utc": 1676864402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_svzps9h1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Combat Human Trafficking Using Hidden Cameras And Machine Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_11749x1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tM2EXFsXosM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Combat Human Trafficking Using Hidden Cameras And Machine Learning\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Combat Human Trafficking Using Hidden Cameras And Machine Learning", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tM2EXFsXosM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Combat Human Trafficking Using Hidden Cameras And Machine Learning\"&gt;&lt;/iframe&gt;", "author_name": "OnePageCode", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tM2EXFsXosM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@onepagecode"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tM2EXFsXosM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Combat Human Trafficking Using Hidden Cameras And Machine Learning\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/11749x1", "height": 200}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/mNngQdov8LEuPd2kjXChKECpaG_JAGrZ6kfTNh0uW20.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676892257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=tM2EXFsXosM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_y2NsDgJJc58t9BuvItP9o-cAEvMac-9AUyJjsAMXBo.jpg?auto=webp&amp;v=enabled&amp;s=a719bf267045b34b5951d9b21c399fd416710c9e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/_y2NsDgJJc58t9BuvItP9o-cAEvMac-9AUyJjsAMXBo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e3287fadc6ebad43bac54d789dc0b5b88535aae", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/_y2NsDgJJc58t9BuvItP9o-cAEvMac-9AUyJjsAMXBo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5108da451e106b049780b139018f52bcf92a91d1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/_y2NsDgJJc58t9BuvItP9o-cAEvMac-9AUyJjsAMXBo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8adfa2c1075bab3f367e20e63c582312cf961e2", "width": 320, "height": 240}], "variants": {}, "id": "IFVBx-NZ1_fw39YpcIhDof3E0ATftAmyW5HgRB0phKw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "11749x1", "is_robot_indexable": true, "report_reasons": null, "author": "st1275857", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11749x1/combat_human_trafficking_using_hidden_cameras_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=tM2EXFsXosM", "subreddit_subscribers": 849581, "created_utc": 1676892257.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Combat Human Trafficking Using Hidden Cameras And Machine Learning", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tM2EXFsXosM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Combat Human Trafficking Using Hidden Cameras And Machine Learning\"&gt;&lt;/iframe&gt;", "author_name": "OnePageCode", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tM2EXFsXosM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@onepagecode"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to pursue a career in data. My laptop broke and looking for a replacement. Is[this](https://www.bestbuy.com/site/hp-victus-15-6-gaming-laptop-intel-core-i5-12450h-8gb-memory-nvidia-geforce-gtx-1650-512gb-ssd-mica-silver/6503849.p?skuId=6503849) good enough? To work on personal projects, build up a portfolio, do hackathons, etc\u2026 (Basically, is it enough for me to improve myself until I can land a job and upgrade?) I\u2019m planning to upgrade the ram to 16 gb.", "author_fullname": "t2_u5jtyok", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Laptop Recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171kvk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676881739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pursue a career in data. My laptop broke and looking for a replacement. Is&lt;a href=\"https://www.bestbuy.com/site/hp-victus-15-6-gaming-laptop-intel-core-i5-12450h-8gb-memory-nvidia-geforce-gtx-1650-512gb-ssd-mica-silver/6503849.p?skuId=6503849\"&gt;this&lt;/a&gt; good enough? To work on personal projects, build up a portfolio, do hackathons, etc\u2026 (Basically, is it enough for me to improve myself until I can land a job and upgrade?) I\u2019m planning to upgrade the ram to 16 gb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?auto=webp&amp;v=enabled&amp;s=82d309dcc0a3f4b06f43eea82cc1f6f7361f3027", "width": 1240, "height": 1058}, "resolutions": [{"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78b2b17ee55ebee49c6dd6687dfdd043929fc91c", "width": 108, "height": 92}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=907a9f43766c88b04d763cf016b5c0e8ac4d2094", "width": 216, "height": 184}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81ada832c9cb2954de3aa683e4cc27f863e6a57", "width": 320, "height": 273}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54258cc1b18f6b8a3007a08322b8a83e52cdface", "width": 640, "height": 546}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3e14fecb8401d7404124369b21e40f25d7e6d51", "width": 960, "height": 819}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7deb74ca4d9b75a3bb0be3871e4b1ecbf26454b3", "width": 1080, "height": 921}], "variants": {}, "id": "ogusEl8GfwyZ-TFcxoIYQGnIluKtIjKc9j3PLFWmF48"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1171kvk", "is_robot_indexable": true, "report_reasons": null, "author": "hazurdv", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1171kvk/laptop_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1171kvk/laptop_recommendation/", "subreddit_subscribers": 849581, "created_utc": 1676881739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, I am interested to see what tools are in high demand right now. In addition, are any automation tools part of your data process?", "author_fullname": "t2_s2o8t6mt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tool and framework are you currently using in your data process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116jvbv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676830712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am interested to see what tools are in high demand right now. In addition, are any automation tools part of your data process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116jvbv", "is_robot_indexable": true, "report_reasons": null, "author": "osa1501", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/116jvbv/what_tool_and_framework_are_you_currently_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/116jvbv/what_tool_and_framework_are_you_currently_using/", "subreddit_subscribers": 849581, "created_utc": 1676830712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to pursue a career in data. My laptop broke and looking for a replacement. Is[this](https://www.bestbuy.com/site/hp-victus-15-6-gaming-laptop-intel-core-i5-12450h-8gb-memory-nvidia-geforce-gtx-1650-512gb-ssd-mica-silver/6503849.p?skuId=6503849) good enough? To work on personal projects, build up a portfolio, do hackathons, etc\u2026 (Basically, is it enough for me to improve myself until I can land a job and upgrade?) I\u2019m planning to upgrade the ram to 16 gb.", "author_fullname": "t2_u5jtyok", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Laptop Recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171kvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676881739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pursue a career in data. My laptop broke and looking for a replacement. Is&lt;a href=\"https://www.bestbuy.com/site/hp-victus-15-6-gaming-laptop-intel-core-i5-12450h-8gb-memory-nvidia-geforce-gtx-1650-512gb-ssd-mica-silver/6503849.p?skuId=6503849\"&gt;this&lt;/a&gt; good enough? To work on personal projects, build up a portfolio, do hackathons, etc\u2026 (Basically, is it enough for me to improve myself until I can land a job and upgrade?) I\u2019m planning to upgrade the ram to 16 gb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?auto=webp&amp;v=enabled&amp;s=82d309dcc0a3f4b06f43eea82cc1f6f7361f3027", "width": 1240, "height": 1058}, "resolutions": [{"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78b2b17ee55ebee49c6dd6687dfdd043929fc91c", "width": 108, "height": 92}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=907a9f43766c88b04d763cf016b5c0e8ac4d2094", "width": 216, "height": 184}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81ada832c9cb2954de3aa683e4cc27f863e6a57", "width": 320, "height": 273}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54258cc1b18f6b8a3007a08322b8a83e52cdface", "width": 640, "height": 546}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3e14fecb8401d7404124369b21e40f25d7e6d51", "width": 960, "height": 819}, {"url": "https://external-preview.redd.it/47oQA6eXfbucG6moslAmQSrvXQJ29Nd-c717BybGYKE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7deb74ca4d9b75a3bb0be3871e4b1ecbf26454b3", "width": 1080, "height": 921}], "variants": {}, "id": "ogusEl8GfwyZ-TFcxoIYQGnIluKtIjKc9j3PLFWmF48"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1171kvr", "is_robot_indexable": true, "report_reasons": null, "author": "hazurdv", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1171kvr/laptop_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1171kvr/laptop_recommendation/", "subreddit_subscribers": 849581, "created_utc": 1676881739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I used AI to transform a nostalgic radio drama into a breathtaking graphic novel - Light's in the Old Fort The Graphic Novelization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1170un7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hmltihb", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aFSmNXqavjM6hxeXmEPgy8o9oqG2lq5TK61_FdQtWGo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "u_Brothercast", "selftext": "&amp;#x200B;\n\n[A Pioneering Experiment in AI-Augmented Creative Expression](https://reddit.com/link/116wgia/video/ciidx2j719ja1/player)\n\nAfter months of hard work, I'm thrilled to finally share with you the result of an ambitious experiment in storytelling: \"Lights in the Old Fort: The Graphic Novelization.\" \n\nBy fusing human creativity with cutting-edge generative technologies, this story blurs the line between what's possible with traditional methods and what's possible with the latest advances in AI.\n\nAs someone with an insatiable passion for both creative writing and data science, I've always been drawn to the magic of storytelling. But when I gained access to GPT-3, I discovered a whole new world of creative potential that I knew I had to explore. What started as a passion project has turned into a groundbreaking example of the possibilities of blending human creativity with artificial intelligence and large language models.\n\nWith nods to classic young adult adventure fiction like the Hardy Boys and Nancy Drew, \"Lights in the Old Fort\" is a thrilling throwback to a time when adventure was around every corner and anything was possible. But with the help of cutting-edge technology, I've taken that sense of adventure to a whole new level, creating a story that's part human, part machine, and 100% unlike anything you've seen before.\n\n At the heart of the story are two sibling sleuths, Anthonio and Dathaniel, and their plucky friend Wally. Together, they uncover a web of deception surrounding a mysterious crate of counterfeit ballots and a giant diamond they find on the shore. It's a mystery that will keep you on the edge of your seat, and a tale that's guaranteed to captivate your imagination. \n\nBut what makes \"Lights in the Old Fort\" truly unprecedented is the way it was created. With careful tweaking and curation, I used fine-tuned GPT-3 models to generate narration that seamlessly blended with the rest of the manuscript. And with generative image technology from Midjourney AI, I was able to bring the world of Jetty Bay to life in dozens of stunning illustrations based on 1920s lithography. The result is a story that represents a true fusion of human and machine creativity, and I want to share with everyone what I've learned along the way.\n\nNow, I'm excited to share this project with the community and invite you to be a part of this groundbreaking journey. I'm currently crowdfunding a self-published first edition on Kickstarter, and your support can make a difference. Head over to our Kickstarter page now to learn more and pledge your support. \n\nLet's make \"Lights in the Old Fort: The Graphic Novelization\" a reality and a shining example of the power of creativity and technology!\n\n[https://www.kickstarter.com/projects/ambp/lights-in-the-old-fort-the-novelization?ref=d5x96b](https://www.kickstarter.com/projects/ambp/lights-in-the-old-fort-the-novelization?ref=d5x96b)\n\n**Tone Pettit** and **Pettitshire Studios**\n\n[Follow Anthonio and Dathaniel, the two daring sibling detectives, and their young friend Wally, as they embark on a journey to uncover the truth behind a web of deception surrounding a mysterious crate of counterfeit ballots and a giant diamond found on the beach.](https://preview.redd.it/8fjp8uqrw9ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d3740c774c0086eec457dd84b9d7aa90b557d37a)\n\n[Journey back in time to the Eastern Seaboard in the Roaring Twenties!](https://preview.redd.it/7qwegmn559ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ae5111d67ee9e56c25c222dabcc74d43d5b9e6f6)\n\n[Featuring dozens of stunning illustrations based on 1920s lithography, the book is a visual treat!](https://preview.redd.it/pn353uzoq9ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aee2248ae8d7e8a6a69c3fabb3293119d4bea925)\n\n[What will the boys find hidden within the decaying ruins of Fort Bowden?](https://preview.redd.it/p48av6w759ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=566214d9a2505447b91e6c532ba8a63bd1a44ddd)\n\n[Character illustrations from the novel, inspired by the voices of the actors who portrayed them in the original podcast series](https://preview.redd.it/najol167r9ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=33169a64644e93bb5ce15b688f95b072c3d9f8fa)\n\n[Lights in the Old Fort: The Graphic Novelization by Tone Pettit- Pledge your support for the future of creative expression today!](https://preview.redd.it/wi2temjd59ja1.png?width=1344&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d1b58c76e076a1d736b4570fefd667bec74be716)", "author_fullname": "t2_hmltihb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I used AI to transform a nostalgic radio drama into a breathtaking graphic novel - Light's in the Old Fort The Graphic Novelization", "link_flair_richtext": [], "subreddit_name_prefixed": "u/Brothercast", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"wi2temjd59ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1936c4a5e674d80d1e2f5122b14ea8930bd87a3f"}, {"y": 164, "x": 216, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9af844ab75d619b078fdbfe1aeccac738a6c09ca"}, {"y": 243, "x": 320, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81dfb91257c272e479d513ec97d0c8c8e895928c"}, {"y": 487, "x": 640, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fab6b7d13ab3b2d261dac17495fa17b20fc781f"}, {"y": 731, "x": 960, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=004b0a2c3f51e91cac9ec29423c430ba09c69706"}, {"y": 822, "x": 1080, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3980713ea7f3689817d6c6f757234f92967f7419"}], "s": {"y": 1024, "x": 1344, "u": "https://preview.redd.it/wi2temjd59ja1.png?width=1344&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d1b58c76e076a1d736b4570fefd667bec74be716"}, "id": "wi2temjd59ja1"}, "p48av6w759ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/p48av6w759ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c195e33e08875d547e401fd417352d96ff653585"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/p48av6w759ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a48581595fa9871f31e22cdc0916f143b8655fc9"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/p48av6w759ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ea1ec4f262ac9d4e3572dc2250262aa83e000b3"}, {"y": 640, "x": 640, "u": "https://preview.redd.it/p48av6w759ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b47bc36e8f141c4f473de963a922cc09f5fefefe"}, {"y": 960, "x": 960, "u": "https://preview.redd.it/p48av6w759ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=214e7c25a1187f82770b4910f3a7a9ec48c35003"}], "s": {"y": 1024, "x": 1024, "u": "https://preview.redd.it/p48av6w759ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=566214d9a2505447b91e6c532ba8a63bd1a44ddd"}, "id": "p48av6w759ja1"}, "ciidx2j719ja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/116wgia/asset/ciidx2j719ja1/DASHPlaylist.mpd?a=1679490024%2CMjUyNmI0M2E5YmNmYzJlZWZhOWMxMmZmMGQ3NGVkOTcxMzU1OWQwMmZjNTU5ZjEzNzllZmM5NmZjYmZmYmQxMg%3D%3D&amp;v=1&amp;f=sd", "x": 1920, "y": 1080, "hlsUrl": "https://v.redd.it/link/116wgia/asset/ciidx2j719ja1/HLSPlaylist.m3u8?a=1679490024%2CODU0ZTQxZjJlNDQzMjBlMWQ2MTlkNzQ3Y2M4MjI3MDI4OGY3YzNmOGIyMTc1MGU1YjllNjcyNDBmOGIzYmE3OA%3D%3D&amp;v=1&amp;f=sd", "id": "ciidx2j719ja1", "isGif": false}, "pn353uzoq9ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/pn353uzoq9ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f875a48719ad1a1a69842f7add243cbbd1ee2700"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/pn353uzoq9ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8130c3b50ad7998dfc39c45164a335c65d3d1f3b"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/pn353uzoq9ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b94d0ad5d55c68258fe3180d68985476dc4f5d45"}, {"y": 640, "x": 640, "u": "https://preview.redd.it/pn353uzoq9ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dee49d5d24116b4082ff6cfa9b76150df37bb45d"}, {"y": 960, "x": 960, "u": "https://preview.redd.it/pn353uzoq9ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a371eae76c8bff97e1cf7633fbaa67710055c20c"}], "s": {"y": 1024, "x": 1024, "u": "https://preview.redd.it/pn353uzoq9ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aee2248ae8d7e8a6a69c3fabb3293119d4bea925"}, "id": "pn353uzoq9ja1"}, "7qwegmn559ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/7qwegmn559ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b29d8d8f82aec57bfbc7029896aa9db4ab8bf9b"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/7qwegmn559ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29f08fd6949b78d4f0b097ce8d8ab4c7c88befc8"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/7qwegmn559ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8cea835dd3de81589cff63abafe72b980c002b10"}, {"y": 640, "x": 640, "u": "https://preview.redd.it/7qwegmn559ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=590eb82e6504c8aa50453b13df28ff202c63ca3e"}, {"y": 960, "x": 960, "u": "https://preview.redd.it/7qwegmn559ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3db0fa647d26462c497065617254d355dbb1b420"}], "s": {"y": 1024, "x": 1024, "u": "https://preview.redd.it/7qwegmn559ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ae5111d67ee9e56c25c222dabcc74d43d5b9e6f6"}, "id": "7qwegmn559ja1"}, "8fjp8uqrw9ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/8fjp8uqrw9ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c020312c63ef28f1b5d384489f2cc39579580a1c"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/8fjp8uqrw9ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d92990d7b25e2071257806325593be75dbec073"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/8fjp8uqrw9ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9693179afe6f61632b49815f6716f1551ff311d"}, {"y": 640, "x": 640, "u": "https://preview.redd.it/8fjp8uqrw9ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d8c905e97347ed47f6f9f02dbb1d15c12782d37"}, {"y": 960, "x": 960, "u": "https://preview.redd.it/8fjp8uqrw9ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13213fc7a44165067d1f4dff6929485fff5679b9"}], "s": {"y": 1024, "x": 1024, "u": "https://preview.redd.it/8fjp8uqrw9ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d3740c774c0086eec457dd84b9d7aa90b557d37a"}, "id": "8fjp8uqrw9ja1"}, "najol167r9ja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/najol167r9ja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cefb572faf8375ee0b6ed3788aae117b0d8bc398"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/najol167r9ja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80b2d9e469636201815b3257ae7041e77b8c3779"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/najol167r9ja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c83d4d7e1cf50d630c2f5de2ff7fc3825a351a02"}, {"y": 640, "x": 640, "u": "https://preview.redd.it/najol167r9ja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=909bd204e1e81261dc32504e2f4f34f0ed7eb6da"}, {"y": 960, "x": 960, "u": "https://preview.redd.it/najol167r9ja1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6622db36646e2a1fb3b151a2caf816b47be3f9e3"}], "s": {"y": 1024, "x": 1024, "u": "https://preview.redd.it/najol167r9ja1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=33169a64644e93bb5ce15b688f95b072c3d9f8fa"}, "id": "najol167r9ja1"}}, "name": "t3_116wgia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "user", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aFSmNXqavjM6hxeXmEPgy8o9oqG2lq5TK61_FdQtWGo.jpg", "edited": 1676878617.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676863497.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Brothercast", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/116wgia/video/ciidx2j719ja1/player\"&gt;A Pioneering Experiment in AI-Augmented Creative Expression&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After months of hard work, I&amp;#39;m thrilled to finally share with you the result of an ambitious experiment in storytelling: &amp;quot;Lights in the Old Fort: The Graphic Novelization.&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;By fusing human creativity with cutting-edge generative technologies, this story blurs the line between what&amp;#39;s possible with traditional methods and what&amp;#39;s possible with the latest advances in AI.&lt;/p&gt;\n\n&lt;p&gt;As someone with an insatiable passion for both creative writing and data science, I&amp;#39;ve always been drawn to the magic of storytelling. But when I gained access to GPT-3, I discovered a whole new world of creative potential that I knew I had to explore. What started as a passion project has turned into a groundbreaking example of the possibilities of blending human creativity with artificial intelligence and large language models.&lt;/p&gt;\n\n&lt;p&gt;With nods to classic young adult adventure fiction like the Hardy Boys and Nancy Drew, &amp;quot;Lights in the Old Fort&amp;quot; is a thrilling throwback to a time when adventure was around every corner and anything was possible. But with the help of cutting-edge technology, I&amp;#39;ve taken that sense of adventure to a whole new level, creating a story that&amp;#39;s part human, part machine, and 100% unlike anything you&amp;#39;ve seen before.&lt;/p&gt;\n\n&lt;p&gt;At the heart of the story are two sibling sleuths, Anthonio and Dathaniel, and their plucky friend Wally. Together, they uncover a web of deception surrounding a mysterious crate of counterfeit ballots and a giant diamond they find on the shore. It&amp;#39;s a mystery that will keep you on the edge of your seat, and a tale that&amp;#39;s guaranteed to captivate your imagination. &lt;/p&gt;\n\n&lt;p&gt;But what makes &amp;quot;Lights in the Old Fort&amp;quot; truly unprecedented is the way it was created. With careful tweaking and curation, I used fine-tuned GPT-3 models to generate narration that seamlessly blended with the rest of the manuscript. And with generative image technology from Midjourney AI, I was able to bring the world of Jetty Bay to life in dozens of stunning illustrations based on 1920s lithography. The result is a story that represents a true fusion of human and machine creativity, and I want to share with everyone what I&amp;#39;ve learned along the way.&lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;m excited to share this project with the community and invite you to be a part of this groundbreaking journey. I&amp;#39;m currently crowdfunding a self-published first edition on Kickstarter, and your support can make a difference. Head over to our Kickstarter page now to learn more and pledge your support. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s make &amp;quot;Lights in the Old Fort: The Graphic Novelization&amp;quot; a reality and a shining example of the power of creativity and technology!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kickstarter.com/projects/ambp/lights-in-the-old-fort-the-novelization?ref=d5x96b\"&gt;https://www.kickstarter.com/projects/ambp/lights-in-the-old-fort-the-novelization?ref=d5x96b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tone Pettit&lt;/strong&gt; and &lt;strong&gt;Pettitshire Studios&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8fjp8uqrw9ja1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d3740c774c0086eec457dd84b9d7aa90b557d37a\"&gt;Follow Anthonio and Dathaniel, the two daring sibling detectives, and their young friend Wally, as they embark on a journey to uncover the truth behind a web of deception surrounding a mysterious crate of counterfeit ballots and a giant diamond found on the beach.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7qwegmn559ja1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ae5111d67ee9e56c25c222dabcc74d43d5b9e6f6\"&gt;Journey back in time to the Eastern Seaboard in the Roaring Twenties!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pn353uzoq9ja1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=aee2248ae8d7e8a6a69c3fabb3293119d4bea925\"&gt;Featuring dozens of stunning illustrations based on 1920s lithography, the book is a visual treat!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/p48av6w759ja1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=566214d9a2505447b91e6c532ba8a63bd1a44ddd\"&gt;What will the boys find hidden within the decaying ruins of Fort Bowden?&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/najol167r9ja1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=33169a64644e93bb5ce15b688f95b072c3d9f8fa\"&gt;Character illustrations from the novel, inspired by the voices of the actors who portrayed them in the original podcast series&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wi2temjd59ja1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d1b58c76e076a1d736b4570fefd667bec74be716\"&gt;Lights in the Old Fort: The Graphic Novelization by Tone Pettit- Pledge your support for the future of creative expression today!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "qa", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_gbeek", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "116wgia", "is_robot_indexable": true, "report_reasons": null, "author": "Brothercast", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/u_Brothercast/comments/116wgia/how_i_used_ai_to_transform_a_nostalgic_radio/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/u_Brothercast/comments/116wgia/how_i_used_ai_to_transform_a_nostalgic_radio/", "subreddit_subscribers": 0, "created_utc": 1676863497.0, "num_crossposts": 10, "media": null, "is_video": false}], "created": 1676878885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Brothercast", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/user/Brothercast/comments/116wgia/how_i_used_ai_to_transform_a_nostalgic_radio/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1170un7", "is_robot_indexable": true, "report_reasons": null, "author": "Brothercast", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_116wgia", "author_flair_text_color": null, "permalink": "/r/datascience/comments/1170un7/how_i_used_ai_to_transform_a_nostalgic_radio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/user/Brothercast/comments/116wgia/how_i_used_ai_to_transform_a_nostalgic_radio/", "subreddit_subscribers": 849581, "created_utc": 1676878885.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}