{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "*Hoping for an interesting thread*", "author_fullname": "t2_27lz615w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the toughest DE problem you faced in your work career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116a03p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 69, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 69, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676812430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hoping for an interesting thread&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116a03p", "is_robot_indexable": true, "report_reasons": null, "author": "priprocks", "discussion_type": null, "num_comments": 80, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/", "subreddit_subscribers": 90281, "created_utc": 1676812430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will start.\n\nI would create a standardized framework to create airflow dags. Due to our inexperience with airflow, everyone created their own version of dags to achieve the same ETL tasks. This has led to unnecessary duplication of code and maintenance nightmare. And since all the jobs are prod critical and interdependent, we cannot phase these dags out. Hence we are perpetually stuck with them", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you could redo your company's data warehouse/data lake/data infrastructure, what would you do differently due to the benefit of hindsight?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1169v89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676812023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will start.&lt;/p&gt;\n\n&lt;p&gt;I would create a standardized framework to create airflow dags. Due to our inexperience with airflow, everyone created their own version of dags to achieve the same ETL tasks. This has led to unnecessary duplication of code and maintenance nightmare. And since all the jobs are prod critical and interdependent, we cannot phase these dags out. Hence we are perpetually stuck with them&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1169v89", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1169v89/if_you_could_redo_your_companys_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1169v89/if_you_could_redo_your_companys_data/", "subreddit_subscribers": 90281, "created_utc": 1676812023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to this field I feel I want to have a clear understanding of data modeling.\nPlease suggest some good resource beginner friendly\nApart from Kimball book", "author_fullname": "t2_pwnfr6q3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good data modeling course for a beginner", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116tx9y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676855873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to this field I feel I want to have a clear understanding of data modeling.\nPlease suggest some good resource beginner friendly\nApart from Kimball book&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116tx9y", "is_robot_indexable": true, "report_reasons": null, "author": "reddituseless", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116tx9y/good_data_modeling_course_for_a_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116tx9y/good_data_modeling_course_for_a_beginner/", "subreddit_subscribers": 90281, "created_utc": 1676855873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I'd say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!", "author_fullname": "t2_7qam9xn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No coding. Is it bad for career", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171ps4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676882464.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676882269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I&amp;#39;d say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1171ps4", "is_robot_indexable": true, "report_reasons": null, "author": "ApprehensiveIce792", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1171ps4/no_coding_is_it_bad_for_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1171ps4/no_coding_is_it_bad_for_career/", "subreddit_subscribers": 90281, "created_utc": 1676882269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote a post that you might enjoy if you're working with Delta tables and are struggling to keep them at max performance. It's a hands-on tutorial with a default dataset that shows how Delta's maintenance commands work at a lower level.\n\n[https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e](https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e)\n\nIf you like it, don't hesitate to drop a follow as I write often about Data and the Apache Spark ecosystem", "author_fullname": "t2_jxqw29xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake - Keeping it fast and clean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1169lak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676811193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a post that you might enjoy if you&amp;#39;re working with Delta tables and are struggling to keep them at max performance. It&amp;#39;s a hands-on tutorial with a default dataset that shows how Delta&amp;#39;s maintenance commands work at a lower level.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e\"&gt;https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you like it, don&amp;#39;t hesitate to drop a follow as I write often about Data and the Apache Spark ecosystem&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?auto=webp&amp;v=enabled&amp;s=00b7fe2c8dc3d0d997638f61ae9d08eba78b699d", "width": 1063, "height": 777}, "resolutions": [{"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60633c68620342e9e456784cd2273bfb52644bca", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1da243eda34aa2295752f812e454828c560ac14", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2d16d68365e7c4bc1f7a1047f974d6c03299f55", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc8c922a41a754aa05dbd16081f985e3c2120aad", "width": 640, "height": 467}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbd639b9a8ec5091e663fe3dcdce6ed195b7cef3", "width": 960, "height": 701}], "variants": {}, "id": "vyVHtKourTs-IBsK9lRJgvoNCNNc6PhgHIcVh1WBfuE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1169lak", "is_robot_indexable": true, "report_reasons": null, "author": "orpheuz24", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1169lak/delta_lake_keeping_it_fast_and_clean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1169lak/delta_lake_keeping_it_fast_and_clean/", "subreddit_subscribers": 90281, "created_utc": 1676811193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm reading about Dagster for some time. I was wondering to use it as an orchestrator in my organization's data lake (Azure, Databricks). But as I dig into it I realized that the only way to schedule Databricks jobs is to use Dagster's 'op'. For me using ops only looks like a regular task-based orchestrator rather than asset-based (as Dagster claims to be). Am I missing something or Dagster, for now, is good only for small projects running in single-node systems?", "author_fullname": "t2_h57m9xk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Dagster well suited for data lakes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116k3dv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676831252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m reading about Dagster for some time. I was wondering to use it as an orchestrator in my organization&amp;#39;s data lake (Azure, Databricks). But as I dig into it I realized that the only way to schedule Databricks jobs is to use Dagster&amp;#39;s &amp;#39;op&amp;#39;. For me using ops only looks like a regular task-based orchestrator rather than asset-based (as Dagster claims to be). Am I missing something or Dagster, for now, is good only for small projects running in single-node systems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116k3dv", "is_robot_indexable": true, "report_reasons": null, "author": "Advanced_Turnip_6090", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116k3dv/is_dagster_well_suited_for_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116k3dv/is_dagster_well_suited_for_data_lakes/", "subreddit_subscribers": 90281, "created_utc": 1676831252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in a startup and I am pushing to implement a separate data warehouse for analytics. But the problem is I am not sure what solution works best if our transactional data is stored in DynamoDB. \n\nThe tech stack right now is mostly for software engineers so orchestration and transformation tools are not present as well. Use cases are powering dashboards, making analytical queries, and process automations. \n\nDWHs I am considering are BigQuery, Snowflake, Databricks, and Redshift.\n\nFor Orchestration I was considering something along the lines of Dagster, GCP Dataflow, and Airflow.\n\nPlease feel free to suggest.", "author_fullname": "t2_7pbhfups", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Data Warehouse for a Startup if our transactions are stored in DynamoDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116yriu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676872480.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676871165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in a startup and I am pushing to implement a separate data warehouse for analytics. But the problem is I am not sure what solution works best if our transactional data is stored in DynamoDB. &lt;/p&gt;\n\n&lt;p&gt;The tech stack right now is mostly for software engineers so orchestration and transformation tools are not present as well. Use cases are powering dashboards, making analytical queries, and process automations. &lt;/p&gt;\n\n&lt;p&gt;DWHs I am considering are BigQuery, Snowflake, Databricks, and Redshift.&lt;/p&gt;\n\n&lt;p&gt;For Orchestration I was considering something along the lines of Dagster, GCP Dataflow, and Airflow.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116yriu", "is_robot_indexable": true, "report_reasons": null, "author": "eej107", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116yriu/good_data_warehouse_for_a_startup_if_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116yriu/good_data_warehouse_for_a_startup_if_our/", "subreddit_subscribers": 90281, "created_utc": 1676871165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the best way to call pyspark on dbt run and create a table with the results of the pyspark's job?", "author_fullname": "t2_11bm9f4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to call pyspark on dbt run and create a table with the results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116vkxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676860786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best way to call pyspark on dbt run and create a table with the results of the pyspark&amp;#39;s job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116vkxc", "is_robot_indexable": true, "report_reasons": null, "author": "ar405", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116vkxc/how_to_call_pyspark_on_dbt_run_and_create_a_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116vkxc/how_to_call_pyspark_on_dbt_run_and_create_a_table/", "subreddit_subscribers": 90281, "created_utc": 1676860786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Correct me if I\u2019m wrong but in overall it seems like ML folks are better off these days, considering the recent shift to AI. FAANG is currently only hiring for AI positions. Comp has always been slightly higher on average imho.\n\nWas wondering if anyone has gone through similar transition and hoping to get some advice on whether it was worth it or not.", "author_fullname": "t2_ntpsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering switching to ML/AI after 8 years in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qth0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676847830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Correct me if I\u2019m wrong but in overall it seems like ML folks are better off these days, considering the recent shift to AI. FAANG is currently only hiring for AI positions. Comp has always been slightly higher on average imho.&lt;/p&gt;\n\n&lt;p&gt;Was wondering if anyone has gone through similar transition and hoping to get some advice on whether it was worth it or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "116qth0", "is_robot_indexable": true, "report_reasons": null, "author": "DCman1993", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116qth0/considering_switching_to_mlai_after_8_years_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116qth0/considering_switching_to_mlai_after_8_years_in_de/", "subreddit_subscribers": 90281, "created_utc": 1676847830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone familiar with how AWS DMS works with CDC? From what I understand, DMS requires a worker node to do the replication tasks to move data from source to destination. For CDC, does the worker aways run? If so, is it possible to only run it in batches so it\u2019s not always running?\n\nAnd anyone know how much the cost is compared to other cdc solutions?", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS DMS Pricing for CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qd2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone familiar with how AWS DMS works with CDC? From what I understand, DMS requires a worker node to do the replication tasks to move data from source to destination. For CDC, does the worker aways run? If so, is it possible to only run it in batches so it\u2019s not always running?&lt;/p&gt;\n\n&lt;p&gt;And anyone know how much the cost is compared to other cdc solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116qd2k", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116qd2k/aws_dms_pricing_for_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116qd2k/aws_dms_pricing_for_cdc/", "subreddit_subscribers": 90281, "created_utc": 1676846720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\nI'm not sure if this is the best place to ask - and I've searched, but my search-fu is possibly not as great as your search-fu and I'd be happy with a link?\n\nWe have an SSAS data warehouse we use for multiple Power BI models and our central Fact table (splitting into star schema) is starting to get very wide (80+) and is looking to get wider. On top of this it is very long, millions of records, basically think orders but with many checkpoints, rule checks, dates, values, pricing, etc, etc.\n\nI am looking at the best solution I can make to model this.  There are already 30+ relationships to Dimensions, all of the additional columns are values.\n\nAt this point if you're asking why don't we aggregate this data? Well, we kind of want to use it as a dimension, because we have many lower grain tables that we also want insight into and it's all I can do to keep things at the Order grain level.\n\nSo, should we keep this as a super-wide table (ultimately it could be 100's of columns), or what's the pain of segmenting it by the type or section of the company for the data and using the Order number to do a Fact-to-Fact link?\n\nIf we choose one solution or the other for the SSAS model, how should we approach the SQL layer - should we also have wide tables there or segment and join?\n\nPart of me keeps thinking that wide tables are the way to go, that JOINs cost, but I want to know how folks with much more experience than my incredibly limited view would approach this.", "author_fullname": "t2_erqka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dimensional modelling and Vertically Splitting Facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116zt9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676874935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if this is the best place to ask - and I&amp;#39;ve searched, but my search-fu is possibly not as great as your search-fu and I&amp;#39;d be happy with a link?&lt;/p&gt;\n\n&lt;p&gt;We have an SSAS data warehouse we use for multiple Power BI models and our central Fact table (splitting into star schema) is starting to get very wide (80+) and is looking to get wider. On top of this it is very long, millions of records, basically think orders but with many checkpoints, rule checks, dates, values, pricing, etc, etc.&lt;/p&gt;\n\n&lt;p&gt;I am looking at the best solution I can make to model this.  There are already 30+ relationships to Dimensions, all of the additional columns are values.&lt;/p&gt;\n\n&lt;p&gt;At this point if you&amp;#39;re asking why don&amp;#39;t we aggregate this data? Well, we kind of want to use it as a dimension, because we have many lower grain tables that we also want insight into and it&amp;#39;s all I can do to keep things at the Order grain level.&lt;/p&gt;\n\n&lt;p&gt;So, should we keep this as a super-wide table (ultimately it could be 100&amp;#39;s of columns), or what&amp;#39;s the pain of segmenting it by the type or section of the company for the data and using the Order number to do a Fact-to-Fact link?&lt;/p&gt;\n\n&lt;p&gt;If we choose one solution or the other for the SSAS model, how should we approach the SQL layer - should we also have wide tables there or segment and join?&lt;/p&gt;\n\n&lt;p&gt;Part of me keeps thinking that wide tables are the way to go, that JOINs cost, but I want to know how folks with much more experience than my incredibly limited view would approach this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116zt9p", "is_robot_indexable": true, "report_reasons": null, "author": "-crucible-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116zt9p/dimensional_modelling_and_vertically_splitting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116zt9p/dimensional_modelling_and_vertically_splitting/", "subreddit_subscribers": 90281, "created_utc": 1676874935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Small and simple organizations have no need for a data mesh architecture. Huge organizations (potentially) benefit from data mesh concepts. What size should an organization be before they would even consider this socio-technical architecture? I'm thinking number of engineers, data teams, sources, consumers, etc.", "author_fullname": "t2_rmmatfaw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Size of organization for data mesh viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116xnhg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676867426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Small and simple organizations have no need for a data mesh architecture. Huge organizations (potentially) benefit from data mesh concepts. What size should an organization be before they would even consider this socio-technical architecture? I&amp;#39;m thinking number of engineers, data teams, sources, consumers, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116xnhg", "is_robot_indexable": true, "report_reasons": null, "author": "realitydevice", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116xnhg/size_of_organization_for_data_mesh_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116xnhg/size_of_organization_for_data_mesh_viability/", "subreddit_subscribers": 90281, "created_utc": 1676867426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I apologize I don't really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don't fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I'm really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.", "author_fullname": "t2_4yh9ceg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would community-owned data centers help with the sovereignty of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wr4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize I don&amp;#39;t really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don&amp;#39;t fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I&amp;#39;m really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116wr4l", "is_robot_indexable": true, "report_reasons": null, "author": "proudmisfit", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116wr4l/would_communityowned_data_centers_help_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116wr4l/would_communityowned_data_centers_help_with_the/", "subreddit_subscribers": 90281, "created_utc": 1676864449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nThe project I'm currently on is using Vertica and we have been asked to go over the vertica Certification course but don't have to actually get the certification.\n\nAs this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?", "author_fullname": "t2_digu9bnx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vertica Certification worth it ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11700w4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676875729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;The project I&amp;#39;m currently on is using Vertica and we have been asked to go over the vertica Certification course but don&amp;#39;t have to actually get the certification.&lt;/p&gt;\n\n&lt;p&gt;As this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11700w4", "is_robot_indexable": true, "report_reasons": null, "author": "Coffeee-Cat", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11700w4/vertica_certification_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11700w4/vertica_certification_worth_it/", "subreddit_subscribers": 90281, "created_utc": 1676875729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\n\nI have a bi layer of data in my data lake that gets periodically updated and a list of tables defined in the thrift server that serves BI tools like tableau, Metabase etc over JDBC.\n\nWhen I update (overwrite) a table location, the definition is no longer valid because underlying files are being changed, is there a general way of solving this issue or I have to refresh the tables in thrift server manually every time?", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark thrift server auto refresh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wzxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676865266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I have a bi layer of data in my data lake that gets periodically updated and a list of tables defined in the thrift server that serves BI tools like tableau, Metabase etc over JDBC.&lt;/p&gt;\n\n&lt;p&gt;When I update (overwrite) a table location, the definition is no longer valid because underlying files are being changed, is there a general way of solving this issue or I have to refresh the tables in thrift server manually every time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116wzxk", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116wzxk/spark_thrift_server_auto_refresh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116wzxk/spark_thrift_server_auto_refresh/", "subreddit_subscribers": 90281, "created_utc": 1676865266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n\nI am new in the field and I am trying to design an architecture to improve our current system. At first I have two cases:\n\n \n\n1. Several ETL pipelines that will be triggered periodically at specified times.\n\n2. ETL pipelines that will be triggered manually by users with custom parameters.\n\n\n\nThe user-triggered pipelines would often be the same as the automatic ones. I wonder if Prefect could be used to queue up the same automatic and manual pipelines with different parameters and distribute this execution queue to different machines for processing. Is there a better tool to solve this problem? Below is a diagram of the problem.\n\n\n\nhttps://imgur.com/a/ru3Jl4X", "author_fullname": "t2_9ca451os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone give me suggestions for modeling my system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ujw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676857694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new in the field and I am trying to design an architecture to improve our current system. At first I have two cases:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Several ETL pipelines that will be triggered periodically at specified times.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ETL pipelines that will be triggered manually by users with custom parameters.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The user-triggered pipelines would often be the same as the automatic ones. I wonder if Prefect could be used to queue up the same automatic and manual pipelines with different parameters and distribute this execution queue to different machines for processing. Is there a better tool to solve this problem? Below is a diagram of the problem.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/ru3Jl4X\"&gt;https://imgur.com/a/ru3Jl4X&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?auto=webp&amp;v=enabled&amp;s=945488c3866319a1db45f35a7dd7a0b75d58637e", "width": 2725, "height": 1187}, "resolutions": [{"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e819550c7aad4347e30a20ab362faf42fb9a127", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf0ade7cbd53490686b72b64275d59d0c4786beb", "width": 216, "height": 94}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b2fd01a454dc7573b32420d284ad3a6e7c80977", "width": 320, "height": 139}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c342f245a6a772fe6ad49ae0477bf87c4b0ca4", "width": 640, "height": 278}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74a22171349a0ac3bd0a3b40aaa5670958d12182", "width": 960, "height": 418}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8da0c8e2c743cdc77a1b3223c0c5c8aad8dc271", "width": 1080, "height": 470}], "variants": {}, "id": "-DDE0WNCgkhPiOHSL0UQnIcnULKFa1wd7xsE3cytSXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116ujw1", "is_robot_indexable": true, "report_reasons": null, "author": "MiserableAstronaut77", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116ujw1/can_anyone_give_me_suggestions_for_modeling_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116ujw1/can_anyone_give_me_suggestions_for_modeling_my/", "subreddit_subscribers": 90281, "created_utc": 1676857694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw this video form this data analyst and thought to myself most of the tools he is using to conduct this research are tools that I always see on data engineering projects. I was wondering if the following project could be classified as a data engineering project ,data analyst or data scientitst project. Mayber all three?\n\nI'm fairly new to this field and was wondering how do differentiate between those three fields of data work. \n\nAlso, if you believe this a data engineer project rate it on a scale of 10. \n\n1-''meh'' uses very little data engineering tools, don't put on CV\n\n5- uses some\n\n10-excellent definitely recommend having on CV\n\nVideo Link: [https://www.youtube.com/watch?v=7G\\_Kz5MOqps&amp;t=1s](https://www.youtube.com/watch?v=7G_Kz5MOqps&amp;t=1s)\n\nLink to site: [https://datanerd.tech/](https://datanerd.tech/)\n\n&amp;#x200B;\n\nthank you for your time and input in advance", "author_fullname": "t2_4jav853u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Classify if the following project is a data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116mutn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676837998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw this video form this data analyst and thought to myself most of the tools he is using to conduct this research are tools that I always see on data engineering projects. I was wondering if the following project could be classified as a data engineering project ,data analyst or data scientitst project. Mayber all three?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly new to this field and was wondering how do differentiate between those three fields of data work. &lt;/p&gt;\n\n&lt;p&gt;Also, if you believe this a data engineer project rate it on a scale of 10. &lt;/p&gt;\n\n&lt;p&gt;1-&amp;#39;&amp;#39;meh&amp;#39;&amp;#39; uses very little data engineering tools, don&amp;#39;t put on CV&lt;/p&gt;\n\n&lt;p&gt;5- uses some&lt;/p&gt;\n\n&lt;p&gt;10-excellent definitely recommend having on CV&lt;/p&gt;\n\n&lt;p&gt;Video Link: &lt;a href=\"https://www.youtube.com/watch?v=7G_Kz5MOqps&amp;amp;t=1s\"&gt;https://www.youtube.com/watch?v=7G_Kz5MOqps&amp;amp;t=1s&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Link to site: &lt;a href=\"https://datanerd.tech/\"&gt;https://datanerd.tech/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thank you for your time and input in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fKcjglWlBNvglJyKxq_VNCRqKFIPP-BYTnYK5MDh3gA.jpg?auto=webp&amp;v=enabled&amp;s=7c8eb5b3561c82cf378d4094e36b52ec0725ab5b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/fKcjglWlBNvglJyKxq_VNCRqKFIPP-BYTnYK5MDh3gA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c741f9cf018304aef2f02a7df1cc9326870dbd7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/fKcjglWlBNvglJyKxq_VNCRqKFIPP-BYTnYK5MDh3gA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1343a3cd9ae1409f0d9163a4cfa2c55c421de954", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/fKcjglWlBNvglJyKxq_VNCRqKFIPP-BYTnYK5MDh3gA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=767e774660121ed324da3f82e6c88b57bf554355", "width": 320, "height": 240}], "variants": {}, "id": "VLgRjvWOt1lZyzDyBI6R6SNzWIQF7OwlhKQmZb15aVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116mutn", "is_robot_indexable": true, "report_reasons": null, "author": "RatedRForRegression", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116mutn/classify_if_the_following_project_is_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116mutn/classify_if_the_following_project_is_a_data/", "subreddit_subscribers": 90281, "created_utc": 1676837998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How did your company or your team handle the massive influx of data that is currently being generated?", "author_fullname": "t2_s2o8t6mt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What techniques you or your company used to handle the massive amount of data being generated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116jxv3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676830879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How did your company or your team handle the massive influx of data that is currently being generated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116jxv3", "is_robot_indexable": true, "report_reasons": null, "author": "osa1501", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116jxv3/what_techniques_you_or_your_company_used_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116jxv3/what_techniques_you_or_your_company_used_to/", "subreddit_subscribers": 90281, "created_utc": 1676830879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At the end of our pipeline there's an aggregate table `'agg_table'` serving user requests from a webpage in realtime. The user request consists of just 3 parameter values (`A, B, C`) identifying the row in the table, s.t. it can basically be interpreted as the query `SELECT * FROM agg_table WHERE key_1=A AND key_2=B AND key_3=C`. However, it is rarely the case that a row with all 3 (or even 2) values exists in the table, s.t. it then defaults (according to business/domain logic) to `WHERE key_1=A AND key_2=B`, if that doesn't exist to `WHERE key_1=A AND key_3=C`, if that doesn't exist to `WHERE key_2=B AND key_3=C`, if that doesn't exist to `WHERE key_1=A` etc.\n\nIs there a recommended way to model the analytics table, with respect to dimensional modelling?", "author_fullname": "t2_cgxiixth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with fallbacks for webpage-serving analytics table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1168siq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676808397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At the end of our pipeline there&amp;#39;s an aggregate table &lt;code&gt;&amp;#39;agg_table&amp;#39;&lt;/code&gt; serving user requests from a webpage in realtime. The user request consists of just 3 parameter values (&lt;code&gt;A, B, C&lt;/code&gt;) identifying the row in the table, s.t. it can basically be interpreted as the query &lt;code&gt;SELECT * FROM agg_table WHERE key_1=A AND key_2=B AND key_3=C&lt;/code&gt;. However, it is rarely the case that a row with all 3 (or even 2) values exists in the table, s.t. it then defaults (according to business/domain logic) to &lt;code&gt;WHERE key_1=A AND key_2=B&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_1=A AND key_3=C&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_2=B AND key_3=C&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_1=A&lt;/code&gt; etc.&lt;/p&gt;\n\n&lt;p&gt;Is there a recommended way to model the analytics table, with respect to dimensional modelling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1168siq", "is_robot_indexable": true, "report_reasons": null, "author": "BusyFture", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1168siq/how_to_deal_with_fallbacks_for_webpageserving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1168siq/how_to_deal_with_fallbacks_for_webpageserving/", "subreddit_subscribers": 90281, "created_utc": 1676808397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was trying to implement this process to create data warehouse. I could think of possible ways.\n\n1. Use AWS services. \n\nTo get data from apps i am using appflow to store data as S3. \nTo get database data, i am using DMS service to get data from databases and store it as S3 bucket with CDC. \n\nNow i want to transfer these source data to Redshift. I checked and most of the documentation mentioned using copy command. \n\nBut in this case, i have to create all tables again in redshift???? Is there any way to do this without creating tables again in redshift i.e, where redshift just takes all tables from s3 bucket. \n\n2. Use snowflake and stitch.\n\nAny help or recommendations in this regard ll be really helpful.\ud83d\ude4f", "author_fullname": "t2_8jfrjnc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline options from source to destination [data warehouse]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11738k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676888361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to implement this process to create data warehouse. I could think of possible ways.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use AWS services. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To get data from apps i am using appflow to store data as S3. \nTo get database data, i am using DMS service to get data from databases and store it as S3 bucket with CDC. &lt;/p&gt;\n\n&lt;p&gt;Now i want to transfer these source data to Redshift. I checked and most of the documentation mentioned using copy command. &lt;/p&gt;\n\n&lt;p&gt;But in this case, i have to create all tables again in redshift???? Is there any way to do this without creating tables again in redshift i.e, where redshift just takes all tables from s3 bucket. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use snowflake and stitch.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help or recommendations in this regard ll be really helpful.\ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11738k2", "is_robot_indexable": true, "report_reasons": null, "author": "lost_soul1995", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11738k2/data_pipeline_options_from_source_to_destination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11738k2/data_pipeline_options_from_source_to_destination/", "subreddit_subscribers": 90281, "created_utc": 1676888361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can task like data cleansing, data structuring, data validation, and data transformation be automated using GPT 2? Has it been done already? I don't believe it is feasible because it isn't that great at formatting.", "author_fullname": "t2_s2o8t6mt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My manager wants me to use GPT2 to automate certain tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116kf89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676832036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can task like data cleansing, data structuring, data validation, and data transformation be automated using GPT 2? Has it been done already? I don&amp;#39;t believe it is feasible because it isn&amp;#39;t that great at formatting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116kf89", "is_robot_indexable": true, "report_reasons": null, "author": "osa1501", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116kf89/my_manager_wants_me_to_use_gpt2_to_automate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116kf89/my_manager_wants_me_to_use_gpt2_to_automate/", "subreddit_subscribers": 90281, "created_utc": 1676832036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm in the final interview stage which should be scheduled for sometime next week. \n\nThey've told me there will be an hour allocated to a 'case study' with no extra details. I have asked for some extra info but if they don't reply soon what can I do to prepare for in the meantime? Also they said in the previous stage there's no live coding test.", "author_fullname": "t2_4ba5z1zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case study for a final interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11692c3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676809409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m in the final interview stage which should be scheduled for sometime next week. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;ve told me there will be an hour allocated to a &amp;#39;case study&amp;#39; with no extra details. I have asked for some extra info but if they don&amp;#39;t reply soon what can I do to prepare for in the meantime? Also they said in the previous stage there&amp;#39;s no live coding test.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11692c3", "is_robot_indexable": true, "report_reasons": null, "author": "dreamr49", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11692c3/case_study_for_a_final_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11692c3/case_study_for_a_final_interview/", "subreddit_subscribers": 90281, "created_utc": 1676809409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure I understand datalake folder structuring correctly:\n\nIf I get everyday new file for lets say sales for some region I would save this file like this:\n\nsales/region/yyyy/mm/dd/file\n\nor\n\nsales/file ?\n\n\nBecause if I understand it correctly - e.g. in Trino I pass the folder with the data (parquet files) and Trino itself loads the files. But if I would have each file in separate folder it would be a problem? I would have to create table for each file manually? Also if I go with \u201csales/region\u201d I can create table with partitions by date and Trino automatically would create these \u201cyyyy/mm/dd\u201d folders? Sorry not sure I understand it correctly.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake - partitions vs folder structure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116iqu0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676827986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure I understand datalake folder structuring correctly:&lt;/p&gt;\n\n&lt;p&gt;If I get everyday new file for lets say sales for some region I would save this file like this:&lt;/p&gt;\n\n&lt;p&gt;sales/region/yyyy/mm/dd/file&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;sales/file ?&lt;/p&gt;\n\n&lt;p&gt;Because if I understand it correctly - e.g. in Trino I pass the folder with the data (parquet files) and Trino itself loads the files. But if I would have each file in separate folder it would be a problem? I would have to create table for each file manually? Also if I go with \u201csales/region\u201d I can create table with partitions by date and Trino automatically would create these \u201cyyyy/mm/dd\u201d folders? Sorry not sure I understand it correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116iqu0", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116iqu0/datalake_partitions_vs_folder_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116iqu0/datalake_partitions_vs_folder_structure/", "subreddit_subscribers": 90281, "created_utc": 1676827986.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}