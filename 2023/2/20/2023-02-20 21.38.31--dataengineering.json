{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I'd say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!", "author_fullname": "t2_7qam9xn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No coding. Is it bad for career", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171ps4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676882464.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676882269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I&amp;#39;d say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1171ps4", "is_robot_indexable": true, "report_reasons": null, "author": "ApprehensiveIce792", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1171ps4/no_coding_is_it_bad_for_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1171ps4/no_coding_is_it_bad_for_career/", "subreddit_subscribers": 90318, "created_utc": 1676882269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.\n\nRecently I've cleared DP-203 and received the Data Engineer Associate certificate. I [shared a post on here](https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&amp;utm_medium=android_app&amp;utm_name=androidcss&amp;utm_term=1&amp;utm_content=share_button) as well.\n\nI prepared some notes on Notion while preparing for the certification. And I'd like to share it with others so that It could help others while doing revision for the exam.\n\nNotes link: [dp203-azure-data-engineering-notes](https://github.com/jithendray/dp203-azure-data-engineering).\n\nTips that helped me:\n\n- I did a decent course on the Udemy. \n- Made notes while watching tbe last lecture videos.\n- The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.\n- Finally, revised the notes that I made a day before the exam.\n\nAll the best, for anyone who is preparing for the exam. Feel free to add \u2b50 to my repo ;)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got certified recently and prepared some notes while preparing for Azure DP-203", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117djjy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676912073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.&lt;/p&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve cleared DP-203 and received the Data Engineer Associate certificate. I &lt;a href=\"https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&amp;amp;utm_medium=android_app&amp;amp;utm_name=androidcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;shared a post on here&lt;/a&gt; as well.&lt;/p&gt;\n\n&lt;p&gt;I prepared some notes on Notion while preparing for the certification. And I&amp;#39;d like to share it with others so that It could help others while doing revision for the exam.&lt;/p&gt;\n\n&lt;p&gt;Notes link: &lt;a href=\"https://github.com/jithendray/dp203-azure-data-engineering\"&gt;dp203-azure-data-engineering-notes&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Tips that helped me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I did a decent course on the Udemy. &lt;/li&gt;\n&lt;li&gt;Made notes while watching tbe last lecture videos.&lt;/li&gt;\n&lt;li&gt;The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.&lt;/li&gt;\n&lt;li&gt;Finally, revised the notes that I made a day before the exam.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All the best, for anyone who is preparing for the exam. Feel free to add \u2b50 to my repo ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?auto=webp&amp;v=enabled&amp;s=09779b28cd66cfd5ff2aa08fc530f1fa0831fc49", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f769aa1c0a1f249552dbc00dfc441edfac9f2659", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9297afeb8d68686fb38859746cdecf9fc40943c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343a56e772e5d82dd7581f48f181b9440b5b5e3e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b29be5481f5b76e130a75650be4590f6e7b98f3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=598766feafb9e3390ec187a00834507d7d716873", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=553c08a009245ad293de85e9851266d1ff557d60", "width": 1080, "height": 540}], "variants": {}, "id": "4SDWg8PO0pIHRKtfw-MN6uvnndNQPozdBuf03lZJ82A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "117djjy", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/", "subreddit_subscribers": 90318, "created_utc": 1676912073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to this field I feel I want to have a clear understanding of data modeling.\nPlease suggest some good resource beginner friendly\nApart from Kimball book", "author_fullname": "t2_pwnfr6q3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good data modeling course for a beginner", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116tx9y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676855873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to this field I feel I want to have a clear understanding of data modeling.\nPlease suggest some good resource beginner friendly\nApart from Kimball book&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116tx9y", "is_robot_indexable": true, "report_reasons": null, "author": "reddituseless", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116tx9y/good_data_modeling_course_for_a_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116tx9y/good_data_modeling_course_for_a_beginner/", "subreddit_subscribers": 90318, "created_utc": 1676855873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in a startup and I am pushing to implement a separate data warehouse for analytics. But the problem is I am not sure what solution works best if our transactional data is stored in DynamoDB. \n\nThe tech stack right now is mostly for software engineers so orchestration and transformation tools are not present as well. Use cases are powering dashboards, making analytical queries, and process automations. \n\nDWHs I am considering are BigQuery, Snowflake, Databricks, and Redshift.\n\nFor Orchestration I was considering something along the lines of Dagster, GCP Dataflow, and Airflow.\n\nPlease feel free to suggest.", "author_fullname": "t2_7pbhfups", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Data Warehouse for a Startup if our transactions are stored in DynamoDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116yriu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676872480.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676871165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in a startup and I am pushing to implement a separate data warehouse for analytics. But the problem is I am not sure what solution works best if our transactional data is stored in DynamoDB. &lt;/p&gt;\n\n&lt;p&gt;The tech stack right now is mostly for software engineers so orchestration and transformation tools are not present as well. Use cases are powering dashboards, making analytical queries, and process automations. &lt;/p&gt;\n\n&lt;p&gt;DWHs I am considering are BigQuery, Snowflake, Databricks, and Redshift.&lt;/p&gt;\n\n&lt;p&gt;For Orchestration I was considering something along the lines of Dagster, GCP Dataflow, and Airflow.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116yriu", "is_robot_indexable": true, "report_reasons": null, "author": "eej107", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116yriu/good_data_warehouse_for_a_startup_if_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116yriu/good_data_warehouse_for_a_startup_if_our/", "subreddit_subscribers": 90318, "created_utc": 1676871165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Correct me if I\u2019m wrong but in overall it seems like ML folks are better off these days, considering the recent shift to AI. FAANG is currently only hiring for AI positions. Comp has always been slightly higher on average imho.\n\nWas wondering if anyone has gone through similar transition and hoping to get some advice on whether it was worth it or not.", "author_fullname": "t2_ntpsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering switching to ML/AI after 8 years in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qth0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676847830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Correct me if I\u2019m wrong but in overall it seems like ML folks are better off these days, considering the recent shift to AI. FAANG is currently only hiring for AI positions. Comp has always been slightly higher on average imho.&lt;/p&gt;\n\n&lt;p&gt;Was wondering if anyone has gone through similar transition and hoping to get some advice on whether it was worth it or not.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "116qth0", "is_robot_indexable": true, "report_reasons": null, "author": "DCman1993", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116qth0/considering_switching_to_mlai_after_8_years_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116qth0/considering_switching_to_mlai_after_8_years_in_de/", "subreddit_subscribers": 90318, "created_utc": 1676847830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\nI'm not sure if this is the best place to ask - and I've searched, but my search-fu is possibly not as great as your search-fu and I'd be happy with a link?\n\nWe have an SSAS data warehouse we use for multiple Power BI models and our central Fact table (splitting into star schema) is starting to get very wide (80+) and is looking to get wider. On top of this it is very long, millions of records, basically think orders but with many checkpoints, rule checks, dates, values, pricing, etc, etc.\n\nI am looking at the best solution I can make to model this.  There are already 30+ relationships to Dimensions, all of the additional columns are values.\n\nAt this point if you're asking why don't we aggregate this data? Well, we kind of want to use it as a dimension, because we have many lower grain tables that we also want insight into and it's all I can do to keep things at the Order grain level.\n\nSo, should we keep this as a super-wide table (ultimately it could be 100's of columns), or what's the pain of segmenting it by the type or section of the company for the data and using the Order number to do a Fact-to-Fact link?\n\nIf we choose one solution or the other for the SSAS model, how should we approach the SQL layer - should we also have wide tables there or segment and join?\n\nPart of me keeps thinking that wide tables are the way to go, that JOINs cost, but I want to know how folks with much more experience than my incredibly limited view would approach this.", "author_fullname": "t2_erqka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dimensional modelling and Vertically Splitting Facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116zt9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676874935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if this is the best place to ask - and I&amp;#39;ve searched, but my search-fu is possibly not as great as your search-fu and I&amp;#39;d be happy with a link?&lt;/p&gt;\n\n&lt;p&gt;We have an SSAS data warehouse we use for multiple Power BI models and our central Fact table (splitting into star schema) is starting to get very wide (80+) and is looking to get wider. On top of this it is very long, millions of records, basically think orders but with many checkpoints, rule checks, dates, values, pricing, etc, etc.&lt;/p&gt;\n\n&lt;p&gt;I am looking at the best solution I can make to model this.  There are already 30+ relationships to Dimensions, all of the additional columns are values.&lt;/p&gt;\n\n&lt;p&gt;At this point if you&amp;#39;re asking why don&amp;#39;t we aggregate this data? Well, we kind of want to use it as a dimension, because we have many lower grain tables that we also want insight into and it&amp;#39;s all I can do to keep things at the Order grain level.&lt;/p&gt;\n\n&lt;p&gt;So, should we keep this as a super-wide table (ultimately it could be 100&amp;#39;s of columns), or what&amp;#39;s the pain of segmenting it by the type or section of the company for the data and using the Order number to do a Fact-to-Fact link?&lt;/p&gt;\n\n&lt;p&gt;If we choose one solution or the other for the SSAS model, how should we approach the SQL layer - should we also have wide tables there or segment and join?&lt;/p&gt;\n\n&lt;p&gt;Part of me keeps thinking that wide tables are the way to go, that JOINs cost, but I want to know how folks with much more experience than my incredibly limited view would approach this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116zt9p", "is_robot_indexable": true, "report_reasons": null, "author": "-crucible-", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116zt9p/dimensional_modelling_and_vertically_splitting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116zt9p/dimensional_modelling_and_vertically_splitting/", "subreddit_subscribers": 90318, "created_utc": 1676874935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to pull data from multiple sql tables into a lake. The ingest should be scheduled every night and pull only new data (incremental load, e.g. from yesterday). Is there some magic offered by databricks to accomplish this? I read about the autoloader, but it seems that it only supports blob storage as a source.\n\nOutside of databricks I would go for a simple spark script, that connects via jdbc connector and pulls records that where created e.g. yesterday.\n\nHow would you solve this?", "author_fullname": "t2_kh9d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using databricks to incrementally load data from a sql server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1179krk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676906617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pull data from multiple sql tables into a lake. The ingest should be scheduled every night and pull only new data (incremental load, e.g. from yesterday). Is there some magic offered by databricks to accomplish this? I read about the autoloader, but it seems that it only supports blob storage as a source.&lt;/p&gt;\n\n&lt;p&gt;Outside of databricks I would go for a simple spark script, that connects via jdbc connector and pulls records that where created e.g. yesterday.&lt;/p&gt;\n\n&lt;p&gt;How would you solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1179krk", "is_robot_indexable": true, "report_reasons": null, "author": "artworkad", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1179krk/using_databricks_to_incrementally_load_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1179krk/using_databricks_to_incrementally_load_data_from/", "subreddit_subscribers": 90318, "created_utc": 1676906617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Small and simple organizations have no need for a data mesh architecture. Huge organizations (potentially) benefit from data mesh concepts. What size should an organization be before they would even consider this socio-technical architecture? I'm thinking number of engineers, data teams, sources, consumers, etc.", "author_fullname": "t2_rmmatfaw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Size of organization for data mesh viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116xnhg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676867426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Small and simple organizations have no need for a data mesh architecture. Huge organizations (potentially) benefit from data mesh concepts. What size should an organization be before they would even consider this socio-technical architecture? I&amp;#39;m thinking number of engineers, data teams, sources, consumers, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116xnhg", "is_robot_indexable": true, "report_reasons": null, "author": "realitydevice", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116xnhg/size_of_organization_for_data_mesh_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116xnhg/size_of_organization_for_data_mesh_viability/", "subreddit_subscribers": 90318, "created_utc": 1676867426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I apologize I don't really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don't fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I'm really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.", "author_fullname": "t2_4yh9ceg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would community-owned data centers help with the sovereignty of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wr4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize I don&amp;#39;t really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don&amp;#39;t fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I&amp;#39;m really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116wr4l", "is_robot_indexable": true, "report_reasons": null, "author": "proudmisfit", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116wr4l/would_communityowned_data_centers_help_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116wr4l/would_communityowned_data_centers_help_with_the/", "subreddit_subscribers": 90318, "created_utc": 1676864449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone familiar with how AWS DMS works with CDC? From what I understand, DMS requires a worker node to do the replication tasks to move data from source to destination. For CDC, does the worker aways run? If so, is it possible to only run it in batches so it\u2019s not always running?\n\nAnd anyone know how much the cost is compared to other cdc solutions?", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS DMS Pricing for CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qd2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone familiar with how AWS DMS works with CDC? From what I understand, DMS requires a worker node to do the replication tasks to move data from source to destination. For CDC, does the worker aways run? If so, is it possible to only run it in batches so it\u2019s not always running?&lt;/p&gt;\n\n&lt;p&gt;And anyone know how much the cost is compared to other cdc solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116qd2k", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116qd2k/aws_dms_pricing_for_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116qd2k/aws_dms_pricing_for_cdc/", "subreddit_subscribers": 90318, "created_utc": 1676846720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\n\nI have a bi layer of data in my data lake that gets periodically updated and a list of tables defined in the thrift server that serves BI tools like tableau, Metabase etc over JDBC.\n\nWhen I update (overwrite) a table location, the definition is no longer valid because underlying files are being changed, is there a general way of solving this issue or I have to refresh the tables in thrift server manually every time?", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark thrift server auto refresh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wzxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676865266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I have a bi layer of data in my data lake that gets periodically updated and a list of tables defined in the thrift server that serves BI tools like tableau, Metabase etc over JDBC.&lt;/p&gt;\n\n&lt;p&gt;When I update (overwrite) a table location, the definition is no longer valid because underlying files are being changed, is there a general way of solving this issue or I have to refresh the tables in thrift server manually every time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116wzxk", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116wzxk/spark_thrift_server_auto_refresh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116wzxk/spark_thrift_server_auto_refresh/", "subreddit_subscribers": 90318, "created_utc": 1676865266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the best way to call pyspark on dbt run and create a table with the results of the pyspark's job?", "author_fullname": "t2_11bm9f4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to call pyspark on dbt run and create a table with the results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116vkxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676860786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best way to call pyspark on dbt run and create a table with the results of the pyspark&amp;#39;s job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116vkxc", "is_robot_indexable": true, "report_reasons": null, "author": "ar405", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116vkxc/how_to_call_pyspark_on_dbt_run_and_create_a_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116vkxc/how_to_call_pyspark_on_dbt_run_and_create_a_table/", "subreddit_subscribers": 90318, "created_utc": 1676860786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Changes from PostgreSQL to Any Destination with Change Data Capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117est6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1676914063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cloudquery.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.cloudquery.io/blog/postgres-cdc-to-any-destination", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "117est6", "is_robot_indexable": true, "report_reasons": null, "author": "jekapats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117est6/stream_changes_from_postgresql_to_any_destination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.cloudquery.io/blog/postgres-cdc-to-any-destination", "subreddit_subscribers": 90318, "created_utc": 1676914063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone had success with ELT tools or technologies that allowed them to source CDC changes from multiple (see: HUNDREDS) of source SQL databases for streaming said changes to a lake/warehouse? It would need to be robust enough that all these connectors could be scripted, especially as more, newly created Tenants are created. Any help or suggestions are appreciated -- thanks!", "author_fullname": "t2_5h5g8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best ELT tool for sourcing from numerous multi-tenant SQL databases for CDC changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117e7nx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676913058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone had success with ELT tools or technologies that allowed them to source CDC changes from multiple (see: HUNDREDS) of source SQL databases for streaming said changes to a lake/warehouse? It would need to be robust enough that all these connectors could be scripted, especially as more, newly created Tenants are created. Any help or suggestions are appreciated -- thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117e7nx", "is_robot_indexable": true, "report_reasons": null, "author": "Zilean", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117e7nx/best_elt_tool_for_sourcing_from_numerous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117e7nx/best_elt_tool_for_sourcing_from_numerous/", "subreddit_subscribers": 90318, "created_utc": 1676913058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jh2dwngo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coinbase CEO tells his team: APIs not meetings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_117bpnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KrVPNY4yYpPhHgV7eoocFPDxlsBNpArRv-P83VDJvWM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676909552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thestatuscode.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thestatuscode.co/p/coinbase-ceo-tells-his-team-apis", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?auto=webp&amp;v=enabled&amp;s=b974687b5992bd4d8567e839be2a08dad45c6d27", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a299fd9b173a81a1c00bdb256660aff7e4b22725", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92892193b0bcafaebf3dcf4d16ecaee107d9cd99", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=994c596fd68b034b12f8a431f3fd223a5c85a822", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2000e27d0c48fa8d29753f0839469134e4451b61", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2f7419c21fd8ff66fdeb1848255d96bf86e1934", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca770fe8e599d1d3bccb2c33af32059317c13eb1", "width": 1080, "height": 720}], "variants": {}, "id": "wS6NnZbV8bwcv_3l3IB3QdEQZAAi8NkDDtk88IIgrEw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117bpnt", "is_robot_indexable": true, "report_reasons": null, "author": "foundersblock", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117bpnt/coinbase_ceo_tells_his_team_apis_not_meetings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thestatuscode.co/p/coinbase-ceo-tells-his-team-apis", "subreddit_subscribers": 90318, "created_utc": 1676909552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n\nI am new in the field and I am trying to design an architecture to improve our current system. At first I have two cases:\n\n \n\n1. Several ETL pipelines that will be triggered periodically at specified times.\n\n2. ETL pipelines that will be triggered manually by users with custom parameters.\n\n\n\nThe user-triggered pipelines would often be the same as the automatic ones. I wonder if Prefect could be used to queue up the same automatic and manual pipelines with different parameters and distribute this execution queue to different machines for processing. Is there a better tool to solve this problem? Below is a diagram of the problem.\n\n\n\nhttps://imgur.com/a/ru3Jl4X", "author_fullname": "t2_9ca451os", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone give me suggestions for modeling my system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ujw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676857694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new in the field and I am trying to design an architecture to improve our current system. At first I have two cases:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Several ETL pipelines that will be triggered periodically at specified times.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ETL pipelines that will be triggered manually by users with custom parameters.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The user-triggered pipelines would often be the same as the automatic ones. I wonder if Prefect could be used to queue up the same automatic and manual pipelines with different parameters and distribute this execution queue to different machines for processing. Is there a better tool to solve this problem? Below is a diagram of the problem.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/ru3Jl4X\"&gt;https://imgur.com/a/ru3Jl4X&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?auto=webp&amp;v=enabled&amp;s=945488c3866319a1db45f35a7dd7a0b75d58637e", "width": 2725, "height": 1187}, "resolutions": [{"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e819550c7aad4347e30a20ab362faf42fb9a127", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf0ade7cbd53490686b72b64275d59d0c4786beb", "width": 216, "height": 94}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b2fd01a454dc7573b32420d284ad3a6e7c80977", "width": 320, "height": 139}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c342f245a6a772fe6ad49ae0477bf87c4b0ca4", "width": 640, "height": 278}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74a22171349a0ac3bd0a3b40aaa5670958d12182", "width": 960, "height": 418}, {"url": "https://external-preview.redd.it/AdxMNeOC3p1LMf-suYv3_QnETH7zCjefye6XxWfV3fQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8da0c8e2c743cdc77a1b3223c0c5c8aad8dc271", "width": 1080, "height": 470}], "variants": {}, "id": "-DDE0WNCgkhPiOHSL0UQnIcnULKFa1wd7xsE3cytSXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116ujw1", "is_robot_indexable": true, "report_reasons": null, "author": "MiserableAstronaut77", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116ujw1/can_anyone_give_me_suggestions_for_modeling_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116ujw1/can_anyone_give_me_suggestions_for_modeling_my/", "subreddit_subscribers": 90318, "created_utc": 1676857694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nThe project I'm currently on is using Vertica and we have been asked to go over the vertica Certification course but don't have to actually get the certification.\n\nAs this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?", "author_fullname": "t2_digu9bnx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vertica Certification worth it ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11700w4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676875729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;The project I&amp;#39;m currently on is using Vertica and we have been asked to go over the vertica Certification course but don&amp;#39;t have to actually get the certification.&lt;/p&gt;\n\n&lt;p&gt;As this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11700w4", "is_robot_indexable": true, "report_reasons": null, "author": "Coffeee-Cat", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11700w4/vertica_certification_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11700w4/vertica_certification_worth_it/", "subreddit_subscribers": 90318, "created_utc": 1676875729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_27c3plee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog post: Metadata-driven pipelines in Azure Data Factory | Part 4 - Analytical Processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_117jyag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zpy9fyhlUkl11M_XfA6lhAEKh4-fOg0k0ZlLQvSCMzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676925802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanrg.blogspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?auto=webp&amp;v=enabled&amp;s=acf6b162314a17c0afbe67b2c22c8ab66e5c7332", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=976c7f9865030935e649d015169ac172cc6f5e6c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85dd501dc2bb75e6a72a1a225332106070cbcc91", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=212afa4d32af2f1746ea2623c764edf1f12a0a68", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57d89326fd6c1b4391a282e522ca8df8fc10add5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cc3e3e5d8e02be53ed74cabf9db3c4e2e051c9f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33a493bbc6aef040b48a9a0e128e6d65f2e0c3e0", "width": 1080, "height": 567}], "variants": {}, "id": "_z5Q-MwAZcOLYV1qeyPkktnXQXLmlsXbk1lpBU4Gubg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "117jyag", "is_robot_indexable": true, "report_reasons": null, "author": "RayisImayev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117jyag/blog_post_metadatadriven_pipelines_in_azure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "subreddit_subscribers": 90318, "created_utc": 1676925802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was trying to implement this process to create data warehouse. I could think of possible ways.\n\n1. Use AWS services. \n\nTo get data from apps i am using appflow to store data as S3. \nTo get database data, i am using DMS service to get data from databases and store it as S3 bucket with CDC. \n\nNow i want to transfer these source data to Redshift. I checked and most of the documentation mentioned using copy command. \n\nBut in this case, i have to create all tables again in redshift???? Is there any way to do this without creating tables again in redshift i.e, where redshift just takes all tables from s3 bucket. \n\n2. Use snowflake and stitch.\n\nAny help or recommendations in this regard ll be really helpful.\ud83d\ude4f", "author_fullname": "t2_8jfrjnc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline options from source to destination [data warehouse]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11738k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676888361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to implement this process to create data warehouse. I could think of possible ways.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use AWS services. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To get data from apps i am using appflow to store data as S3. \nTo get database data, i am using DMS service to get data from databases and store it as S3 bucket with CDC. &lt;/p&gt;\n\n&lt;p&gt;Now i want to transfer these source data to Redshift. I checked and most of the documentation mentioned using copy command. &lt;/p&gt;\n\n&lt;p&gt;But in this case, i have to create all tables again in redshift???? Is there any way to do this without creating tables again in redshift i.e, where redshift just takes all tables from s3 bucket. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use snowflake and stitch.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help or recommendations in this regard ll be really helpful.\ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11738k2", "is_robot_indexable": true, "report_reasons": null, "author": "lost_soul1995", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11738k2/data_pipeline_options_from_source_to_destination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11738k2/data_pipeline_options_from_source_to_destination/", "subreddit_subscribers": 90318, "created_utc": 1676888361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing a test for a job application where one of the tasks is to do the following:\n\nFor each Id number in df['AccountId'] check if it exists in that same column as a preffix to another value.\n\nEg.\n\ndf['AccountId'] is 1.1 for row X  and df['AccountId'] is 1.1.1 for row Y\n\nThat will determine wether column df['Type'] will be filled with value 'X', 'Y' or 'Z'.\n\nSo if there's no entry in df['AccountId'] where row X is a preffix to it, then row X will have 'Z' as value for df['Type']. But if there is, then row X will have 'Y' as value for df['Type'].\n\nAny idea what's the optimal way to do this?\n\nI've tried many approaches but I keep running on all sorts of errors like 'The truth value of a Series is ambiguous'.\n\nSorry if this is not the place to post this and thanks for any help!", "author_fullname": "t2_32b255ac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help checking for a certain condition for each value in a column in a Pandas dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_117iu5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676923155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing a test for a job application where one of the tasks is to do the following:&lt;/p&gt;\n\n&lt;p&gt;For each Id number in df[&amp;#39;AccountId&amp;#39;] check if it exists in that same column as a preffix to another value.&lt;/p&gt;\n\n&lt;p&gt;Eg.&lt;/p&gt;\n\n&lt;p&gt;df[&amp;#39;AccountId&amp;#39;] is 1.1 for row X  and df[&amp;#39;AccountId&amp;#39;] is 1.1.1 for row Y&lt;/p&gt;\n\n&lt;p&gt;That will determine wether column df[&amp;#39;Type&amp;#39;] will be filled with value &amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39; or &amp;#39;Z&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;So if there&amp;#39;s no entry in df[&amp;#39;AccountId&amp;#39;] where row X is a preffix to it, then row X will have &amp;#39;Z&amp;#39; as value for df[&amp;#39;Type&amp;#39;]. But if there is, then row X will have &amp;#39;Y&amp;#39; as value for df[&amp;#39;Type&amp;#39;].&lt;/p&gt;\n\n&lt;p&gt;Any idea what&amp;#39;s the optimal way to do this?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried many approaches but I keep running on all sorts of errors like &amp;#39;The truth value of a Series is ambiguous&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is not the place to post this and thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117iu5h", "is_robot_indexable": true, "report_reasons": null, "author": "torvi97", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117iu5h/need_some_help_checking_for_a_certain_condition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117iu5h/need_some_help_checking_for_a_certain_condition/", "subreddit_subscribers": 90318, "created_utc": 1676923155.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}