{"kind": "Listing", "data": {"after": "t3_116fg1v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bv1iezjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Your fellow film archivist here to show off how I clean, scan, and digitally restore (some) of my 35mm slides that come through the door! I hit 45,000 photos recently and have no plans to stop! Take a look! (Portrait orientation, terribly sorry) (All captioned, DEAF FRIENDLY).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1162tgv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 1185, "total_awards_received": 3, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/8tr6bh1023ja1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/8tr6bh1023ja1/DASH_96.mp4", "dash_url": "https://v.redd.it/8tr6bh1023ja1/DASHPlaylist.mpd?a=1679456495%2CZTAwN2NhZjlhNjU5N2UwNWU1ODUxOTM3OGVlYTA5ODBmODI5ODgzODkyZjQ0MGQ4ZjMyZDAwNjY5OWI2M2E3MQ%3D%3D&amp;v=1&amp;f=sd", "duration": 162, "hls_url": "https://v.redd.it/8tr6bh1023ja1/HLSPlaylist.m3u8?a=1679456495%2CMjYzNzliYWYyMTdjODZmMDcwNDkzNTdkZGJlYjY5ZDE4ZDA2MjNlMDA3ZDQ0NTZiZjI1NjBiZDFmZDZiZTkyMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1185, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/o9X37fwEU-SJJ0ykF9E8qX9wYz9pP5xyCNOpxreLAKw.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {"gid_3": 1}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676785486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/8tr6bh1023ja1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8e603841d385c6ac91ee31da2e88b4b2b643099c", "width": 1080, "height": 1920}, "resolutions": [{"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6d6458bafa86934f36bc3f2601833949c7226d56", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c6e4a2e87bacc6abec1fc52e006e084edfe8d5e6", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=03ebf17a5b1b882bb1253a8ba693052f3fb665fd", "width": 320, "height": 568}, {"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5295d615d41a8f6eaf98897768c76afe6b25210b", "width": 640, "height": 1137}, {"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=54b359b0cbcbdeb7d7ab7768083779bbd256f21b", "width": 960, "height": 1706}, {"url": "https://external-preview.redd.it/0gNsRXM0kAeN7XrKTiMvI8e_HPBnvcX8Dat31vRkH_k.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=423ddb09ea3689ad109d06a64bc904efa8810f0c", "width": 1080, "height": 1920}], "variants": {}, "id": "OGii38pEveTn5AeYWT_4xQluw1a-qH5vIxlyE5_-4SI"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": 31, "coin_price": 1800, "id": "gid_3", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/platinum_512.png", "days_of_premium": 31, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/platinum_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 700 Reddit Coins and a month of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Platinum", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/platinum_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/platinum_512.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 125, "id": "award_611ff347-196b-4a14-ad4b-0076f2d8f9d2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Faith_in_Humanity_Restored_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Faith_in_Humanity_Restored_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Faith_in_Humanity_Restored_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Faith_in_Humanity_Restored_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Faith_in_Humanity_Restored_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Faith_in_Humanity_Restored_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "This goes a long way to restore my faith in the people of Earth", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Faith In Humanity Restored", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/jlwc6uspakd61_FaithInHumanityRestored.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=08c73f1595b2f969da73ad8bf9fef83e9e3f79db", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/jlwc6uspakd61_FaithInHumanityRestored.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=58da3d6a8c678afc324db8ada6103e2e8b953bdc", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/jlwc6uspakd61_FaithInHumanityRestored.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=49a6559ef36478537654d37e2c929a6b9d512d01", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/jlwc6uspakd61_FaithInHumanityRestored.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=dcb92afecb2d9b8c637f1b38344bec6a91873838", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/jlwc6uspakd61_FaithInHumanityRestored.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=23d55b0d7a762eef629b7ab829fb3250d0dc4746", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/jlwc6uspakd61_FaithInHumanityRestored.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The 35mm Slide Knight - 6TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1162tgv", "is_robot_indexable": true, "report_reasons": null, "author": "SalmonSnail", "discussion_type": null, "num_comments": 105, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1162tgv/your_fellow_film_archivist_here_to_show_off_how_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/8tr6bh1023ja1", "subreddit_subscribers": 670502, "created_utc": 1676785486.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/8tr6bh1023ja1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/8tr6bh1023ja1/DASH_96.mp4", "dash_url": "https://v.redd.it/8tr6bh1023ja1/DASHPlaylist.mpd?a=1679456495%2CZTAwN2NhZjlhNjU5N2UwNWU1ODUxOTM3OGVlYTA5ODBmODI5ODgzODkyZjQ0MGQ4ZjMyZDAwNjY5OWI2M2E3MQ%3D%3D&amp;v=1&amp;f=sd", "duration": 162, "hls_url": "https://v.redd.it/8tr6bh1023ja1/HLSPlaylist.m3u8?a=1679456495%2CMjYzNzliYWYyMTdjODZmMDcwNDkzNTdkZGJlYjY5ZDE4ZDA2MjNlMDA3ZDQ0NTZiZjI1NjBiZDFmZDZiZTkyMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For some reason, Apple only numbers them IMG_0000 to IMG_9999 and avoids conflicts by sorting them into folders by month. This is so stupid I can't even fathom. What if you want to reorganize them by category? I was moving my photos to a hard drive and overwrote quite a few, I have no idea which ones they were. Literally a time stamp in the name would fix this. Why would they not do that?\n\nAnyway, I guess the solution when backing up iPhone photos is to preserve the date folders.\n\nEdit: After thinking about this in terms of interoperability, maybe it doesn't matter. If you have multiple cameras or phones, even if they were all using MMDDYYYY along with a rotating IMG nomenclature, you could still end up with duplicates. I was thinking about this in terms of my specific use case, but in a production environment, you'd have multiple identical cameras and you'd have to rename them anyway.", "author_fullname": "t2_8nbrguzr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA/rant about iPhone photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1161zab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676828142.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676782542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some reason, Apple only numbers them IMG_0000 to IMG_9999 and avoids conflicts by sorting them into folders by month. This is so stupid I can&amp;#39;t even fathom. What if you want to reorganize them by category? I was moving my photos to a hard drive and overwrote quite a few, I have no idea which ones they were. Literally a time stamp in the name would fix this. Why would they not do that?&lt;/p&gt;\n\n&lt;p&gt;Anyway, I guess the solution when backing up iPhone photos is to preserve the date folders.&lt;/p&gt;\n\n&lt;p&gt;Edit: After thinking about this in terms of interoperability, maybe it doesn&amp;#39;t matter. If you have multiple cameras or phones, even if they were all using MMDDYYYY along with a rotating IMG nomenclature, you could still end up with duplicates. I was thinking about this in terms of my specific use case, but in a production environment, you&amp;#39;d have multiple identical cameras and you&amp;#39;d have to rename them anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1161zab", "is_robot_indexable": true, "report_reasons": null, "author": "parastro", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1161zab/psarant_about_iphone_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1161zab/psarant_about_iphone_photos/", "subreddit_subscribers": 670502, "created_utc": 1676782542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just posted a video of some of my work, but I wanted to reach out and ask... where all the motion picture and vhs digitizing homies at?! I do amateur film photography still from estate sales along with vintage photo prints.\n\nI also have a hoard of \\~4,000 FEET of 8mm amateur home movies from the 50's i'm looking to make a deal with someone to scan, but I can't seem to find anyone!! :(\n\nI feel like we could exchange a ton of knowledge and have a laugh or two! \n\nlol this is so embarrassing. Is there anyone here who digitizes this media and wants to shoot the shit? I'd looove to hear all about your setup! \n\nMuch love,\n\n'snail", "author_fullname": "t2_bv1iezjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Embarrassing) - I'm looking for hoarders who digitize and archive VHS or 8mm/S8mm/16mm/35mm movie film, amateur or professionally produced. Not to make this a personal classifieds ad, but can we be datahoarding friends!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11630ga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676786198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just posted a video of some of my work, but I wanted to reach out and ask... where all the motion picture and vhs digitizing homies at?! I do amateur film photography still from estate sales along with vintage photo prints.&lt;/p&gt;\n\n&lt;p&gt;I also have a hoard of ~4,000 FEET of 8mm amateur home movies from the 50&amp;#39;s i&amp;#39;m looking to make a deal with someone to scan, but I can&amp;#39;t seem to find anyone!! :(&lt;/p&gt;\n\n&lt;p&gt;I feel like we could exchange a ton of knowledge and have a laugh or two! &lt;/p&gt;\n\n&lt;p&gt;lol this is so embarrassing. Is there anyone here who digitizes this media and wants to shoot the shit? I&amp;#39;d looove to hear all about your setup! &lt;/p&gt;\n\n&lt;p&gt;Much love,&lt;/p&gt;\n\n&lt;p&gt;&amp;#39;snail&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The 35mm Slide Knight - 6TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11630ga", "is_robot_indexable": true, "report_reasons": null, "author": "SalmonSnail", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/11630ga/embarrassing_im_looking_for_hoarders_who_digitize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11630ga/embarrassing_im_looking_for_hoarders_who_digitize/", "subreddit_subscribers": 670502, "created_utc": 1676786198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Had a bit of a rude awakening recently when I found that one of my youtube archives (which includes many videos no longer on the creator's channel), was not actually searchable, not with the search function, or by clicking on any of the search term tags for the page.\n\nI emailed them and they told me \"Social Media content does not generate metadata and are not searchable\" and that for some reason, youtube videos fall under social media.\n\nThis absolutely baffles me, there are tons of channels out there that get unfairly nuked out of existence, or lost videos because of DMCAs etc, and it would seem the 'only' reason any of them are findable on archive.org is because they're not correctly catagorised, a complete fluke. (Basically, they start in Community Video and are searchable, but if an admin changes that, you might just be out of luck)\n\nI myself in the past have tried to search for lost videos on archive.org before, and now I don't know if they simply weren't uploaded there by anyone, or if they lacked the metadata to search because one of the admins deemed it so.\n\nJust figured I'd share this information for anyone who wasn't aware because this seems like a really stupid decision, you can't even search within the catagories! If you go to the Mirror Tube catagory for instance, you can scroll through them (all 100 thousand plus...) but you can't search. What on earth are they doing?", "author_fullname": "t2_hs20g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive.org : Youtube videos are not ment to be searchable, apparently.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11697u7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676809939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a bit of a rude awakening recently when I found that one of my youtube archives (which includes many videos no longer on the creator&amp;#39;s channel), was not actually searchable, not with the search function, or by clicking on any of the search term tags for the page.&lt;/p&gt;\n\n&lt;p&gt;I emailed them and they told me &amp;quot;Social Media content does not generate metadata and are not searchable&amp;quot; and that for some reason, youtube videos fall under social media.&lt;/p&gt;\n\n&lt;p&gt;This absolutely baffles me, there are tons of channels out there that get unfairly nuked out of existence, or lost videos because of DMCAs etc, and it would seem the &amp;#39;only&amp;#39; reason any of them are findable on archive.org is because they&amp;#39;re not correctly catagorised, a complete fluke. (Basically, they start in Community Video and are searchable, but if an admin changes that, you might just be out of luck)&lt;/p&gt;\n\n&lt;p&gt;I myself in the past have tried to search for lost videos on archive.org before, and now I don&amp;#39;t know if they simply weren&amp;#39;t uploaded there by anyone, or if they lacked the metadata to search because one of the admins deemed it so.&lt;/p&gt;\n\n&lt;p&gt;Just figured I&amp;#39;d share this information for anyone who wasn&amp;#39;t aware because this seems like a really stupid decision, you can&amp;#39;t even search within the catagories! If you go to the Mirror Tube catagory for instance, you can scroll through them (all 100 thousand plus...) but you can&amp;#39;t search. What on earth are they doing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11697u7", "is_robot_indexable": true, "report_reasons": null, "author": "Xyrexus", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11697u7/archiveorg_youtube_videos_are_not_ment_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11697u7/archiveorg_youtube_videos_are_not_ment_to_be/", "subreddit_subscribers": 670502, "created_utc": 1676809939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Censoring Roald Dahl? I\u2019ll be keeping my original copies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_116fqiy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_6ilbp", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/orBUY8t8L8JIrDRbbSv9MWBS1MjjjIRnN7X1SirTEJ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "books", "selftext": "", "author_fullname": "t2_915gm5zi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Censoring Roald Dahl? I\u2019ll be keeping my original copies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/books", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1168fhx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6974, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 6974, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/orBUY8t8L8JIrDRbbSv9MWBS1MjjjIRnN7X1SirTEJ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1676806928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thetimes.co.uk", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?auto=webp&amp;v=enabled&amp;s=a653df65434c355a48265268b6cebe4178dd4568", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1fdce132b567f08beb26ee04588d0ec18b85d24", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cb34a70991697933639ed2ed85e0ce54311949b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91351761f8b259ad24015d4e6dd982380d334e8c", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e09c7ba5808f051fce6e5fc4a521997b206ac489", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acebb9fe8441300bc90cf693961cbc132a650f8", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ecdfc5dd89441de7a206873513d875bf2488ca4", "width": 1080, "height": 606}], "variants": {}, "id": "c4G_Az5480aPDW7fE7v_Z3a4cpI7PbFC9Uwrmx_iqLA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh4i", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1168fhx", "is_robot_indexable": true, "report_reasons": null, "author": "Elliottafc1", "discussion_type": null, "num_comments": 1231, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/books/comments/1168fhx/censoring_roald_dahl_ill_be_keeping_my_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "subreddit_subscribers": 22234294, "created_utc": 1676806928.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1676822138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thetimes.co.uk", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?auto=webp&amp;v=enabled&amp;s=a653df65434c355a48265268b6cebe4178dd4568", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1fdce132b567f08beb26ee04588d0ec18b85d24", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cb34a70991697933639ed2ed85e0ce54311949b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91351761f8b259ad24015d4e6dd982380d334e8c", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e09c7ba5808f051fce6e5fc4a521997b206ac489", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acebb9fe8441300bc90cf693961cbc132a650f8", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ecdfc5dd89441de7a206873513d875bf2488ca4", "width": 1080, "height": 606}], "variants": {}, "id": "c4G_Az5480aPDW7fE7v_Z3a4cpI7PbFC9Uwrmx_iqLA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116fqiy", "is_robot_indexable": true, "report_reasons": null, "author": "zuperfly", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1168fhx", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116fqiy/censoring_roald_dahl_ill_be_keeping_my_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "subreddit_subscribers": 670502, "created_utc": 1676822138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[A reminder of the data quality scale](https://xkcd.com/2739/).", "author_fullname": "t2_8zyu4htp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA (courtesy of xkcg): Data Quality Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1162g06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676784119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://xkcd.com/2739/\"&gt;A reminder of the data quality scale&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?auto=webp&amp;v=enabled&amp;s=c236379352705b361b2c40f172ba4c59555f79da", "width": 1343, "height": 421}, "resolutions": [{"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01801815d81b9ebee9cd1915fb344dcf7e72d4c8", "width": 108, "height": 33}, {"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0b6e5d0140627f342bef19ea868bea3ec0cf34b", "width": 216, "height": 67}, {"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59d4e91b9f894da1ac91fe1072e1842e8e917ac5", "width": 320, "height": 100}, {"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7f30b4274028be3266e7547d7acd6d761e09ddb", "width": 640, "height": 200}, {"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=316df6787856723abbfdccedc982e156c19ad850", "width": 960, "height": 300}, {"url": "https://external-preview.redd.it/Z5BMx_PziZ7uFi6LiuAji8ULAlYoA8O_19QPjcKM2Aw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fb28871b1e90e63f2594fc66a3421402c804243", "width": 1080, "height": 338}], "variants": {}, "id": "K9hyWLcubylMi04TFXIB9KVFIcRhWGiecR7d_x86eco"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "70TB usable, 48TB backup, 70TB cloud backup", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1162g06", "is_robot_indexable": true, "report_reasons": null, "author": "ComputingElephant", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1162g06/psa_courtesy_of_xkcg_data_quality_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1162g06/psa_courtesy_of_xkcg_data_quality_scale/", "subreddit_subscribers": 670502, "created_utc": 1676784119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to move a heap of data from my phone to my backup SSD. Nothing is corrupted or damaged on my phone storage.\n\nWhen I transfer data to my SSD, some of it shows up corrupted and I can't work out why. I've used different cables, and connected the phone to a laptop and then SSD, as well as directly to the SSD.\n\nIt looks like the corruption of data affects video files only - not all of them, but some of them. Maybe 1 in 20. There's 190GB of files, a mix of images and videos, and it's a bit of a mess.\n\nI have a Sandisk 1TB Gen1 SSD that I'm moving data to from my Galaxy S23 1TB phone via USB-C to USB-C.\n\nAnyone know why this could be happening?", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been battling data corruption issues all day.. narrowed it down to video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116k8br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676831576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to move a heap of data from my phone to my backup SSD. Nothing is corrupted or damaged on my phone storage.&lt;/p&gt;\n\n&lt;p&gt;When I transfer data to my SSD, some of it shows up corrupted and I can&amp;#39;t work out why. I&amp;#39;ve used different cables, and connected the phone to a laptop and then SSD, as well as directly to the SSD.&lt;/p&gt;\n\n&lt;p&gt;It looks like the corruption of data affects video files only - not all of them, but some of them. Maybe 1 in 20. There&amp;#39;s 190GB of files, a mix of images and videos, and it&amp;#39;s a bit of a mess.&lt;/p&gt;\n\n&lt;p&gt;I have a Sandisk 1TB Gen1 SSD that I&amp;#39;m moving data to from my Galaxy S23 1TB phone via USB-C to USB-C.&lt;/p&gt;\n\n&lt;p&gt;Anyone know why this could be happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116k8br", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116k8br/been_battling_data_corruption_issues_all_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116k8br/been_battling_data_corruption_issues_all_day/", "subreddit_subscribers": 670502, "created_utc": 1676831576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nSo I\u2019ve got a bunch of really old SIM cards from my childhood (nano, micro, and standard), and I\u2019m pretty sure I may have sms messages saved on them. What\u2019s the easiest way of viewing their contents and extracting them?\n\nThanks!", "author_fullname": "t2_u1k87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to extract data from old Sim Cards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116q78n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;So I\u2019ve got a bunch of really old SIM cards from my childhood (nano, micro, and standard), and I\u2019m pretty sure I may have sms messages saved on them. What\u2019s the easiest way of viewing their contents and extracting them?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116q78n", "is_robot_indexable": true, "report_reasons": null, "author": "MurmurOfTheCine", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116q78n/how_to_extract_data_from_old_sim_cards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116q78n/how_to_extract_data_from_old_sim_cards/", "subreddit_subscribers": 670502, "created_utc": 1676846321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Do they have a list of files that are lost for you to know which to restore from your backup?", "author_fullname": "t2_woqd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When RAID fails, do you know what files are lost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1162lic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676784684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do they have a list of files that are lost for you to know which to restore from your backup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1162lic", "is_robot_indexable": true, "report_reasons": null, "author": "slaiyfer", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1162lic/when_raid_fails_do_you_know_what_files_are_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1162lic/when_raid_fails_do_you_know_what_files_are_lost/", "subreddit_subscribers": 670502, "created_utc": 1676784684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a way to checksum files from a cold storage HDD to my current NAS, as the source.\n\nI have an external HDD I keep offsite that I update about once every 6 months. I have a NAS running Unraid that I use as my main storage.\n\nI would like a way to compare checksums of the files, to see if any corruption occured on the cold storage drive while it was unpowered. I only copy over new files, usually with FreeFileSync, so I could have corruption / bitrot on the older files and not know.\n\nDealing with about 5TB of data\n\nIs Teracopy the best way to do this? I have read horror stories of corruption caused by teracopy.\n\nMain OS is Windows.\n\n&amp;#x200B;\n\nThanks everyone!", "author_fullname": "t2_becvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum for cold storage backup, compare to source before backing up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116q5qi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a way to checksum files from a cold storage HDD to my current NAS, as the source.&lt;/p&gt;\n\n&lt;p&gt;I have an external HDD I keep offsite that I update about once every 6 months. I have a NAS running Unraid that I use as my main storage.&lt;/p&gt;\n\n&lt;p&gt;I would like a way to compare checksums of the files, to see if any corruption occured on the cold storage drive while it was unpowered. I only copy over new files, usually with FreeFileSync, so I could have corruption / bitrot on the older files and not know.&lt;/p&gt;\n\n&lt;p&gt;Dealing with about 5TB of data&lt;/p&gt;\n\n&lt;p&gt;Is Teracopy the best way to do this? I have read horror stories of corruption caused by teracopy.&lt;/p&gt;\n\n&lt;p&gt;Main OS is Windows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116q5qi", "is_robot_indexable": true, "report_reasons": null, "author": "technica-", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116q5qi/checksum_for_cold_storage_backup_compare_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116q5qi/checksum_for_cold_storage_backup_compare_to/", "subreddit_subscribers": 670502, "created_utc": 1676846215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tonight I learned about the 604 3.3v rails that need to be taped over. Figured that out. I have 6 new to me drives. 4x604 and 4x6L4.  All four of the 604s work fine. The two 6L4s do not.  Anyone know what I might be missing? The 6l4's spin up but when testing in windows they suffer a I/O error when trying to initialize.  I'm going to be putting all of these in an unraid setup with a backplane when it arrives but that might be a few steps from now.  Is there some \"gotcha\" about these drives I don't know about or are they just bad drives?", "author_fullname": "t2_7olt1k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the difference between the WD WUH721414ALE6L4 and the 604?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ud2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676857132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tonight I learned about the 604 3.3v rails that need to be taped over. Figured that out. I have 6 new to me drives. 4x604 and 4x6L4.  All four of the 604s work fine. The two 6L4s do not.  Anyone know what I might be missing? The 6l4&amp;#39;s spin up but when testing in windows they suffer a I/O error when trying to initialize.  I&amp;#39;m going to be putting all of these in an unraid setup with a backplane when it arrives but that might be a few steps from now.  Is there some &amp;quot;gotcha&amp;quot; about these drives I don&amp;#39;t know about or are they just bad drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116ud2r", "is_robot_indexable": true, "report_reasons": null, "author": "ElBigBad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116ud2r/whats_the_difference_between_the_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116ud2r/whats_the_difference_between_the_wd/", "subreddit_subscribers": 670502, "created_utc": 1676857132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using TeraCopy to manually sync folders between my Windows laptop and a remote Synology NAS.  Folders are created and populated with files on my laptop just once for each folder, with no further modifications, so syncing manually has thus been pretty simple so far.  The relevant NAS directory is mapped as a network drive, and I must connect via VPN in order to access it remotely.\n\nTo verify the copy operations I always use TeraCopy's \"Verify\" option, which is great, but very slow when I'm connected to the NAS remotely, I believe because it's actually resending all the data as part of the verification process ([https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p](https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p)).\n\nAnyone know of a simple, ideally automated way of doing file verification faster (consuming less throughput) than TeraCopy?\n\nIn the near future I'm thinking of setting up Free File Sync to handle the syncing, but I still need to research and test it out more, and I'm not sure how it handles verification.", "author_fullname": "t2_9i6f7eeq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File verification optimized for remote network file copy (Windows)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116nrde", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676840273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using TeraCopy to manually sync folders between my Windows laptop and a remote Synology NAS.  Folders are created and populated with files on my laptop just once for each folder, with no further modifications, so syncing manually has thus been pretty simple so far.  The relevant NAS directory is mapped as a network drive, and I must connect via VPN in order to access it remotely.&lt;/p&gt;\n\n&lt;p&gt;To verify the copy operations I always use TeraCopy&amp;#39;s &amp;quot;Verify&amp;quot; option, which is great, but very slow when I&amp;#39;m connected to the NAS remotely, I believe because it&amp;#39;s actually resending all the data as part of the verification process (&lt;a href=\"https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p\"&gt;https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;Anyone know of a simple, ideally automated way of doing file verification faster (consuming less throughput) than TeraCopy?&lt;/p&gt;\n\n&lt;p&gt;In the near future I&amp;#39;m thinking of setting up Free File Sync to handle the syncing, but I still need to research and test it out more, and I&amp;#39;m not sure how it handles verification.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?auto=webp&amp;v=enabled&amp;s=5bf61032a6a11077440cf0421aceee21a943a62d", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbd71549aa2d678c4628f02ae8cd5b7ddaf23d73", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=451335daa7d9e8aa739cd3a851acc983948bf130", "width": 216, "height": 216}], "variants": {}, "id": "PJ2AnV34BC-Bc5jLZ5IhQtzJPf-P0PW2LhpwYCeTNyE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116nrde", "is_robot_indexable": true, "report_reasons": null, "author": "VladsBestFriend", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116nrde/file_verification_optimized_for_remote_network/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116nrde/file_verification_optimized_for_remote_network/", "subreddit_subscribers": 670502, "created_utc": 1676840273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't know if this is the right sub to ask this but I have a Pixel 1st gen that still has the ability to upload unlimited original quality picture through google photos. \n\nThe problem is that I have the 32GB version of the phone and only has 18GB of free space. So that means I have to upload 1.5TB of pictures and videos in ~18GB intervals. I would like to know if there's a solution to split a 1.5TB folder into multiple folders of approximately 18GB. That way I could just use syncthings or external drive and just transfer each ~18GB folder everytime the last one uploads. It doesn't have to be exact, it just has to be no more than 18GB. \n     \nI tried googling this but the solution they give is to zip and split the big folder into multiple zipped pieces. Which doesn't really help me since I need to transfer the original jpg or mp4 to the phone in order to upload. They won't upload if they are zipped.\n   \nI also tried chatgpt but it is giving me command/script that has invalid syntax/parameter. \n    \n`xcopy /s &lt;source folder&gt; &lt;destination folder&gt;\\&lt;prefix&gt; /f /max &lt;maximum file size&gt;`\n\n`xcopy /s my_folder_ subdirectories\\my_folder_ /f /max 18000000000`\n\nI told it that /max is an invalid parameter and it sent me to powershell to do this\n\n`Get-ChildItem -Path \"&lt;source folder&gt;\" -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object (robocopy $_ \"&lt;destination folder&gt;\\$($_.Substring($_. LastIndexOf(\"\\\")+1))\" /mov /sizemax: &lt;maximum file size&gt;}`\n       \n`Get-ChildItem -Path \"C:\\Users\\me\\Desktop\\my_folder\" -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object {robocopy $_ \"C:\\Users\\me\\Desktop\\subdirectories\\$($_.Substring($_.LastIndexOf(\"\\\")+1))\" /mov /sizemax:18000000000}`\n\nI told it once again sizemax is not a valid parameter and it just errored out.\n      \nTo test both of these scripts/commands I just created a folder called \"my folder\" and another one called \"subdirectories\" under desktop. Also used fsutils to create different sized dummy files inside \"my folder\". I didn't run these scripts with my actual files.", "author_fullname": "t2_vcp81gxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to split a large folder into multiple smaller folders of no more than a certain size without zipping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116kxal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676833470.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676833260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if this is the right sub to ask this but I have a Pixel 1st gen that still has the ability to upload unlimited original quality picture through google photos. &lt;/p&gt;\n\n&lt;p&gt;The problem is that I have the 32GB version of the phone and only has 18GB of free space. So that means I have to upload 1.5TB of pictures and videos in ~18GB intervals. I would like to know if there&amp;#39;s a solution to split a 1.5TB folder into multiple folders of approximately 18GB. That way I could just use syncthings or external drive and just transfer each ~18GB folder everytime the last one uploads. It doesn&amp;#39;t have to be exact, it just has to be no more than 18GB. &lt;/p&gt;\n\n&lt;p&gt;I tried googling this but the solution they give is to zip and split the big folder into multiple zipped pieces. Which doesn&amp;#39;t really help me since I need to transfer the original jpg or mp4 to the phone in order to upload. They won&amp;#39;t upload if they are zipped.&lt;/p&gt;\n\n&lt;p&gt;I also tried chatgpt but it is giving me command/script that has invalid syntax/parameter. &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;xcopy /s &amp;lt;source folder&amp;gt; &amp;lt;destination folder&amp;gt;\\&amp;lt;prefix&amp;gt; /f /max &amp;lt;maximum file size&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;xcopy /s my_folder_ subdirectories\\my_folder_ /f /max 18000000000&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I told it that /max is an invalid parameter and it sent me to powershell to do this&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Get-ChildItem -Path &amp;quot;&amp;lt;source folder&amp;gt;&amp;quot; -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object (robocopy $_ &amp;quot;&amp;lt;destination folder&amp;gt;\\$($_.Substring($_. LastIndexOf(&amp;quot;\\&amp;quot;)+1))&amp;quot; /mov /sizemax: &amp;lt;maximum file size&amp;gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Get-ChildItem -Path &amp;quot;C:\\Users\\me\\Desktop\\my_folder&amp;quot; -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object {robocopy $_ &amp;quot;C:\\Users\\me\\Desktop\\subdirectories\\$($_.Substring($_.LastIndexOf(&amp;quot;\\&amp;quot;)+1))&amp;quot; /mov /sizemax:18000000000}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I told it once again sizemax is not a valid parameter and it just errored out.&lt;/p&gt;\n\n&lt;p&gt;To test both of these scripts/commands I just created a folder called &amp;quot;my folder&amp;quot; and another one called &amp;quot;subdirectories&amp;quot; under desktop. Also used fsutils to create different sized dummy files inside &amp;quot;my folder&amp;quot;. I didn&amp;#39;t run these scripts with my actual files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116kxal", "is_robot_indexable": true, "report_reasons": null, "author": "arcohex", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116kxal/is_there_a_way_to_split_a_large_folder_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116kxal/is_there_a_way_to_split_a_large_folder_into/", "subreddit_subscribers": 670502, "created_utc": 1676833260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI am looking for some cloud storage to store my family photos. I use Google Drive but I have ONE problem with it - you cannot rotate the images. Do you know any cloud storage good for keeping photos and basic editing? I know that dropbox can do this editing but it's quite expensive. Also, Amazon Photos can rotate photos but It will save the photo as a new one so you now have 2 of the same images.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_1lyo8mc6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Cloud storage for photos and videos with basic editing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116k2bs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676831181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am looking for some cloud storage to store my family photos. I use Google Drive but I have ONE problem with it - you cannot rotate the images. Do you know any cloud storage good for keeping photos and basic editing? I know that dropbox can do this editing but it&amp;#39;s quite expensive. Also, Amazon Photos can rotate photos but It will save the photo as a new one so you now have 2 of the same images.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116k2bs", "is_robot_indexable": true, "report_reasons": null, "author": "Xperr1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116k2bs/looking_for_cloud_storage_for_photos_and_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116k2bs/looking_for_cloud_storage_for_photos_and_videos/", "subreddit_subscribers": 670502, "created_utc": 1676831181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI recently decided space was getting a little too cosy on my NAS (under 1.5 TB free!) and so purchased another hard drive to upgrade.\n\nBefore the upgrade, I had 2x Seagate Exos 16 TB running in RAID1 through BTRFS (so 16 TB of usable space).\n\nTo upgrade, I simply bought another Seagate Exos 16 TB. My understanding was thanks to the magic of BTRFS, adding a single drive to a RAID1 configuration was fine, since there wasn't a need for any single drive in the array to hold over half its total capacity, so it would 'just work'.\n\nWhat actually happened was that I only got 4 TB of additional usable space after installing and rebalancing the drives, whereas I was expecting 8 TB.\n\nCan anyone explain to me why this has happened please? Thanks! :)", "author_fullname": "t2_3b1qo66c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expanding my NAS' capacity: why did I not get as much extra usable space as I expected?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116di0n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676819068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently decided space was getting a little too cosy on my NAS (under 1.5 TB free!) and so purchased another hard drive to upgrade.&lt;/p&gt;\n\n&lt;p&gt;Before the upgrade, I had 2x Seagate Exos 16 TB running in RAID1 through BTRFS (so 16 TB of usable space).&lt;/p&gt;\n\n&lt;p&gt;To upgrade, I simply bought another Seagate Exos 16 TB. My understanding was thanks to the magic of BTRFS, adding a single drive to a RAID1 configuration was fine, since there wasn&amp;#39;t a need for any single drive in the array to hold over half its total capacity, so it would &amp;#39;just work&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;What actually happened was that I only got 4 TB of additional usable space after installing and rebalancing the drives, whereas I was expecting 8 TB.&lt;/p&gt;\n\n&lt;p&gt;Can anyone explain to me why this has happened please? Thanks! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "24TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116di0n", "is_robot_indexable": true, "report_reasons": null, "author": "justapotplant", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/116di0n/expanding_my_nas_capacity_why_did_i_not_get_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116di0n/expanding_my_nas_capacity_why_did_i_not_get_as/", "subreddit_subscribers": 670502, "created_utc": 1676819068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all I have around 6x 20 TB WD external HDDs. Used for torrenting and Plex server. Just wondering if there\u2019s anything I can do software or practices to prolong the life of the drives. I\u2019m on macOS by the way.", "author_fullname": "t2_vs43uoy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tips on keeping external HDDs healthy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_116vxkk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676861873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I have around 6x 20 TB WD external HDDs. Used for torrenting and Plex server. Just wondering if there\u2019s anything I can do software or practices to prolong the life of the drives. I\u2019m on macOS by the way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116vxkk", "is_robot_indexable": true, "report_reasons": null, "author": "CosmicCobbler", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116vxkk/any_tips_on_keeping_external_hdds_healthy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116vxkk/any_tips_on_keeping_external_hdds_healthy/", "subreddit_subscribers": 670502, "created_utc": 1676861873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help understanding the current hard drive market available to be purchased now.\n\nStandard Disclaimer 37M Autistic, lives with parents, fully employed, not earn a lot of money, parents loaded.\n\nAny money I did have, went to buying hard drives, last count was 23 Wired drives ranging from 1TB - 18TB, 2 of them are NAS Synology units (8 and 16TB) and a WD 16TB NAS Home Duo.\n\nI have not calculated for a long time, but the total size is somewhere under 300TB but definitely above 150TB. \n\nParents are finally snapping and want a better more energy efficient solution, with a crap ton of space, so they have said they will buy me something, for our new house, but I have no idea what it is going to look like, I have no input and I am not a noob, but unless I am wrong, the highest capacity drives that can be purchased are 22TB. \n\nOf course, I won't be transferring everything from those 23 drives that will go onto this new unit, whatever it is, I will need to be very cautious going forward of what stays on it, and what gets archived, just so I can make the space last as long as possible without me forking out the money, for more space in the future. \n\nHoping 22TB will eventually get cheaper than they are now, but don't know how long that will take.\n\nAny help would be appreciated", "author_fullname": "t2_c7lcxyz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help to speculate exactly what I am getting.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116sygz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676853279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help understanding the current hard drive market available to be purchased now.&lt;/p&gt;\n\n&lt;p&gt;Standard Disclaimer 37M Autistic, lives with parents, fully employed, not earn a lot of money, parents loaded.&lt;/p&gt;\n\n&lt;p&gt;Any money I did have, went to buying hard drives, last count was 23 Wired drives ranging from 1TB - 18TB, 2 of them are NAS Synology units (8 and 16TB) and a WD 16TB NAS Home Duo.&lt;/p&gt;\n\n&lt;p&gt;I have not calculated for a long time, but the total size is somewhere under 300TB but definitely above 150TB. &lt;/p&gt;\n\n&lt;p&gt;Parents are finally snapping and want a better more energy efficient solution, with a crap ton of space, so they have said they will buy me something, for our new house, but I have no idea what it is going to look like, I have no input and I am not a noob, but unless I am wrong, the highest capacity drives that can be purchased are 22TB. &lt;/p&gt;\n\n&lt;p&gt;Of course, I won&amp;#39;t be transferring everything from those 23 drives that will go onto this new unit, whatever it is, I will need to be very cautious going forward of what stays on it, and what gets archived, just so I can make the space last as long as possible without me forking out the money, for more space in the future. &lt;/p&gt;\n\n&lt;p&gt;Hoping 22TB will eventually get cheaper than they are now, but don&amp;#39;t know how long that will take.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116sygz", "is_robot_indexable": true, "report_reasons": null, "author": "massivlybored", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116sygz/need_help_to_speculate_exactly_what_i_am_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116sygz/need_help_to_speculate_exactly_what_i_am_getting/", "subreddit_subscribers": 670502, "created_utc": 1676853279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!  \n\n\nI'm sorry to bother, but I wanted to run something by the experts here since I couldn't find a definitive answer online or on my mobo manual.\n\n&amp;#x200B;\n\nIs it possible to use my motherboard, (AMD MSI X570 Tomahawk Mag) to run 2 nvme ssds in their slots, two sata drives, and a third nvme drive run through a pcie to nvme adapter?\n\n&amp;#x200B;\n\nThanks in advance!", "author_fullname": "t2_2lxg4v2b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about NV.ME Adapter and Sata Ports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ruyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676850425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sorry to bother, but I wanted to run something by the experts here since I couldn&amp;#39;t find a definitive answer online or on my mobo manual.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is it possible to use my motherboard, (AMD MSI X570 Tomahawk Mag) to run 2 nvme ssds in their slots, two sata drives, and a third nvme drive run through a pcie to nvme adapter?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116ruyd", "is_robot_indexable": true, "report_reasons": null, "author": "Scholar_Erasmus", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116ruyd/questions_about_nvme_adapter_and_sata_ports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116ruyd/questions_about_nvme_adapter_and_sata_ports/", "subreddit_subscribers": 670502, "created_utc": 1676850425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I thought it would be OK but apparently I was VERY wrong. The drive did not have issues prior and was used as a cold external storage. I got a replacement and wanted to wipe it before selling it. No issues prior to wiping. \n\nI had the SeaTools utility app open for a new Seagate drive I got and I figured that I try to use their \"Erase\" option on my WD drive as well... Big mistake.\n\nI run \"Sanitize Overwrite\" in SeaTools on my WD 10 TB Elements drive after which the drive shows as \"Not Initialized\" with no partitions (all space unallocated). I get I/O error dialogue (*The request could not be performed because of an I/O device error)* when attempting to initialize the drive. Similar with Linux Mint, could not format/repartition. \n\n**Is there a way to save this drive?**\n\nContacted WD via chat, that did not lead me anywhere as essentially got sent a list of \"data recovery experts\" as they declared the drive dead.\n\nSeaTools is able to show *some* data about the drive: Detail is correct, interface good but SMART data is missing. Power and security is populated. Interestingly here I do have an option to upload a new firmware but I suspect uploading a firmware from SeaTools wont work for my WD drive.. I tried looking for firmware 83.HOA83 that I saw in SeaTools for this drive but could not find a download option online.\n\nI pulled the drive out of its housing and hooked up via sata. Then I was also able to get WD software to see the drive via its \"Western Digital Dashboard\" that has a number of options but still missing SMART and really cannot do anything. No way to update firmware or repair firmware.", "author_fullname": "t2_ldc282h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bricked 10TB WD Elements with SeaTools by Sanitize Overwrite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116rf18", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676849307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought it would be OK but apparently I was VERY wrong. The drive did not have issues prior and was used as a cold external storage. I got a replacement and wanted to wipe it before selling it. No issues prior to wiping. &lt;/p&gt;\n\n&lt;p&gt;I had the SeaTools utility app open for a new Seagate drive I got and I figured that I try to use their &amp;quot;Erase&amp;quot; option on my WD drive as well... Big mistake.&lt;/p&gt;\n\n&lt;p&gt;I run &amp;quot;Sanitize Overwrite&amp;quot; in SeaTools on my WD 10 TB Elements drive after which the drive shows as &amp;quot;Not Initialized&amp;quot; with no partitions (all space unallocated). I get I/O error dialogue (&lt;em&gt;The request could not be performed because of an I/O device error)&lt;/em&gt; when attempting to initialize the drive. Similar with Linux Mint, could not format/repartition. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there a way to save this drive?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Contacted WD via chat, that did not lead me anywhere as essentially got sent a list of &amp;quot;data recovery experts&amp;quot; as they declared the drive dead.&lt;/p&gt;\n\n&lt;p&gt;SeaTools is able to show &lt;em&gt;some&lt;/em&gt; data about the drive: Detail is correct, interface good but SMART data is missing. Power and security is populated. Interestingly here I do have an option to upload a new firmware but I suspect uploading a firmware from SeaTools wont work for my WD drive.. I tried looking for firmware 83.HOA83 that I saw in SeaTools for this drive but could not find a download option online.&lt;/p&gt;\n\n&lt;p&gt;I pulled the drive out of its housing and hooked up via sata. Then I was also able to get WD software to see the drive via its &amp;quot;Western Digital Dashboard&amp;quot; that has a number of options but still missing SMART and really cannot do anything. No way to update firmware or repair firmware.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116rf18", "is_robot_indexable": true, "report_reasons": null, "author": "A2251", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116rf18/bricked_10tb_wd_elements_with_seatools_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116rf18/bricked_10tb_wd_elements_with_seatools_by/", "subreddit_subscribers": 670502, "created_utc": 1676849307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, so I found a tool on Github that archives Discord channels, but there doesn't seem to be a way to automate it (Tyrrrz/DiscordChatExporter). If anyone knows a way to automatically update the json files with new messages every 30 minutes or so that would be great.", "author_fullname": "t2_vonfwqpk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically archiving Discord servers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116qzna", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676848255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, so I found a tool on Github that archives Discord channels, but there doesn&amp;#39;t seem to be a way to automate it (Tyrrrz/DiscordChatExporter). If anyone knows a way to automatically update the json files with new messages every 30 minutes or so that would be great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116qzna", "is_robot_indexable": true, "report_reasons": null, "author": "viperr081", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116qzna/automatically_archiving_discord_servers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116qzna/automatically_archiving_discord_servers/", "subreddit_subscribers": 670502, "created_utc": 1676848255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey I'm looking for advice on how to manage/rename movie/tv series folders? Everything is messy and I've got Full DVD, Full Blu-ray and lots of scene rips and I want to get everything looking the same but I'm worried that if I change the name I won't be ae to reseed, datahoarding can be hard at times lol", "author_fullname": "t2_5dkgegkg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on renaming movies/tv series as everything looks messy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116lq3b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676835177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I&amp;#39;m looking for advice on how to manage/rename movie/tv series folders? Everything is messy and I&amp;#39;ve got Full DVD, Full Blu-ray and lots of scene rips and I want to get everything looking the same but I&amp;#39;m worried that if I change the name I won&amp;#39;t be ae to reseed, datahoarding can be hard at times lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116lq3b", "is_robot_indexable": true, "report_reasons": null, "author": "craftywizard1983", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116lq3b/advice_on_renaming_moviestv_series_as_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116lq3b/advice_on_renaming_moviestv_series_as_everything/", "subreddit_subscribers": 670502, "created_utc": 1676835177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not exactly sure if this question fits in here but I will try. I'm sort of desperate to see (and backup) a particular area in 2009 while all the Street View offers is 2011. Perhaps it would be possible to access those older images somehow? I don't think they are gone from their servers. What do you say?", "author_fullname": "t2_gltvm32x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to access old Street View panoramic images of Google Maps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116kygi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676833330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not exactly sure if this question fits in here but I will try. I&amp;#39;m sort of desperate to see (and backup) a particular area in 2009 while all the Street View offers is 2011. Perhaps it would be possible to access those older images somehow? I don&amp;#39;t think they are gone from their servers. What do you say?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116kygi", "is_robot_indexable": true, "report_reasons": null, "author": "botcraft_net", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116kygi/is_there_a_way_to_access_old_street_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116kygi/is_there_a_way_to_access_old_street_view/", "subreddit_subscribers": 670502, "created_utc": 1676833330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Heya good day!\n\nIt's been ages since I've gone hard drive shopping.\n\nMust have been at least like 5 years ago. around 2016 or something.\n\nI got myself a pair of ST4000DM004 drives. 4 tb each back then.\n\nWhile not the best. And to be fair still universally hated for whatever reason. They worked fine for me.\n\n\nBeen in multiple rigs. One of them even repurposed into multiple servers. And then back to my main rig.\n\nNow one of them has developed bad sectors. So I backed everything up about a month ago. \n\nAnd today when I was going to update the backup. The drive's condition has worsened quickly.\n\n\nso I've been browsing around for hard drives and pricing to replace my tired bois with.\n\n\ni've noticed that the ST4000DM004  is still for sale today here.\n\nand is also currently the lowest Euro to TB. at about 17.16 Euro per TB. 68.65 per 4tb drive.\n\nshould i just grab these drives again. or should i opt for other drives? \n\n\nfor example. \n\nSeagate Skyhawk ST4000VX016  +Rescue 4TB for 18.27 per TB. \n\nand Seagate Skyhawk Surveillance HDD ST4000VX013 4TB for 19.50 per TB.\n\n\ngoogled around a bit. and heard people talk about that starting/stopping them often could lead to increased wear. so might not be the decent option.\n\n\nso after a bit more research i came opun these 2.\n\nThe WD purple WD42PURZ 256mb cache for 20.82 per TB\n\nand Seagate NAS ST4000VN006 Iron wolf for 20.98 per TB\n\n\n\nwhat should I go for?\n\n these will not live in raid. these will just be chucked into my main rig as storage drives.\n \nthese drives wont be used for constant high throughput applications like editing or streaming to multiple devices from multiple sources.\n\n\nthese will just be used for simple data storage. think of hundreds of GB of ISO's. that i have to occasionally pull off to copy to a usb drive.\n\none of them will be used as a steam library drive though. which is currently at a chunky size of 2+ TB. so far these Seagate drives have been perfectly fine running most games from.\n\nhowever most games larger than about 60 gigs i tend to install to my SSD when I play them anyways.\n\n\n\nthank you in avance for your advice!\n\nP.S i know i should get a NAS with zfs and such to have proper 1-2-3 backup solutions. I'm working on that. plan is 8x4 or 8x8TB server when i move in with my partner ^ ^", "author_fullname": "t2_157dz0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been ages since I've gone HDD shopping. I need some help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116jigb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676829844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heya good day!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been ages since I&amp;#39;ve gone hard drive shopping.&lt;/p&gt;\n\n&lt;p&gt;Must have been at least like 5 years ago. around 2016 or something.&lt;/p&gt;\n\n&lt;p&gt;I got myself a pair of ST4000DM004 drives. 4 tb each back then.&lt;/p&gt;\n\n&lt;p&gt;While not the best. And to be fair still universally hated for whatever reason. They worked fine for me.&lt;/p&gt;\n\n&lt;p&gt;Been in multiple rigs. One of them even repurposed into multiple servers. And then back to my main rig.&lt;/p&gt;\n\n&lt;p&gt;Now one of them has developed bad sectors. So I backed everything up about a month ago. &lt;/p&gt;\n\n&lt;p&gt;And today when I was going to update the backup. The drive&amp;#39;s condition has worsened quickly.&lt;/p&gt;\n\n&lt;p&gt;so I&amp;#39;ve been browsing around for hard drives and pricing to replace my tired bois with.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve noticed that the ST4000DM004  is still for sale today here.&lt;/p&gt;\n\n&lt;p&gt;and is also currently the lowest Euro to TB. at about 17.16 Euro per TB. 68.65 per 4tb drive.&lt;/p&gt;\n\n&lt;p&gt;should i just grab these drives again. or should i opt for other drives? &lt;/p&gt;\n\n&lt;p&gt;for example. &lt;/p&gt;\n\n&lt;p&gt;Seagate Skyhawk ST4000VX016  +Rescue 4TB for 18.27 per TB. &lt;/p&gt;\n\n&lt;p&gt;and Seagate Skyhawk Surveillance HDD ST4000VX013 4TB for 19.50 per TB.&lt;/p&gt;\n\n&lt;p&gt;googled around a bit. and heard people talk about that starting/stopping them often could lead to increased wear. so might not be the decent option.&lt;/p&gt;\n\n&lt;p&gt;so after a bit more research i came opun these 2.&lt;/p&gt;\n\n&lt;p&gt;The WD purple WD42PURZ 256mb cache for 20.82 per TB&lt;/p&gt;\n\n&lt;p&gt;and Seagate NAS ST4000VN006 Iron wolf for 20.98 per TB&lt;/p&gt;\n\n&lt;p&gt;what should I go for?&lt;/p&gt;\n\n&lt;p&gt;these will not live in raid. these will just be chucked into my main rig as storage drives.&lt;/p&gt;\n\n&lt;p&gt;these drives wont be used for constant high throughput applications like editing or streaming to multiple devices from multiple sources.&lt;/p&gt;\n\n&lt;p&gt;these will just be used for simple data storage. think of hundreds of GB of ISO&amp;#39;s. that i have to occasionally pull off to copy to a usb drive.&lt;/p&gt;\n\n&lt;p&gt;one of them will be used as a steam library drive though. which is currently at a chunky size of 2+ TB. so far these Seagate drives have been perfectly fine running most games from.&lt;/p&gt;\n\n&lt;p&gt;however most games larger than about 60 gigs i tend to install to my SSD when I play them anyways.&lt;/p&gt;\n\n&lt;p&gt;thank you in avance for your advice!&lt;/p&gt;\n\n&lt;p&gt;P.S i know i should get a NAS with zfs and such to have proper 1-2-3 backup solutions. I&amp;#39;m working on that. plan is 8x4 or 8x8TB server when i move in with my partner ^ ^&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116jigb", "is_robot_indexable": true, "report_reasons": null, "author": "appletechgeek", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116jigb/been_ages_since_ive_gone_hdd_shopping_i_need_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116jigb/been_ages_since_ive_gone_hdd_shopping_i_need_some/", "subreddit_subscribers": 670502, "created_utc": 1676829844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Folks at r/sysadmin suggested y'all might be able to provide suggestions.  My backup data is outgrowing a 6TB QNAP TR-004 Raid 5 box, and I would have thought a 5 or 6 bay DAS with hardware RAID 6 wouldn't be hard to find.  It needs to be DAS because VEEAM recommends against NAS storage for local backup repositories (and anyway it won't be shared at all).  But hours of searching has turned up nothing.  I'm surporised that someone hasn't made an enclosure that you can use as either NAS or DAS depemding on how it's attached.  Please point me to something if you have ideas, thanks.", "author_fullname": "t2_slg66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 or 6 Bay DAS that supports RAID 6?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116h22p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676824179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Folks at &lt;a href=\"/r/sysadmin\"&gt;r/sysadmin&lt;/a&gt; suggested y&amp;#39;all might be able to provide suggestions.  My backup data is outgrowing a 6TB QNAP TR-004 Raid 5 box, and I would have thought a 5 or 6 bay DAS with hardware RAID 6 wouldn&amp;#39;t be hard to find.  It needs to be DAS because VEEAM recommends against NAS storage for local backup repositories (and anyway it won&amp;#39;t be shared at all).  But hours of searching has turned up nothing.  I&amp;#39;m surporised that someone hasn&amp;#39;t made an enclosure that you can use as either NAS or DAS depemding on how it&amp;#39;s attached.  Please point me to something if you have ideas, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116h22p", "is_robot_indexable": true, "report_reasons": null, "author": "jimshilliday", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116h22p/5_or_6_bay_das_that_supports_raid_6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116h22p/5_or_6_bay_das_that_supports_raid_6/", "subreddit_subscribers": 670502, "created_utc": 1676824179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen a couple options for scraping stuff from instagram, OTT all I can think of is instaloader. I've seen lots of advice on using it by setting up a temp account and using that to download. The thing is, I don't so much need accounts, I want the stuff from my collections. AFAIK there's no way I can get access to those from another account unless I send each post in a message. I have way too many posts saved in my collections for that.\n\nI use the account a lot and have invested a lot of time building up these collections (photography, architecture, plants, a variety of things). I love instagram for discovery but hate it for browsing. I'd love to get stuff downloaded and tagged so I can better browse/manage my collections.\n\nAll I can think of is piping instaloader through a python script that heavily spaces and randomly staggers the requests (perhaps 2 a minute on average is slow enough?). I'm still concerned instagram might catch it out though, unless the instaloader requests are indistinguishable from user requests other than frequency.\n\nOr perhaps I could just write some python to actually use the browser to get the information? Would be a huge PITA but I'm pretty desperate here. My architecture collection has been particularly educational, and I could learn/memorize info from it much better if I had control over the images/account names/descriptions as files and a spreadsheet.", "author_fullname": "t2_2qatlocm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading my collections from Instagram without killing account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116fg1v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676821973.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676821756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a couple options for scraping stuff from instagram, OTT all I can think of is instaloader. I&amp;#39;ve seen lots of advice on using it by setting up a temp account and using that to download. The thing is, I don&amp;#39;t so much need accounts, I want the stuff from my collections. AFAIK there&amp;#39;s no way I can get access to those from another account unless I send each post in a message. I have way too many posts saved in my collections for that.&lt;/p&gt;\n\n&lt;p&gt;I use the account a lot and have invested a lot of time building up these collections (photography, architecture, plants, a variety of things). I love instagram for discovery but hate it for browsing. I&amp;#39;d love to get stuff downloaded and tagged so I can better browse/manage my collections.&lt;/p&gt;\n\n&lt;p&gt;All I can think of is piping instaloader through a python script that heavily spaces and randomly staggers the requests (perhaps 2 a minute on average is slow enough?). I&amp;#39;m still concerned instagram might catch it out though, unless the instaloader requests are indistinguishable from user requests other than frequency.&lt;/p&gt;\n\n&lt;p&gt;Or perhaps I could just write some python to actually use the browser to get the information? Would be a huge PITA but I&amp;#39;m pretty desperate here. My architecture collection has been particularly educational, and I could learn/memorize info from it much better if I had control over the images/account names/descriptions as files and a spreadsheet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116fg1v", "is_robot_indexable": true, "report_reasons": null, "author": "dealingwitholddata", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116fg1v/downloading_my_collections_from_instagram_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116fg1v/downloading_my_collections_from_instagram_without/", "subreddit_subscribers": 670502, "created_utc": 1676821756.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}