{"kind": "Listing", "data": {"after": "t3_116h22p", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all I have around 6x 20 TB WD external HDDs. Used for torrenting and Plex server. Just wondering if there\u2019s anything I can do software or practices to prolong the life of the drives. I\u2019m on macOS by the way.", "author_fullname": "t2_vs43uoy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tips on keeping external HDDs healthy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116vxkk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676861873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all I have around 6x 20 TB WD external HDDs. Used for torrenting and Plex server. Just wondering if there\u2019s anything I can do software or practices to prolong the life of the drives. I\u2019m on macOS by the way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116vxkk", "is_robot_indexable": true, "report_reasons": null, "author": "CosmicCobbler", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116vxkk/any_tips_on_keeping_external_hdds_healthy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116vxkk/any_tips_on_keeping_external_hdds_healthy/", "subreddit_subscribers": 670529, "created_utc": 1676861873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Censoring Roald Dahl? I\u2019ll be keeping my original copies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_116fqiy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_6ilbp", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/orBUY8t8L8JIrDRbbSv9MWBS1MjjjIRnN7X1SirTEJ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "books", "selftext": "", "author_fullname": "t2_915gm5zi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Censoring Roald Dahl? I\u2019ll be keeping my original copies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/books", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1168fhx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8412, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 8412, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/orBUY8t8L8JIrDRbbSv9MWBS1MjjjIRnN7X1SirTEJ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1676806928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thetimes.co.uk", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?auto=webp&amp;v=enabled&amp;s=a653df65434c355a48265268b6cebe4178dd4568", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1fdce132b567f08beb26ee04588d0ec18b85d24", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cb34a70991697933639ed2ed85e0ce54311949b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91351761f8b259ad24015d4e6dd982380d334e8c", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e09c7ba5808f051fce6e5fc4a521997b206ac489", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acebb9fe8441300bc90cf693961cbc132a650f8", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ecdfc5dd89441de7a206873513d875bf2488ca4", "width": 1080, "height": 606}], "variants": {}, "id": "c4G_Az5480aPDW7fE7v_Z3a4cpI7PbFC9Uwrmx_iqLA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh4i", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1168fhx", "is_robot_indexable": true, "report_reasons": null, "author": "Elliottafc1", "discussion_type": null, "num_comments": 1228, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/books/comments/1168fhx/censoring_roald_dahl_ill_be_keeping_my_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "subreddit_subscribers": 22235196, "created_utc": 1676806928.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1676822138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thetimes.co.uk", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?auto=webp&amp;v=enabled&amp;s=a653df65434c355a48265268b6cebe4178dd4568", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1fdce132b567f08beb26ee04588d0ec18b85d24", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cb34a70991697933639ed2ed85e0ce54311949b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91351761f8b259ad24015d4e6dd982380d334e8c", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e09c7ba5808f051fce6e5fc4a521997b206ac489", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acebb9fe8441300bc90cf693961cbc132a650f8", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/qRgndBIu0gKb2GKq7eP_MsTYHQXltg7r42SRGH8RQ7k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ecdfc5dd89441de7a206873513d875bf2488ca4", "width": 1080, "height": 606}], "variants": {}, "id": "c4G_Az5480aPDW7fE7v_Z3a4cpI7PbFC9Uwrmx_iqLA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116fqiy", "is_robot_indexable": true, "report_reasons": null, "author": "zuperfly", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1168fhx", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116fqiy/censoring_roald_dahl_ill_be_keeping_my_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thetimes.co.uk/article/sensitivity-readers-are-twits-to-mess-with-the-magic-of-roald-dahl-zxk928mdz", "subreddit_subscribers": 670529, "created_utc": 1676822138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Had a bit of a rude awakening recently when I found that one of my youtube archives (which includes many videos no longer on the creator's channel), was not actually searchable, not with the search function, or by clicking on any of the search term tags for the page.\n\nI emailed them and they told me \"Social Media content does not generate metadata and are not searchable\" and that for some reason, youtube videos fall under social media.\n\nThis absolutely baffles me, there are tons of channels out there that get unfairly nuked out of existence, or lost videos because of DMCAs etc, and it would seem the 'only' reason any of them are findable on archive.org is because they're not correctly catagorised, a complete fluke. (Basically, they start in Community Video and are searchable, but if an admin changes that, you might just be out of luck)\n\nI myself in the past have tried to search for lost videos on archive.org before, and now I don't know if they simply weren't uploaded there by anyone, or if they lacked the metadata to search because one of the admins deemed it so.\n\nJust figured I'd share this information for anyone who wasn't aware because this seems like a really stupid decision, you can't even search within the catagories! If you go to the Mirror Tube catagory for instance, you can scroll through them (all 100 thousand plus...) but you can't search. What on earth are they doing?", "author_fullname": "t2_hs20g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive.org : Youtube videos are not ment to be searchable, apparently.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11697u7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676809939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a bit of a rude awakening recently when I found that one of my youtube archives (which includes many videos no longer on the creator&amp;#39;s channel), was not actually searchable, not with the search function, or by clicking on any of the search term tags for the page.&lt;/p&gt;\n\n&lt;p&gt;I emailed them and they told me &amp;quot;Social Media content does not generate metadata and are not searchable&amp;quot; and that for some reason, youtube videos fall under social media.&lt;/p&gt;\n\n&lt;p&gt;This absolutely baffles me, there are tons of channels out there that get unfairly nuked out of existence, or lost videos because of DMCAs etc, and it would seem the &amp;#39;only&amp;#39; reason any of them are findable on archive.org is because they&amp;#39;re not correctly catagorised, a complete fluke. (Basically, they start in Community Video and are searchable, but if an admin changes that, you might just be out of luck)&lt;/p&gt;\n\n&lt;p&gt;I myself in the past have tried to search for lost videos on archive.org before, and now I don&amp;#39;t know if they simply weren&amp;#39;t uploaded there by anyone, or if they lacked the metadata to search because one of the admins deemed it so.&lt;/p&gt;\n\n&lt;p&gt;Just figured I&amp;#39;d share this information for anyone who wasn&amp;#39;t aware because this seems like a really stupid decision, you can&amp;#39;t even search within the catagories! If you go to the Mirror Tube catagory for instance, you can scroll through them (all 100 thousand plus...) but you can&amp;#39;t search. What on earth are they doing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "11697u7", "is_robot_indexable": true, "report_reasons": null, "author": "Xyrexus", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/11697u7/archiveorg_youtube_videos_are_not_ment_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/11697u7/archiveorg_youtube_videos_are_not_ment_to_be/", "subreddit_subscribers": 670529, "created_utc": 1676809939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nSo I\u2019ve got a bunch of really old SIM cards from my childhood (nano, micro, and standard), and I\u2019m pretty sure I may have sms messages saved on them. What\u2019s the easiest way of viewing their contents and extracting them?\n\nThanks!", "author_fullname": "t2_u1k87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to extract data from old Sim Cards?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116q78n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;So I\u2019ve got a bunch of really old SIM cards from my childhood (nano, micro, and standard), and I\u2019m pretty sure I may have sms messages saved on them. What\u2019s the easiest way of viewing their contents and extracting them?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116q78n", "is_robot_indexable": true, "report_reasons": null, "author": "MurmurOfTheCine", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116q78n/how_to_extract_data_from_old_sim_cards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116q78n/how_to_extract_data_from_old_sim_cards/", "subreddit_subscribers": 670529, "created_utc": 1676846321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to move a heap of data from my phone to my backup SSD. Nothing is corrupted or damaged on my phone storage.\n\nWhen I transfer data to my SSD, some of it shows up corrupted and I can't work out why. I've used different cables, and connected the phone to a laptop and then SSD, as well as directly to the SSD.\n\nIt looks like the corruption of data affects video files only - not all of them, but some of them. Maybe 1 in 20. There's 190GB of files, a mix of images and videos, and it's a bit of a mess.\n\nI have a Sandisk 1TB Gen1 SSD that I'm moving data to from my Galaxy S23 1TB phone via USB-C to USB-C.\n\nAnyone know why this could be happening?", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been battling data corruption issues all day.. narrowed it down to video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116k8br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676831576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to move a heap of data from my phone to my backup SSD. Nothing is corrupted or damaged on my phone storage.&lt;/p&gt;\n\n&lt;p&gt;When I transfer data to my SSD, some of it shows up corrupted and I can&amp;#39;t work out why. I&amp;#39;ve used different cables, and connected the phone to a laptop and then SSD, as well as directly to the SSD.&lt;/p&gt;\n\n&lt;p&gt;It looks like the corruption of data affects video files only - not all of them, but some of them. Maybe 1 in 20. There&amp;#39;s 190GB of files, a mix of images and videos, and it&amp;#39;s a bit of a mess.&lt;/p&gt;\n\n&lt;p&gt;I have a Sandisk 1TB Gen1 SSD that I&amp;#39;m moving data to from my Galaxy S23 1TB phone via USB-C to USB-C.&lt;/p&gt;\n\n&lt;p&gt;Anyone know why this could be happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116k8br", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116k8br/been_battling_data_corruption_issues_all_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116k8br/been_battling_data_corruption_issues_all_day/", "subreddit_subscribers": 670529, "created_utc": 1676831576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have been processing my use case for a few weeks now and I have realized the following:\n\n1. I really only care for syncing data remotely and streaming short clips (5-15 minutes maximum). I will be syncing quite a lot of data (my main repo is 170GB in size and I expect 3-6 GB in changes weekly) but I will not be streaming very many clips. I don't actually want additional features on top of this since it would probably just serve as bloatware to me.\n\n2. I think a 4-bay NAS would work for my purposes as far as extensibility of storage space goes.\n\n3. I am simply not prepared for building my own NAS as I lack to many competencies. I need a progression/baby step into a homebrew NAS rather than starting out building one from scratch.\n\n4. Aspirationally, I'm looking for something that costs as little idle energy as possible.\n\n\nWhat cues should I look for in retail NAS/es that would indicate that satisfying of these 4 criteria?", "author_fullname": "t2_ahrudp61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything specific I should look for in a NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116yslu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676871275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been processing my use case for a few weeks now and I have realized the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I really only care for syncing data remotely and streaming short clips (5-15 minutes maximum). I will be syncing quite a lot of data (my main repo is 170GB in size and I expect 3-6 GB in changes weekly) but I will not be streaming very many clips. I don&amp;#39;t actually want additional features on top of this since it would probably just serve as bloatware to me.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I think a 4-bay NAS would work for my purposes as far as extensibility of storage space goes.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am simply not prepared for building my own NAS as I lack to many competencies. I need a progression/baby step into a homebrew NAS rather than starting out building one from scratch.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Aspirationally, I&amp;#39;m looking for something that costs as little idle energy as possible.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What cues should I look for in retail NAS/es that would indicate that satisfying of these 4 criteria?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116yslu", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane_Grab_8727", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116yslu/is_there_anything_specific_i_should_look_for_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116yslu/is_there_anything_specific_i_should_look_for_in_a/", "subreddit_subscribers": 670529, "created_utc": 1676871275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a way to checksum files from a cold storage HDD to my current NAS, as the source.\n\nI have an external HDD I keep offsite that I update about once every 6 months. I have a NAS running Unraid that I use as my main storage.\n\nI would like a way to compare checksums of the files, to see if any corruption occured on the cold storage drive while it was unpowered. I only copy over new files, usually with FreeFileSync, so I could have corruption / bitrot on the older files and not know.\n\nDealing with about 5TB of data\n\nIs Teracopy the best way to do this? I have read horror stories of corruption caused by teracopy.\n\nMain OS is Windows.\n\n&amp;#x200B;\n\nThanks everyone!", "author_fullname": "t2_becvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum for cold storage backup, compare to source before backing up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116q5qi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676846215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a way to checksum files from a cold storage HDD to my current NAS, as the source.&lt;/p&gt;\n\n&lt;p&gt;I have an external HDD I keep offsite that I update about once every 6 months. I have a NAS running Unraid that I use as my main storage.&lt;/p&gt;\n\n&lt;p&gt;I would like a way to compare checksums of the files, to see if any corruption occured on the cold storage drive while it was unpowered. I only copy over new files, usually with FreeFileSync, so I could have corruption / bitrot on the older files and not know.&lt;/p&gt;\n\n&lt;p&gt;Dealing with about 5TB of data&lt;/p&gt;\n\n&lt;p&gt;Is Teracopy the best way to do this? I have read horror stories of corruption caused by teracopy.&lt;/p&gt;\n\n&lt;p&gt;Main OS is Windows.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116q5qi", "is_robot_indexable": true, "report_reasons": null, "author": "technica-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116q5qi/checksum_for_cold_storage_backup_compare_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116q5qi/checksum_for_cold_storage_backup_compare_to/", "subreddit_subscribers": 670529, "created_utc": 1676846215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using TeraCopy to manually sync folders between my Windows laptop and a remote Synology NAS.  Folders are created and populated with files on my laptop just once for each folder, with no further modifications, so syncing manually has thus been pretty simple so far.  The relevant NAS directory is mapped as a network drive, and I must connect via VPN in order to access it remotely.\n\nTo verify the copy operations I always use TeraCopy's \"Verify\" option, which is great, but very slow when I'm connected to the NAS remotely, I believe because it's actually resending all the data as part of the verification process ([https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p](https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p)).\n\nAnyone know of a simple, ideally automated way of doing file verification faster (consuming less throughput) than TeraCopy?\n\nIn the near future I'm thinking of setting up Free File Sync to handle the syncing, but I still need to research and test it out more, and I'm not sure how it handles verification.", "author_fullname": "t2_9i6f7eeq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File verification optimized for remote network file copy (Windows)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116nrde", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676840273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using TeraCopy to manually sync folders between my Windows laptop and a remote Synology NAS.  Folders are created and populated with files on my laptop just once for each folder, with no further modifications, so syncing manually has thus been pretty simple so far.  The relevant NAS directory is mapped as a network drive, and I must connect via VPN in order to access it remotely.&lt;/p&gt;\n\n&lt;p&gt;To verify the copy operations I always use TeraCopy&amp;#39;s &amp;quot;Verify&amp;quot; option, which is great, but very slow when I&amp;#39;m connected to the NAS remotely, I believe because it&amp;#39;s actually resending all the data as part of the verification process (&lt;a href=\"https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p\"&gt;https://superuser.com/questions/582403/over-gigabit-connection-teracopy-does-31mb-s-but-windows-8-does-it-at-109mb-p&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;Anyone know of a simple, ideally automated way of doing file verification faster (consuming less throughput) than TeraCopy?&lt;/p&gt;\n\n&lt;p&gt;In the near future I&amp;#39;m thinking of setting up Free File Sync to handle the syncing, but I still need to research and test it out more, and I&amp;#39;m not sure how it handles verification.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?auto=webp&amp;v=enabled&amp;s=5bf61032a6a11077440cf0421aceee21a943a62d", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbd71549aa2d678c4628f02ae8cd5b7ddaf23d73", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=451335daa7d9e8aa739cd3a851acc983948bf130", "width": 216, "height": 216}], "variants": {}, "id": "PJ2AnV34BC-Bc5jLZ5IhQtzJPf-P0PW2LhpwYCeTNyE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116nrde", "is_robot_indexable": true, "report_reasons": null, "author": "VladsBestFriend", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116nrde/file_verification_optimized_for_remote_network/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116nrde/file_verification_optimized_for_remote_network/", "subreddit_subscribers": 670529, "created_utc": 1676840273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "DigitalOcean have laid off [Geoff Graham](https://geoffgraham.me/goodbye-css-tricks/), and I'm sceptical of the availability of the site in the long-term, so I'd like to see if there's any way to properly archive the site.\n\n[Article on hn](https://news.ycombinator.com/item?id=34864701).", "author_fullname": "t2_3x9pv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to properly archive css-tricks? It's an invaluable resource", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1170xkb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676879194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DigitalOcean have laid off &lt;a href=\"https://geoffgraham.me/goodbye-css-tricks/\"&gt;Geoff Graham&lt;/a&gt;, and I&amp;#39;m sceptical of the availability of the site in the long-term, so I&amp;#39;d like to see if there&amp;#39;s any way to properly archive the site.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://news.ycombinator.com/item?id=34864701\"&gt;Article on hn&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?auto=webp&amp;v=enabled&amp;s=3ac9af9d3f9bb819c97b3887e3cd3f9df7c90ffd", "width": 538, "height": 348}, "resolutions": [{"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0df5aab32d4f1d33a7b85c824e1867fbfee0c051", "width": 108, "height": 69}, {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6bbea08544072cd4a354472aa0911e6520306ea1", "width": 216, "height": 139}, {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c595b929668178505faa315382be71404c953840", "width": 320, "height": 206}], "variants": {}, "id": "yvtCkFuqQWxfc5Je6a9ovKSdFy3Fh72jg35_lkR_3bU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1170xkb", "is_robot_indexable": true, "report_reasons": null, "author": "EmSixTeen", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1170xkb/is_there_any_way_to_properly_archive_csstricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1170xkb/is_there_any_way_to_properly_archive_csstricks/", "subreddit_subscribers": 670529, "created_utc": 1676879194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've restored images to many single drives before with Macrium being my go-to, but I currently need to image multiple drives at once and can't for the life of me find reliable software to do so. Sadly, Macrium is only one at a time and I'm trying to avoid sitting there to start a new drive myself one after another. Either restoring an image or cloning from an existing drive is fine, either way, I just need it to be able to batch together up to 10 drives plugged in automatically. Balena Etcher can do batch cloning but it's taking forever and seems have trouble actually completing that many drives at a time. Anything else you guys have used for something like this?", "author_fullname": "t2_id2ll0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for imaging to multiple drives at a time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116z62v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676872635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve restored images to many single drives before with Macrium being my go-to, but I currently need to image multiple drives at once and can&amp;#39;t for the life of me find reliable software to do so. Sadly, Macrium is only one at a time and I&amp;#39;m trying to avoid sitting there to start a new drive myself one after another. Either restoring an image or cloning from an existing drive is fine, either way, I just need it to be able to batch together up to 10 drives plugged in automatically. Balena Etcher can do batch cloning but it&amp;#39;s taking forever and seems have trouble actually completing that many drives at a time. Anything else you guys have used for something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116z62v", "is_robot_indexable": true, "report_reasons": null, "author": "TheMaster627", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116z62v/software_for_imaging_to_multiple_drives_at_a_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116z62v/software_for_imaging_to_multiple_drives_at_a_time/", "subreddit_subscribers": 670529, "created_utc": 1676872635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tonight I learned about the 604 3.3v rails that need to be taped over. Figured that out. I have 6 new to me drives. 4x604 and 4x6L4.  All four of the 604s work fine. The two 6L4s do not.  Anyone know what I might be missing? The 6l4's spin up but when testing in windows they suffer a I/O error when trying to initialize.  I'm going to be putting all of these in an unraid setup with a backplane when it arrives but that might be a few steps from now.  Is there some \"gotcha\" about these drives I don't know about or are they just bad drives?", "author_fullname": "t2_7olt1k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the difference between the WD WUH721414ALE6L4 and the 604?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ud2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676857132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tonight I learned about the 604 3.3v rails that need to be taped over. Figured that out. I have 6 new to me drives. 4x604 and 4x6L4.  All four of the 604s work fine. The two 6L4s do not.  Anyone know what I might be missing? The 6l4&amp;#39;s spin up but when testing in windows they suffer a I/O error when trying to initialize.  I&amp;#39;m going to be putting all of these in an unraid setup with a backplane when it arrives but that might be a few steps from now.  Is there some &amp;quot;gotcha&amp;quot; about these drives I don&amp;#39;t know about or are they just bad drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116ud2r", "is_robot_indexable": true, "report_reasons": null, "author": "ElBigBad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116ud2r/whats_the_difference_between_the_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116ud2r/whats_the_difference_between_the_wd/", "subreddit_subscribers": 670529, "created_utc": 1676857132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "every single hard drive has disappeared and cant be ordered.\n\ni only needed to wait a few more days to order my 20TB Exos :( \n\nnow i dont know what to do.", "author_fullname": "t2_8l9xfrvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what happened on serverpartdeals.com today", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1172lpi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676885843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;every single hard drive has disappeared and cant be ordered.&lt;/p&gt;\n\n&lt;p&gt;i only needed to wait a few more days to order my 20TB Exos :( &lt;/p&gt;\n\n&lt;p&gt;now i dont know what to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1172lpi", "is_robot_indexable": true, "report_reasons": null, "author": "seqvirtualtours", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1172lpi/what_happened_on_serverpartdealscom_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1172lpi/what_happened_on_serverpartdealscom_today/", "subreddit_subscribers": 670529, "created_utc": 1676885843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, after going through a bit of a digital house clean, moving content around and clearing out any duplicates (and there were a lot !!) - I\u2019m now left with loads of redundant directories, yet none seem to be completely empty to do a simple `find ./ -type d -empty -delete ` command, as it seems they either have empty subfolder and/or hidden subfolder within them a big culprit is e.g `.@thumbs`..  Does anyone have a good clean up approach, perhaps some command lines  that would allow me to remove (or just move) all my now redundant folders ?", "author_fullname": "t2_tvy84", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Remove redundant folders after data clean up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1171yxm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676883299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, after going through a bit of a digital house clean, moving content around and clearing out any duplicates (and there were a lot !!) - I\u2019m now left with loads of redundant directories, yet none seem to be completely empty to do a simple &lt;code&gt;find ./ -type d -empty -delete&lt;/code&gt; command, as it seems they either have empty subfolder and/or hidden subfolder within them a big culprit is e.g &lt;code&gt;.@thumbs&lt;/code&gt;..  Does anyone have a good clean up approach, perhaps some command lines  that would allow me to remove (or just move) all my now redundant folders ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1171yxm", "is_robot_indexable": true, "report_reasons": null, "author": "parkercp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1171yxm/remove_redundant_folders_after_data_clean_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1171yxm/remove_redundant_folders_after_data_clean_up/", "subreddit_subscribers": 670529, "created_utc": 1676883299.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have these messages within the chrome console logs in developer tools and there\u2019s at least 3000 of them but everytime I wanna save one of them I have to right click on one and hit \u201csave as\u201d, is there a tool or another way I could just save them all at once?\n\n[https://linuxhint.com/chrome-dev-tools/](https://linuxhint.com/chrome-dev-tools/)", "author_fullname": "t2_7dvw50ti", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save console logs (chrome)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116zovj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676874490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have these messages within the chrome console logs in developer tools and there\u2019s at least 3000 of them but everytime I wanna save one of them I have to right click on one and hit \u201csave as\u201d, is there a tool or another way I could just save them all at once?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://linuxhint.com/chrome-dev-tools/\"&gt;https://linuxhint.com/chrome-dev-tools/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116zovj", "is_robot_indexable": true, "report_reasons": null, "author": "JohnCoss192", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116zovj/save_console_logs_chrome/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116zovj/save_console_logs_chrome/", "subreddit_subscribers": 670529, "created_utc": 1676874490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Edited with solution using a powershell script\n\n~~I don't know if this is the right sub to ask this but I have a Pixel 1st gen that still has the ability to upload unlimited original quality picture through google photos.~~\n\n~~The problem is that I have the 32GB version of the phone and only has 18GB of free space. So that means I have to upload 1.5TB of pictures and videos in \\~18GB intervals. I would like to know if there's a solution to split a 1.5TB folder into multiple folders of approximately 18GB. That way I could just use syncthings or external drive and just transfer each \\~18GB folder everytime the last one uploads. It doesn't have to be exact, it just has to be no more than 18GB.~~\n\n~~I tried googling this but the solution they give is to zip and split the big folder into multiple zipped pieces. Which doesn't really help me since I need to transfer the original jpg or mp4 to the phone in order to upload. They won't upload if they are zipped.~~\n\n~~I also tried chatgpt but it is giving me command/script that has invalid syntax/parameter.~~\n\n`xcopy /s &lt;source folder&gt; &lt;destination folder&gt;\\&lt;prefix&gt; /f /max &lt;maximum file size&gt;`\n\n~~I told it that /max is an invalid parameter and it sent me to powershell to do this~~\n\n`Get-ChildItem -Path \"&lt;source folder&gt;\" -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object (robocopy $_ \"&lt;destination folder&gt;\\$($_.Substring($_. LastIndexOf(\"\\\")+1))\" /mov /sizemax: &lt;maximum file size&gt;}`\n\n~~I told it once again sizemax is not a valid parameter and it just errored out.~~\n\n~~To test both of these scripts/commands I just created a folder called \"my folder\" and another one called \"subdirectories\" under desktop. Also used fsutils to create different sized dummy files inside \"my folder\". I didn't run these scripts with my actual files.~~\n\nedit:for anyone that stumbles upon this in the future here's the powershell script I used to do this.\n\n`$sourcePath = \"Source\"` \n\n`$destinationPath = \"Destination\"` \n\n`$maxSize = 18GB`\n\n`$currentSize = 0 $currentFolder = 1 $destination = Join-Path -Path $destinationPath -ChildPath \"Folder$currentFolder\"`\n\n`if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null }`\n\n`Get-ChildItem -Path $sourcePath -File | ForEach-Object { if ($currentSize + $_.Length -ge $maxSize) { $currentFolder++ $destination = Join-Path -Path $destinationPath -ChildPath \"Folder$currentFolder\" if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null } $currentSize = 0 }`\n\n`$destinationFile = Join-Path -Path $destination -ChildPath $_.Name Copy-Item -Path $_.FullName -Destination $destinationFile -Force`\n\n`$currentSize += $_.Length }`\n\n`Write-Output \"Folders created: $currentFolder\"`\n\nAll you have to do is change the source and destination path to match yours and also the maxSize to whatever you want. No need to change anything else after the first 3 lines. This script will copy files from your source directory into the giving destination and will automatically create a new folder when it reaches the threshold or if and when the next file will put it above the threshold given.\n\n&amp;#x200B;", "author_fullname": "t2_vcp81gxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to split a large folder into multiple smaller folders of no more than a certain size without zipping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116kxal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676870537.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676833260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edited with solution using a powershell script&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I don&amp;#39;t know if this is the right sub to ask this but I have a Pixel 1st gen that still has the ability to upload unlimited original quality picture through google photos.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;The problem is that I have the 32GB version of the phone and only has 18GB of free space. So that means I have to upload 1.5TB of pictures and videos in ~18GB intervals. I would like to know if there&amp;#39;s a solution to split a 1.5TB folder into multiple folders of approximately 18GB. That way I could just use syncthings or external drive and just transfer each ~18GB folder everytime the last one uploads. It doesn&amp;#39;t have to be exact, it just has to be no more than 18GB.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I tried googling this but the solution they give is to zip and split the big folder into multiple zipped pieces. Which doesn&amp;#39;t really help me since I need to transfer the original jpg or mp4 to the phone in order to upload. They won&amp;#39;t upload if they are zipped.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I also tried chatgpt but it is giving me command/script that has invalid syntax/parameter.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;xcopy /s &amp;lt;source folder&amp;gt; &amp;lt;destination folder&amp;gt;\\&amp;lt;prefix&amp;gt; /f /max &amp;lt;maximum file size&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I told it that /max is an invalid parameter and it sent me to powershell to do this&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Get-ChildItem -Path &amp;quot;&amp;lt;source folder&amp;gt;&amp;quot; -Recurse -File | Split-Path -Parent | Get-Unique | ForEach-Object (robocopy $_ &amp;quot;&amp;lt;destination folder&amp;gt;\\$($_.Substring($_. LastIndexOf(&amp;quot;\\&amp;quot;)+1))&amp;quot; /mov /sizemax: &amp;lt;maximum file size&amp;gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;I told it once again sizemax is not a valid parameter and it just errored out.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;To test both of these scripts/commands I just created a folder called &amp;quot;my folder&amp;quot; and another one called &amp;quot;subdirectories&amp;quot; under desktop. Also used fsutils to create different sized dummy files inside &amp;quot;my folder&amp;quot;. I didn&amp;#39;t run these scripts with my actual files.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;edit:for anyone that stumbles upon this in the future here&amp;#39;s the powershell script I used to do this.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$sourcePath = &amp;quot;Source&amp;quot;&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$destinationPath = &amp;quot;Destination&amp;quot;&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$maxSize = 18GB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$currentSize = 0 $currentFolder = 1 $destination = Join-Path -Path $destinationPath -ChildPath &amp;quot;Folder$currentFolder&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null }&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Get-ChildItem -Path $sourcePath -File | ForEach-Object { if ($currentSize + $_.Length -ge $maxSize) { $currentFolder++ $destination = Join-Path -Path $destinationPath -ChildPath &amp;quot;Folder$currentFolder&amp;quot; if (-not (Test-Path $destination)) { New-Item -ItemType Directory -Path $destination | Out-Null } $currentSize = 0 }&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$destinationFile = Join-Path -Path $destination -ChildPath $_.Name Copy-Item -Path $_.FullName -Destination $destinationFile -Force&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;$currentSize += $_.Length }&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Write-Output &amp;quot;Folders created: $currentFolder&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;All you have to do is change the source and destination path to match yours and also the maxSize to whatever you want. No need to change anything else after the first 3 lines. This script will copy files from your source directory into the giving destination and will automatically create a new folder when it reaches the threshold or if and when the next file will put it above the threshold given.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116kxal", "is_robot_indexable": true, "report_reasons": null, "author": "arcohex", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116kxal/is_there_a_way_to_split_a_large_folder_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116kxal/is_there_a_way_to_split_a_large_folder_into/", "subreddit_subscribers": 670529, "created_utc": 1676833260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI am looking for some cloud storage to store my family photos. I use Google Drive but I have ONE problem with it - you cannot rotate the images. Do you know any cloud storage good for keeping photos and basic editing? I know that dropbox can do this editing but it's quite expensive. Also, Amazon Photos can rotate photos but It will save the photo as a new one so you now have 2 of the same images.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_1lyo8mc6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Cloud storage for photos and videos with basic editing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116k2bs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676831181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am looking for some cloud storage to store my family photos. I use Google Drive but I have ONE problem with it - you cannot rotate the images. Do you know any cloud storage good for keeping photos and basic editing? I know that dropbox can do this editing but it&amp;#39;s quite expensive. Also, Amazon Photos can rotate photos but It will save the photo as a new one so you now have 2 of the same images.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116k2bs", "is_robot_indexable": true, "report_reasons": null, "author": "Xperr1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116k2bs/looking_for_cloud_storage_for_photos_and_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116k2bs/looking_for_cloud_storage_for_photos_and_videos/", "subreddit_subscribers": 670529, "created_utc": 1676831181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What kind of 50 or 100 or possibly even 200 year plans have y'all come up with for storing data long term in a scenario where new storage medium might not be obtainable?\n\nIt's been a sort of thought experiment of mine ever since I saw Beyond Thunderdome as a kid, to wonder how these societies would maintain digital media. Obviously computer components could be scavenged and if your good enough at soldering, at least the motherboards and maybe flash memory could be maintained for a very long time. Other parts might have to be continuously scavenged, I haven't the foggiest how one would physically repair a dead MCU chip for example, but the problem of storing digital media long term, and possibly having to eventually convert that into analog media or lose it forever, is one that definitely should be worked out as the data is a big reason why we are scavenging computers together in this hypothetical scenario, so if we can't work that out, then maintaining a computer is somewhat moot.\n\nMy best idea so far is basically just layers redundancy. An upfront server of sorts for accessing this information would be the first layer obviously, then a backup server that only boots up to perform raid operations seems the best way to go, but then after that it gets harder. Digital discs a sketchy long term, as with any digital storage media. I don't know how long an HDD can be preserved, but I'd imagine it's be near impossible for one to survive 50 years from what I hear about breakdown while even in airtight radiation proof containers like some satellites and hardened facilities have. So that brings me to tape, but encoding digital data onto tape when the capacity to extract that digital data might be lost eventually makes it pointless. Analog seems to be the best way to go. After the backup server layer I currently feel the only possible course of action is analog. A punchcard sort of thing could encode all sorts of technical data in a way that could be replicated again, vinyl records or possibly records made with other materials seems to be a good way to store high quality audio, and regular film seems to be the best way to preserve video. Museums have thought about this a lot for their archives and they basically store everything important as analog media as well. 2 or even 3 copies should be kept on site. One for recovery purposes in case the first layers are lost, and then another that does not get opened unless the recovery copy is also lost, as well as further off-site backups for redundancy. \n\nWhich brings me to my next question. How would one store these forms of analog media, ideally indefinitely? I know indefinitely isn't practical. Heat death of the universe and all that. What measures can be taken to ensure that a collection of analog storage media could hypothetically last as long as it needs to until society develops the technology to start being able to distribute it again, even if that is potentially hundreds of years?\n\nI've even entertained the thought of using geometric ratios encoded into megaliths, which sent me down a somewhat disturbing yet extremely fascinating rabbit hole in which I learnt of the unit of measure the megalithic yard, read the Fingerprints of the Gods by Graham Hancock, and also found out that the Great Pyramid of Giza is so mathematically sophisticated that the level of advanced knowledge it required to build the damn thing is absolutely mind boggling, but I digress...\n\nAnyways, yeah. Apocalypse library. Ideas?", "author_fullname": "t2_c06g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is y'all's 'apocalypse library' solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1167zeg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676805054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What kind of 50 or 100 or possibly even 200 year plans have y&amp;#39;all come up with for storing data long term in a scenario where new storage medium might not be obtainable?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a sort of thought experiment of mine ever since I saw Beyond Thunderdome as a kid, to wonder how these societies would maintain digital media. Obviously computer components could be scavenged and if your good enough at soldering, at least the motherboards and maybe flash memory could be maintained for a very long time. Other parts might have to be continuously scavenged, I haven&amp;#39;t the foggiest how one would physically repair a dead MCU chip for example, but the problem of storing digital media long term, and possibly having to eventually convert that into analog media or lose it forever, is one that definitely should be worked out as the data is a big reason why we are scavenging computers together in this hypothetical scenario, so if we can&amp;#39;t work that out, then maintaining a computer is somewhat moot.&lt;/p&gt;\n\n&lt;p&gt;My best idea so far is basically just layers redundancy. An upfront server of sorts for accessing this information would be the first layer obviously, then a backup server that only boots up to perform raid operations seems the best way to go, but then after that it gets harder. Digital discs a sketchy long term, as with any digital storage media. I don&amp;#39;t know how long an HDD can be preserved, but I&amp;#39;d imagine it&amp;#39;s be near impossible for one to survive 50 years from what I hear about breakdown while even in airtight radiation proof containers like some satellites and hardened facilities have. So that brings me to tape, but encoding digital data onto tape when the capacity to extract that digital data might be lost eventually makes it pointless. Analog seems to be the best way to go. After the backup server layer I currently feel the only possible course of action is analog. A punchcard sort of thing could encode all sorts of technical data in a way that could be replicated again, vinyl records or possibly records made with other materials seems to be a good way to store high quality audio, and regular film seems to be the best way to preserve video. Museums have thought about this a lot for their archives and they basically store everything important as analog media as well. 2 or even 3 copies should be kept on site. One for recovery purposes in case the first layers are lost, and then another that does not get opened unless the recovery copy is also lost, as well as further off-site backups for redundancy. &lt;/p&gt;\n\n&lt;p&gt;Which brings me to my next question. How would one store these forms of analog media, ideally indefinitely? I know indefinitely isn&amp;#39;t practical. Heat death of the universe and all that. What measures can be taken to ensure that a collection of analog storage media could hypothetically last as long as it needs to until society develops the technology to start being able to distribute it again, even if that is potentially hundreds of years?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve even entertained the thought of using geometric ratios encoded into megaliths, which sent me down a somewhat disturbing yet extremely fascinating rabbit hole in which I learnt of the unit of measure the megalithic yard, read the Fingerprints of the Gods by Graham Hancock, and also found out that the Great Pyramid of Giza is so mathematically sophisticated that the level of advanced knowledge it required to build the damn thing is absolutely mind boggling, but I digress...&lt;/p&gt;\n\n&lt;p&gt;Anyways, yeah. Apocalypse library. Ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1167zeg", "is_robot_indexable": true, "report_reasons": null, "author": "ONEOFHAM", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1167zeg/what_is_yalls_apocalypse_library_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1167zeg/what_is_yalls_apocalypse_library_solution/", "subreddit_subscribers": 670529, "created_utc": 1676805054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently had an incident of data corruption and I'm concerned about how this sort of thing can happen, and what the implications could be for all my data, across devices, NASes, etc.  \n\nIn this case, I was upgrading a phone and diffing copied data, but I can't imagine the situation isn't unique to a phone.  In this case I'd copied the internal \"sd card\" data from the source (old phone) to the destination (new phone).  I used Beyond Compare to diff the data to make sure everything transferred accurately.  To my horror, one image was different on the destination than it was on the source.  I looked at the source, and the source JPEG was itself corrupted, you load the image and sections are visually shifted with garbage displayed.  That seemed odd, so  I went to a backup I made three months ago, and the backup had the undamaged image.  The nature of the corruption was that large sections of nulls were replacing valid data.  What struct me as particularly odd here is that there have been two corruption incidents, at least, the one that I find on the source now, and the one I find on the destination.  How can that happen?  \n\nBasically how did the file originally become corrupt, and then how did the copy I made to another device become more corrupt than the source?  And how worried do I need to be that many other files on the source were corrupted and I just haven't realized it.\n\nThanks for any input.", "author_fullname": "t2_b5s4o14r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does this kind of corruption happen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116xpfq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676867608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had an incident of data corruption and I&amp;#39;m concerned about how this sort of thing can happen, and what the implications could be for all my data, across devices, NASes, etc.  &lt;/p&gt;\n\n&lt;p&gt;In this case, I was upgrading a phone and diffing copied data, but I can&amp;#39;t imagine the situation isn&amp;#39;t unique to a phone.  In this case I&amp;#39;d copied the internal &amp;quot;sd card&amp;quot; data from the source (old phone) to the destination (new phone).  I used Beyond Compare to diff the data to make sure everything transferred accurately.  To my horror, one image was different on the destination than it was on the source.  I looked at the source, and the source JPEG was itself corrupted, you load the image and sections are visually shifted with garbage displayed.  That seemed odd, so  I went to a backup I made three months ago, and the backup had the undamaged image.  The nature of the corruption was that large sections of nulls were replacing valid data.  What struct me as particularly odd here is that there have been two corruption incidents, at least, the one that I find on the source now, and the one I find on the destination.  How can that happen?  &lt;/p&gt;\n\n&lt;p&gt;Basically how did the file originally become corrupt, and then how did the copy I made to another device become more corrupt than the source?  And how worried do I need to be that many other files on the source were corrupted and I just haven&amp;#39;t realized it.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116xpfq", "is_robot_indexable": true, "report_reasons": null, "author": "asking4afriend40631", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116xpfq/how_does_this_kind_of_corruption_happen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116xpfq/how_does_this_kind_of_corruption_happen/", "subreddit_subscribers": 670529, "created_utc": 1676867608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hope I'm not just being an idiot here, but I recently got an lsi 9207-8i card, supposedly flashed to IT mode but it's definitely one of those fake Chinese ones that looks brand new. Anyway I installed and made sure it was seated well in the slot, but wasn't able to see it in bios anywhere and it didn't show up in device manager either. I tried another slot and same issue. It also isn't even getting warm to the touch. Is there anything I should try to get this working if it's not even showing up to the system, or should I assume I got a faulty junk card and go with a more reputable seller? Fwiw the seller I picked has tens of thousands of very solid reviews.", "author_fullname": "t2_8gagi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hba card not showing in bios or in device manager?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wusr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hope I&amp;#39;m not just being an idiot here, but I recently got an lsi 9207-8i card, supposedly flashed to IT mode but it&amp;#39;s definitely one of those fake Chinese ones that looks brand new. Anyway I installed and made sure it was seated well in the slot, but wasn&amp;#39;t able to see it in bios anywhere and it didn&amp;#39;t show up in device manager either. I tried another slot and same issue. It also isn&amp;#39;t even getting warm to the touch. Is there anything I should try to get this working if it&amp;#39;s not even showing up to the system, or should I assume I got a faulty junk card and go with a more reputable seller? Fwiw the seller I picked has tens of thousands of very solid reviews.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116wusr", "is_robot_indexable": true, "report_reasons": null, "author": "ben7337", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116wusr/hba_card_not_showing_in_bios_or_in_device_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116wusr/hba_card_not_showing_in_bios_or_in_device_manager/", "subreddit_subscribers": 670529, "created_utc": 1676864790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m stuck between Samsung t7 shield or sandisk extreme. Both are similar price and 1TB. I notice the Sandisk has a longer warranty (5 vs 3 years) but heard they tend to fail quickly? Any suggestions?", "author_fullname": "t2_7pg7o64p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "M1 air ssd dillema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wukb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m stuck between Samsung t7 shield or sandisk extreme. Both are similar price and 1TB. I notice the Sandisk has a longer warranty (5 vs 3 years) but heard they tend to fail quickly? Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116wukb", "is_robot_indexable": true, "report_reasons": null, "author": "samurai489", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116wukb/m1_air_ssd_dillema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116wukb/m1_air_ssd_dillema/", "subreddit_subscribers": 670529, "created_utc": 1676864768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!  \n\n\nI'm sorry to bother, but I wanted to run something by the experts here since I couldn't find a definitive answer online or on my mobo manual.\n\n&amp;#x200B;\n\nIs it possible to use my motherboard, (AMD MSI X570 Tomahawk Mag) to run 2 nvme ssds in their slots, two sata drives, and a third nvme drive run through a pcie to nvme adapter?\n\n&amp;#x200B;\n\nThanks in advance!", "author_fullname": "t2_2lxg4v2b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about NV.ME Adapter and Sata Ports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116ruyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676850425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sorry to bother, but I wanted to run something by the experts here since I couldn&amp;#39;t find a definitive answer online or on my mobo manual.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is it possible to use my motherboard, (AMD MSI X570 Tomahawk Mag) to run 2 nvme ssds in their slots, two sata drives, and a third nvme drive run through a pcie to nvme adapter?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116ruyd", "is_robot_indexable": true, "report_reasons": null, "author": "Scholar_Erasmus", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116ruyd/questions_about_nvme_adapter_and_sata_ports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116ruyd/questions_about_nvme_adapter_and_sata_ports/", "subreddit_subscribers": 670529, "created_utc": 1676850425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey I'm looking for advice on how to manage/rename movie/tv series folders? Everything is messy and I've got Full DVD, Full Blu-ray and lots of scene rips and I want to get everything looking the same but I'm worried that if I change the name I won't be ae to reseed, datahoarding can be hard at times lol", "author_fullname": "t2_5dkgegkg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on renaming movies/tv series as everything looks messy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116lq3b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676835177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I&amp;#39;m looking for advice on how to manage/rename movie/tv series folders? Everything is messy and I&amp;#39;ve got Full DVD, Full Blu-ray and lots of scene rips and I want to get everything looking the same but I&amp;#39;m worried that if I change the name I won&amp;#39;t be ae to reseed, datahoarding can be hard at times lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116lq3b", "is_robot_indexable": true, "report_reasons": null, "author": "craftywizard1983", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116lq3b/advice_on_renaming_moviestv_series_as_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116lq3b/advice_on_renaming_moviestv_series_as_everything/", "subreddit_subscribers": 670529, "created_utc": 1676835177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not exactly sure if this question fits in here but I will try. I'm sort of desperate to see (and backup) a particular area in 2009 while all the Street View offers is 2011. Perhaps it would be possible to access those older images somehow? I don't think they are gone from their servers. What do you say?", "author_fullname": "t2_gltvm32x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to access old Street View panoramic images of Google Maps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116kygi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676833330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not exactly sure if this question fits in here but I will try. I&amp;#39;m sort of desperate to see (and backup) a particular area in 2009 while all the Street View offers is 2011. Perhaps it would be possible to access those older images somehow? I don&amp;#39;t think they are gone from their servers. What do you say?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116kygi", "is_robot_indexable": true, "report_reasons": null, "author": "botcraft_net", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116kygi/is_there_a_way_to_access_old_street_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116kygi/is_there_a_way_to_access_old_street_view/", "subreddit_subscribers": 670529, "created_utc": 1676833330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Heya good day!\n\nIt's been ages since I've gone hard drive shopping.\n\nMust have been at least like 5 years ago. around 2016 or something.\n\nI got myself a pair of ST4000DM004 drives. 4 tb each back then.\n\nWhile not the best. And to be fair still universally hated for whatever reason. They worked fine for me.\n\n\nBeen in multiple rigs. One of them even repurposed into multiple servers. And then back to my main rig.\n\nNow one of them has developed bad sectors. So I backed everything up about a month ago. \n\nAnd today when I was going to update the backup. The drive's condition has worsened quickly.\n\n\nso I've been browsing around for hard drives and pricing to replace my tired bois with.\n\n\ni've noticed that the ST4000DM004  is still for sale today here.\n\nand is also currently the lowest Euro to TB. at about 17.16 Euro per TB. 68.65 per 4tb drive.\n\nshould i just grab these drives again. or should i opt for other drives? \n\n\nfor example. \n\nSeagate Skyhawk ST4000VX016  +Rescue 4TB for 18.27 per TB. \n\nand Seagate Skyhawk Surveillance HDD ST4000VX013 4TB for 19.50 per TB.\n\n\ngoogled around a bit. and heard people talk about that starting/stopping them often could lead to increased wear. so might not be the decent option.\n\n\nso after a bit more research i came opun these 2.\n\nThe WD purple WD42PURZ 256mb cache for 20.82 per TB\n\nand Seagate NAS ST4000VN006 Iron wolf for 20.98 per TB\n\n\n\nwhat should I go for?\n\n these will not live in raid. these will just be chucked into my main rig as storage drives.\n \nthese drives wont be used for constant high throughput applications like editing or streaming to multiple devices from multiple sources.\n\n\nthese will just be used for simple data storage. think of hundreds of GB of ISO's. that i have to occasionally pull off to copy to a usb drive.\n\none of them will be used as a steam library drive though. which is currently at a chunky size of 2+ TB. so far these Seagate drives have been perfectly fine running most games from.\n\nhowever most games larger than about 60 gigs i tend to install to my SSD when I play them anyways.\n\n\n\nthank you in avance for your advice!\n\nP.S i know i should get a NAS with zfs and such to have proper 1-2-3 backup solutions. I'm working on that. plan is 8x4 or 8x8TB server when i move in with my partner ^ ^", "author_fullname": "t2_157dz0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been ages since I've gone HDD shopping. I need some help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116jigb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676829844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heya good day!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been ages since I&amp;#39;ve gone hard drive shopping.&lt;/p&gt;\n\n&lt;p&gt;Must have been at least like 5 years ago. around 2016 or something.&lt;/p&gt;\n\n&lt;p&gt;I got myself a pair of ST4000DM004 drives. 4 tb each back then.&lt;/p&gt;\n\n&lt;p&gt;While not the best. And to be fair still universally hated for whatever reason. They worked fine for me.&lt;/p&gt;\n\n&lt;p&gt;Been in multiple rigs. One of them even repurposed into multiple servers. And then back to my main rig.&lt;/p&gt;\n\n&lt;p&gt;Now one of them has developed bad sectors. So I backed everything up about a month ago. &lt;/p&gt;\n\n&lt;p&gt;And today when I was going to update the backup. The drive&amp;#39;s condition has worsened quickly.&lt;/p&gt;\n\n&lt;p&gt;so I&amp;#39;ve been browsing around for hard drives and pricing to replace my tired bois with.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve noticed that the ST4000DM004  is still for sale today here.&lt;/p&gt;\n\n&lt;p&gt;and is also currently the lowest Euro to TB. at about 17.16 Euro per TB. 68.65 per 4tb drive.&lt;/p&gt;\n\n&lt;p&gt;should i just grab these drives again. or should i opt for other drives? &lt;/p&gt;\n\n&lt;p&gt;for example. &lt;/p&gt;\n\n&lt;p&gt;Seagate Skyhawk ST4000VX016  +Rescue 4TB for 18.27 per TB. &lt;/p&gt;\n\n&lt;p&gt;and Seagate Skyhawk Surveillance HDD ST4000VX013 4TB for 19.50 per TB.&lt;/p&gt;\n\n&lt;p&gt;googled around a bit. and heard people talk about that starting/stopping them often could lead to increased wear. so might not be the decent option.&lt;/p&gt;\n\n&lt;p&gt;so after a bit more research i came opun these 2.&lt;/p&gt;\n\n&lt;p&gt;The WD purple WD42PURZ 256mb cache for 20.82 per TB&lt;/p&gt;\n\n&lt;p&gt;and Seagate NAS ST4000VN006 Iron wolf for 20.98 per TB&lt;/p&gt;\n\n&lt;p&gt;what should I go for?&lt;/p&gt;\n\n&lt;p&gt;these will not live in raid. these will just be chucked into my main rig as storage drives.&lt;/p&gt;\n\n&lt;p&gt;these drives wont be used for constant high throughput applications like editing or streaming to multiple devices from multiple sources.&lt;/p&gt;\n\n&lt;p&gt;these will just be used for simple data storage. think of hundreds of GB of ISO&amp;#39;s. that i have to occasionally pull off to copy to a usb drive.&lt;/p&gt;\n\n&lt;p&gt;one of them will be used as a steam library drive though. which is currently at a chunky size of 2+ TB. so far these Seagate drives have been perfectly fine running most games from.&lt;/p&gt;\n\n&lt;p&gt;however most games larger than about 60 gigs i tend to install to my SSD when I play them anyways.&lt;/p&gt;\n\n&lt;p&gt;thank you in avance for your advice!&lt;/p&gt;\n\n&lt;p&gt;P.S i know i should get a NAS with zfs and such to have proper 1-2-3 backup solutions. I&amp;#39;m working on that. plan is 8x4 or 8x8TB server when i move in with my partner ^ ^&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116jigb", "is_robot_indexable": true, "report_reasons": null, "author": "appletechgeek", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116jigb/been_ages_since_ive_gone_hdd_shopping_i_need_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116jigb/been_ages_since_ive_gone_hdd_shopping_i_need_some/", "subreddit_subscribers": 670529, "created_utc": 1676829844.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Folks at r/sysadmin suggested y'all might be able to provide suggestions.  My backup data is outgrowing a 6TB QNAP TR-004 Raid 5 box, and I would have thought a 5 or 6 bay DAS with hardware RAID 6 wouldn't be hard to find.  It needs to be DAS because VEEAM recommends against NAS storage for local backup repositories (and anyway it won't be shared at all).  But hours of searching has turned up nothing.  I'm surporised that someone hasn't made an enclosure that you can use as either NAS or DAS depemding on how it's attached.  Please point me to something if you have ideas, thanks.", "author_fullname": "t2_slg66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 or 6 Bay DAS that supports RAID 6?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116h22p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676824179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Folks at &lt;a href=\"/r/sysadmin\"&gt;r/sysadmin&lt;/a&gt; suggested y&amp;#39;all might be able to provide suggestions.  My backup data is outgrowing a 6TB QNAP TR-004 Raid 5 box, and I would have thought a 5 or 6 bay DAS with hardware RAID 6 wouldn&amp;#39;t be hard to find.  It needs to be DAS because VEEAM recommends against NAS storage for local backup repositories (and anyway it won&amp;#39;t be shared at all).  But hours of searching has turned up nothing.  I&amp;#39;m surporised that someone hasn&amp;#39;t made an enclosure that you can use as either NAS or DAS depemding on how it&amp;#39;s attached.  Please point me to something if you have ideas, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116h22p", "is_robot_indexable": true, "report_reasons": null, "author": "jimshilliday", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116h22p/5_or_6_bay_das_that_supports_raid_6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116h22p/5_or_6_bay_das_that_supports_raid_6/", "subreddit_subscribers": 670529, "created_utc": 1676824179.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}