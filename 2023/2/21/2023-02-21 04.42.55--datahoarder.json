{"kind": "Listing", "data": {"after": "t3_117knj5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "(crosspost from r/kiwix but relevant to the Data hoarding crowd I believe)\n\nAs a reminder, Kiwix is an offline reader: once you download your zim file (Wikipedia, StackOverflow or whatever) you can browse it without any further need for internet connectivity. There's much talk that one could fit Wikipedia into 21 Gb, but that would be a text-only, compressed and unformatted (ie not human readable) dump. Kiwix, on the other hand, is ready for consumption and use cases range from preppers to rural schools to Antarctic bases and anything inbetween.\n\nLast update was from May last year, but we've solved quite a number of issues since and so expect to be able to resume our monthly update schedule.\n\nThis new zim file contains 6,608,280 articles, about 97GB's worth of the Sum of All Human Knowledge. Other large wikis (FR, DE, anything &gt; 1M articles really) are also on their way.\n\nThe scrape lasted this time less than a week (5 days and 10 hours exactly). This is a substantial difference from 2022-05, which took approximately 11 days, and 2021-12, with 8 and a half days. \n\nThe download link is here ([http](https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim)) or here ([torrent](https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim.torrent), **recommended**).\n\nKiwix is free, open-source and is run as a non-profit. Thanks to everyone who helped with [fixing bugs](https://github.com/openzim/mwoffliner/issues) and / or [donated to support](https://support.kiwix.org) the project.", "author_fullname": "t2_24n3qggu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latest Wikipedia zim dump (97 GB) is available for download", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1178fz2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 644, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 644, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676904546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(crosspost from &lt;a href=\"/r/kiwix\"&gt;r/kiwix&lt;/a&gt; but relevant to the Data hoarding crowd I believe)&lt;/p&gt;\n\n&lt;p&gt;As a reminder, Kiwix is an offline reader: once you download your zim file (Wikipedia, StackOverflow or whatever) you can browse it without any further need for internet connectivity. There&amp;#39;s much talk that one could fit Wikipedia into 21 Gb, but that would be a text-only, compressed and unformatted (ie not human readable) dump. Kiwix, on the other hand, is ready for consumption and use cases range from preppers to rural schools to Antarctic bases and anything inbetween.&lt;/p&gt;\n\n&lt;p&gt;Last update was from May last year, but we&amp;#39;ve solved quite a number of issues since and so expect to be able to resume our monthly update schedule.&lt;/p&gt;\n\n&lt;p&gt;This new zim file contains 6,608,280 articles, about 97GB&amp;#39;s worth of the Sum of All Human Knowledge. Other large wikis (FR, DE, anything &amp;gt; 1M articles really) are also on their way.&lt;/p&gt;\n\n&lt;p&gt;The scrape lasted this time less than a week (5 days and 10 hours exactly). This is a substantial difference from 2022-05, which took approximately 11 days, and 2021-12, with 8 and a half days. &lt;/p&gt;\n\n&lt;p&gt;The download link is here (&lt;a href=\"https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim\"&gt;http&lt;/a&gt;) or here (&lt;a href=\"https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-02.zim.torrent\"&gt;torrent&lt;/a&gt;, &lt;strong&gt;recommended&lt;/strong&gt;).&lt;/p&gt;\n\n&lt;p&gt;Kiwix is free, open-source and is run as a non-profit. Thanks to everyone who helped with &lt;a href=\"https://github.com/openzim/mwoffliner/issues\"&gt;fixing bugs&lt;/a&gt; and / or &lt;a href=\"https://support.kiwix.org\"&gt;donated to support&lt;/a&gt; the project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1178fz2", "is_robot_indexable": true, "report_reasons": null, "author": "The_other_kiwix_guy", "discussion_type": null, "num_comments": 128, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1178fz2/latest_wikipedia_zim_dump_97_gb_is_available_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1178fz2/latest_wikipedia_zim_dump_97_gb_is_available_for/", "subreddit_subscribers": 670647, "created_utc": 1676904546.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hopefully this is a right sub for this!\n\nUnfortunately I have been tasked to manage and backup a deceased person's data. They had a computer, a phone, two cloud-services, an e-mail and messenger accounts and possibly some other services that I would like to keep backed-up. There are numerous duplicates, since for example all of their photos were backed-up to two different cloud services and are also stored locally on their phone. All in all I estimate that there is 1 Tb of data at most, likely a lot less. The data consists of personal photos and files. The photos have obviously strong sentimental value, files not so much. Nothing is objectively valuable or sensitive, since all their possessions will be shared, their identity can no longer be stolen and there are no work related things etc.\n\nI currently have access to all of their devices and services. All paid subscriptions are still active.\n\n&amp;#x200B;\n\nI need advice on how I should proceed from here.\n\n&amp;#x200B;\n\nI was planning on buying a 2 Tb portable SSD. I would transfer all of their files from the computer and the phone, and import everything from cloud, e-mail and other services that they had. I would simply store everything as is and only do some very basic archiving like a different folder for each source of data, essentially just accepting that there will be numerous duplicates (especially photos).\n\nThen I would label the SSD and leave it to their family member, and I would also upload all that data to that family member's cloud under a clearly labelled folder. The SSD would remain as an un-accessed local back-up. I am confident that the cloud service will be managed and kept active indefinitely.\n\n&amp;#x200B;\n\nIs that a good plan, or should I do something differently?\n\n&amp;#x200B;\n\nWhat kind of portable SSD would you recommend? I'm completely out of the loop of portable SSD's, and googling doesn't really help. Initially I wanted to buy one without a separate power supply for convenience, but I realised that if the power supply fails in a couple of years, the drive might be irreparable. On the other hand, if it comes with a separate proprietary power supply, the same problem remains and there is also the added possibility that the power supply may get misplaced and lost. Aside from that, I just want it to last as long as possible, 5 - 10 years at minimum (I don't know what is realistic).\n\nI understand this sub heavily advocates for a 3-2-1 rule, but that is not realistic in this situation. In this case it is important that the data is easily accessible from the cloud service which the family member is already familiar with. The SSD serves as a device that makes it easier for me to first compile everything together and the upload all of it to the cloud, but it would also act as a local backup for now until technical failure. It is extremely unlikely that anyone will even attempt to access it ever again. To reiterate, the cloud would be the main backup.", "author_fullname": "t2_13g02v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage the data of a deceased person?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1174q1b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676893891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully this is a right sub for this!&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I have been tasked to manage and backup a deceased person&amp;#39;s data. They had a computer, a phone, two cloud-services, an e-mail and messenger accounts and possibly some other services that I would like to keep backed-up. There are numerous duplicates, since for example all of their photos were backed-up to two different cloud services and are also stored locally on their phone. All in all I estimate that there is 1 Tb of data at most, likely a lot less. The data consists of personal photos and files. The photos have obviously strong sentimental value, files not so much. Nothing is objectively valuable or sensitive, since all their possessions will be shared, their identity can no longer be stolen and there are no work related things etc.&lt;/p&gt;\n\n&lt;p&gt;I currently have access to all of their devices and services. All paid subscriptions are still active.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need advice on how I should proceed from here.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was planning on buying a 2 Tb portable SSD. I would transfer all of their files from the computer and the phone, and import everything from cloud, e-mail and other services that they had. I would simply store everything as is and only do some very basic archiving like a different folder for each source of data, essentially just accepting that there will be numerous duplicates (especially photos).&lt;/p&gt;\n\n&lt;p&gt;Then I would label the SSD and leave it to their family member, and I would also upload all that data to that family member&amp;#39;s cloud under a clearly labelled folder. The SSD would remain as an un-accessed local back-up. I am confident that the cloud service will be managed and kept active indefinitely.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is that a good plan, or should I do something differently?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What kind of portable SSD would you recommend? I&amp;#39;m completely out of the loop of portable SSD&amp;#39;s, and googling doesn&amp;#39;t really help. Initially I wanted to buy one without a separate power supply for convenience, but I realised that if the power supply fails in a couple of years, the drive might be irreparable. On the other hand, if it comes with a separate proprietary power supply, the same problem remains and there is also the added possibility that the power supply may get misplaced and lost. Aside from that, I just want it to last as long as possible, 5 - 10 years at minimum (I don&amp;#39;t know what is realistic).&lt;/p&gt;\n\n&lt;p&gt;I understand this sub heavily advocates for a 3-2-1 rule, but that is not realistic in this situation. In this case it is important that the data is easily accessible from the cloud service which the family member is already familiar with. The SSD serves as a device that makes it easier for me to first compile everything together and the upload all of it to the cloud, but it would also act as a local backup for now until technical failure. It is extremely unlikely that anyone will even attempt to access it ever again. To reiterate, the cloud would be the main backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1174q1b", "is_robot_indexable": true, "report_reasons": null, "author": "RsSime", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1174q1b/how_to_manage_the_data_of_a_deceased_person/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1174q1b/how_to_manage_the_data_of_a_deceased_person/", "subreddit_subscribers": 670647, "created_utc": 1676893891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "DigitalOcean have laid off [Geoff Graham](https://geoffgraham.me/goodbye-css-tricks/), and I'm sceptical of the availability of the site in the long-term, so I'd like to see if there's any way to properly archive the site.\n\n[Article on hn](https://news.ycombinator.com/item?id=34864701).", "author_fullname": "t2_3x9pv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to properly archive css-tricks? It's an invaluable resource", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1170xkb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676879194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DigitalOcean have laid off &lt;a href=\"https://geoffgraham.me/goodbye-css-tricks/\"&gt;Geoff Graham&lt;/a&gt;, and I&amp;#39;m sceptical of the availability of the site in the long-term, so I&amp;#39;d like to see if there&amp;#39;s any way to properly archive the site.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://news.ycombinator.com/item?id=34864701\"&gt;Article on hn&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?auto=webp&amp;v=enabled&amp;s=3ac9af9d3f9bb819c97b3887e3cd3f9df7c90ffd", "width": 538, "height": 348}, "resolutions": [{"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0df5aab32d4f1d33a7b85c824e1867fbfee0c051", "width": 108, "height": 69}, {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6bbea08544072cd4a354472aa0911e6520306ea1", "width": 216, "height": 139}, {"url": "https://external-preview.redd.it/cIiTlDrdHfHSJiicA7xpHVVvoP-XnHOcMhpkNVyWupY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c595b929668178505faa315382be71404c953840", "width": 320, "height": 206}], "variants": {}, "id": "yvtCkFuqQWxfc5Je6a9ovKSdFy3Fh72jg35_lkR_3bU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1170xkb", "is_robot_indexable": true, "report_reasons": null, "author": "EmSixTeen", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1170xkb/is_there_any_way_to_properly_archive_csstricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1170xkb/is_there_any_way_to_properly_archive_csstricks/", "subreddit_subscribers": 670647, "created_utc": 1676879194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8ux9h3d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I bought this 12TB MyBook and wanna shuck it, if somebody knows if it contains the Helium filled drive or the Air Cooled drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_117378j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_mS5EaY-ei-VtN6rK4g_Y1REy29FVe0P3II7btA00Ns.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676888222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t1cs9nhyjbja1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?auto=webp&amp;v=enabled&amp;s=c35a9f2369c12fafec8a012089bd195fa5e3b743", "width": 899, "height": 1599}, "resolutions": [{"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d817b4f686782da6dc127f105daa754349ffedfc", "width": 108, "height": 192}, {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8a8e1d74f21b14ea02cc7bf680228678978d361", "width": 216, "height": 384}, {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e73686c543e4c508b4f6af3ca8b60783aa9b8f6d", "width": 320, "height": 569}, {"url": "https://preview.redd.it/t1cs9nhyjbja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c7063ec2507b314421f6bb08436c098848e3067", "width": 640, "height": 1138}], "variants": {}, "id": "eqy3h4_1KCV-JbvrQsb572oLzpqik9oGCTZcuz-9PLg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117378j", "is_robot_indexable": true, "report_reasons": null, "author": "petyr420", "discussion_type": null, "num_comments": 29, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117378j/i_bought_this_12tb_mybook_and_wanna_shuck_it_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t1cs9nhyjbja1.jpg", "subreddit_subscribers": 670647, "created_utc": 1676888222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_byyu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Infinite Storage Glitch: YouTube as cloud storage for ANY files, not just video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_117crzx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/u4dBt_Cyukywdh1o7WCwXwEGIEGyVrjvVzKyuGN7XXc.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676910985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/DvorakDwarf/Infinite-Storage-Glitch", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?auto=webp&amp;v=enabled&amp;s=5d92c3536aa3ba854f1f2972af10f4f8a169461e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e0c091c087c5860358bf1956e7e6713c3f86d4e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b085543f0bd4ec2336c0fed6a9770f2960d2124", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f31f1552b861c17786d70e761a5a5b58a2ee620b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67ec9132580bdd95a6e524068aae587f741a3c6a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68b562d506b8fcf07489199cc9883f93c09fa283", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/84xWn0jVvcn1MCc2yWkBkgM3xIJaqeMAWB5r9HDtT9A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a7a5fb0dc736063a1f44bf3d54b0bcebfbae029", "width": 1080, "height": 540}], "variants": {}, "id": "Thm20V0Y9NRyUvE0KHYnyzfalmsEiKIh0XJD4MdN1fU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "90TiB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "117crzx", "is_robot_indexable": true, "report_reasons": null, "author": "2bluesc", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/117crzx/infinite_storage_glitch_youtube_as_cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/DvorakDwarf/Infinite-Storage-Glitch", "subreddit_subscribers": 670647, "created_utc": 1676910985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "every single hard drive has disappeared and cant be ordered.\n\ni only needed to wait a few more days to order my 20TB Exos :( \n\nnow i dont know what to do.", "author_fullname": "t2_8l9xfrvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what happened on serverpartdeals.com today", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1172lpi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676885843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;every single hard drive has disappeared and cant be ordered.&lt;/p&gt;\n\n&lt;p&gt;i only needed to wait a few more days to order my 20TB Exos :( &lt;/p&gt;\n\n&lt;p&gt;now i dont know what to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1172lpi", "is_robot_indexable": true, "report_reasons": null, "author": "seqvirtualtours", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1172lpi/what_happened_on_serverpartdealscom_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1172lpi/what_happened_on_serverpartdealscom_today/", "subreddit_subscribers": 670647, "created_utc": 1676885843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a software with which i can select a drive and have it auto convert every file to the least file size with the same destination and file name. Like just give it a drive and have it convert every video file with the best h.265 preset.", "author_fullname": "t2_tylmkfsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "auto handbrake video converter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1176kot", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676899479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a software with which i can select a drive and have it auto convert every file to the least file size with the same destination and file name. Like just give it a drive and have it convert every video file with the best h.265 preset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1176kot", "is_robot_indexable": true, "report_reasons": null, "author": "Fun_Activity7114", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1176kot/auto_handbrake_video_converter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1176kot/auto_handbrake_video_converter/", "subreddit_subscribers": 670647, "created_utc": 1676899479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Amazing how they differ. If noise is a consideration I would go with the 12tb. Speeds seem the same ~190 in benchmarks, 120-160/MBps in real life.", "author_fullname": "t2_54wan", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Plus 12tb are much quieter than the 14tb.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117pnxc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676939941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Amazing how they differ. If noise is a consideration I would go with the 12tb. Speeds seem the same ~190 in benchmarks, 120-160/MBps in real life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "117pnxc", "is_robot_indexable": true, "report_reasons": null, "author": "eyecrax", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117pnxc/wd_red_plus_12tb_are_much_quieter_than_the_14tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117pnxc/wd_red_plus_12tb_are_much_quieter_than_the_14tb/", "subreddit_subscribers": 670647, "created_utc": 1676939941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've restored images to many single drives before with Macrium being my go-to, but I currently need to image multiple drives at once and can't for the life of me find reliable software to do so. Sadly, Macrium is only one at a time and I'm trying to avoid sitting there to start a new drive myself one after another. Either restoring an image or cloning from an existing drive is fine, either way, I just need it to be able to batch together up to 10 drives plugged in automatically. Balena Etcher can do batch cloning but it's taking forever and seems have trouble actually completing that many drives at a time. Anything else you guys have used for something like this?", "author_fullname": "t2_id2ll0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for imaging to multiple drives at a time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116z62v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676872635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve restored images to many single drives before with Macrium being my go-to, but I currently need to image multiple drives at once and can&amp;#39;t for the life of me find reliable software to do so. Sadly, Macrium is only one at a time and I&amp;#39;m trying to avoid sitting there to start a new drive myself one after another. Either restoring an image or cloning from an existing drive is fine, either way, I just need it to be able to batch together up to 10 drives plugged in automatically. Balena Etcher can do batch cloning but it&amp;#39;s taking forever and seems have trouble actually completing that many drives at a time. Anything else you guys have used for something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116z62v", "is_robot_indexable": true, "report_reasons": null, "author": "TheMaster627", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116z62v/software_for_imaging_to_multiple_drives_at_a_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116z62v/software_for_imaging_to_multiple_drives_at_a_time/", "subreddit_subscribers": 670647, "created_utc": 1676872635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have terabytes of content from onlyfans and want to find a way to organize and catalog using an app or a platform where I can gain access easily and browse without issues. \n\nthink of a plex server but for pictures and videos. currently I have it all stored in picture and video folders within folders of the models, but I want to be able to view said content within a few taps/swipers/clips. \n\nany recommendations?", "author_fullname": "t2_6fu21ud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "downloaded onlyfans content management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117i1pn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676921292.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have terabytes of content from onlyfans and want to find a way to organize and catalog using an app or a platform where I can gain access easily and browse without issues. &lt;/p&gt;\n\n&lt;p&gt;think of a plex server but for pictures and videos. currently I have it all stored in picture and video folders within folders of the models, but I want to be able to view said content within a few taps/swipers/clips. &lt;/p&gt;\n\n&lt;p&gt;any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "117i1pn", "is_robot_indexable": true, "report_reasons": null, "author": "Itsmymainthrowaway", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117i1pn/downloaded_onlyfans_content_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117i1pn/downloaded_onlyfans_content_management/", "subreddit_subscribers": 670647, "created_utc": 1676921292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have been processing my use case for a few weeks now and I have realized the following:\n\n1. I really only care for syncing data remotely and streaming short clips (5-15 minutes maximum). I will be syncing quite a lot of data (my main repo is 170GB in size and I expect 3-6 GB in changes weekly) but I will not be streaming very many clips. I don't actually want additional features on top of this since it would probably just serve as bloatware to me.\n\n2. I think a 4-bay NAS would work for my purposes as far as extensibility of storage space goes.\n\n3. I am simply not prepared for building my own NAS as I lack to many competencies. I need a progression/baby step into a homebrew NAS rather than starting out building one from scratch.\n\n4. Aspirationally, I'm looking for something that costs as little idle energy as possible.\n\n\nWhat cues should I look for in retail NAS/es that would indicate that satisfying of these 4 criteria?", "author_fullname": "t2_ahrudp61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything specific I should look for in a NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116yslu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676871275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been processing my use case for a few weeks now and I have realized the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I really only care for syncing data remotely and streaming short clips (5-15 minutes maximum). I will be syncing quite a lot of data (my main repo is 170GB in size and I expect 3-6 GB in changes weekly) but I will not be streaming very many clips. I don&amp;#39;t actually want additional features on top of this since it would probably just serve as bloatware to me.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I think a 4-bay NAS would work for my purposes as far as extensibility of storage space goes.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am simply not prepared for building my own NAS as I lack to many competencies. I need a progression/baby step into a homebrew NAS rather than starting out building one from scratch.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Aspirationally, I&amp;#39;m looking for something that costs as little idle energy as possible.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What cues should I look for in retail NAS/es that would indicate that satisfying of these 4 criteria?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "116yslu", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane_Grab_8727", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/116yslu/is_there_anything_specific_i_should_look_for_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/116yslu/is_there_anything_specific_i_should_look_for_in_a/", "subreddit_subscribers": 670647, "created_utc": 1676871275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow Data Horder,\n\nI've got 9 old disks with various sizes (80GB-1TB) and run times (&lt;1k hours to &gt;40k hours), a LSI 9240 8i Raid controller and a Mobo with 4 Sata connector. All of this stuff is basically e-waste but I kind of want to use it until it is truly defective. When I'm upgrading my home server (zfs raid 6 \u00e0 6x4TB) or my first backup (zfs raid 5 \u00e0 4x2TB) I'll probably getting additional disks. So I might upgrade this into a real thing in the future. The server will be at an other location than where my home server is located, which is the second reason why I'm bothering with it, I would like to mirror some of my files on there.\n\nSo basically what I want is a storge solution, which has the following properties:\n- using different sized disks.\n- any 2 disks out of 4 can fail or 3 out of 8.\n- one big pool.\n- adding and removing disks to the array.\n- checksumming like zfs or btrfs posses.\n- fully automatic, when writing to the array mirroring/parity should be done.\n\nwhat I do not care about is:\n- power consumption (it wont be running a lot).\n- wasting space for parity.\n- wasting disks for hot spares.\n- read/write speed.\n\nI rather like my zfs pool since it has dealt with disk failures, defective cables and a migration to bigger disks, however I would loose to much space using it.\nMy second backup uses btrfs raid 1 (2 different sized disks), where I do not loose any space (I can just add another small disks when I'm running out of mirrored space). I feel comfortable dealing with zfs, btrfs and mdadm. I was thinking of creating mdadm raid 1s and 5s (with about same disks sizes) with a btrfs raid 1 on top. Has anyone had a similar goal or any idea/opinion on that matter?\n\nWhat are you doing with your outdated disks?", "author_fullname": "t2_w9un1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "using old disks until they are truly dead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117mbid", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676931433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow Data Horder,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got 9 old disks with various sizes (80GB-1TB) and run times (&amp;lt;1k hours to &amp;gt;40k hours), a LSI 9240 8i Raid controller and a Mobo with 4 Sata connector. All of this stuff is basically e-waste but I kind of want to use it until it is truly defective. When I&amp;#39;m upgrading my home server (zfs raid 6 \u00e0 6x4TB) or my first backup (zfs raid 5 \u00e0 4x2TB) I&amp;#39;ll probably getting additional disks. So I might upgrade this into a real thing in the future. The server will be at an other location than where my home server is located, which is the second reason why I&amp;#39;m bothering with it, I would like to mirror some of my files on there.&lt;/p&gt;\n\n&lt;p&gt;So basically what I want is a storge solution, which has the following properties:\n- using different sized disks.\n- any 2 disks out of 4 can fail or 3 out of 8.\n- one big pool.\n- adding and removing disks to the array.\n- checksumming like zfs or btrfs posses.\n- fully automatic, when writing to the array mirroring/parity should be done.&lt;/p&gt;\n\n&lt;p&gt;what I do not care about is:\n- power consumption (it wont be running a lot).\n- wasting space for parity.\n- wasting disks for hot spares.\n- read/write speed.&lt;/p&gt;\n\n&lt;p&gt;I rather like my zfs pool since it has dealt with disk failures, defective cables and a migration to bigger disks, however I would loose to much space using it.\nMy second backup uses btrfs raid 1 (2 different sized disks), where I do not loose any space (I can just add another small disks when I&amp;#39;m running out of mirrored space). I feel comfortable dealing with zfs, btrfs and mdadm. I was thinking of creating mdadm raid 1s and 5s (with about same disks sizes) with a btrfs raid 1 on top. Has anyone had a similar goal or any idea/opinion on that matter?&lt;/p&gt;\n\n&lt;p&gt;What are you doing with your outdated disks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117mbid", "is_robot_indexable": true, "report_reasons": null, "author": "edmontdantes", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117mbid/using_old_disks_until_they_are_truly_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117mbid/using_old_disks_until_they_are_truly_dead/", "subreddit_subscribers": 670647, "created_utc": 1676931433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Doing some quick Google searches, it seems that although there is a slight speed advantage with external SSDs, the gap is closing. Secondly, it seems that external SSDs are slightly more expensive than thumb drives.   \n\n\nI'm looking for 2 TB solutions for a 3-2-1 data backup scheme, and I want to keep costs low, but I don't want to sacrifice reliability too much.  \n\n\nThe only downside I see to external SSDs is that the power consumption is a little higher than a USB drive. This means that on some USB Hubs, or KVM switches, you may not get enough power, at times.", "author_fullname": "t2_tbm9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the differences in reliability, speed, and cost between external SSDs and USB drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1178ew9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676904463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing some quick Google searches, it seems that although there is a slight speed advantage with external SSDs, the gap is closing. Secondly, it seems that external SSDs are slightly more expensive than thumb drives.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for 2 TB solutions for a 3-2-1 data backup scheme, and I want to keep costs low, but I don&amp;#39;t want to sacrifice reliability too much.  &lt;/p&gt;\n\n&lt;p&gt;The only downside I see to external SSDs is that the power consumption is a little higher than a USB drive. This means that on some USB Hubs, or KVM switches, you may not get enough power, at times.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1178ew9", "is_robot_indexable": true, "report_reasons": null, "author": "pantshirtshoes", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1178ew9/what_are_the_differences_in_reliability_speed_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1178ew9/what_are_the_differences_in_reliability_speed_and/", "subreddit_subscribers": 670647, "created_utc": 1676904463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, this is my first post on this subreddit (few on reddit in general). I have searched all over the internet for a mounting solution for hard drives that are using the new 4 screw mounting points instead of the 6  screw scheme. I purchased 2 helium filled 10 tb exos drives to use for a new NAS / media server build in a CoolerMaster N400 case. The only solutions I could find were case specific for different brands or 3D printed options. I do not have access to a 3d printer unfortunely. I have provided pictures to show my dilemma.\n\nI understand that this mounting scheme was implemented due to fitting more discs on the inside of the drive. Does anyone know of / use any better mounting mechanisms? I am currently just using the 2 mounting holes that I have access to. It does not seem very safe and I dont like it!\n\n&amp;#x200B;\n\n[Hard drive cage inside of case](https://preview.redd.it/rzbbe2sspgja1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d1354e21364e745c3c7cea8b622a85fed7158949)\n\n&amp;#x200B;\n\n[Insert obvious hole missing photo](https://preview.redd.it/xqsb8hrupgja1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=934226367a5d4252d0ba055dfd04e1079b9d372b)\n\n&amp;#x200B;", "author_fullname": "t2_uka2m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mounting solution for new hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"rzbbe2sspgja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2aff7ff2527cb46fc635f5533c014ba782837af1"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=534ca90f01648459701ea362ac7a27a1247aac05"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2e06c58dd4bd1ee42d9cf6001e5b8db78e01176"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df2523b50766e741ddc0923673c399f6b0fd0ecd"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=546d74413f8ed5030e2d7d436a0e009671a1bb00"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31b59ff22e471f632b4cbfdaf4ebf2908e075534"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/rzbbe2sspgja1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d1354e21364e745c3c7cea8b622a85fed7158949"}, "id": "rzbbe2sspgja1"}, "xqsb8hrupgja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cdea13d9b3bba0ba5437cbe864e283282758dcf"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1b1792a81c787f5c5a5953afe5c9b6387ed80de"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbd192663b3679ce8686e596bdeab9b568088171"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cd7a06bd54215ffe1d0f1f243662de3e801f709"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5b41f71d24d83ddcaf260e5a000aa141673c600"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ec16570d65140706f07104e5f307bee963cdcdd"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/xqsb8hrupgja1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=934226367a5d4252d0ba055dfd04e1079b9d372b"}, "id": "xqsb8hrupgja1"}}, "name": "t3_117td40", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TjMzhZH-soRcHAbZzfVVQCk6ZTVv0Zi2fraijcmjATk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676950633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, this is my first post on this subreddit (few on reddit in general). I have searched all over the internet for a mounting solution for hard drives that are using the new 4 screw mounting points instead of the 6  screw scheme. I purchased 2 helium filled 10 tb exos drives to use for a new NAS / media server build in a CoolerMaster N400 case. The only solutions I could find were case specific for different brands or 3D printed options. I do not have access to a 3d printer unfortunely. I have provided pictures to show my dilemma.&lt;/p&gt;\n\n&lt;p&gt;I understand that this mounting scheme was implemented due to fitting more discs on the inside of the drive. Does anyone know of / use any better mounting mechanisms? I am currently just using the 2 mounting holes that I have access to. It does not seem very safe and I dont like it!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rzbbe2sspgja1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d1354e21364e745c3c7cea8b622a85fed7158949\"&gt;Hard drive cage inside of case&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xqsb8hrupgja1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=934226367a5d4252d0ba055dfd04e1079b9d372b\"&gt;Insert obvious hole missing photo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117td40", "is_robot_indexable": true, "report_reasons": null, "author": "WhyIsQwentonStolen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117td40/mounting_solution_for_new_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117td40/mounting_solution_for_new_hard_drives/", "subreddit_subscribers": 670647, "created_utc": 1676950633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using Ripme for downloading and it either will stop with this error or only download partial things.\n\n I have tried deleting the appdata and redownloading and it still is not working. The history in the program was clear when I re-opened it.  \nDoes anyone know what is happening and how to fix it?", "author_fullname": "t2_sywj8398", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripme Error JSON Video Not Found", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117r8fg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676944379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Ripme for downloading and it either will stop with this error or only download partial things.&lt;/p&gt;\n\n&lt;p&gt;I have tried deleting the appdata and redownloading and it still is not working. The history in the program was clear when I re-opened it.&lt;br/&gt;\nDoes anyone know what is happening and how to fix it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117r8fg", "is_robot_indexable": true, "report_reasons": null, "author": "inkabusx", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117r8fg/ripme_error_json_video_not_found/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117r8fg/ripme_error_json_video_not_found/", "subreddit_subscribers": 670647, "created_utc": 1676944379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to copy all my old dvdfab (some split over two discs) and DVD shrink backup discs to my NAS so I can locally stream them.  How do I recompose them back on the HDDs?  Is there software to do this?\n\nThanks!", "author_fullname": "t2_4yovgcex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVDFab Split Disks and DVD Shrink to HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117q60c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676941336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to copy all my old dvdfab (some split over two discs) and DVD shrink backup discs to my NAS so I can locally stream them.  How do I recompose them back on the HDDs?  Is there software to do this?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117q60c", "is_robot_indexable": true, "report_reasons": null, "author": "Rune456", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117q60c/dvdfab_split_disks_and_dvd_shrink_to_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117q60c/dvdfab_split_disks_and_dvd_shrink_to_hdd/", "subreddit_subscribers": 670647, "created_utc": 1676941336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "## I am so glad to run this giveaway in the r/DataHoarder. Also, thanks to the mod team for helping me to organize the amazing campaign! I hope you will like the prizes of FlexiSpot.\n\nPrizes:\n\n**1x** [**FlexiSpot E7 standing desk with a bamboo top**](https://www.flexispot.com/flexispot-pro-standing-desk-e7)**($600)** \u00a0\n\n**1x** [**FlexiSpot OC14 Ergonomic Chair**](https://www.flexispot.com/ergonomic-chair-pro)**($599.99)** \u00a0\n\nWe will give away our prizes\u00a0based on the total number of entrants when this campaign ends. For every 500\u00a0entrants above, we will give away one more E7 standing desk(up to 2x E7 standing desk). For instance, we will give away 2x E7 standing desk\u00a0when we reach 1000 entrants.\n\n&amp;#x200B;\n\n**How to Enter?**\n\nSimply answer this question:\n\nWhich giveaway(FlexiSpot E7 or FlexiSpot OC14)\u00a0would you like? And why you choose it\n\n&amp;#x200B;\n\n**\\[Ends on Feb 24th at 11:59 PM PST, 2023\\]**\n\nWinners will be selected randomly and announced on Feb 24th\u00a0in this thread. If you do want to secure one, [**FlexiSpot is on sale to celebrate the President\u2019s Day**](https://www.flexispot.com/)**, up to 40% off for standing desk and chair.** If the winner happens to be a customer of the FlexiSpot\u00a0President's Day sale, **I will offer double the order amount refund.**\n\n**Region**\n\nOnly in USA and Canada\n\nMany thanks for the Mod Team's support! Feel free to ask me anything or leave a message at r/flexispot_official(a sub to discuss the ergonomic products and get the opportunity to try new ergonomic products). Good Luck Everyone!", "author_fullname": "t2_njye31ba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[GIVEAWAY] FlexiSpot Standing Desk and Ergonomic Chair [US and Canada]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117pqr9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676940163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;I am so glad to run this giveaway in the &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;. Also, thanks to the mod team for helping me to organize the amazing campaign! I hope you will like the prizes of FlexiSpot.&lt;/h2&gt;\n\n&lt;p&gt;Prizes:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1x&lt;/strong&gt; &lt;a href=\"https://www.flexispot.com/flexispot-pro-standing-desk-e7\"&gt;&lt;strong&gt;FlexiSpot E7 standing desk with a bamboo top&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;($600)&lt;/strong&gt; \u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1x&lt;/strong&gt; &lt;a href=\"https://www.flexispot.com/ergonomic-chair-pro\"&gt;&lt;strong&gt;FlexiSpot OC14 Ergonomic Chair&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;($599.99)&lt;/strong&gt; \u00a0&lt;/p&gt;\n\n&lt;p&gt;We will give away our prizes\u00a0based on the total number of entrants when this campaign ends. For every 500\u00a0entrants above, we will give away one more E7 standing desk(up to 2x E7 standing desk). For instance, we will give away 2x E7 standing desk\u00a0when we reach 1000 entrants.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to Enter?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Simply answer this question:&lt;/p&gt;\n\n&lt;p&gt;Which giveaway(FlexiSpot E7 or FlexiSpot OC14)\u00a0would you like? And why you choose it&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;[Ends on Feb 24th at 11:59 PM PST, 2023]&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Winners will be selected randomly and announced on Feb 24th\u00a0in this thread. If you do want to secure one, &lt;a href=\"https://www.flexispot.com/\"&gt;&lt;strong&gt;FlexiSpot is on sale to celebrate the President\u2019s Day&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;, up to 40% off for standing desk and chair.&lt;/strong&gt; If the winner happens to be a customer of the FlexiSpot\u00a0President&amp;#39;s Day sale, &lt;strong&gt;I will offer double the order amount refund.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Region&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Only in USA and Canada&lt;/p&gt;\n\n&lt;p&gt;Many thanks for the Mod Team&amp;#39;s support! Feel free to ask me anything or leave a message at &lt;a href=\"/r/flexispot_official\"&gt;r/flexispot_official&lt;/a&gt;(a sub to discuss the ergonomic products and get the opportunity to try new ergonomic products). Good Luck Everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "117pqr9", "is_robot_indexable": true, "report_reasons": null, "author": "Ramzes888", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117pqr9/giveaway_flexispot_standing_desk_and_ergonomic/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/117pqr9/giveaway_flexispot_standing_desk_and_ergonomic/", "subreddit_subscribers": 670647, "created_utc": 1676940163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\n\nSo I have these messages within the chrome console logs in developer tools and there\u2019s at least 3000 of them but everytime I wanna save one of them I have to right click on one and hit \u201csave as\u201d, is there a tool or another way I could just save them all at once?\n\n[https://linuxhint.com/chrome-dev-tools/](https://linuxhint.com/chrome-dev-tools/)", "author_fullname": "t2_7dvw50ti", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save console input (chrome)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117oeqr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676936555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have these messages within the chrome console logs in developer tools and there\u2019s at least 3000 of them but everytime I wanna save one of them I have to right click on one and hit \u201csave as\u201d, is there a tool or another way I could just save them all at once?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://linuxhint.com/chrome-dev-tools/\"&gt;https://linuxhint.com/chrome-dev-tools/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117oeqr", "is_robot_indexable": true, "report_reasons": null, "author": "JohnCoss192", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117oeqr/save_console_input_chrome/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117oeqr/save_console_input_chrome/", "subreddit_subscribers": 670647, "created_utc": 1676936555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I noticed on my 14TB WD Elements external HDD (which has about 12.7 TB of space total) that the total \"used space\" under the drive properties (12.00 TB or 13,203,715,649,536 bytes) is higher than the total used space calculated by adding the \"size on disk\" number under folder properties of the two parent folders on the HDD (11.97 TB or 13,178,469,289,984 bytes). This seems to be about a 25GB difference. The HDD does contain about 29,000 folders according to the properties, but I thought the size of folders would be included in the \"size on disk\" for the two parent folders. Does anyone know what the reason for this is? In other words, what is this extra \\~25 GB of data and where is it?", "author_fullname": "t2_caggo8i8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about discrepancies between total \"used space\" and \"size on disk\" numbers on 14TB external HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117ntey", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676935052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed on my 14TB WD Elements external HDD (which has about 12.7 TB of space total) that the total &amp;quot;used space&amp;quot; under the drive properties (12.00 TB or 13,203,715,649,536 bytes) is higher than the total used space calculated by adding the &amp;quot;size on disk&amp;quot; number under folder properties of the two parent folders on the HDD (11.97 TB or 13,178,469,289,984 bytes). This seems to be about a 25GB difference. The HDD does contain about 29,000 folders according to the properties, but I thought the size of folders would be included in the &amp;quot;size on disk&amp;quot; for the two parent folders. Does anyone know what the reason for this is? In other words, what is this extra ~25 GB of data and where is it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117ntey", "is_robot_indexable": true, "report_reasons": null, "author": "Platographer", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117ntey/question_about_discrepancies_between_total_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117ntey/question_about_discrepancies_between_total_used/", "subreddit_subscribers": 670647, "created_utc": 1676935052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I have a Synology DS2415+ NAS that has 12 bays. It currently has 4 x 750GB drives with RAID 5. I use it for personal files, backups and as a media server.\n\nI recently came upon a lot of used disks of different sizes and I'm planning on using them on my NAS. Most of the disks are 500GB and 2 or 3 are 1TB. I have more than enough to fill up my NAS and more for swapping faulty drives.\n\nMy concern is that some of the drives may be of poor health, so I expect at least some of them to go bad sooner than later.\n\nMy question is, what type of RAID configuration should I use, so that I don't \"waste\" a ton of space due to the different sizes and I can handle 2 disk failures. Perhaps even have a hot-swappable configuration.\n\nThanks for any responses!", "author_fullname": "t2_dnsns", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID Suggestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117n9g7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676933690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a Synology DS2415+ NAS that has 12 bays. It currently has 4 x 750GB drives with RAID 5. I use it for personal files, backups and as a media server.&lt;/p&gt;\n\n&lt;p&gt;I recently came upon a lot of used disks of different sizes and I&amp;#39;m planning on using them on my NAS. Most of the disks are 500GB and 2 or 3 are 1TB. I have more than enough to fill up my NAS and more for swapping faulty drives.&lt;/p&gt;\n\n&lt;p&gt;My concern is that some of the drives may be of poor health, so I expect at least some of them to go bad sooner than later.&lt;/p&gt;\n\n&lt;p&gt;My question is, what type of RAID configuration should I use, so that I don&amp;#39;t &amp;quot;waste&amp;quot; a ton of space due to the different sizes and I can handle 2 disk failures. Perhaps even have a hot-swappable configuration.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any responses!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117n9g7", "is_robot_indexable": true, "report_reasons": null, "author": "panosttm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117n9g7/raid_suggestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117n9g7/raid_suggestion/", "subreddit_subscribers": 670647, "created_utc": 1676933690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "WD 20TB Elements Desktop External Hard Drive, USB 3.0 external hard drive for plug-and-play storage - WDBWLG0200HBK-NESN https://a.co/d/aX2NXmu\n\nI recently purchased two of these HDDs, one a 20TB drive and the other an 18TB drive. So far, I've only connected the 18TB drive, and it's performance has been lacking. When not being read or written to, the drive will spin down, and roughly 10 seconds later, it seems to enter a sort of sleep mode. When attempting to access the drive, it will take roughly 15 seconds, and when done, it'll happen all over again the next time. As you may imagine, it's incredibly frustrating, and I'm wondering if anyone else has experienced this and what they did to resolve it. Thank you.", "author_fullname": "t2_8wh3fdih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Having Difficulties w/ This HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117lp3k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676929939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;WD 20TB Elements Desktop External Hard Drive, USB 3.0 external hard drive for plug-and-play storage - WDBWLG0200HBK-NESN &lt;a href=\"https://a.co/d/aX2NXmu\"&gt;https://a.co/d/aX2NXmu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I recently purchased two of these HDDs, one a 20TB drive and the other an 18TB drive. So far, I&amp;#39;ve only connected the 18TB drive, and it&amp;#39;s performance has been lacking. When not being read or written to, the drive will spin down, and roughly 10 seconds later, it seems to enter a sort of sleep mode. When attempting to access the drive, it will take roughly 15 seconds, and when done, it&amp;#39;ll happen all over again the next time. As you may imagine, it&amp;#39;s incredibly frustrating, and I&amp;#39;m wondering if anyone else has experienced this and what they did to resolve it. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117lp3k", "is_robot_indexable": true, "report_reasons": null, "author": "TOGRiaDR", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117lp3k/having_difficulties_w_this_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117lp3k/having_difficulties_w_this_hdd/", "subreddit_subscribers": 670647, "created_utc": 1676929939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a set-up where I have an old Sony Handycam (Model TR57) plugged into the IN of an old VHS I got, then an OUT from the VHS to my PC using this DIGITNOW capture card: https://a.co/d/bj2citf. I use OBS to capture the footage and record it, and it works fine to pick-up both audio and video from the camcorder.\n\nThe current issue I'm having is that the video quality of the tapes starts to go bad about halfway to 3/4 way through the tapes. Looking at the camcorder viewfinder, the quality still looks the same there, but there's a sudden quality drop in what I'm getting on my PC on OBS. I thought maybe the VCR connections were bad, but I previously used it to convert a bunch of VHS and VHS-C tapes over, and there were no quality issues throughout; likewise, the capture card ran fine through all that so I'm certain it's not the card.\n\nThe issue seems to happen to all the tapes I've tried. Is it possible that the camcorder itself is at fault, with however it's playing the tapes back, but only on output and not in the camcorder? Cause I'm uncertain what's causing the issue.", "author_fullname": "t2_9nw2b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issue converting old Sony Hi8 MP tapes to PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117kyto", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676928207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a set-up where I have an old Sony Handycam (Model TR57) plugged into the IN of an old VHS I got, then an OUT from the VHS to my PC using this DIGITNOW capture card: &lt;a href=\"https://a.co/d/bj2citf\"&gt;https://a.co/d/bj2citf&lt;/a&gt;. I use OBS to capture the footage and record it, and it works fine to pick-up both audio and video from the camcorder.&lt;/p&gt;\n\n&lt;p&gt;The current issue I&amp;#39;m having is that the video quality of the tapes starts to go bad about halfway to 3/4 way through the tapes. Looking at the camcorder viewfinder, the quality still looks the same there, but there&amp;#39;s a sudden quality drop in what I&amp;#39;m getting on my PC on OBS. I thought maybe the VCR connections were bad, but I previously used it to convert a bunch of VHS and VHS-C tapes over, and there were no quality issues throughout; likewise, the capture card ran fine through all that so I&amp;#39;m certain it&amp;#39;s not the card.&lt;/p&gt;\n\n&lt;p&gt;The issue seems to happen to all the tapes I&amp;#39;ve tried. Is it possible that the camcorder itself is at fault, with however it&amp;#39;s playing the tapes back, but only on output and not in the camcorder? Cause I&amp;#39;m uncertain what&amp;#39;s causing the issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117kyto", "is_robot_indexable": true, "report_reasons": null, "author": "kaialb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117kyto/issue_converting_old_sony_hi8_mp_tapes_to_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117kyto/issue_converting_old_sony_hi8_mp_tapes_to_pc/", "subreddit_subscribers": 670647, "created_utc": 1676928207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys. \nI am lately got interested in preserving old materials. Old shows and games that might someone would look out for in the future. I am also trying to do some AI upscale of these stuff to maybe make them more presentable for current day viewing.   \n\n\nI tried hooking some of my old 5.25 drives and only one of them worked. I need more.   \nGoing external is the way to go from what I read around here in the subreddit.   \nThe question is what specs to go for? I maybe won't be able to buy exact models but still knowing some good ones will be a nice comparison point.   \nInformation about dvd drives are hard to find. I will not buy a blu-ray drive because they are way too expansive and old stuff are not coming in the form of blu-ray anyway.   \n\nEdit: The goal is taking old CD's and DVD's and turn them to digital files. \nThese discs contains mostly TV shows, Movies and Musics. But sometimes Live performance recordings and video games.\n\n1. What specs should I go for?   \n2. I will do both music and video burning. Can dvd burner drive do both without a loss of quality/performance?  \n3. Can quality be affected by the drive when I do the burning (Extraction)?\n4. Any specific models that I can use as a reference?\n5. Any other resource you think my help me out would be appreciated as well.\n6. How can I tell which burner has lock in them? I wasn't aware of this before reading here so I am not sure what is the way to verify before purchase.", "author_fullname": "t2_kpguk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for recommended specs to look out for when buying DVD burners/discs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117gtya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676937052.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676918491.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys. \nI am lately got interested in preserving old materials. Old shows and games that might someone would look out for in the future. I am also trying to do some AI upscale of these stuff to maybe make them more presentable for current day viewing.   &lt;/p&gt;\n\n&lt;p&gt;I tried hooking some of my old 5.25 drives and only one of them worked. I need more.&lt;br/&gt;\nGoing external is the way to go from what I read around here in the subreddit.&lt;br/&gt;\nThe question is what specs to go for? I maybe won&amp;#39;t be able to buy exact models but still knowing some good ones will be a nice comparison point.&lt;br/&gt;\nInformation about dvd drives are hard to find. I will not buy a blu-ray drive because they are way too expansive and old stuff are not coming in the form of blu-ray anyway.   &lt;/p&gt;\n\n&lt;p&gt;Edit: The goal is taking old CD&amp;#39;s and DVD&amp;#39;s and turn them to digital files. \nThese discs contains mostly TV shows, Movies and Musics. But sometimes Live performance recordings and video games.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What specs should I go for?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;I will do both music and video burning. Can dvd burner drive do both without a loss of quality/performance?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Can quality be affected by the drive when I do the burning (Extraction)?&lt;/li&gt;\n&lt;li&gt;Any specific models that I can use as a reference?&lt;/li&gt;\n&lt;li&gt;Any other resource you think my help me out would be appreciated as well.&lt;/li&gt;\n&lt;li&gt;How can I tell which burner has lock in them? I wasn&amp;#39;t aware of this before reading here so I am not sure what is the way to verify before purchase.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117gtya", "is_robot_indexable": true, "report_reasons": null, "author": "x1996x", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117gtya/looking_for_recommended_specs_to_look_out_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117gtya/looking_for_recommended_specs_to_look_out_for/", "subreddit_subscribers": 670647, "created_utc": 1676918491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What kind of storage medium should i use if the following aspects are important:\nIt needs to be reliable and durable    \nLarge capacity to price ratio\nDon't necessarily to be rewritable   \nCloud not an option because I don't want to store my stuff in someone else computer    \n    \n    \n    \n(I need to make backup some software and archive more than 2 terabyte of data, mostly movies and games)", "author_fullname": "t2_7572ek6v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommend me a storage medium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117mc5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676931477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What kind of storage medium should i use if the following aspects are important:\nIt needs to be reliable and durable&lt;br/&gt;\nLarge capacity to price ratio\nDon&amp;#39;t necessarily to be rewritable&lt;br/&gt;\nCloud not an option because I don&amp;#39;t want to store my stuff in someone else computer    &lt;/p&gt;\n\n&lt;p&gt;(I need to make backup some software and archive more than 2 terabyte of data, mostly movies and games)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117mc5b", "is_robot_indexable": true, "report_reasons": null, "author": "zsombor12312312312", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/117mc5b/recommend_me_a_storage_medium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117mc5b/recommend_me_a_storage_medium/", "subreddit_subscribers": 670647, "created_utc": 1676931477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is a good way / scraper to download tiktok live streams? Automated would be really nice but not necessary", "author_fullname": "t2_j48q8e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TikTok Live Scraper?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117knj5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676927460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is a good way / scraper to download tiktok live streams? Automated would be really nice but not necessary&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "117knj5", "is_robot_indexable": true, "report_reasons": null, "author": "RuiningSimps", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/117knj5/tiktok_live_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/117knj5/tiktok_live_scraper/", "subreddit_subscribers": 670647, "created_utc": 1676927460.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}