{"kind": "Listing", "data": {"after": "t3_1180hok", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please share what lakehouse stack do you use. \n\nFor example: \n\n* Storage: S3\n* File formats: parquet\n* Table formats: Iceberg\n* Realtime: Clickhouse\n* Modeling/Transformations: dbt, Spark\n* Orchestration: Airflow\n* Semantic layer: Cube\n* BI: Tableau\n* Data quality: Great expectations\n* Data catalog: Amundsen\n* ML: mlflow, kubeflow\n* Other: lakeFS", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What lakehouse stack do you use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117x39e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676969076.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676962861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please share what lakehouse stack do you use. &lt;/p&gt;\n\n&lt;p&gt;For example: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Storage: S3&lt;/li&gt;\n&lt;li&gt;File formats: parquet&lt;/li&gt;\n&lt;li&gt;Table formats: Iceberg&lt;/li&gt;\n&lt;li&gt;Realtime: Clickhouse&lt;/li&gt;\n&lt;li&gt;Modeling/Transformations: dbt, Spark&lt;/li&gt;\n&lt;li&gt;Orchestration: Airflow&lt;/li&gt;\n&lt;li&gt;Semantic layer: Cube&lt;/li&gt;\n&lt;li&gt;BI: Tableau&lt;/li&gt;\n&lt;li&gt;Data quality: Great expectations&lt;/li&gt;\n&lt;li&gt;Data catalog: Amundsen&lt;/li&gt;\n&lt;li&gt;ML: mlflow, kubeflow&lt;/li&gt;\n&lt;li&gt;Other: lakeFS&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117x39e", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117x39e/what_lakehouse_stack_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117x39e/what_lakehouse_stack_do_you_use/", "subreddit_subscribers": 90439, "created_utc": 1676962861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE fellows,\n\nI'm a Data Engineer for 1.5 years with a business background, I do ETLs mostly, query data from PostgreSQL server, do some calculation then load to another PostgreSQL table, automate using Airflow. I want to improve my skills so I can add to CVs and impress new employers, which one should I stay focus on for the next 6 months? Doing many DE side projects or get a Cloud certificate? If the answer is Cloud cert then which one is better, GCP or AWS?\n\nThank you for your advice.", "author_fullname": "t2_fgtgvlw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing DE side projects or getting Cloud certification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117tm6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676951407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE fellows,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a Data Engineer for 1.5 years with a business background, I do ETLs mostly, query data from PostgreSQL server, do some calculation then load to another PostgreSQL table, automate using Airflow. I want to improve my skills so I can add to CVs and impress new employers, which one should I stay focus on for the next 6 months? Doing many DE side projects or get a Cloud certificate? If the answer is Cloud cert then which one is better, GCP or AWS?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117tm6i", "is_robot_indexable": true, "report_reasons": null, "author": "J-Huynh", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117tm6i/doing_de_side_projects_or_getting_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117tm6i/doing_de_side_projects_or_getting_cloud/", "subreddit_subscribers": 90439, "created_utc": 1676951407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you handle schema evolution in your data lake files?\n\nLet us assume, I have a csv file landing in my data lake everyday named 'emi.csv'\n\nMy pipeline reads this file using spark, converts it into a dataframe and stores it into an external hive table. The hive external table is partitioned using 'delivery_date' column. So the file arriving on June 1, 2023 will be stored in partition 'delivery_date=20230601'.\n\nNow suddenly, on June 10, the file has the following changes: 2 new columns were added, 1 column was renamed and 1 column was deleted. How do I handle this schema evolution?\n\nShould I retire the old table and create a new table with the new schema?\n\nShould I add the 2 new columns to existing hive table, delete the existing column which was deleted in new file and rename the existing column was renamed?\n\n\nHow do you handle schema evolution such as above scenario in your data lake? Do you manually handle it or is there some automated response to handle it?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle schema evolution in your data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11868u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676993683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you handle schema evolution in your data lake files?&lt;/p&gt;\n\n&lt;p&gt;Let us assume, I have a csv file landing in my data lake everyday named &amp;#39;emi.csv&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;My pipeline reads this file using spark, converts it into a dataframe and stores it into an external hive table. The hive external table is partitioned using &amp;#39;delivery_date&amp;#39; column. So the file arriving on June 1, 2023 will be stored in partition &amp;#39;delivery_date=20230601&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Now suddenly, on June 10, the file has the following changes: 2 new columns were added, 1 column was renamed and 1 column was deleted. How do I handle this schema evolution?&lt;/p&gt;\n\n&lt;p&gt;Should I retire the old table and create a new table with the new schema?&lt;/p&gt;\n\n&lt;p&gt;Should I add the 2 new columns to existing hive table, delete the existing column which was deleted in new file and rename the existing column was renamed?&lt;/p&gt;\n\n&lt;p&gt;How do you handle schema evolution such as above scenario in your data lake? Do you manually handle it or is there some automated response to handle it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11868u6", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11868u6/how_do_you_handle_schema_evolution_in_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11868u6/how_do_you_handle_schema_evolution_in_your_data/", "subreddit_subscribers": 90439, "created_utc": 1676993683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started two weeks ago an internship in data engineering, but so far my manager is struggling to give me work to do. It seems like the team, him included, is overloaded and simply don't have the time for me. I have been asked to complete some courses on Coursera (Scala / spark), discover my environment etc, but it is becoming quite obvious that my manager is avoiding me because he has no work for me (I have already told him I have nothing left to do). What should I do ? I'm worried not to learn enough things to get a job after this internship. The project of my internship is running on spark and opensearch, but I can't do anything without help. What can I learn on my own online that will help me in the future ?", "author_fullname": "t2_qexqkrk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New intern, no work to to do, what should I do ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118005b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676974087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started two weeks ago an internship in data engineering, but so far my manager is struggling to give me work to do. It seems like the team, him included, is overloaded and simply don&amp;#39;t have the time for me. I have been asked to complete some courses on Coursera (Scala / spark), discover my environment etc, but it is becoming quite obvious that my manager is avoiding me because he has no work for me (I have already told him I have nothing left to do). What should I do ? I&amp;#39;m worried not to learn enough things to get a job after this internship. The project of my internship is running on spark and opensearch, but I can&amp;#39;t do anything without help. What can I learn on my own online that will help me in the future ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "118005b", "is_robot_indexable": true, "report_reasons": null, "author": "165817566995", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118005b/new_intern_no_work_to_to_do_what_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118005b/new_intern_no_work_to_to_do_what_should_i_do/", "subreddit_subscribers": 90439, "created_utc": 1676974087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I recently got back into data engineering after venturing off to do more traditional backend development for a couple years. My experience is as follows:\n\n2 years DE in the US at Fortune 500 company, 1 year as backend SWE at mid sized tech startup, and a little under a year at two very small startups in Korea (backend at 1st, DE at 2nd).\n\nLong story short, developer jobs are horrible here and I am looking at leaving my 2nd job in less than a year because I am not getting paid anymore. They ran out of money and I expect the startup to die soon.\n\nAt this point, I am open to basically any functional (stable, reasonable dev practices, reasonable expectations) job that is remote even for low pay. I took a 60% pay cut from my US job and then another 20% pay cut from my 1st Korea job in the hopes that the new job is better. I have basically lost all hope for a good job in Korea and my co-workers and developer friends here either say their job is not much better or it requires fluent Korean (or commuting 2+ hours away from where I am).\n\nThus I looked into global remote jobs and I have been having a difficult time there as well. My resume is basically the same as the one I've been using for years, just updated, and has given me great results over the years with high interview rates. Lately I have been getting extremely few call backs to the global remote jobs I apply to and the few times I do, it usually ends at the recruiter call as they missed something that is a hard blocker (time zone, not realizing I live in Korea, etc.) I haven't had good luck with the large foreign/international companies (Coupang, FB, G, A, etc.) here either as they seem to mostly hire seniors if not fluent in Korean. I didn't get any callbacks. But either way I think it would have been difficult as most do not operate remotely and I live a bit further from Seoul.\n\nThere also seems to be very little global remote DE jobs to start with. Mostly full stack, front end, or mobile developer jobs. I have searched through many different job sites and applied to anything I could see myself potentially working at. I'm at my wit's end here and it unfortunately looks like we have to leave Korea if I can't find a job by the end of spring. We originally moved here due to my spouse's job (which has been mostly great) but it's hard to justify staying in Korea for longer if I am going to be unemployed or working jobs I hate for very little pay.\n\nI also tried Upwork but it's not been going well and I'm competing against people who have cheaper rates than even minimum wage here. Doesn't seem like a great option.\n\nI asked my last company if I could be a contractor for them but they said they'd only be willing if I went through a contracting company they already contract with. No individual contractors. The company they work with is in the EU where I don't have a visa to work in so no dice there.\n\nIf anyone has any advice or leads on finding such jobs, please let me know. If needed I can try anonymizing my resume and sharing it but it'll take a little bit to edit.\n\nI'd be happy with any job that lets me work even half my hours in my time zone (4 hours on theirs) and pays at least 40k USD per year. I have experience with AWS/GCP and worked a good amount on cloud native products. Experienced with Python/JS/SQL and some experience with Spark. I had to use a variety of tech stacks and tools throughout my jobs so I'm pretty good at picking up something new and getting productive quick.", "author_fullname": "t2_116gfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difficulty finding global remote DE job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117xfdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676964105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I recently got back into data engineering after venturing off to do more traditional backend development for a couple years. My experience is as follows:&lt;/p&gt;\n\n&lt;p&gt;2 years DE in the US at Fortune 500 company, 1 year as backend SWE at mid sized tech startup, and a little under a year at two very small startups in Korea (backend at 1st, DE at 2nd).&lt;/p&gt;\n\n&lt;p&gt;Long story short, developer jobs are horrible here and I am looking at leaving my 2nd job in less than a year because I am not getting paid anymore. They ran out of money and I expect the startup to die soon.&lt;/p&gt;\n\n&lt;p&gt;At this point, I am open to basically any functional (stable, reasonable dev practices, reasonable expectations) job that is remote even for low pay. I took a 60% pay cut from my US job and then another 20% pay cut from my 1st Korea job in the hopes that the new job is better. I have basically lost all hope for a good job in Korea and my co-workers and developer friends here either say their job is not much better or it requires fluent Korean (or commuting 2+ hours away from where I am).&lt;/p&gt;\n\n&lt;p&gt;Thus I looked into global remote jobs and I have been having a difficult time there as well. My resume is basically the same as the one I&amp;#39;ve been using for years, just updated, and has given me great results over the years with high interview rates. Lately I have been getting extremely few call backs to the global remote jobs I apply to and the few times I do, it usually ends at the recruiter call as they missed something that is a hard blocker (time zone, not realizing I live in Korea, etc.) I haven&amp;#39;t had good luck with the large foreign/international companies (Coupang, FB, G, A, etc.) here either as they seem to mostly hire seniors if not fluent in Korean. I didn&amp;#39;t get any callbacks. But either way I think it would have been difficult as most do not operate remotely and I live a bit further from Seoul.&lt;/p&gt;\n\n&lt;p&gt;There also seems to be very little global remote DE jobs to start with. Mostly full stack, front end, or mobile developer jobs. I have searched through many different job sites and applied to anything I could see myself potentially working at. I&amp;#39;m at my wit&amp;#39;s end here and it unfortunately looks like we have to leave Korea if I can&amp;#39;t find a job by the end of spring. We originally moved here due to my spouse&amp;#39;s job (which has been mostly great) but it&amp;#39;s hard to justify staying in Korea for longer if I am going to be unemployed or working jobs I hate for very little pay.&lt;/p&gt;\n\n&lt;p&gt;I also tried Upwork but it&amp;#39;s not been going well and I&amp;#39;m competing against people who have cheaper rates than even minimum wage here. Doesn&amp;#39;t seem like a great option.&lt;/p&gt;\n\n&lt;p&gt;I asked my last company if I could be a contractor for them but they said they&amp;#39;d only be willing if I went through a contracting company they already contract with. No individual contractors. The company they work with is in the EU where I don&amp;#39;t have a visa to work in so no dice there.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any advice or leads on finding such jobs, please let me know. If needed I can try anonymizing my resume and sharing it but it&amp;#39;ll take a little bit to edit.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be happy with any job that lets me work even half my hours in my time zone (4 hours on theirs) and pays at least 40k USD per year. I have experience with AWS/GCP and worked a good amount on cloud native products. Experienced with Python/JS/SQL and some experience with Spark. I had to use a variety of tech stacks and tools throughout my jobs so I&amp;#39;m pretty good at picking up something new and getting productive quick.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117xfdg", "is_robot_indexable": true, "report_reasons": null, "author": "rohrohroh", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117xfdg/difficulty_finding_global_remote_de_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117xfdg/difficulty_finding_global_remote_de_job/", "subreddit_subscribers": 90439, "created_utc": 1676964105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, recently I got the feel that I'm not learning the core skills of a DE. I've been working for a Fintech company for about 1 year now.  I've mainly been working with migrating old logics into our new data warehouse and write some SQL queries for some adhoc requests. Occasionally I get the chance to refactor some Airflow DAGs and write some custom Python packages that we use internally. Data modeling haven't been the focus during the migration. We have now started to planning on data modeling works, but it's always down prioritized by other requests from the business, so I haven't had the chance to learn data modeling. Since I came from an mechanical engineering background, I found myself not so interested in the financial data either...\n\nRecently we are setting up some pubsub pipelines and I would really like to learn how to set up on the infrastructure's side, ie. setting up kinesis/sqs and etc. Besides that, we are also planning for a redesign for our DBT setup. Which I think is a lot more interesting than creating tables for some adhoc requests and validating the migrated codes. But we have a dedicated team for those tasks and due to the ongoing migration project I don't really have time to dive into those parts that are outside of my main area.\n\nI really feel that I'm not learning as much as I should for the past year or am I just too impatient? Should I move on and look for other opportunities?\n\nAny input is highly appreciated, thanks in advance :)", "author_fullname": "t2_uvw997aw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feeling a bit stagnant as jr DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1180y2c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676977711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, recently I got the feel that I&amp;#39;m not learning the core skills of a DE. I&amp;#39;ve been working for a Fintech company for about 1 year now.  I&amp;#39;ve mainly been working with migrating old logics into our new data warehouse and write some SQL queries for some adhoc requests. Occasionally I get the chance to refactor some Airflow DAGs and write some custom Python packages that we use internally. Data modeling haven&amp;#39;t been the focus during the migration. We have now started to planning on data modeling works, but it&amp;#39;s always down prioritized by other requests from the business, so I haven&amp;#39;t had the chance to learn data modeling. Since I came from an mechanical engineering background, I found myself not so interested in the financial data either...&lt;/p&gt;\n\n&lt;p&gt;Recently we are setting up some pubsub pipelines and I would really like to learn how to set up on the infrastructure&amp;#39;s side, ie. setting up kinesis/sqs and etc. Besides that, we are also planning for a redesign for our DBT setup. Which I think is a lot more interesting than creating tables for some adhoc requests and validating the migrated codes. But we have a dedicated team for those tasks and due to the ongoing migration project I don&amp;#39;t really have time to dive into those parts that are outside of my main area.&lt;/p&gt;\n\n&lt;p&gt;I really feel that I&amp;#39;m not learning as much as I should for the past year or am I just too impatient? Should I move on and look for other opportunities?&lt;/p&gt;\n\n&lt;p&gt;Any input is highly appreciated, thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1180y2c", "is_robot_indexable": true, "report_reasons": null, "author": "RawPotatoMan1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1180y2c/feeling_a_bit_stagnant_as_jr_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1180y2c/feeling_a_bit_stagnant_as_jr_de/", "subreddit_subscribers": 90439, "created_utc": 1676977711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm a second year student and I have a question relating to Data Engineer track. Does OOP and Design pattern knowledge is really necesssary for a DE? \nSince next semester I have a program elective course (which will teach in-depth OOP and design pattern), I'm really confused to choose or not choose this course.\nHere is the course info:\n\"This course provides students with an advanced understanding of software\ndevelopment with an emphasis on architecture and design and how this\nrelates to programming and implementation. This course builds upon\nexisting programming knowledge using Java as the implementation platform\nand assumes students already have a basic understanding of Java\nProgramming and basic OO concepts. Students will explore advanced OOP\nconcepts and the relationship between design in UML and its expression in code\nand how this is supported by modelling tools and development platforms\"\n\f \nMany thanks", "author_fullname": "t2_7g9xr3h3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OOP and Design pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117z5ii", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676970781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m a second year student and I have a question relating to Data Engineer track. Does OOP and Design pattern knowledge is really necesssary for a DE? \nSince next semester I have a program elective course (which will teach in-depth OOP and design pattern), I&amp;#39;m really confused to choose or not choose this course.\nHere is the course info:\n&amp;quot;This course provides students with an advanced understanding of software\ndevelopment with an emphasis on architecture and design and how this\nrelates to programming and implementation. This course builds upon\nexisting programming knowledge using Java as the implementation platform\nand assumes students already have a basic understanding of Java\nProgramming and basic OO concepts. Students will explore advanced OOP\nconcepts and the relationship between design in UML and its expression in code\nand how this is supported by modelling tools and development platforms&amp;quot;\n \nMany thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117z5ii", "is_robot_indexable": true, "report_reasons": null, "author": "GLizard0611", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117z5ii/oop_and_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117z5ii/oop_and_design_pattern/", "subreddit_subscribers": 90439, "created_utc": 1676970781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to learn an open source data ingestion tool and use it in some personal projects, primary I want to ingest data from APIs and postgres and orchestrte this with airflow. I can't seem to make up my mind on which tool to pick, please help", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source ingestion tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117r4gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676944062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to learn an open source data ingestion tool and use it in some personal projects, primary I want to ingest data from APIs and postgres and orchestrte this with airflow. I can&amp;#39;t seem to make up my mind on which tool to pick, please help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117r4gd", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117r4gd/open_source_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117r4gd/open_source_ingestion_tool/", "subreddit_subscribers": 90439, "created_utc": 1676944062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, looking to get some insight between 2 ELT strategies on micro-batched data. If you extract micro-batches from a source (1 min intervals) and load the batch as a new parquet file in an object store (ADLS), the next obvious task is to process that data (as soon as a new file lands) and load it into a data warehouse for business reporting needs. The micro-batches are pretty small, no more than 50-100ish records each minute.\n\nOne way to process the data is to do a full history transformation, where as soon as the new files land, the entire history of data in the lake is processed and the data in the warehouse is completely replaced. However, this seems very excessive to do every minute, and doesn't even seem like a logical solution?\n\nThe other (and maybe more obvious) is to process just the new files that are landed in the lake every minute.  This way, we are just processing and inserting only new records into the existing warehouse tables.\n\nHowever, this confuses me - what if there is business logic that is being applied to the data before loading into the warehouse? If there is a change in business requirements that alters the transformation step, wouldn't the existing data in the warehouse be irrelevant, since the data is not affected by the change? At least with a full-history transformation, then all the data would affected by the change in requirements.\n\nHow do you guys handle process like this, while considering the need to future proof the system?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Transformation on full history or just new data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117norj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676934719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, looking to get some insight between 2 ELT strategies on micro-batched data. If you extract micro-batches from a source (1 min intervals) and load the batch as a new parquet file in an object store (ADLS), the next obvious task is to process that data (as soon as a new file lands) and load it into a data warehouse for business reporting needs. The micro-batches are pretty small, no more than 50-100ish records each minute.&lt;/p&gt;\n\n&lt;p&gt;One way to process the data is to do a full history transformation, where as soon as the new files land, the entire history of data in the lake is processed and the data in the warehouse is completely replaced. However, this seems very excessive to do every minute, and doesn&amp;#39;t even seem like a logical solution?&lt;/p&gt;\n\n&lt;p&gt;The other (and maybe more obvious) is to process just the new files that are landed in the lake every minute.  This way, we are just processing and inserting only new records into the existing warehouse tables.&lt;/p&gt;\n\n&lt;p&gt;However, this confuses me - what if there is business logic that is being applied to the data before loading into the warehouse? If there is a change in business requirements that alters the transformation step, wouldn&amp;#39;t the existing data in the warehouse be irrelevant, since the data is not affected by the change? At least with a full-history transformation, then all the data would affected by the change in requirements.&lt;/p&gt;\n\n&lt;p&gt;How do you guys handle process like this, while considering the need to future proof the system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117norj", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117norj/data_transformation_on_full_history_or_just_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117norj/data_transformation_on_full_history_or_just_new/", "subreddit_subscribers": 90439, "created_utc": 1676934719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The project aims to demonstrate how to work with real-time data using Kafka, KSQL, Elasticsearch, and Flask. It shows how to perform joins on Kafka topics, ingest data into Elasticsearch using Kafka Connect, and build a REST API to provide real-time metrics to end-users. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3\n\n&amp;#x200B;\n\n[https://medium.com/@stefentaime\\_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78](https://medium.com/@stefentaime_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Data Processing and Analysis with Kafka, Connect, KSQL, Elasticsearch, and Flask", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"rqhxu6fggkja1": {"status": "valid", "e": "Image", "m": "image/png", "o": [{"y": 500, "x": 500, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=320&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=353fc995873e21bf781c2b6d6850a1101b8ad0e2"}], "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8bb4d8d78959ebafa56a7d316cf8a10b643f3765"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efb1b74b01612aa5d3d5f62ff845b1a08fca4c51"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e603d2799cda843b1fa195212b9445b1952aff16"}], "s": {"y": 500, "x": 500, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3"}, "id": "rqhxu6fggkja1"}}, "name": "t3_1187yvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1676996049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The project aims to demonstrate how to work with real-time data using Kafka, KSQL, Elasticsearch, and Flask. It shows how to perform joins on Kafka topics, ingest data into Elasticsearch using Kafka Connect, and build a REST API to provide real-time metrics to end-users. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3\"&gt;https://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78\"&gt;https://medium.com/@stefentaime_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?auto=webp&amp;v=enabled&amp;s=a9005179359307951a1e81d401ea5152ea3d1551", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61c9ac705a57f544b2d6d23928c46a99c340d9a1", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a25d5b750580dc67c058b057a7bb0f59ef9eb1e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eed4e3a9f5d7e40c79d6eff16e1bba7455f9d1c6", "width": 320, "height": 320}], "variants": {"obfuscated": {"source": {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b5425ca7f6718912fbefda88e725d875323bc17c", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6c2513d5eb275c57cb4ac6ef8b5168986ca25c11", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=10ea348e2b0c52689eba2a3d5030cbaab1527c4a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c694b19d9c601a47cf58445e69e2c643fd3603b9", "width": 320, "height": 320}]}}, "id": "SaJ6-HMH0_rD4sVnfWReybYIyaTg716qwqx1Cd1dAYY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1187yvr", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1187yvr/realtime_data_processing_and_analysis_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1187yvr/realtime_data_processing_and_analysis_with_kafka/", "subreddit_subscribers": 90439, "created_utc": 1676996049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our Product Manager keeps telling us we\u2019re not abiding by true agile, and we need to change. We build data pipelines. Data starts at one or multiple points, and the business wants it modelled and sent to a destination (an API for instance).\n\nWhen the business defines their needs, and then we build the pipeline, can someone explain to me where Agile comes into play? I get the building in increments, which makes sense. But given a complex pipeline that feeds to an API that then validates the data, I don\u2019t see where agile fits. The data either makes it through and gets validated or it doesn\u2019t. The business rules that make it useful are either implemented, or the data is not useful.\n\nCan someone please help me understand? Can agile work in a data pipeline team?\n\nEdit: reworded for clarity.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Agile with Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117y5gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676969302.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676966887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our Product Manager keeps telling us we\u2019re not abiding by true agile, and we need to change. We build data pipelines. Data starts at one or multiple points, and the business wants it modelled and sent to a destination (an API for instance).&lt;/p&gt;\n\n&lt;p&gt;When the business defines their needs, and then we build the pipeline, can someone explain to me where Agile comes into play? I get the building in increments, which makes sense. But given a complex pipeline that feeds to an API that then validates the data, I don\u2019t see where agile fits. The data either makes it through and gets validated or it doesn\u2019t. The business rules that make it useful are either implemented, or the data is not useful.&lt;/p&gt;\n\n&lt;p&gt;Can someone please help me understand? Can agile work in a data pipeline team?&lt;/p&gt;\n\n&lt;p&gt;Edit: reworded for clarity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117y5gj", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117y5gj/agile_with_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117y5gj/agile_with_data_pipelines/", "subreddit_subscribers": 90439, "created_utc": 1676966887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the last couple of years I've been quite typical full-stack dev, with a bit more accent on the backend side. Lot of java, node.js, on frontend some jquery, react, plus some work around infra (deployments on gitlab, dockerizations, etc.).  \nIn last two years I was more drawn into DE - I was picked for a project of building up internal data analytics system in our company. Lots of AWS work, DBs (Redshift, DynamoDB, relational DBs), moving tons data between AWS components and learning on the road what proper DE is.\n\nI'm sure a lot of people went down the similar route. How do you find such transition - do you prefer more DE work to typical backend/full-stack work?    \n\n\nPersonally I like now flexibility in my daily work and playing with a lot of tools but on other hand I miss sometimes focusing  more on a good coding in a single app, rather than jumping between smaller apps and scripts and working on a setting proper tooling (which seems to me a core of DE work)", "author_fullname": "t2_vyrlctas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from classic software engineering into DE - opinions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117lwqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676930438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the last couple of years I&amp;#39;ve been quite typical full-stack dev, with a bit more accent on the backend side. Lot of java, node.js, on frontend some jquery, react, plus some work around infra (deployments on gitlab, dockerizations, etc.).&lt;br/&gt;\nIn last two years I was more drawn into DE - I was picked for a project of building up internal data analytics system in our company. Lots of AWS work, DBs (Redshift, DynamoDB, relational DBs), moving tons data between AWS components and learning on the road what proper DE is.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure a lot of people went down the similar route. How do you find such transition - do you prefer more DE work to typical backend/full-stack work?    &lt;/p&gt;\n\n&lt;p&gt;Personally I like now flexibility in my daily work and playing with a lot of tools but on other hand I miss sometimes focusing  more on a good coding in a single app, rather than jumping between smaller apps and scripts and working on a setting proper tooling (which seems to me a core of DE work)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117lwqw", "is_robot_indexable": true, "report_reasons": null, "author": "IronEider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117lwqw/transition_from_classic_software_engineering_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117lwqw/transition_from_classic_software_engineering_into/", "subreddit_subscribers": 90439, "created_utc": 1676930438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_27c3plee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog post: Metadata-driven pipelines in Azure Data Factory | Part 4 - Analytical Processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_117jyag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zpy9fyhlUkl11M_XfA6lhAEKh4-fOg0k0ZlLQvSCMzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676925802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanrg.blogspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?auto=webp&amp;v=enabled&amp;s=acf6b162314a17c0afbe67b2c22c8ab66e5c7332", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=976c7f9865030935e649d015169ac172cc6f5e6c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85dd501dc2bb75e6a72a1a225332106070cbcc91", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=212afa4d32af2f1746ea2623c764edf1f12a0a68", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57d89326fd6c1b4391a282e522ca8df8fc10add5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cc3e3e5d8e02be53ed74cabf9db3c4e2e051c9f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33a493bbc6aef040b48a9a0e128e6d65f2e0c3e0", "width": 1080, "height": 567}], "variants": {}, "id": "_z5Q-MwAZcOLYV1qeyPkktnXQXLmlsXbk1lpBU4Gubg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "117jyag", "is_robot_indexable": true, "report_reasons": null, "author": "RayisImayev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117jyag/blog_post_metadatadriven_pipelines_in_azure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "subreddit_subscribers": 90439, "created_utc": 1676925802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI\u2019ve been working on a side project that does some basic web scraping of fantasy basketball projections into a Postgres DB, using dbt for further modeling and airflow for orchestrating the whole thing. I currently have these services dockerized and since I don\u2019t have any cloud experience on my job, I thought it would be interesting to try messing around with AWS and see if I could get it deployed on a small EC2 instance.\n\nA friend of mine (software engineer) told me it would be way more worthwhile if I instead start up a raw Ubuntu server on a DigitalOcean droplet and set everything up from scratch on there, saying AWS would be a breeze to use once I am able to do that.\n\nI know he is a more traditional software engineer, but wanted to hear anyone\u2019s thoughts if this would be true since I technically don\u2019t need S3 or any additional AWS services for this project. I feel that it is important to get experience with AWS for future positions, so I guess I\u2019m curious to see if I should try deploying to DigitalOcean for practice and then scrap that/redeploy the project to AWS from there, or if I should just dive straight into AWS?", "author_fullname": "t2_bwp6e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DigitalOcean vs AWS for Side Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1185npa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676992269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working on a side project that does some basic web scraping of fantasy basketball projections into a Postgres DB, using dbt for further modeling and airflow for orchestrating the whole thing. I currently have these services dockerized and since I don\u2019t have any cloud experience on my job, I thought it would be interesting to try messing around with AWS and see if I could get it deployed on a small EC2 instance.&lt;/p&gt;\n\n&lt;p&gt;A friend of mine (software engineer) told me it would be way more worthwhile if I instead start up a raw Ubuntu server on a DigitalOcean droplet and set everything up from scratch on there, saying AWS would be a breeze to use once I am able to do that.&lt;/p&gt;\n\n&lt;p&gt;I know he is a more traditional software engineer, but wanted to hear anyone\u2019s thoughts if this would be true since I technically don\u2019t need S3 or any additional AWS services for this project. I feel that it is important to get experience with AWS for future positions, so I guess I\u2019m curious to see if I should try deploying to DigitalOcean for practice and then scrap that/redeploy the project to AWS from there, or if I should just dive straight into AWS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1185npa", "is_robot_indexable": true, "report_reasons": null, "author": "wild_bill34", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1185npa/digitalocean_vs_aws_for_side_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1185npa/digitalocean_vs_aws_for_side_project/", "subreddit_subscribers": 90439, "created_utc": 1676992269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, i am new to reddit stuff. This is the 1dt que. Pardon me if i have violated something.\n\nMy question:- I am in mid of doing post graduation in data science from good reputed institute. Now I have already finished course on Data Tools and machine learning. I like technical things but really shit scared with hardcore programming stuff. I have gone through python basics with pandas numpy and Visualization libraries but i enjoy SQL more and feel little more gripped doing SQL than python. My institute offers specialisations and i have 3 option 1: Data engineering, 2:- Data analyst 3:- business analyst. I am not from Tech background. I am self employed and into health and wellness. but after having a kid i pushed myself for academics. Now I am confused. Any idea at what depth Coding will be required in data engineering as I want to do that but what if Coding is too in depth and throw me out of the bus???", "author_fullname": "t2_b66mo7az", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data engineering or Business analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117z50r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676974500.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676970717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, i am new to reddit stuff. This is the 1dt que. Pardon me if i have violated something.&lt;/p&gt;\n\n&lt;p&gt;My question:- I am in mid of doing post graduation in data science from good reputed institute. Now I have already finished course on Data Tools and machine learning. I like technical things but really shit scared with hardcore programming stuff. I have gone through python basics with pandas numpy and Visualization libraries but i enjoy SQL more and feel little more gripped doing SQL than python. My institute offers specialisations and i have 3 option 1: Data engineering, 2:- Data analyst 3:- business analyst. I am not from Tech background. I am self employed and into health and wellness. but after having a kid i pushed myself for academics. Now I am confused. Any idea at what depth Coding will be required in data engineering as I want to do that but what if Coding is too in depth and throw me out of the bus???&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117z50r", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Play6458", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117z50r/data_engineering_or_business_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117z50r/data_engineering_or_business_analyst/", "subreddit_subscribers": 90439, "created_utc": 1676970717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My startup has built a knowledge management SaaS product, and I\u2019ve been thinking of launching our backend platform as a separate API service. \n\nIt would be like Hasura for unstructured data.\n\nWould this API be useful for any applications you\u2019re building?  Trying to evaluate if there\u2019s a market need.\n\n\ud83d\udca1 It would work like this:\n\nYou can upload any file type, like PDFs, videos, images or even CAD drawings.  Or, you can create \u201cfeeds\u201d from places like Google Drive or an S3 bucket.\n\nWe run ML on the files to extract embedded data, like references to companies, places, topics, or even visual objects.  Audio and video files will generate searchable transcripts. ML is configurable, if you have an endpoint for your own model.\n\nYou can configure enrichment of that data to provide more context, like layering in Crunchbase data for companies or LinkedIn for people.\n\nThen, you can use our GraphQL API to query the extracted data: filter by properties, and similarity search by time, geolocation or full text.  Also it supports ChatGPT-like Q&amp;A over the data.  \n\nWe are also considering adding data publishing to deliver the extracted data to other data pipelines, if you wouldn\u2019t want the GraphQL API.", "author_fullname": "t2_15wnt5aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Files -&gt; ML -&gt; GraphQL API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117qdy0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676949971.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676941961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My startup has built a knowledge management SaaS product, and I\u2019ve been thinking of launching our backend platform as a separate API service. &lt;/p&gt;\n\n&lt;p&gt;It would be like Hasura for unstructured data.&lt;/p&gt;\n\n&lt;p&gt;Would this API be useful for any applications you\u2019re building?  Trying to evaluate if there\u2019s a market need.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udca1 It would work like this:&lt;/p&gt;\n\n&lt;p&gt;You can upload any file type, like PDFs, videos, images or even CAD drawings.  Or, you can create \u201cfeeds\u201d from places like Google Drive or an S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;We run ML on the files to extract embedded data, like references to companies, places, topics, or even visual objects.  Audio and video files will generate searchable transcripts. ML is configurable, if you have an endpoint for your own model.&lt;/p&gt;\n\n&lt;p&gt;You can configure enrichment of that data to provide more context, like layering in Crunchbase data for companies or LinkedIn for people.&lt;/p&gt;\n\n&lt;p&gt;Then, you can use our GraphQL API to query the extracted data: filter by properties, and similarity search by time, geolocation or full text.  Also it supports ChatGPT-like Q&amp;amp;A over the data.  &lt;/p&gt;\n\n&lt;p&gt;We are also considering adding data publishing to deliver the extracted data to other data pipelines, if you wouldn\u2019t want the GraphQL API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117qdy0", "is_robot_indexable": true, "report_reasons": null, "author": "DeadPukka", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117qdy0/files_ml_graphql_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117qdy0/files_ml_graphql_api/", "subreddit_subscribers": 90439, "created_utc": 1676941961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Primarily 15-445/645 - INTRO TO DATABASE SYSTEMS \n\nI came across it on Youtube and I guess it must be interesting for people who work with relational DBs a lot (esp. OLTP systems). The syllabus mostly focuses on internals of an abstract database like Buffer Pool, Query Optimizer, Concurrency Control, ACID Transactions etc.   \n\n\nAside from that they have an active community on Discord and all coding assignments are available to the general public for submission and auto grading.  \n   \nHas anyone taken this course recently? Would be great to hear your thoughts on it.", "author_fullname": "t2_ntpsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on MOOC CMU DB courses by Andy Pavlo?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_118ccej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677004072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Primarily 15-445/645 - INTRO TO DATABASE SYSTEMS &lt;/p&gt;\n\n&lt;p&gt;I came across it on Youtube and I guess it must be interesting for people who work with relational DBs a lot (esp. OLTP systems). The syllabus mostly focuses on internals of an abstract database like Buffer Pool, Query Optimizer, Concurrency Control, ACID Transactions etc.   &lt;/p&gt;\n\n&lt;p&gt;Aside from that they have an active community on Discord and all coding assignments are available to the general public for submission and auto grading.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone taken this course recently? Would be great to hear your thoughts on it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118ccej", "is_robot_indexable": true, "report_reasons": null, "author": "DCman1993", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ccej/what_are_your_thoughts_on_mooc_cmu_db_courses_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118ccej/what_are_your_thoughts_on_mooc_cmu_db_courses_by/", "subreddit_subscribers": 90439, "created_utc": 1677004072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data source that's only accessible through rest API calls and I want to do some light transformation on the data before dumping into databricks table. One way I am thinking is to do this in python notebook and just schedule this notebook as part of the automation pipeline.\n\nIs this reasonable?  what downside is there to it?   I am asking because I haven't seen any tutorial suggesting loading data this way. All the info I see online is basically loading data from another storage infra like SQL table, kafka, or S3 for example.", "author_fullname": "t2_1qgl3bp9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to ingest data from API to databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_118cb99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677003996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data source that&amp;#39;s only accessible through rest API calls and I want to do some light transformation on the data before dumping into databricks table. One way I am thinking is to do this in python notebook and just schedule this notebook as part of the automation pipeline.&lt;/p&gt;\n\n&lt;p&gt;Is this reasonable?  what downside is there to it?   I am asking because I haven&amp;#39;t seen any tutorial suggesting loading data this way. All the info I see online is basically loading data from another storage infra like SQL table, kafka, or S3 for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118cb99", "is_robot_indexable": true, "report_reasons": null, "author": "jakebigman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118cb99/best_way_to_ingest_data_from_api_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118cb99/best_way_to_ingest_data_from_api_to_databricks/", "subreddit_subscribers": 90439, "created_utc": 1677003996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some common challenges in scaling machine learning systems?", "author_fullname": "t2_9u34jt5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some common challenges in scaling machine learning systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_118c3li", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677003474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common challenges in scaling machine learning systems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "118c3li", "is_robot_indexable": true, "report_reasons": null, "author": "Nice-Tomorrow2926", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118c3li/what_are_some_common_challenges_in_scaling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118c3li/what_are_some_common_challenges_in_scaling/", "subreddit_subscribers": 90439, "created_utc": 1677003474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like this is becoming more prevalent in my organization where some new vendor comes in and promises the moon to replace an on-premises system. \n\nThe problem is that although these new SaaS solutions offer a lot of flexibility, what appears to be secondary is their ability to provide access to the data. Most vendors I've spoken with almost appear a little surprised that it's even a requirement, and they show off their pie charts indicating that reporting and analytics can be done within platform just fine. This typically falls into one of three categories:\n\n&amp;#x200B;\n\n1. They can offer access to the system's data, but it's through a REST API. The problem with REST APIs is I feel like we don't have enough control over the data we receive (i.e. if a new attribute is added, or the hierarchy of an item changes, or if their pagination goes wonky.) \n2. They can offer access, but it's all NoSQL, which isn't as conducive as a relational database.\n3. They just can't offer access to the data.\n\nMy questions are:\n\n1. How do you convince business that having access to their own data is in their self interest.\n2. How important has it been to your organization's decision making on vendor selection.\n3. How much of a problem has this been for your team(s).", "author_fullname": "t2_ry4ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deal with SaaS solutions and getting access to data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_118bj0p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677002104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like this is becoming more prevalent in my organization where some new vendor comes in and promises the moon to replace an on-premises system. &lt;/p&gt;\n\n&lt;p&gt;The problem is that although these new SaaS solutions offer a lot of flexibility, what appears to be secondary is their ability to provide access to the data. Most vendors I&amp;#39;ve spoken with almost appear a little surprised that it&amp;#39;s even a requirement, and they show off their pie charts indicating that reporting and analytics can be done within platform just fine. This typically falls into one of three categories:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;They can offer access to the system&amp;#39;s data, but it&amp;#39;s through a REST API. The problem with REST APIs is I feel like we don&amp;#39;t have enough control over the data we receive (i.e. if a new attribute is added, or the hierarchy of an item changes, or if their pagination goes wonky.) &lt;/li&gt;\n&lt;li&gt;They can offer access, but it&amp;#39;s all NoSQL, which isn&amp;#39;t as conducive as a relational database.&lt;/li&gt;\n&lt;li&gt;They just can&amp;#39;t offer access to the data.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you convince business that having access to their own data is in their self interest.&lt;/li&gt;\n&lt;li&gt;How important has it been to your organization&amp;#39;s decision making on vendor selection.&lt;/li&gt;\n&lt;li&gt;How much of a problem has this been for your team(s).&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118bj0p", "is_robot_indexable": true, "report_reasons": null, "author": "vincentx99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118bj0p/how_do_you_deal_with_saas_solutions_and_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118bj0p/how_do_you_deal_with_saas_solutions_and_getting/", "subreddit_subscribers": 90439, "created_utc": 1677002104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Newbie here. Please be gentle.\n\nIn order to gain a understanding of the tables and their contents in our company, I have implemented one of the existing [data discovery platforms](https://github.com/opendatadiscovery/awesome-data-catalogs) (in my case [Amundsen](https://www.amundsen.io/)). Unfortunately, Amundsen can only display the tables it has access to.\n\nWhat I am looking for is a solution (similar to Amundsen or [Datahub](https://datahubproject.io/)) that also allows to add tables and their metadata manually.\n\nI would be glad for any kind of help.", "author_fullname": "t2_762zbgq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an \"offline\" data discovery platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_118b8f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677001910.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677001374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newbie here. Please be gentle.&lt;/p&gt;\n\n&lt;p&gt;In order to gain a understanding of the tables and their contents in our company, I have implemented one of the existing &lt;a href=\"https://github.com/opendatadiscovery/awesome-data-catalogs\"&gt;data discovery platforms&lt;/a&gt; (in my case &lt;a href=\"https://www.amundsen.io/\"&gt;Amundsen&lt;/a&gt;). Unfortunately, Amundsen can only display the tables it has access to.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is a solution (similar to Amundsen or &lt;a href=\"https://datahubproject.io/\"&gt;Datahub&lt;/a&gt;) that also allows to add tables and their metadata manually.&lt;/p&gt;\n\n&lt;p&gt;I would be glad for any kind of help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?auto=webp&amp;v=enabled&amp;s=92b9653a9cf2ce71110179b3709e066b6cb0eefe", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7fd913fec3e90e5089dbba711b2a6daef030c296", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d1f64f9bba16400bfd2ae22d7ba430051dcf64e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1360b393dd90be71a2aef6091457a269419d8932", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0174751cc19e56081b7b5bf498af3780218f962", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeacb5dc6f37022e5ba8e3396a20513f609fb209", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8176f19e4228c9e5ce1add94e90d83919af5a556", "width": 1080, "height": 540}], "variants": {}, "id": "-EHj7XvhM7vQSeFel5uITngODMjFcKcpaC6j9DVqNZ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118b8f7", "is_robot_indexable": true, "report_reasons": null, "author": "rocket9001", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118b8f7/looking_for_an_offline_data_discovery_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118b8f7/looking_for_an_offline_data_discovery_platform/", "subreddit_subscribers": 90439, "created_utc": 1677001374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everybody, \ud83d\udc4b\ud83d\udc4b\n\nWe are really excited to open source YoBulk today!\n\nYoBulk is an **open-source CSV importer** for any SaaS application - It's a free alternative to [https://flatfile.com/](https://flatfile.com/)\n\nWe realized that more than 70% of business data is shared in CSV and Excel formats, and only a small percentage use API integrations for data exchange. As developers and product managers, we have experienced the difficulties of building a scalable CSV importer, and we know that many others face the same challenges. Our goal is to solve this problem by taking an open-source AI and developer-centric approach\n\n**Who can use YoBulk:**\n\nYoBulk is a highly beneficial tool for a variety of professionals, such as Developers, Product Managers, Customer Success teams, and Marketers. It simplifies the process of onboarding and verifying customer data, making it an indispensable asset for those who deal with frequent CSV data uploads to a system with a predetermined schema or template.\n\nThis tool is particularly valuable for updating sales CRM or product catalog data, and it effectively solves the initial challenge of customer data ingestion.\n\n&amp;#x200B;\n\n# The Problem:\n\n**Importing a CSV is a really hard problem to solve. Some of the key problems are:**\n\n**1. Missing Collaboration and Automation in CSV importing workflow:** In a usual situation, the customer success team responsible for receiving CSV data has to engage in extensive back-and-forth communication with the customer to address unintentional manual errors present in a CSV.\n\n**2. Scale:** CRM CSV files can sometimes reach sizes as large as 4 GB, making it nearly impossible to open them on a standalone machine for data correction. This presents a significant challenge for small businesses that cannot afford to invest in big data technologies such as EMR, Databrick, and ETL tools to address CSV import scaling problems.\n\n**3. Countless complex validation Types:** single date format can have as many as 100 different variations, such as dd-mm-yyyy, mm-dd-yyyy, and dd.mm.yyyy. Manually setting validation rules for each of these formats is almost impossible, and correcting errors manually will be difficult.\n\n**4. Data mapping issues**: In a typical scenario, the recipient of CSV data provides a template to the data donor and creates a CSV column for template mapping before importing\n\n**5. Data Security and Privacy**: It is always risky to share your customer data with third-party companies for data cleaning purposes.\n\n**6. Non-availability of low code/No code tool:** Product managers and customer success teams, who are typically no-code users, often rely on data analysts to create a programmed CSV template with validation rules, which must be shared with customers to receive CSV data in a specific format.\n\n**7.** **Vague error messages:** Unclear error messages do not provide users with enough context to confidently resolve their issues before uploading their data.\n\n&amp;#x200B;\n\n# How YoBulk helps address the above issues :\n\n\ud83d\ude80 **Smart Spreadsheet View:** Designed to be a data exchange hub for any business that utilizes CSV files, YoBulk makes it easy to import and transform any CSV into a smart spreadsheet interface. This user-friendly interface highlights errors in a clear, concise manner, simplifying the task of cleaning data.\n\n[Spreadsheet view](https://preview.redd.it/mifmavo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8cfd6f44d3372f019e550c0547d3d7d11c0361d2)\n\n\ud83d\ude80 **Bring your validation function:** YoBulk offers a platform for Developers to create a custom CSV importer that includes personalized **validation rules based on JSON schema.** With this functionality, developers can design an importer that meets their specific needs and preferences.\n\n[JSON Validator](https://preview.redd.it/42dh3wo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e0b34c17390cc1b4fc9fe6124608f2ad1c035d9b)\n\n\ud83d\ude80 **AI first:** YoBulk harnesses the power of OpenAI to provide advanced column matching, data cleaning, and JSON schema generation features.\n\n[Power of AI](https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=cd76a30304fe90365aa0c6b3fde38c1abb40f592)\n\n\ud83d\ude80 **Build for Scale:** YoBulk is designed for large-scale CSV validation, with the ability to process files in the gigabyte range without any glitches or errors.\n\n&amp;#x200B;\n\n\ud83d\ude80 **Embeddable:** Take advantage of YoBulk's customizable import button feature, which can be embedded in any SaaS or App. This allows you to receive CSV data in the exact format you require, streamlining your workflows.\n\n[Template builder](https://preview.redd.it/8bexoro5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0ce1e1b95cd5374551b110962b4e58d98c5636f3)\n\n[Embed with ease](https://preview.redd.it/he2d1vo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ee65ea7b06c5e59c751168980096cf7fa650ba16)\n\n&amp;#x200B;\n\n# Quick Demo\n\n[String validation](https://reddit.com/link/118ait5/video/d9xo4n5frkja1/player)\n\n[GPT](https://reddit.com/link/118ait5/video/xwtzbi5frkja1/player)\n\n&amp;#x200B;\n\n**Hosting and Deployment:**\n\nYoBulk can be self-hosted and currently running on Mongo.\n\nGithub: git clone [git@github.com](mailto:git@github.com):yobulkdev/yobulkdev.git\n\n**Getting started is really simple :**\n\nPlease refer [https://doc.yobulk.dev/GetStarted/Installation](https://doc.yobulk.dev/GetStarted/Installation)\n\nDocker command:\n\n&gt;git clone [https://github.com/yobulkdev/yobulkdev.git](https://github.com/yobulkdev/yobulkdev.git)  \ncd yobulkdev  \ndocker-compose up -d\n\nOr\n\n&gt;docker run --rm -it -p 5050:5050/tcp yobulk/yobulk\n\nOr\n\n&gt;git clone [https://github.com/yobulkdev/yobulkdev](https://github.com/yobulkdev/yobulkdev)  \ncd yobulkdev  \nyarn install  \nyarn run dev\n\n&amp;#x200B;\n\n**Also please join our community at :**\n\n**\ud83d\udce3** Github: [https://github.com/yobulkdev/yobulkdev](https://github.com/yobulkdev/yobulkdev)\n\n**\ud83d\udce3** Slack: [https://join.slack.com/t/yobulkdev/signup](https://join.slack.com/t/yobulkdev/signup).\n\n**\ud83d\udce3** Twitter: [https://twitter.com/YoBulkDev](https://twitter.com/YoBulkDev)\n\n**\ud83d\udce3** Reditt: [https://reddit.com/r/YoBulk](https://reddit.com/r/YoBulk)\n\nWould love to hear your feedback &amp; how we can make this better.\n\nThank you,\n\n**Team YoBulk**", "author_fullname": "t2_e5nsso8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YoBulk: Open Source CSV importer powered by GPT3 (Free flatfile.com alternative)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zmrmpuo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ada2ab8b2e6d085ababc8828403e887bb18bae7b"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f3b690eea82cabdbb5d794c2793a8b7fb3dfa61"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e94190fc78c28583766c6584d3372ad7b03ec34"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=309adf6955fbe3ace7552c99f4b9a613ccbd4e3b"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2953550dcbe89fa6e27636983f3f8fb3142f7d4"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47850b45caf01a142637bd9ad00350f3b37c017b"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=cd76a30304fe90365aa0c6b3fde38c1abb40f592"}, "id": "zmrmpuo5rkja1"}, "d9xo4n5frkja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/118ait5/asset/d9xo4n5frkja1/DASHPlaylist.mpd?a=1679600290%2CZGIyZDRmOTk5YmUwMjY0NjYxMzNmOTYyODMxMjUxOWFjN2MxMTgwOWIzMGRmYjcxOGNjNTdmMjY5OTEyMjI3Yw%3D%3D&amp;v=1&amp;f=sd", "x": 1728, "y": 1080, "hlsUrl": "https://v.redd.it/link/118ait5/asset/d9xo4n5frkja1/HLSPlaylist.m3u8?a=1679600290%2CNTE2YTE3MDQ0MjQyMmI0NDZiMjkwYmIzM2ZjMTczNDI0YzZiYjgzY2VmY2QwNzM2ZTM2MDNiNDljYTFlNjRiMg%3D%3D&amp;v=1&amp;f=sd", "id": "d9xo4n5frkja1", "isGif": false}, "42dh3wo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0af9167ec55bf0baad399a747a0d465d8283115a"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e60d32fa66e854044bfff093f9731809a22c30d4"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38a91c124a622d0e4621247d298a7ba0dee8cf85"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c33b70f2d81dd3c8cbbaf26312a96a9117299132"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0b501331d1219e12ae1920b9e21757d2ea0f895"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd0d3e37b6f67186d1083a0c452276007b059408"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e0b34c17390cc1b4fc9fe6124608f2ad1c035d9b"}, "id": "42dh3wo5rkja1"}, "xwtzbi5frkja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/118ait5/asset/xwtzbi5frkja1/DASHPlaylist.mpd?a=1679600290%2CYTJmN2VmMTNiYWNkNzdhMmI4ZWZlMGMwZWE3YzdkY2EyNjQ5OWYzNjBjMDJlNGIyYmY2Y2M1NTk2OTFlYTlmYw%3D%3D&amp;v=1&amp;f=sd", "x": 1728, "y": 1080, "hlsUrl": "https://v.redd.it/link/118ait5/asset/xwtzbi5frkja1/HLSPlaylist.m3u8?a=1679600290%2CNjAwODVjYjk1YTBlMDc5MTQwZTNmY2JlZTM4ZDI3ZmRkMjBjOGJiNDQ0NjZmNzBhZjY5OTI4Njk0YzEwZjM4NQ%3D%3D&amp;v=1&amp;f=sd", "id": "xwtzbi5frkja1", "isGif": false}, "8bexoro5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6583ec18f72a4fe01169498cd426e662cc8adb9"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a39e8977e7e8408d6ba7ab9da312df2e3f443bc1"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5e23fc922a349c4056fc34cbdd3d0ff36f3abea"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5417eba5b12dd839173efc659626be255bb19fc"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5b41dee24fae39aa49388080ad10ee9dfefd351"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95b0bb98d03d577a3c547768c0cc5c821ee99753"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0ce1e1b95cd5374551b110962b4e58d98c5636f3"}, "id": "8bexoro5rkja1"}, "he2d1vo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58ef5edbd3e2b87ed54fec3db7939037be4e1fc3"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb96c9e9322a19b2852b0d8ff5e00118f9fb7ddf"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f79ec7aa6f0ab4cec02043c2bbfb66c2ac110c0"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d94cea3b2010a42479a7d7190c1a465407cb936"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=100c7fce3e59826e596e38f99c01b04f84c2aee2"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac6b580c10b22f983a865565d6c285d79091b155"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ee65ea7b06c5e59c751168980096cf7fa650ba16"}, "id": "he2d1vo5rkja1"}, "mifmavo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb00214ed2915a5c84e002b77d5b3871a6438317"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=165de6dbe0e08cef82fe0e8d7acd918826a75924"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5afe85e8962c35347ba8b088f5b33228a94356e7"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d855503bd8046c45520eeed710bbd1a73a5f647f"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6045f17645f8d666c44166a0224f204265c9657"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3585ce13cc1c47d7ae0ddf9b3180c10141f17821"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8cfd6f44d3372f019e550c0547d3d7d11c0361d2"}, "id": "mifmavo5rkja1"}}, "name": "t3_118ait5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/k50OptU2Vj9lv7XzmTuRSwWc6hnyQKNXFIhjf2uGr2U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1676999778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everybody, \ud83d\udc4b\ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;We are really excited to open source YoBulk today!&lt;/p&gt;\n\n&lt;p&gt;YoBulk is an &lt;strong&gt;open-source CSV importer&lt;/strong&gt; for any SaaS application - It&amp;#39;s a free alternative to &lt;a href=\"https://flatfile.com/\"&gt;https://flatfile.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We realized that more than 70% of business data is shared in CSV and Excel formats, and only a small percentage use API integrations for data exchange. As developers and product managers, we have experienced the difficulties of building a scalable CSV importer, and we know that many others face the same challenges. Our goal is to solve this problem by taking an open-source AI and developer-centric approach&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Who can use YoBulk:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;YoBulk is a highly beneficial tool for a variety of professionals, such as Developers, Product Managers, Customer Success teams, and Marketers. It simplifies the process of onboarding and verifying customer data, making it an indispensable asset for those who deal with frequent CSV data uploads to a system with a predetermined schema or template.&lt;/p&gt;\n\n&lt;p&gt;This tool is particularly valuable for updating sales CRM or product catalog data, and it effectively solves the initial challenge of customer data ingestion.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;The Problem:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Importing a CSV is a really hard problem to solve. Some of the key problems are:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Missing Collaboration and Automation in CSV importing workflow:&lt;/strong&gt; In a usual situation, the customer success team responsible for receiving CSV data has to engage in extensive back-and-forth communication with the customer to address unintentional manual errors present in a CSV.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Scale:&lt;/strong&gt; CRM CSV files can sometimes reach sizes as large as 4 GB, making it nearly impossible to open them on a standalone machine for data correction. This presents a significant challenge for small businesses that cannot afford to invest in big data technologies such as EMR, Databrick, and ETL tools to address CSV import scaling problems.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Countless complex validation Types:&lt;/strong&gt; single date format can have as many as 100 different variations, such as dd-mm-yyyy, mm-dd-yyyy, and dd.mm.yyyy. Manually setting validation rules for each of these formats is almost impossible, and correcting errors manually will be difficult.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Data mapping issues&lt;/strong&gt;: In a typical scenario, the recipient of CSV data provides a template to the data donor and creates a CSV column for template mapping before importing&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Data Security and Privacy&lt;/strong&gt;: It is always risky to share your customer data with third-party companies for data cleaning purposes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6. Non-availability of low code/No code tool:&lt;/strong&gt; Product managers and customer success teams, who are typically no-code users, often rely on data analysts to create a programmed CSV template with validation rules, which must be shared with customers to receive CSV data in a specific format.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; &lt;strong&gt;Vague error messages:&lt;/strong&gt; Unclear error messages do not provide users with enough context to confidently resolve their issues before uploading their data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;How YoBulk helps address the above issues :&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Smart Spreadsheet View:&lt;/strong&gt; Designed to be a data exchange hub for any business that utilizes CSV files, YoBulk makes it easy to import and transform any CSV into a smart spreadsheet interface. This user-friendly interface highlights errors in a clear, concise manner, simplifying the task of cleaning data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mifmavo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8cfd6f44d3372f019e550c0547d3d7d11c0361d2\"&gt;Spreadsheet view&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Bring your validation function:&lt;/strong&gt; YoBulk offers a platform for Developers to create a custom CSV importer that includes personalized &lt;strong&gt;validation rules based on JSON schema.&lt;/strong&gt; With this functionality, developers can design an importer that meets their specific needs and preferences.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/42dh3wo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e0b34c17390cc1b4fc9fe6124608f2ad1c035d9b\"&gt;JSON Validator&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;AI first:&lt;/strong&gt; YoBulk harnesses the power of OpenAI to provide advanced column matching, data cleaning, and JSON schema generation features.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cd76a30304fe90365aa0c6b3fde38c1abb40f592\"&gt;Power of AI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Build for Scale:&lt;/strong&gt; YoBulk is designed for large-scale CSV validation, with the ability to process files in the gigabyte range without any glitches or errors.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Embeddable:&lt;/strong&gt; Take advantage of YoBulk&amp;#39;s customizable import button feature, which can be embedded in any SaaS or App. This allows you to receive CSV data in the exact format you require, streamlining your workflows.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8bexoro5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0ce1e1b95cd5374551b110962b4e58d98c5636f3\"&gt;Template builder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/he2d1vo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ee65ea7b06c5e59c751168980096cf7fa650ba16\"&gt;Embed with ease&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;Quick Demo&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/118ait5/video/d9xo4n5frkja1/player\"&gt;String validation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/118ait5/video/xwtzbi5frkja1/player\"&gt;GPT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hosting and Deployment:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;YoBulk can be self-hosted and currently running on Mongo.&lt;/p&gt;\n\n&lt;p&gt;Github: git clone [&lt;a href=\"mailto:git@github.com\"&gt;git@github.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:git@github.com\"&gt;git@github.com&lt;/a&gt;):yobulkdev/yobulkdev.git&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Getting started is really simple :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Please refer &lt;a href=\"https://doc.yobulk.dev/GetStarted/Installation\"&gt;https://doc.yobulk.dev/GetStarted/Installation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Docker command:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;git clone &lt;a href=\"https://github.com/yobulkdev/yobulkdev.git\"&gt;https://github.com/yobulkdev/yobulkdev.git&lt;/a&gt;&lt;br/&gt;\ncd yobulkdev&lt;br/&gt;\ndocker-compose up -d&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;docker run --rm -it -p 5050:5050/tcp yobulk/yobulk&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;git clone &lt;a href=\"https://github.com/yobulkdev/yobulkdev\"&gt;https://github.com/yobulkdev/yobulkdev&lt;/a&gt;&lt;br/&gt;\ncd yobulkdev&lt;br/&gt;\nyarn install&lt;br/&gt;\nyarn run dev&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Also please join our community at :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Github: &lt;a href=\"https://github.com/yobulkdev/yobulkdev\"&gt;https://github.com/yobulkdev/yobulkdev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Slack: &lt;a href=\"https://join.slack.com/t/yobulkdev/signup\"&gt;https://join.slack.com/t/yobulkdev/signup&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Twitter: &lt;a href=\"https://twitter.com/YoBulkDev\"&gt;https://twitter.com/YoBulkDev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Reditt: &lt;a href=\"https://reddit.com/r/YoBulk\"&gt;https://reddit.com/r/YoBulk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your feedback &amp;amp; how we can make this better.&lt;/p&gt;\n\n&lt;p&gt;Thank you,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Team YoBulk&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?auto=webp&amp;v=enabled&amp;s=37744d5163d376765ca256576a6e8dc8b5b5b0a3", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a265b5f1ef8b75d86212284540a796f627e0cb7e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be2a8811203462e9b854ea10eae0c3386197c6e0", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c303167c7987abf079d24bb474f344e98b9dbc1", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=105c05bb236804bed5cc2c8679ac7a623e51ce9b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e10c99c20fdabf0612f26c5d81a3f253f32e0ba", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4675912b97773b7899a78db7d1119eabc9ac6e1b", "width": 1080, "height": 564}], "variants": {}, "id": "Ok4CUIAgW7uB_hl0dnfVegWrTM9wJo9ASZ1FvnRrJE8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "118ait5", "is_robot_indexable": true, "report_reasons": null, "author": "dstala", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ait5/yobulk_open_source_csv_importer_powered_by_gpt3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118ait5/yobulk_open_source_csv_importer_powered_by_gpt3/", "subreddit_subscribers": 90439, "created_utc": 1676999778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please help, getting this error when using Delta \"replace where\" SQL (not Python):\n\n    ParseException:  mismatched input 'replace' expecting {'(', 'DESC', 'DESCRIBE', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES', 'WITH'}(line 1, pos 72)  \n    == SQL == insert into table delta.`/path-to-table/` replace where replace_key in ('a', 'b') select * from vwt_df\n\nI am using this guide (sql version): [https://docs.databricks.com/delta/selective-overwrite.html#language-sql](https://docs.databricks.com/delta/selective-overwrite.html#language-sql)\n\nP.S. I am trying to perform selective overwrite.", "author_fullname": "t2_5u5812sd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake Replace Where SQL Clause Error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118a58k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676999106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please help, getting this error when using Delta &amp;quot;replace where&amp;quot; SQL (not Python):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ParseException:  mismatched input &amp;#39;replace&amp;#39; expecting {&amp;#39;(&amp;#39;, &amp;#39;DESC&amp;#39;, &amp;#39;DESCRIBE&amp;#39;, &amp;#39;FROM&amp;#39;, &amp;#39;MAP&amp;#39;, &amp;#39;REDUCE&amp;#39;, &amp;#39;SELECT&amp;#39;, &amp;#39;TABLE&amp;#39;, &amp;#39;VALUES&amp;#39;, &amp;#39;WITH&amp;#39;}(line 1, pos 72)  \n== SQL == insert into table delta.`/path-to-table/` replace where replace_key in (&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;) select * from vwt_df\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am using this guide (sql version): &lt;a href=\"https://docs.databricks.com/delta/selective-overwrite.html#language-sql\"&gt;https://docs.databricks.com/delta/selective-overwrite.html#language-sql&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;P.S. I am trying to perform selective overwrite.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "118a58k", "is_robot_indexable": true, "report_reasons": null, "author": "mike_bds", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118a58k/delta_lake_replace_where_sql_clause_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118a58k/delta_lake_replace_where_sql_clause_error/", "subreddit_subscribers": 90439, "created_utc": 1676999106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am currently in the process of designing a new ETL framework from scratch using Apache Airflow. For the purpose of this post, I will only talk about a specific type of process which does extract. In general the sequence is:\n\n1. Execute Query against MySQL Database\n2. Save Query Results to S3\n3. Drop and Create Athena Table, Perform Maintenance on Table\n\nThis sequence would have to be repeated for X extract jobs with different queries. The idea is to have several of these sequences running in parallel.\n\n**Option 1**: Design a DAG template which performs the sequences where each step is a different task. Have one centralized DAG which calls each DAG in parallel and manages the overall orchestration of the other dags.\n\n**Option 2**: Design a DAG generator which places all processes in a single DAG, together with Operators for parallelizing the different tasks, resulting in one big DAG.\n\nI am currently not sure which option to go for, and I was wondering if the community has any ideas. Feel free to offer pros/cons, suggestions or alternatives.", "author_fullname": "t2_j3gqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Discussion: Several DAGs vs Several Tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11895rk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676997699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently in the process of designing a new ETL framework from scratch using Apache Airflow. For the purpose of this post, I will only talk about a specific type of process which does extract. In general the sequence is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Execute Query against MySQL Database&lt;/li&gt;\n&lt;li&gt;Save Query Results to S3&lt;/li&gt;\n&lt;li&gt;Drop and Create Athena Table, Perform Maintenance on Table&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This sequence would have to be repeated for X extract jobs with different queries. The idea is to have several of these sequences running in parallel.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;: Design a DAG template which performs the sequences where each step is a different task. Have one centralized DAG which calls each DAG in parallel and manages the overall orchestration of the other dags.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt;: Design a DAG generator which places all processes in a single DAG, together with Operators for parallelizing the different tasks, resulting in one big DAG.&lt;/p&gt;\n\n&lt;p&gt;I am currently not sure which option to go for, and I was wondering if the community has any ideas. Feel free to offer pros/cons, suggestions or alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11895rk", "is_robot_indexable": true, "report_reasons": null, "author": "exact-approximate", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11895rk/airflow_discussion_several_dags_vs_several_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11895rk/airflow_discussion_several_dags_vs_several_tasks/", "subreddit_subscribers": 90439, "created_utc": 1676997699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nour company has a DWH which still runs on a AWS RDS Postgres Instance (t3.2xlarge, gp3, &gt;1TB storage)\n\nThere are plans to migrate to Snowflake pretty soon, however at the moment all of the pipelines at night completely drain the EBS Bytes Balance, leading to very long running queries.\n\nAs a short term solution, to which different instance type could we migrate? Memory Optimized? I have read, that burstable instances like t3 are not very suitable for that.\n\n&amp;#x200B;\n\nCheers,\n\nMatt", "author_fullname": "t2_5o9ebpsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS RDS Postgres Bursting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1180hok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676976012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;our company has a DWH which still runs on a AWS RDS Postgres Instance (t3.2xlarge, gp3, &amp;gt;1TB storage)&lt;/p&gt;\n\n&lt;p&gt;There are plans to migrate to Snowflake pretty soon, however at the moment all of the pipelines at night completely drain the EBS Bytes Balance, leading to very long running queries.&lt;/p&gt;\n\n&lt;p&gt;As a short term solution, to which different instance type could we migrate? Memory Optimized? I have read, that burstable instances like t3 are not very suitable for that.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Matt&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1180hok", "is_robot_indexable": true, "report_reasons": null, "author": "mosquitsch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1180hok/aws_rds_postgres_bursting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1180hok/aws_rds_postgres_bursting/", "subreddit_subscribers": 90439, "created_utc": 1676976012.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}