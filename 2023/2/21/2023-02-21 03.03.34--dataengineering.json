{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I'd say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!", "author_fullname": "t2_7qam9xn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No coding. Is it bad for career", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1171ps4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676882464.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676882269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I&amp;#39;d say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1171ps4", "is_robot_indexable": true, "report_reasons": null, "author": "ApprehensiveIce792", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1171ps4/no_coding_is_it_bad_for_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1171ps4/no_coding_is_it_bad_for_career/", "subreddit_subscribers": 90352, "created_utc": 1676882269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.\n\nRecently I've cleared DP-203 and received the Data Engineer Associate certificate. I [shared a post on here](https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&amp;utm_medium=android_app&amp;utm_name=androidcss&amp;utm_term=1&amp;utm_content=share_button) as well.\n\nI prepared some notes on Notion while preparing for the certification. And I'd like to share it with others so that It could help others while doing revision for the exam.\n\nNotes link: [dp203-azure-data-engineering-notes](https://github.com/jithendray/dp203-azure-data-engineering).\n\nTips that helped me:\n\n- I did a decent course on the Udemy. \n- Made notes while watching tbe last lecture videos.\n- The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.\n- Finally, revised the notes that I made a day before the exam.\n\nAll the best, for anyone who is preparing for the exam. Feel free to add \u2b50 to my repo ;)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got certified recently and prepared some notes while preparing for Azure DP-203", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117djjy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676912073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.&lt;/p&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve cleared DP-203 and received the Data Engineer Associate certificate. I &lt;a href=\"https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&amp;amp;utm_medium=android_app&amp;amp;utm_name=androidcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;shared a post on here&lt;/a&gt; as well.&lt;/p&gt;\n\n&lt;p&gt;I prepared some notes on Notion while preparing for the certification. And I&amp;#39;d like to share it with others so that It could help others while doing revision for the exam.&lt;/p&gt;\n\n&lt;p&gt;Notes link: &lt;a href=\"https://github.com/jithendray/dp203-azure-data-engineering\"&gt;dp203-azure-data-engineering-notes&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Tips that helped me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I did a decent course on the Udemy. &lt;/li&gt;\n&lt;li&gt;Made notes while watching tbe last lecture videos.&lt;/li&gt;\n&lt;li&gt;The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.&lt;/li&gt;\n&lt;li&gt;Finally, revised the notes that I made a day before the exam.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All the best, for anyone who is preparing for the exam. Feel free to add \u2b50 to my repo ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?auto=webp&amp;v=enabled&amp;s=09779b28cd66cfd5ff2aa08fc530f1fa0831fc49", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f769aa1c0a1f249552dbc00dfc441edfac9f2659", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9297afeb8d68686fb38859746cdecf9fc40943c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343a56e772e5d82dd7581f48f181b9440b5b5e3e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b29be5481f5b76e130a75650be4590f6e7b98f3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=598766feafb9e3390ec187a00834507d7d716873", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=553c08a009245ad293de85e9851266d1ff557d60", "width": 1080, "height": 540}], "variants": {}, "id": "4SDWg8PO0pIHRKtfw-MN6uvnndNQPozdBuf03lZJ82A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "117djjy", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/", "subreddit_subscribers": 90352, "created_utc": 1676912073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to pull data from multiple sql tables into a lake. The ingest should be scheduled every night and pull only new data (incremental load, e.g. from yesterday). Is there some magic offered by databricks to accomplish this? I read about the autoloader, but it seems that it only supports blob storage as a source.\n\nOutside of databricks I would go for a simple spark script, that connects via jdbc connector and pulls records that where created e.g. yesterday.\n\nHow would you solve this?", "author_fullname": "t2_kh9d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using databricks to incrementally load data from a sql server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1179krk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676906617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pull data from multiple sql tables into a lake. The ingest should be scheduled every night and pull only new data (incremental load, e.g. from yesterday). Is there some magic offered by databricks to accomplish this? I read about the autoloader, but it seems that it only supports blob storage as a source.&lt;/p&gt;\n\n&lt;p&gt;Outside of databricks I would go for a simple spark script, that connects via jdbc connector and pulls records that where created e.g. yesterday.&lt;/p&gt;\n\n&lt;p&gt;How would you solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1179krk", "is_robot_indexable": true, "report_reasons": null, "author": "artworkad", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1179krk/using_databricks_to_incrementally_load_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1179krk/using_databricks_to_incrementally_load_data_from/", "subreddit_subscribers": 90352, "created_utc": 1676906617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\nI'm not sure if this is the best place to ask - and I've searched, but my search-fu is possibly not as great as your search-fu and I'd be happy with a link?\n\nWe have an SSAS data warehouse we use for multiple Power BI models and our central Fact table (splitting into star schema) is starting to get very wide (80+) and is looking to get wider. On top of this it is very long, millions of records, basically think orders but with many checkpoints, rule checks, dates, values, pricing, etc, etc.\n\nI am looking at the best solution I can make to model this.  There are already 30+ relationships to Dimensions, all of the additional columns are values.\n\nAt this point if you're asking why don't we aggregate this data? Well, we kind of want to use it as a dimension, because we have many lower grain tables that we also want insight into and it's all I can do to keep things at the Order grain level.\n\nSo, should we keep this as a super-wide table (ultimately it could be 100's of columns), or what's the pain of segmenting it by the type or section of the company for the data and using the Order number to do a Fact-to-Fact link?\n\nIf we choose one solution or the other for the SSAS model, how should we approach the SQL layer - should we also have wide tables there or segment and join?\n\nPart of me keeps thinking that wide tables are the way to go, that JOINs cost, but I want to know how folks with much more experience than my incredibly limited view would approach this.", "author_fullname": "t2_erqka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dimensional modelling and Vertically Splitting Facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116zt9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676874935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if this is the best place to ask - and I&amp;#39;ve searched, but my search-fu is possibly not as great as your search-fu and I&amp;#39;d be happy with a link?&lt;/p&gt;\n\n&lt;p&gt;We have an SSAS data warehouse we use for multiple Power BI models and our central Fact table (splitting into star schema) is starting to get very wide (80+) and is looking to get wider. On top of this it is very long, millions of records, basically think orders but with many checkpoints, rule checks, dates, values, pricing, etc, etc.&lt;/p&gt;\n\n&lt;p&gt;I am looking at the best solution I can make to model this.  There are already 30+ relationships to Dimensions, all of the additional columns are values.&lt;/p&gt;\n\n&lt;p&gt;At this point if you&amp;#39;re asking why don&amp;#39;t we aggregate this data? Well, we kind of want to use it as a dimension, because we have many lower grain tables that we also want insight into and it&amp;#39;s all I can do to keep things at the Order grain level.&lt;/p&gt;\n\n&lt;p&gt;So, should we keep this as a super-wide table (ultimately it could be 100&amp;#39;s of columns), or what&amp;#39;s the pain of segmenting it by the type or section of the company for the data and using the Order number to do a Fact-to-Fact link?&lt;/p&gt;\n\n&lt;p&gt;If we choose one solution or the other for the SSAS model, how should we approach the SQL layer - should we also have wide tables there or segment and join?&lt;/p&gt;\n\n&lt;p&gt;Part of me keeps thinking that wide tables are the way to go, that JOINs cost, but I want to know how folks with much more experience than my incredibly limited view would approach this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116zt9p", "is_robot_indexable": true, "report_reasons": null, "author": "-crucible-", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116zt9p/dimensional_modelling_and_vertically_splitting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116zt9p/dimensional_modelling_and_vertically_splitting/", "subreddit_subscribers": 90352, "created_utc": 1676874935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in a startup and I am pushing to implement a separate data warehouse for analytics. But the problem is I am not sure what solution works best if our transactional data is stored in DynamoDB. \n\nThe tech stack right now is mostly for software engineers so orchestration and transformation tools are not present as well. Use cases are powering dashboards, making analytical queries, and process automations. \n\nDWHs I am considering are BigQuery, Snowflake, Databricks, and Redshift.\n\nFor Orchestration I was considering something along the lines of Dagster, GCP Dataflow, and Airflow.\n\nPlease feel free to suggest.", "author_fullname": "t2_7pbhfups", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Data Warehouse for a Startup if our transactions are stored in DynamoDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116yriu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676872480.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676871165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in a startup and I am pushing to implement a separate data warehouse for analytics. But the problem is I am not sure what solution works best if our transactional data is stored in DynamoDB. &lt;/p&gt;\n\n&lt;p&gt;The tech stack right now is mostly for software engineers so orchestration and transformation tools are not present as well. Use cases are powering dashboards, making analytical queries, and process automations. &lt;/p&gt;\n\n&lt;p&gt;DWHs I am considering are BigQuery, Snowflake, Databricks, and Redshift.&lt;/p&gt;\n\n&lt;p&gt;For Orchestration I was considering something along the lines of Dagster, GCP Dataflow, and Airflow.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116yriu", "is_robot_indexable": true, "report_reasons": null, "author": "eej107", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116yriu/good_data_warehouse_for_a_startup_if_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116yriu/good_data_warehouse_for_a_startup_if_our/", "subreddit_subscribers": 90352, "created_utc": 1676871165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jh2dwngo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coinbase CEO tells his team: APIs not meetings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_117bpnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KrVPNY4yYpPhHgV7eoocFPDxlsBNpArRv-P83VDJvWM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676909552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thestatuscode.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thestatuscode.co/p/coinbase-ceo-tells-his-team-apis", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?auto=webp&amp;v=enabled&amp;s=b974687b5992bd4d8567e839be2a08dad45c6d27", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a299fd9b173a81a1c00bdb256660aff7e4b22725", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92892193b0bcafaebf3dcf4d16ecaee107d9cd99", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=994c596fd68b034b12f8a431f3fd223a5c85a822", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2000e27d0c48fa8d29753f0839469134e4451b61", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2f7419c21fd8ff66fdeb1848255d96bf86e1934", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca770fe8e599d1d3bccb2c33af32059317c13eb1", "width": 1080, "height": 720}], "variants": {}, "id": "wS6NnZbV8bwcv_3l3IB3QdEQZAAi8NkDDtk88IIgrEw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117bpnt", "is_robot_indexable": true, "report_reasons": null, "author": "foundersblock", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117bpnt/coinbase_ceo_tells_his_team_apis_not_meetings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thestatuscode.co/p/coinbase-ceo-tells-his-team-apis", "subreddit_subscribers": 90352, "created_utc": 1676909552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Small and simple organizations have no need for a data mesh architecture. Huge organizations (potentially) benefit from data mesh concepts. What size should an organization be before they would even consider this socio-technical architecture? I'm thinking number of engineers, data teams, sources, consumers, etc.", "author_fullname": "t2_rmmatfaw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Size of organization for data mesh viability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116xnhg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676867426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Small and simple organizations have no need for a data mesh architecture. Huge organizations (potentially) benefit from data mesh concepts. What size should an organization be before they would even consider this socio-technical architecture? I&amp;#39;m thinking number of engineers, data teams, sources, consumers, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116xnhg", "is_robot_indexable": true, "report_reasons": null, "author": "realitydevice", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116xnhg/size_of_organization_for_data_mesh_viability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116xnhg/size_of_organization_for_data_mesh_viability/", "subreddit_subscribers": 90352, "created_utc": 1676867426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I apologize I don't really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don't fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I'm really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.", "author_fullname": "t2_4yh9ceg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would community-owned data centers help with the sovereignty of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wr4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676864449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize I don&amp;#39;t really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don&amp;#39;t fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I&amp;#39;m really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116wr4l", "is_robot_indexable": true, "report_reasons": null, "author": "proudmisfit", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116wr4l/would_communityowned_data_centers_help_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116wr4l/would_communityowned_data_centers_help_with_the/", "subreddit_subscribers": 90352, "created_utc": 1676864449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the best way to call pyspark on dbt run and create a table with the results of the pyspark's job?", "author_fullname": "t2_11bm9f4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to call pyspark on dbt run and create a table with the results?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116vkxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676860786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best way to call pyspark on dbt run and create a table with the results of the pyspark&amp;#39;s job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "116vkxc", "is_robot_indexable": true, "report_reasons": null, "author": "ar405", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116vkxc/how_to_call_pyspark_on_dbt_run_and_create_a_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116vkxc/how_to_call_pyspark_on_dbt_run_and_create_a_table/", "subreddit_subscribers": 90352, "created_utc": 1676860786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone had success with ELT tools or technologies that allowed them to source CDC changes from multiple (see: HUNDREDS) of source SQL databases for streaming said changes to a lake/warehouse? It would need to be robust enough that all these connectors could be scripted, especially as more, newly created Tenants are created. Any help or suggestions are appreciated -- thanks!", "author_fullname": "t2_5h5g8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best ELT tool for sourcing from numerous multi-tenant SQL databases for CDC changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117e7nx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676913058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone had success with ELT tools or technologies that allowed them to source CDC changes from multiple (see: HUNDREDS) of source SQL databases for streaming said changes to a lake/warehouse? It would need to be robust enough that all these connectors could be scripted, especially as more, newly created Tenants are created. Any help or suggestions are appreciated -- thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117e7nx", "is_robot_indexable": true, "report_reasons": null, "author": "Zilean", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117e7nx/best_elt_tool_for_sourcing_from_numerous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117e7nx/best_elt_tool_for_sourcing_from_numerous/", "subreddit_subscribers": 90352, "created_utc": 1676913058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\n\nI have a bi layer of data in my data lake that gets periodically updated and a list of tables defined in the thrift server that serves BI tools like tableau, Metabase etc over JDBC.\n\nWhen I update (overwrite) a table location, the definition is no longer valid because underlying files are being changed, is there a general way of solving this issue or I have to refresh the tables in thrift server manually every time?", "author_fullname": "t2_4clu4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark thrift server auto refresh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116wzxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676865266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I have a bi layer of data in my data lake that gets periodically updated and a list of tables defined in the thrift server that serves BI tools like tableau, Metabase etc over JDBC.&lt;/p&gt;\n\n&lt;p&gt;When I update (overwrite) a table location, the definition is no longer valid because underlying files are being changed, is there a general way of solving this issue or I have to refresh the tables in thrift server manually every time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116wzxk", "is_robot_indexable": true, "report_reasons": null, "author": "inteloid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116wzxk/spark_thrift_server_auto_refresh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116wzxk/spark_thrift_server_auto_refresh/", "subreddit_subscribers": 90352, "created_utc": 1676865266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, looking to get some insight between 2 ELT strategies on micro-batched data. If you extract micro-batches from a source (1 min intervals) and load the batch as a new parquet file in an object store (ADLS), the next obvious task is to process that data (as soon as a new file lands) and load it into a data warehouse for business reporting needs. The micro-batches are pretty small, no more than 50-100ish records each minute.\n\nOne way to process the data is to do a full history transformation, where as soon as the new files land, the entire history of data in the lake is processed and the data in the warehouse is completely replaced. However, this seems very excessive to do every minute, and doesn't even seem like a logical solution?\n\nThe other (and maybe more obvious) is to process just the new files that are landed in the lake every minute.  This way, we are just processing and inserting only new records into the existing warehouse tables.\n\nHowever, this confuses me - what if there is business logic that is being applied to the data before loading into the warehouse? If there is a change in business requirements that alters the transformation step, wouldn't the existing data in the warehouse be irrelevant, since the data is not affected by the change? At least with a full-history transformation, then all the data would affected by the change in requirements.\n\nHow do you guys handle process like this, while considering the need to future proof the system?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Transformation on full history or just new data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117norj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676934719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, looking to get some insight between 2 ELT strategies on micro-batched data. If you extract micro-batches from a source (1 min intervals) and load the batch as a new parquet file in an object store (ADLS), the next obvious task is to process that data (as soon as a new file lands) and load it into a data warehouse for business reporting needs. The micro-batches are pretty small, no more than 50-100ish records each minute.&lt;/p&gt;\n\n&lt;p&gt;One way to process the data is to do a full history transformation, where as soon as the new files land, the entire history of data in the lake is processed and the data in the warehouse is completely replaced. However, this seems very excessive to do every minute, and doesn&amp;#39;t even seem like a logical solution?&lt;/p&gt;\n\n&lt;p&gt;The other (and maybe more obvious) is to process just the new files that are landed in the lake every minute.  This way, we are just processing and inserting only new records into the existing warehouse tables.&lt;/p&gt;\n\n&lt;p&gt;However, this confuses me - what if there is business logic that is being applied to the data before loading into the warehouse? If there is a change in business requirements that alters the transformation step, wouldn&amp;#39;t the existing data in the warehouse be irrelevant, since the data is not affected by the change? At least with a full-history transformation, then all the data would affected by the change in requirements.&lt;/p&gt;\n\n&lt;p&gt;How do you guys handle process like this, while considering the need to future proof the system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117norj", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117norj/data_transformation_on_full_history_or_just_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117norj/data_transformation_on_full_history_or_just_new/", "subreddit_subscribers": 90352, "created_utc": 1676934719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Changes from PostgreSQL to Any Destination with Change Data Capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117est6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1676914063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cloudquery.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.cloudquery.io/blog/postgres-cdc-to-any-destination", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "117est6", "is_robot_indexable": true, "report_reasons": null, "author": "jekapats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117est6/stream_changes_from_postgresql_to_any_destination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.cloudquery.io/blog/postgres-cdc-to-any-destination", "subreddit_subscribers": 90352, "created_utc": 1676914063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to learn an open source data ingestion tool and use it in some personal projects, primary I want to ingest data from APIs and postgres and orchestrte this with airflow. I can't seem to make up my mind on which tool to pick, please help", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source ingestion tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_117r4gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676944062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to learn an open source data ingestion tool and use it in some personal projects, primary I want to ingest data from APIs and postgres and orchestrte this with airflow. I can&amp;#39;t seem to make up my mind on which tool to pick, please help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117r4gd", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117r4gd/open_source_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117r4gd/open_source_ingestion_tool/", "subreddit_subscribers": 90352, "created_utc": 1676944062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My startup has built a knowledge management SaaS product, and I\u2019ve been thinking of launching our backend platform as a separate API service. \n\nIt would be like Hasura for unstructured data.\n\nWould this API be useful for any applications you\u2019re building?  Trying to evaluate if there\u2019s a market need.\n\nIt would work like this:\n\nYou can upload any file type, like PDFs, videos, images or even CAD drawings.  Or, you can create \u201cfeeds\u201d from places like Google Drive or an S3 bucket.\n\nWe run ML on the files to extract embedded data, like references to companies, places, topics, or even visual objects.  Audio and video files will generate searchable transcripts. ML is configurable, if you have an endpoint for your own model.\n\nYou can configure enrichment of that data to provide more context, like layering in Crunchbase data for companies or LinkedIn for people.\n\nThen, you can use our GraphQL API to query the extracted data: filter by properties, and similarity search by time, geolocation or full text.  Also it supports ChatGPT-like Q&amp;A over the data.  \n\nWe are also considering adding data publishing to deliver the extracted data to other data pipelines, if you wouldn\u2019t want the GraphQL API.", "author_fullname": "t2_15wnt5aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Files -&gt; ML -&gt; GraphQL API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_117qdy0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676941961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My startup has built a knowledge management SaaS product, and I\u2019ve been thinking of launching our backend platform as a separate API service. &lt;/p&gt;\n\n&lt;p&gt;It would be like Hasura for unstructured data.&lt;/p&gt;\n\n&lt;p&gt;Would this API be useful for any applications you\u2019re building?  Trying to evaluate if there\u2019s a market need.&lt;/p&gt;\n\n&lt;p&gt;It would work like this:&lt;/p&gt;\n\n&lt;p&gt;You can upload any file type, like PDFs, videos, images or even CAD drawings.  Or, you can create \u201cfeeds\u201d from places like Google Drive or an S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;We run ML on the files to extract embedded data, like references to companies, places, topics, or even visual objects.  Audio and video files will generate searchable transcripts. ML is configurable, if you have an endpoint for your own model.&lt;/p&gt;\n\n&lt;p&gt;You can configure enrichment of that data to provide more context, like layering in Crunchbase data for companies or LinkedIn for people.&lt;/p&gt;\n\n&lt;p&gt;Then, you can use our GraphQL API to query the extracted data: filter by properties, and similarity search by time, geolocation or full text.  Also it supports ChatGPT-like Q&amp;amp;A over the data.  &lt;/p&gt;\n\n&lt;p&gt;We are also considering adding data publishing to deliver the extracted data to other data pipelines, if you wouldn\u2019t want the GraphQL API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117qdy0", "is_robot_indexable": true, "report_reasons": null, "author": "DeadPukka", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117qdy0/files_ml_graphql_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117qdy0/files_ml_graphql_api/", "subreddit_subscribers": 90352, "created_utc": 1676941961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the last couple of years I've been quite typical full-stack dev, with a bit more accent on the backend side. Lot of java, node.js, on frontend some jquery, react, plus some work around infra (deployments on gitlab, dockerizations, etc.).  \nIn last two years I was more drawn into DE - I was picked for a project of building up internal data analytics system in our company. Lots of AWS work, DBs (Redshift, DynamoDB, relational DBs), moving tons data between AWS components and learning on the road what proper DE is.\n\nI'm sure a lot of people went down the similar route. How do you find such transition - do you prefer more DE work to typical backend/full-stack work?    \n\n\nPersonally I like now flexibility in my daily work and playing with a lot of tools but on other hand I miss sometimes focusing  more on a good coding in a single app, rather than jumping between smaller apps and scripts and working on a setting proper tooling (which seems to me a core of DE work)", "author_fullname": "t2_vyrlctas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from classic software engineering into DE - opinions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117lwqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676930438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the last couple of years I&amp;#39;ve been quite typical full-stack dev, with a bit more accent on the backend side. Lot of java, node.js, on frontend some jquery, react, plus some work around infra (deployments on gitlab, dockerizations, etc.).&lt;br/&gt;\nIn last two years I was more drawn into DE - I was picked for a project of building up internal data analytics system in our company. Lots of AWS work, DBs (Redshift, DynamoDB, relational DBs), moving tons data between AWS components and learning on the road what proper DE is.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure a lot of people went down the similar route. How do you find such transition - do you prefer more DE work to typical backend/full-stack work?    &lt;/p&gt;\n\n&lt;p&gt;Personally I like now flexibility in my daily work and playing with a lot of tools but on other hand I miss sometimes focusing  more on a good coding in a single app, rather than jumping between smaller apps and scripts and working on a setting proper tooling (which seems to me a core of DE work)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117lwqw", "is_robot_indexable": true, "report_reasons": null, "author": "IronEider", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117lwqw/transition_from_classic_software_engineering_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117lwqw/transition_from_classic_software_engineering_into/", "subreddit_subscribers": 90352, "created_utc": 1676930438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_27c3plee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog post: Metadata-driven pipelines in Azure Data Factory | Part 4 - Analytical Processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_117jyag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zpy9fyhlUkl11M_XfA6lhAEKh4-fOg0k0ZlLQvSCMzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676925802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanrg.blogspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?auto=webp&amp;v=enabled&amp;s=acf6b162314a17c0afbe67b2c22c8ab66e5c7332", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=976c7f9865030935e649d015169ac172cc6f5e6c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85dd501dc2bb75e6a72a1a225332106070cbcc91", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=212afa4d32af2f1746ea2623c764edf1f12a0a68", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57d89326fd6c1b4391a282e522ca8df8fc10add5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cc3e3e5d8e02be53ed74cabf9db3c4e2e051c9f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33a493bbc6aef040b48a9a0e128e6d65f2e0c3e0", "width": 1080, "height": 567}], "variants": {}, "id": "_z5Q-MwAZcOLYV1qeyPkktnXQXLmlsXbk1lpBU4Gubg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "117jyag", "is_robot_indexable": true, "report_reasons": null, "author": "RayisImayev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117jyag/blog_post_metadatadriven_pipelines_in_azure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "subreddit_subscribers": 90352, "created_utc": 1676925802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was trying to implement this process to create data warehouse. I could think of possible ways.\n\n1. Use AWS services. \n\nTo get data from apps i am using appflow to store data as S3. \nTo get database data, i am using DMS service to get data from databases and store it as S3 bucket with CDC. \n\nNow i want to transfer these source data to Redshift. I checked and most of the documentation mentioned using copy command. \n\nBut in this case, i have to create all tables again in redshift???? Is there any way to do this without creating tables again in redshift i.e, where redshift just takes all tables from s3 bucket. \n\n2. Use snowflake and stitch.\n\nAny help or recommendations in this regard ll be really helpful.\ud83d\ude4f", "author_fullname": "t2_8jfrjnc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline options from source to destination [data warehouse]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11738k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676888361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to implement this process to create data warehouse. I could think of possible ways.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use AWS services. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To get data from apps i am using appflow to store data as S3. \nTo get database data, i am using DMS service to get data from databases and store it as S3 bucket with CDC. &lt;/p&gt;\n\n&lt;p&gt;Now i want to transfer these source data to Redshift. I checked and most of the documentation mentioned using copy command. &lt;/p&gt;\n\n&lt;p&gt;But in this case, i have to create all tables again in redshift???? Is there any way to do this without creating tables again in redshift i.e, where redshift just takes all tables from s3 bucket. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use snowflake and stitch.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help or recommendations in this regard ll be really helpful.\ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11738k2", "is_robot_indexable": true, "report_reasons": null, "author": "lost_soul1995", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11738k2/data_pipeline_options_from_source_to_destination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11738k2/data_pipeline_options_from_source_to_destination/", "subreddit_subscribers": 90352, "created_utc": 1676888361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nThe project I'm currently on is using Vertica and we have been asked to go over the vertica Certification course but don't have to actually get the certification.\n\nAs this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?", "author_fullname": "t2_digu9bnx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vertica Certification worth it ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11700w4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676875729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;The project I&amp;#39;m currently on is using Vertica and we have been asked to go over the vertica Certification course but don&amp;#39;t have to actually get the certification.&lt;/p&gt;\n\n&lt;p&gt;As this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11700w4", "is_robot_indexable": true, "report_reasons": null, "author": "Coffeee-Cat", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11700w4/vertica_certification_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11700w4/vertica_certification_worth_it/", "subreddit_subscribers": 90352, "created_utc": 1676875729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing a test for a job application where one of the tasks is to do the following:\n\nFor each Id number in df['AccountId'] check if it exists in that same column as a preffix to another value.\n\nEg.\n\ndf['AccountId'] is 1.1 for row X  and df['AccountId'] is 1.1.1 for row Y\n\nThat will determine wether column df['Type'] will be filled with value 'X', 'Y' or 'Z'.\n\nSo if there's no entry in df['AccountId'] where row X is a preffix to it, then row X will have 'Z' as value for df['Type']. But if there is, then row X will have 'Y' as value for df['Type'].\n\nAny idea what's the optimal way to do this?\n\nI've tried many approaches but I keep running on all sorts of errors like 'The truth value of a Series is ambiguous'.\n\nSorry if this is not the place to post this and thanks for any help!", "author_fullname": "t2_32b255ac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help checking for a certain condition for each value in a column in a Pandas dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117iu5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676923155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing a test for a job application where one of the tasks is to do the following:&lt;/p&gt;\n\n&lt;p&gt;For each Id number in df[&amp;#39;AccountId&amp;#39;] check if it exists in that same column as a preffix to another value.&lt;/p&gt;\n\n&lt;p&gt;Eg.&lt;/p&gt;\n\n&lt;p&gt;df[&amp;#39;AccountId&amp;#39;] is 1.1 for row X  and df[&amp;#39;AccountId&amp;#39;] is 1.1.1 for row Y&lt;/p&gt;\n\n&lt;p&gt;That will determine wether column df[&amp;#39;Type&amp;#39;] will be filled with value &amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39; or &amp;#39;Z&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;So if there&amp;#39;s no entry in df[&amp;#39;AccountId&amp;#39;] where row X is a preffix to it, then row X will have &amp;#39;Z&amp;#39; as value for df[&amp;#39;Type&amp;#39;]. But if there is, then row X will have &amp;#39;Y&amp;#39; as value for df[&amp;#39;Type&amp;#39;].&lt;/p&gt;\n\n&lt;p&gt;Any idea what&amp;#39;s the optimal way to do this?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried many approaches but I keep running on all sorts of errors like &amp;#39;The truth value of a Series is ambiguous&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is not the place to post this and thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117iu5h", "is_robot_indexable": true, "report_reasons": null, "author": "torvi97", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117iu5h/need_some_help_checking_for_a_certain_condition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117iu5h/need_some_help_checking_for_a_certain_condition/", "subreddit_subscribers": 90352, "created_utc": 1676923155.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}