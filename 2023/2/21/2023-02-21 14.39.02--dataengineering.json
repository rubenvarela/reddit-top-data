{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.\n\nRecently I've cleared DP-203 and received the Data Engineer Associate certificate. I [shared a post on here](https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&amp;utm_medium=android_app&amp;utm_name=androidcss&amp;utm_term=1&amp;utm_content=share_button) as well.\n\nI prepared some notes on Notion while preparing for the certification. And I'd like to share it with others so that It could help others while doing revision for the exam.\n\nNotes link: [dp203-azure-data-engineering-notes](https://github.com/jithendray/dp203-azure-data-engineering).\n\nTips that helped me:\n\n- I did a decent course on the Udemy. \n- Made notes while watching tbe last lecture videos.\n- The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.\n- Finally, revised the notes that I made a day before the exam.\n\nAll the best, for anyone who is preparing for the exam. Feel free to add \u2b50 to my repo ;)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got certified recently and prepared some notes while preparing for Azure DP-203", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117djjy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676912073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.&lt;/p&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve cleared DP-203 and received the Data Engineer Associate certificate. I &lt;a href=\"https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&amp;amp;utm_medium=android_app&amp;amp;utm_name=androidcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;shared a post on here&lt;/a&gt; as well.&lt;/p&gt;\n\n&lt;p&gt;I prepared some notes on Notion while preparing for the certification. And I&amp;#39;d like to share it with others so that It could help others while doing revision for the exam.&lt;/p&gt;\n\n&lt;p&gt;Notes link: &lt;a href=\"https://github.com/jithendray/dp203-azure-data-engineering\"&gt;dp203-azure-data-engineering-notes&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Tips that helped me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I did a decent course on the Udemy. &lt;/li&gt;\n&lt;li&gt;Made notes while watching tbe last lecture videos.&lt;/li&gt;\n&lt;li&gt;The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.&lt;/li&gt;\n&lt;li&gt;Finally, revised the notes that I made a day before the exam.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All the best, for anyone who is preparing for the exam. Feel free to add \u2b50 to my repo ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?auto=webp&amp;v=enabled&amp;s=09779b28cd66cfd5ff2aa08fc530f1fa0831fc49", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f769aa1c0a1f249552dbc00dfc441edfac9f2659", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9297afeb8d68686fb38859746cdecf9fc40943c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343a56e772e5d82dd7581f48f181b9440b5b5e3e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b29be5481f5b76e130a75650be4590f6e7b98f3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=598766feafb9e3390ec187a00834507d7d716873", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rBU-8isCY5L9ahsDpi_BXS86zQJ1jh9GzB2i3sRX040.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=553c08a009245ad293de85e9851266d1ff557d60", "width": 1080, "height": 540}], "variants": {}, "id": "4SDWg8PO0pIHRKtfw-MN6uvnndNQPozdBuf03lZJ82A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "117djjy", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/", "subreddit_subscribers": 90408, "created_utc": 1676912073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE fellows,\n\nI'm a Data Engineer for 1.5 years with a business background, I do ETLs mostly, query data from PostgreSQL server, do some calculation then load to another PostgreSQL table, automate using Airflow. I want to improve my skills so I can add to CVs and impress new employers, which one should I stay focus on for the next 6 months? Doing many DE side projects or get a Cloud certificate? If the answer is Cloud cert then which one is better, GCP or AWS?\n\nThank you for your advice.", "author_fullname": "t2_fgtgvlw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing DE side projects or getting Cloud certification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117tm6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676951407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE fellows,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a Data Engineer for 1.5 years with a business background, I do ETLs mostly, query data from PostgreSQL server, do some calculation then load to another PostgreSQL table, automate using Airflow. I want to improve my skills so I can add to CVs and impress new employers, which one should I stay focus on for the next 6 months? Doing many DE side projects or get a Cloud certificate? If the answer is Cloud cert then which one is better, GCP or AWS?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117tm6i", "is_robot_indexable": true, "report_reasons": null, "author": "J-Huynh", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117tm6i/doing_de_side_projects_or_getting_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117tm6i/doing_de_side_projects_or_getting_cloud/", "subreddit_subscribers": 90408, "created_utc": 1676951407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please share what lakehouse stack do you use. \n\nFor example: \n\n* Storage: S3\n* File formats: parquet\n* Table formats: Iceberg\n* Realtime: Clickhouse\n* Modeling/Transformations: dbt, Spark\n* Orchestration: Airflow\n* Semantic layer: Cube\n* BI: Tableau\n* Data quality: Great expectations\n* Data catalog: Amundsen\n* ML: mlflow, kubeflow\n* Other: lakeFS", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What lakehouse stack do you use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117x39e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676969076.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676962861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please share what lakehouse stack do you use. &lt;/p&gt;\n\n&lt;p&gt;For example: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Storage: S3&lt;/li&gt;\n&lt;li&gt;File formats: parquet&lt;/li&gt;\n&lt;li&gt;Table formats: Iceberg&lt;/li&gt;\n&lt;li&gt;Realtime: Clickhouse&lt;/li&gt;\n&lt;li&gt;Modeling/Transformations: dbt, Spark&lt;/li&gt;\n&lt;li&gt;Orchestration: Airflow&lt;/li&gt;\n&lt;li&gt;Semantic layer: Cube&lt;/li&gt;\n&lt;li&gt;BI: Tableau&lt;/li&gt;\n&lt;li&gt;Data quality: Great expectations&lt;/li&gt;\n&lt;li&gt;Data catalog: Amundsen&lt;/li&gt;\n&lt;li&gt;ML: mlflow, kubeflow&lt;/li&gt;\n&lt;li&gt;Other: lakeFS&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117x39e", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117x39e/what_lakehouse_stack_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117x39e/what_lakehouse_stack_do_you_use/", "subreddit_subscribers": 90408, "created_utc": 1676962861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to pull data from multiple sql tables into a lake. The ingest should be scheduled every night and pull only new data (incremental load, e.g. from yesterday). Is there some magic offered by databricks to accomplish this? I read about the autoloader, but it seems that it only supports blob storage as a source.\n\nOutside of databricks I would go for a simple spark script, that connects via jdbc connector and pulls records that where created e.g. yesterday.\n\nHow would you solve this?", "author_fullname": "t2_kh9d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using databricks to incrementally load data from a sql server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1179krk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676906617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pull data from multiple sql tables into a lake. The ingest should be scheduled every night and pull only new data (incremental load, e.g. from yesterday). Is there some magic offered by databricks to accomplish this? I read about the autoloader, but it seems that it only supports blob storage as a source.&lt;/p&gt;\n\n&lt;p&gt;Outside of databricks I would go for a simple spark script, that connects via jdbc connector and pulls records that where created e.g. yesterday.&lt;/p&gt;\n\n&lt;p&gt;How would you solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1179krk", "is_robot_indexable": true, "report_reasons": null, "author": "artworkad", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1179krk/using_databricks_to_incrementally_load_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1179krk/using_databricks_to_incrementally_load_data_from/", "subreddit_subscribers": 90408, "created_utc": 1676906617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone had success with ELT tools or technologies that allowed them to source CDC changes from multiple (see: HUNDREDS) of source SQL databases for streaming said changes to a lake/warehouse? It would need to be robust enough that all these connectors could be scripted, especially as more, newly created Tenants are created. Any help or suggestions are appreciated -- thanks!", "author_fullname": "t2_5h5g8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best ELT tool for sourcing from numerous multi-tenant SQL databases for CDC changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117e7nx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676913058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone had success with ELT tools or technologies that allowed them to source CDC changes from multiple (see: HUNDREDS) of source SQL databases for streaming said changes to a lake/warehouse? It would need to be robust enough that all these connectors could be scripted, especially as more, newly created Tenants are created. Any help or suggestions are appreciated -- thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117e7nx", "is_robot_indexable": true, "report_reasons": null, "author": "Zilean", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117e7nx/best_elt_tool_for_sourcing_from_numerous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117e7nx/best_elt_tool_for_sourcing_from_numerous/", "subreddit_subscribers": 90408, "created_utc": 1676913058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jh2dwngo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coinbase CEO tells his team: APIs not meetings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_117bpnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KrVPNY4yYpPhHgV7eoocFPDxlsBNpArRv-P83VDJvWM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676909552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thestatuscode.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thestatuscode.co/p/coinbase-ceo-tells-his-team-apis", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?auto=webp&amp;v=enabled&amp;s=b974687b5992bd4d8567e839be2a08dad45c6d27", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a299fd9b173a81a1c00bdb256660aff7e4b22725", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92892193b0bcafaebf3dcf4d16ecaee107d9cd99", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=994c596fd68b034b12f8a431f3fd223a5c85a822", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2000e27d0c48fa8d29753f0839469134e4451b61", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2f7419c21fd8ff66fdeb1848255d96bf86e1934", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/hOAH2y2fXvdVwr5Gx2CAOS2-6AEG4893E0Zk7bKX_8E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca770fe8e599d1d3bccb2c33af32059317c13eb1", "width": 1080, "height": 720}], "variants": {}, "id": "wS6NnZbV8bwcv_3l3IB3QdEQZAAi8NkDDtk88IIgrEw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117bpnt", "is_robot_indexable": true, "report_reasons": null, "author": "foundersblock", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117bpnt/coinbase_ceo_tells_his_team_apis_not_meetings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thestatuscode.co/p/coinbase-ceo-tells-his-team-apis", "subreddit_subscribers": 90408, "created_utc": 1676909552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I recently got back into data engineering after venturing off to do more traditional backend development for a couple years. My experience is as follows:\n\n2 years DE in the US at Fortune 500 company, 1 year as backend SWE at mid sized tech startup, and a little under a year at two very small startups in Korea (backend at 1st, DE at 2nd).\n\nLong story short, developer jobs are horrible here and I am looking at leaving my 2nd job in less than a year because I am not getting paid anymore. They ran out of money and I expect the startup to die soon.\n\nAt this point, I am open to basically any functional (stable, reasonable dev practices, reasonable expectations) job that is remote even for low pay. I took a 60% pay cut from my US job and then another 20% pay cut from my 1st Korea job in the hopes that the new job is better. I have basically lost all hope for a good job in Korea and my co-workers and developer friends here either say their job is not much better or it requires fluent Korean (or commuting 2+ hours away from where I am).\n\nThus I looked into global remote jobs and I have been having a difficult time there as well. My resume is basically the same as the one I've been using for years, just updated, and has given me great results over the years with high interview rates. Lately I have been getting extremely few call backs to the global remote jobs I apply to and the few times I do, it usually ends at the recruiter call as they missed something that is a hard blocker (time zone, not realizing I live in Korea, etc.) I haven't had good luck with the large foreign/international companies (Coupang, FB, G, A, etc.) here either as they seem to mostly hire seniors if not fluent in Korean. I didn't get any callbacks. But either way I think it would have been difficult as most do not operate remotely and I live a bit further from Seoul.\n\nThere also seems to be very little global remote DE jobs to start with. Mostly full stack, front end, or mobile developer jobs. I have searched through many different job sites and applied to anything I could see myself potentially working at. I'm at my wit's end here and it unfortunately looks like we have to leave Korea if I can't find a job by the end of spring. We originally moved here due to my spouse's job (which has been mostly great) but it's hard to justify staying in Korea for longer if I am going to be unemployed or working jobs I hate for very little pay.\n\nI also tried Upwork but it's not been going well and I'm competing against people who have cheaper rates than even minimum wage here. Doesn't seem like a great option.\n\nI asked my last company if I could be a contractor for them but they said they'd only be willing if I went through a contracting company they already contract with. No individual contractors. The company they work with is in the EU where I don't have a visa to work in so no dice there.\n\nIf anyone has any advice or leads on finding such jobs, please let me know. If needed I can try anonymizing my resume and sharing it but it'll take a little bit to edit.\n\nI'd be happy with any job that lets me work even half my hours in my time zone (4 hours on theirs) and pays at least 40k USD per year. I have experience with AWS/GCP and worked a good amount on cloud native products. Experienced with Python/JS/SQL and some experience with Spark. I had to use a variety of tech stacks and tools throughout my jobs so I'm pretty good at picking up something new and getting productive quick.", "author_fullname": "t2_116gfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difficulty finding global remote DE job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117xfdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676964105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I recently got back into data engineering after venturing off to do more traditional backend development for a couple years. My experience is as follows:&lt;/p&gt;\n\n&lt;p&gt;2 years DE in the US at Fortune 500 company, 1 year as backend SWE at mid sized tech startup, and a little under a year at two very small startups in Korea (backend at 1st, DE at 2nd).&lt;/p&gt;\n\n&lt;p&gt;Long story short, developer jobs are horrible here and I am looking at leaving my 2nd job in less than a year because I am not getting paid anymore. They ran out of money and I expect the startup to die soon.&lt;/p&gt;\n\n&lt;p&gt;At this point, I am open to basically any functional (stable, reasonable dev practices, reasonable expectations) job that is remote even for low pay. I took a 60% pay cut from my US job and then another 20% pay cut from my 1st Korea job in the hopes that the new job is better. I have basically lost all hope for a good job in Korea and my co-workers and developer friends here either say their job is not much better or it requires fluent Korean (or commuting 2+ hours away from where I am).&lt;/p&gt;\n\n&lt;p&gt;Thus I looked into global remote jobs and I have been having a difficult time there as well. My resume is basically the same as the one I&amp;#39;ve been using for years, just updated, and has given me great results over the years with high interview rates. Lately I have been getting extremely few call backs to the global remote jobs I apply to and the few times I do, it usually ends at the recruiter call as they missed something that is a hard blocker (time zone, not realizing I live in Korea, etc.) I haven&amp;#39;t had good luck with the large foreign/international companies (Coupang, FB, G, A, etc.) here either as they seem to mostly hire seniors if not fluent in Korean. I didn&amp;#39;t get any callbacks. But either way I think it would have been difficult as most do not operate remotely and I live a bit further from Seoul.&lt;/p&gt;\n\n&lt;p&gt;There also seems to be very little global remote DE jobs to start with. Mostly full stack, front end, or mobile developer jobs. I have searched through many different job sites and applied to anything I could see myself potentially working at. I&amp;#39;m at my wit&amp;#39;s end here and it unfortunately looks like we have to leave Korea if I can&amp;#39;t find a job by the end of spring. We originally moved here due to my spouse&amp;#39;s job (which has been mostly great) but it&amp;#39;s hard to justify staying in Korea for longer if I am going to be unemployed or working jobs I hate for very little pay.&lt;/p&gt;\n\n&lt;p&gt;I also tried Upwork but it&amp;#39;s not been going well and I&amp;#39;m competing against people who have cheaper rates than even minimum wage here. Doesn&amp;#39;t seem like a great option.&lt;/p&gt;\n\n&lt;p&gt;I asked my last company if I could be a contractor for them but they said they&amp;#39;d only be willing if I went through a contracting company they already contract with. No individual contractors. The company they work with is in the EU where I don&amp;#39;t have a visa to work in so no dice there.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any advice or leads on finding such jobs, please let me know. If needed I can try anonymizing my resume and sharing it but it&amp;#39;ll take a little bit to edit.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be happy with any job that lets me work even half my hours in my time zone (4 hours on theirs) and pays at least 40k USD per year. I have experience with AWS/GCP and worked a good amount on cloud native products. Experienced with Python/JS/SQL and some experience with Spark. I had to use a variety of tech stacks and tools throughout my jobs so I&amp;#39;m pretty good at picking up something new and getting productive quick.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117xfdg", "is_robot_indexable": true, "report_reasons": null, "author": "rohrohroh", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117xfdg/difficulty_finding_global_remote_de_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117xfdg/difficulty_finding_global_remote_de_job/", "subreddit_subscribers": 90408, "created_utc": 1676964105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started two weeks ago an internship in data engineering, but so far my manager is struggling to give me work to do. It seems like the team, him included, is overloaded and simply don't have the time for me. I have been asked to complete some courses on Coursera (Scala / spark), discover my environment etc, but it is becoming quite obvious that my manager is avoiding me because he has no work for me (I have already told him I have nothing left to do). What should I do ? I'm worried not to learn enough things to get a job after this internship. The project of my internship is running on spark and opensearch, but I can't do anything without help. What can I learn on my own online that will help me in the future ?", "author_fullname": "t2_qexqkrk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New intern, no work to to do, what should I do ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118005b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676974087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started two weeks ago an internship in data engineering, but so far my manager is struggling to give me work to do. It seems like the team, him included, is overloaded and simply don&amp;#39;t have the time for me. I have been asked to complete some courses on Coursera (Scala / spark), discover my environment etc, but it is becoming quite obvious that my manager is avoiding me because he has no work for me (I have already told him I have nothing left to do). What should I do ? I&amp;#39;m worried not to learn enough things to get a job after this internship. The project of my internship is running on spark and opensearch, but I can&amp;#39;t do anything without help. What can I learn on my own online that will help me in the future ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "118005b", "is_robot_indexable": true, "report_reasons": null, "author": "165817566995", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118005b/new_intern_no_work_to_to_do_what_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118005b/new_intern_no_work_to_to_do_what_should_i_do/", "subreddit_subscribers": 90408, "created_utc": 1676974087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to learn an open source data ingestion tool and use it in some personal projects, primary I want to ingest data from APIs and postgres and orchestrte this with airflow. I can't seem to make up my mind on which tool to pick, please help", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source ingestion tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117r4gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676944062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to learn an open source data ingestion tool and use it in some personal projects, primary I want to ingest data from APIs and postgres and orchestrte this with airflow. I can&amp;#39;t seem to make up my mind on which tool to pick, please help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117r4gd", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117r4gd/open_source_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117r4gd/open_source_ingestion_tool/", "subreddit_subscribers": 90408, "created_utc": 1676944062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, looking to get some insight between 2 ELT strategies on micro-batched data. If you extract micro-batches from a source (1 min intervals) and load the batch as a new parquet file in an object store (ADLS), the next obvious task is to process that data (as soon as a new file lands) and load it into a data warehouse for business reporting needs. The micro-batches are pretty small, no more than 50-100ish records each minute.\n\nOne way to process the data is to do a full history transformation, where as soon as the new files land, the entire history of data in the lake is processed and the data in the warehouse is completely replaced. However, this seems very excessive to do every minute, and doesn't even seem like a logical solution?\n\nThe other (and maybe more obvious) is to process just the new files that are landed in the lake every minute.  This way, we are just processing and inserting only new records into the existing warehouse tables.\n\nHowever, this confuses me - what if there is business logic that is being applied to the data before loading into the warehouse? If there is a change in business requirements that alters the transformation step, wouldn't the existing data in the warehouse be irrelevant, since the data is not affected by the change? At least with a full-history transformation, then all the data would affected by the change in requirements.\n\nHow do you guys handle process like this, while considering the need to future proof the system?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Transformation on full history or just new data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117norj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676934719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, looking to get some insight between 2 ELT strategies on micro-batched data. If you extract micro-batches from a source (1 min intervals) and load the batch as a new parquet file in an object store (ADLS), the next obvious task is to process that data (as soon as a new file lands) and load it into a data warehouse for business reporting needs. The micro-batches are pretty small, no more than 50-100ish records each minute.&lt;/p&gt;\n\n&lt;p&gt;One way to process the data is to do a full history transformation, where as soon as the new files land, the entire history of data in the lake is processed and the data in the warehouse is completely replaced. However, this seems very excessive to do every minute, and doesn&amp;#39;t even seem like a logical solution?&lt;/p&gt;\n\n&lt;p&gt;The other (and maybe more obvious) is to process just the new files that are landed in the lake every minute.  This way, we are just processing and inserting only new records into the existing warehouse tables.&lt;/p&gt;\n\n&lt;p&gt;However, this confuses me - what if there is business logic that is being applied to the data before loading into the warehouse? If there is a change in business requirements that alters the transformation step, wouldn&amp;#39;t the existing data in the warehouse be irrelevant, since the data is not affected by the change? At least with a full-history transformation, then all the data would affected by the change in requirements.&lt;/p&gt;\n\n&lt;p&gt;How do you guys handle process like this, while considering the need to future proof the system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117norj", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117norj/data_transformation_on_full_history_or_just_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117norj/data_transformation_on_full_history_or_just_new/", "subreddit_subscribers": 90408, "created_utc": 1676934719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our Product Manager keeps telling us we\u2019re not abiding by true agile, and we need to change. We build data pipelines. Data starts at one or multiple points, and the business wants it modelled and sent to a destination (an API for instance).\n\nWhen the business defines their needs, and then we build the pipeline, can someone explain to me where Agile comes into play? I get the building in increments, which makes sense. But given a complex pipeline that feeds to an API that then validates the data, I don\u2019t see where agile fits. The data either makes it through and gets validated or it doesn\u2019t. The business rules that make it useful are either implemented, or the data is not useful.\n\nCan someone please help me understand? Can agile work in a data pipeline team?\n\nEdit: reworded for clarity.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Agile with Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117y5gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676969302.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676966887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our Product Manager keeps telling us we\u2019re not abiding by true agile, and we need to change. We build data pipelines. Data starts at one or multiple points, and the business wants it modelled and sent to a destination (an API for instance).&lt;/p&gt;\n\n&lt;p&gt;When the business defines their needs, and then we build the pipeline, can someone explain to me where Agile comes into play? I get the building in increments, which makes sense. But given a complex pipeline that feeds to an API that then validates the data, I don\u2019t see where agile fits. The data either makes it through and gets validated or it doesn\u2019t. The business rules that make it useful are either implemented, or the data is not useful.&lt;/p&gt;\n\n&lt;p&gt;Can someone please help me understand? Can agile work in a data pipeline team?&lt;/p&gt;\n\n&lt;p&gt;Edit: reworded for clarity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117y5gj", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117y5gj/agile_with_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117y5gj/agile_with_data_pipelines/", "subreddit_subscribers": 90408, "created_utc": 1676966887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the last couple of years I've been quite typical full-stack dev, with a bit more accent on the backend side. Lot of java, node.js, on frontend some jquery, react, plus some work around infra (deployments on gitlab, dockerizations, etc.).  \nIn last two years I was more drawn into DE - I was picked for a project of building up internal data analytics system in our company. Lots of AWS work, DBs (Redshift, DynamoDB, relational DBs), moving tons data between AWS components and learning on the road what proper DE is.\n\nI'm sure a lot of people went down the similar route. How do you find such transition - do you prefer more DE work to typical backend/full-stack work?    \n\n\nPersonally I like now flexibility in my daily work and playing with a lot of tools but on other hand I miss sometimes focusing  more on a good coding in a single app, rather than jumping between smaller apps and scripts and working on a setting proper tooling (which seems to me a core of DE work)", "author_fullname": "t2_vyrlctas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from classic software engineering into DE - opinions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117lwqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676930438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the last couple of years I&amp;#39;ve been quite typical full-stack dev, with a bit more accent on the backend side. Lot of java, node.js, on frontend some jquery, react, plus some work around infra (deployments on gitlab, dockerizations, etc.).&lt;br/&gt;\nIn last two years I was more drawn into DE - I was picked for a project of building up internal data analytics system in our company. Lots of AWS work, DBs (Redshift, DynamoDB, relational DBs), moving tons data between AWS components and learning on the road what proper DE is.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure a lot of people went down the similar route. How do you find such transition - do you prefer more DE work to typical backend/full-stack work?    &lt;/p&gt;\n\n&lt;p&gt;Personally I like now flexibility in my daily work and playing with a lot of tools but on other hand I miss sometimes focusing  more on a good coding in a single app, rather than jumping between smaller apps and scripts and working on a setting proper tooling (which seems to me a core of DE work)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "117lwqw", "is_robot_indexable": true, "report_reasons": null, "author": "IronEider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117lwqw/transition_from_classic_software_engineering_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117lwqw/transition_from_classic_software_engineering_into/", "subreddit_subscribers": 90408, "created_utc": 1676930438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Changes from PostgreSQL to Any Destination with Change Data Capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117est6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1676914063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cloudquery.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.cloudquery.io/blog/postgres-cdc-to-any-destination", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "117est6", "is_robot_indexable": true, "report_reasons": null, "author": "jekapats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117est6/stream_changes_from_postgresql_to_any_destination/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.cloudquery.io/blog/postgres-cdc-to-any-destination", "subreddit_subscribers": 90408, "created_utc": 1676914063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm a second year student and I have a question relating to Data Engineer track. Does OOP and Design pattern knowledge is really necesssary for a DE? \nSince next semester I have a program elective course (which will teach in-depth OOP and design pattern), I'm really confused to choose or not choose this course.\nHere is the course info:\n\"This course provides students with an advanced understanding of software\ndevelopment with an emphasis on architecture and design and how this\nrelates to programming and implementation. This course builds upon\nexisting programming knowledge using Java as the implementation platform\nand assumes students already have a basic understanding of Java\nProgramming and basic OO concepts. Students will explore advanced OOP\nconcepts and the relationship between design in UML and its expression in code\nand how this is supported by modelling tools and development platforms\"\n\f \nMany thanks", "author_fullname": "t2_7g9xr3h3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OOP and Design pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117z5ii", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676970781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m a second year student and I have a question relating to Data Engineer track. Does OOP and Design pattern knowledge is really necesssary for a DE? \nSince next semester I have a program elective course (which will teach in-depth OOP and design pattern), I&amp;#39;m really confused to choose or not choose this course.\nHere is the course info:\n&amp;quot;This course provides students with an advanced understanding of software\ndevelopment with an emphasis on architecture and design and how this\nrelates to programming and implementation. This course builds upon\nexisting programming knowledge using Java as the implementation platform\nand assumes students already have a basic understanding of Java\nProgramming and basic OO concepts. Students will explore advanced OOP\nconcepts and the relationship between design in UML and its expression in code\nand how this is supported by modelling tools and development platforms&amp;quot;\n \nMany thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117z5ii", "is_robot_indexable": true, "report_reasons": null, "author": "GLizard0611", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117z5ii/oop_and_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117z5ii/oop_and_design_pattern/", "subreddit_subscribers": 90408, "created_utc": 1676970781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My startup has built a knowledge management SaaS product, and I\u2019ve been thinking of launching our backend platform as a separate API service. \n\nIt would be like Hasura for unstructured data.\n\nWould this API be useful for any applications you\u2019re building?  Trying to evaluate if there\u2019s a market need.\n\n\ud83d\udca1 It would work like this:\n\nYou can upload any file type, like PDFs, videos, images or even CAD drawings.  Or, you can create \u201cfeeds\u201d from places like Google Drive or an S3 bucket.\n\nWe run ML on the files to extract embedded data, like references to companies, places, topics, or even visual objects.  Audio and video files will generate searchable transcripts. ML is configurable, if you have an endpoint for your own model.\n\nYou can configure enrichment of that data to provide more context, like layering in Crunchbase data for companies or LinkedIn for people.\n\nThen, you can use our GraphQL API to query the extracted data: filter by properties, and similarity search by time, geolocation or full text.  Also it supports ChatGPT-like Q&amp;A over the data.  \n\nWe are also considering adding data publishing to deliver the extracted data to other data pipelines, if you wouldn\u2019t want the GraphQL API.", "author_fullname": "t2_15wnt5aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Files -&gt; ML -&gt; GraphQL API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117qdy0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676949971.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676941961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My startup has built a knowledge management SaaS product, and I\u2019ve been thinking of launching our backend platform as a separate API service. &lt;/p&gt;\n\n&lt;p&gt;It would be like Hasura for unstructured data.&lt;/p&gt;\n\n&lt;p&gt;Would this API be useful for any applications you\u2019re building?  Trying to evaluate if there\u2019s a market need.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udca1 It would work like this:&lt;/p&gt;\n\n&lt;p&gt;You can upload any file type, like PDFs, videos, images or even CAD drawings.  Or, you can create \u201cfeeds\u201d from places like Google Drive or an S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;We run ML on the files to extract embedded data, like references to companies, places, topics, or even visual objects.  Audio and video files will generate searchable transcripts. ML is configurable, if you have an endpoint for your own model.&lt;/p&gt;\n\n&lt;p&gt;You can configure enrichment of that data to provide more context, like layering in Crunchbase data for companies or LinkedIn for people.&lt;/p&gt;\n\n&lt;p&gt;Then, you can use our GraphQL API to query the extracted data: filter by properties, and similarity search by time, geolocation or full text.  Also it supports ChatGPT-like Q&amp;amp;A over the data.  &lt;/p&gt;\n\n&lt;p&gt;We are also considering adding data publishing to deliver the extracted data to other data pipelines, if you wouldn\u2019t want the GraphQL API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117qdy0", "is_robot_indexable": true, "report_reasons": null, "author": "DeadPukka", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117qdy0/files_ml_graphql_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117qdy0/files_ml_graphql_api/", "subreddit_subscribers": 90408, "created_utc": 1676941961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_27c3plee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog post: Metadata-driven pipelines in Azure Data Factory | Part 4 - Analytical Processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_117jyag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zpy9fyhlUkl11M_XfA6lhAEKh4-fOg0k0ZlLQvSCMzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676925802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanrg.blogspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?auto=webp&amp;v=enabled&amp;s=acf6b162314a17c0afbe67b2c22c8ab66e5c7332", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=976c7f9865030935e649d015169ac172cc6f5e6c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85dd501dc2bb75e6a72a1a225332106070cbcc91", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=212afa4d32af2f1746ea2623c764edf1f12a0a68", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57d89326fd6c1b4391a282e522ca8df8fc10add5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cc3e3e5d8e02be53ed74cabf9db3c4e2e051c9f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/9y3X63cO3MEkEAdOwcjO7w0QEAFe4H-Qvk9imDiVGyM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33a493bbc6aef040b48a9a0e128e6d65f2e0c3e0", "width": 1080, "height": 567}], "variants": {}, "id": "_z5Q-MwAZcOLYV1qeyPkktnXQXLmlsXbk1lpBU4Gubg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "117jyag", "is_robot_indexable": true, "report_reasons": null, "author": "RayisImayev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117jyag/blog_post_metadatadriven_pipelines_in_azure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datanrg.blogspot.com/2023/02/metadata-driven-pipelines-in-azure-data.html", "subreddit_subscribers": 90408, "created_utc": 1676925802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nour company has a DWH which still runs on a AWS RDS Postgres Instance (t3.2xlarge, gp3, &gt;1TB storage)\n\nThere are plans to migrate to Snowflake pretty soon, however at the moment all of the pipelines at night completely drain the EBS Bytes Balance, leading to very long running queries.\n\nAs a short term solution, to which different instance type could we migrate? Memory Optimized? I have read, that burstable instances like t3 are not very suitable for that.\n\n&amp;#x200B;\n\nCheers,\n\nMatt", "author_fullname": "t2_5o9ebpsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS RDS Postgres Bursting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1180hok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676976012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;our company has a DWH which still runs on a AWS RDS Postgres Instance (t3.2xlarge, gp3, &amp;gt;1TB storage)&lt;/p&gt;\n\n&lt;p&gt;There are plans to migrate to Snowflake pretty soon, however at the moment all of the pipelines at night completely drain the EBS Bytes Balance, leading to very long running queries.&lt;/p&gt;\n\n&lt;p&gt;As a short term solution, to which different instance type could we migrate? Memory Optimized? I have read, that burstable instances like t3 are not very suitable for that.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Matt&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1180hok", "is_robot_indexable": true, "report_reasons": null, "author": "mosquitsch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1180hok/aws_rds_postgres_bursting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1180hok/aws_rds_postgres_bursting/", "subreddit_subscribers": 90408, "created_utc": 1676976012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are working on setting up a data lake house along the lines of Medallion architecture using ADF and Databricks in Azure. \n\nAll the data sources have structured data stored in their databases. Although streaming based use cases can come in future, presently it is mainly for reporting use cases that dont need real time data, and data can be ingested in batch \n\nAs part of the ingestion step - we are having conversations on the Push vs Pull approach. We are leaning towards Push approach , asking Source systems to push their data into the landing zone. We then pick the data and process it across bronze,silver and Gold layers. This is done considering below:\n\n* The advantage of pushing is that source system know their data and they know what they are pushing. No component knows (or should not know) the data better than the component who owns the data, which will theoretically imply a better design and a more robust system.\n\nHowever, source teams are of the opinion to opt for Pull approach as they dont have to spend time and efforts to develop the \"Push\" system. Our reasons asking for \"Push\" approach is not just based on the efforts, but on other factors like - avoid performance related issues on Upstream by firing un-optimized queries, avoid direct access to Upstream system and their data etc..\n\nWhat are your thoughts on this topic? Any major reasons to pick one approach over the another?", "author_fullname": "t2_5wiktsdf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting data into Azure - Push or Pull Approach?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117zurk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676973549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are working on setting up a data lake house along the lines of Medallion architecture using ADF and Databricks in Azure. &lt;/p&gt;\n\n&lt;p&gt;All the data sources have structured data stored in their databases. Although streaming based use cases can come in future, presently it is mainly for reporting use cases that dont need real time data, and data can be ingested in batch &lt;/p&gt;\n\n&lt;p&gt;As part of the ingestion step - we are having conversations on the Push vs Pull approach. We are leaning towards Push approach , asking Source systems to push their data into the landing zone. We then pick the data and process it across bronze,silver and Gold layers. This is done considering below:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The advantage of pushing is that source system know their data and they know what they are pushing. No component knows (or should not know) the data better than the component who owns the data, which will theoretically imply a better design and a more robust system.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, source teams are of the opinion to opt for Pull approach as they dont have to spend time and efforts to develop the &amp;quot;Push&amp;quot; system. Our reasons asking for &amp;quot;Push&amp;quot; approach is not just based on the efforts, but on other factors like - avoid performance related issues on Upstream by firing un-optimized queries, avoid direct access to Upstream system and their data etc..&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this topic? Any major reasons to pick one approach over the another?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117zurk", "is_robot_indexable": true, "report_reasons": null, "author": "jeebee91", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117zurk/getting_data_into_azure_push_or_pull_approach/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117zurk/getting_data_into_azure_push_or_pull_approach/", "subreddit_subscribers": 90408, "created_utc": 1676973549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to AWS, yet there are 6-8 services that I know and I was wondering what options do I have for storing credentials (api keys, passwords and etc) to use inside scripts and stuff within EC2 instances. What are your experiences, preferences and why?\nWould SSM or Vault be a great option to use?", "author_fullname": "t2_se9r0ncn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which service would you recommend for storing credentials in AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117zpn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676973009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to AWS, yet there are 6-8 services that I know and I was wondering what options do I have for storing credentials (api keys, passwords and etc) to use inside scripts and stuff within EC2 instances. What are your experiences, preferences and why?\nWould SSM or Vault be a great option to use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "117zpn3", "is_robot_indexable": true, "report_reasons": null, "author": "Honest_Panic6434", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117zpn3/which_service_would_you_recommend_for_storing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117zpn3/which_service_would_you_recommend_for_storing/", "subreddit_subscribers": 90408, "created_utc": 1676973009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, i am new to reddit stuff. This is the 1dt que. Pardon me if i have violated something.\n\nMy question:- I am in mid of doing post graduation in data science from good reputed institute. Now I have already finished course on Data Tools and machine learning. I like technical things but really shit scared with hardcore programming stuff. I have gone through python basics with pandas numpy and Visualization libraries but i enjoy SQL more and feel little more gripped doing SQL than python. My institute offers specialisations and i have 3 option 1: Data engineering, 2:- Data analyst 3:- business analyst. I am not from Tech background. I am self employed and into health and wellness. but after having a kid i pushed myself for academics. Now I am confused. Any idea at what depth Coding will be required in data engineering as I want to do that but what if Coding is too in depth and throw me out of the bus???", "author_fullname": "t2_b66mo7az", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data engineering or Business analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117z50r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676974500.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676970717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, i am new to reddit stuff. This is the 1dt que. Pardon me if i have violated something.&lt;/p&gt;\n\n&lt;p&gt;My question:- I am in mid of doing post graduation in data science from good reputed institute. Now I have already finished course on Data Tools and machine learning. I like technical things but really shit scared with hardcore programming stuff. I have gone through python basics with pandas numpy and Visualization libraries but i enjoy SQL more and feel little more gripped doing SQL than python. My institute offers specialisations and i have 3 option 1: Data engineering, 2:- Data analyst 3:- business analyst. I am not from Tech background. I am self employed and into health and wellness. but after having a kid i pushed myself for academics. Now I am confused. Any idea at what depth Coding will be required in data engineering as I want to do that but what if Coding is too in depth and throw me out of the bus???&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117z50r", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Play6458", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117z50r/data_engineering_or_business_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117z50r/data_engineering_or_business_analyst/", "subreddit_subscribers": 90408, "created_utc": 1676970717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing a test for a job application where one of the tasks is to do the following:\n\nFor each Id number in df['AccountId'] check if it exists in that same column as a preffix to another value.\n\nEg.\n\ndf['AccountId'] is 1.1 for row X  and df['AccountId'] is 1.1.1 for row Y\n\nThat will determine wether column df['Type'] will be filled with value 'X', 'Y' or 'Z'.\n\nSo if there's no entry in df['AccountId'] where row X is a preffix to it, then row X will have 'Z' as value for df['Type']. But if there is, then row X will have 'Y' as value for df['Type'].\n\nAny idea what's the optimal way to do this?\n\nI've tried many approaches but I keep running on all sorts of errors like 'The truth value of a Series is ambiguous'.\n\nSorry if this is not the place to post this and thanks for any help!", "author_fullname": "t2_32b255ac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help checking for a certain condition for each value in a column in a Pandas dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_117iu5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676923155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing a test for a job application where one of the tasks is to do the following:&lt;/p&gt;\n\n&lt;p&gt;For each Id number in df[&amp;#39;AccountId&amp;#39;] check if it exists in that same column as a preffix to another value.&lt;/p&gt;\n\n&lt;p&gt;Eg.&lt;/p&gt;\n\n&lt;p&gt;df[&amp;#39;AccountId&amp;#39;] is 1.1 for row X  and df[&amp;#39;AccountId&amp;#39;] is 1.1.1 for row Y&lt;/p&gt;\n\n&lt;p&gt;That will determine wether column df[&amp;#39;Type&amp;#39;] will be filled with value &amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39; or &amp;#39;Z&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;So if there&amp;#39;s no entry in df[&amp;#39;AccountId&amp;#39;] where row X is a preffix to it, then row X will have &amp;#39;Z&amp;#39; as value for df[&amp;#39;Type&amp;#39;]. But if there is, then row X will have &amp;#39;Y&amp;#39; as value for df[&amp;#39;Type&amp;#39;].&lt;/p&gt;\n\n&lt;p&gt;Any idea what&amp;#39;s the optimal way to do this?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried many approaches but I keep running on all sorts of errors like &amp;#39;The truth value of a Series is ambiguous&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is not the place to post this and thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "117iu5h", "is_robot_indexable": true, "report_reasons": null, "author": "torvi97", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/117iu5h/need_some_help_checking_for_a_certain_condition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/117iu5h/need_some_help_checking_for_a_certain_condition/", "subreddit_subscribers": 90408, "created_utc": 1676923155.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}