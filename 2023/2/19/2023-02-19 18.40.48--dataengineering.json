{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My boss heard talk of \u201cdata meshes\u201d at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.\n\nOur conversation:\n\n\u201cSo we\u2019re giving ownership of data engineering and architecture to the business units?\u201d \u201cNo, there\u2019ll still be just the central data team continuing to fulfill requests in a round-robin fashion.\u201d\n\n\u201cWhy are we doing this?\u201d \u201cBecause it\u2019s better and we want to be forward thinking.\u201d\n\nAm I missing something here about the magic of data meshes if you\u2019re not actually transferring data ownership to the business units? Isn\u2019t that kinda the point?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boss wants a data mesh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115vbvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676762792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My boss heard talk of \u201cdata meshes\u201d at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.&lt;/p&gt;\n\n&lt;p&gt;Our conversation:&lt;/p&gt;\n\n&lt;p&gt;\u201cSo we\u2019re giving ownership of data engineering and architecture to the business units?\u201d \u201cNo, there\u2019ll still be just the central data team continuing to fulfill requests in a round-robin fashion.\u201d&lt;/p&gt;\n\n&lt;p&gt;\u201cWhy are we doing this?\u201d \u201cBecause it\u2019s better and we want to be forward thinking.\u201d&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here about the magic of data meshes if you\u2019re not actually transferring data ownership to the business units? Isn\u2019t that kinda the point?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115vbvp", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/", "subreddit_subscribers": 90198, "created_utc": 1676762792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn't paying well in DE job market. So would you advise anything which should be my best bet?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are the highest paying tech frameworks or programming languages in Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115lbja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676743691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn&amp;#39;t paying well in DE job market. So would you advise anything which should be my best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "115lbja", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115lbja/which_are_the_highest_paying_tech_frameworks_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115lbja/which_are_the_highest_paying_tech_frameworks_or/", "subreddit_subscribers": 90198, "created_utc": 1676743691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will start.\n\nI would create a standardized framework to create airflow dags. Due to our inexperience with airflow, everyone created their own version of dags to achieve the same ETL tasks. This has led to unnecessary duplication of code and maintenance nightmare. And since all the jobs are prod critical and interdependent, we cannot phase these dags out. Hence we are perpetually stuck with them", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you could redo your company's data warehouse/data lake/data infrastructure, what would you do differently due to the benefit of hindsight?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1169v89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676812023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will start.&lt;/p&gt;\n\n&lt;p&gt;I would create a standardized framework to create airflow dags. Due to our inexperience with airflow, everyone created their own version of dags to achieve the same ETL tasks. This has led to unnecessary duplication of code and maintenance nightmare. And since all the jobs are prod critical and interdependent, we cannot phase these dags out. Hence we are perpetually stuck with them&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1169v89", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1169v89/if_you_could_redo_your_companys_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1169v89/if_you_could_redo_your_companys_data/", "subreddit_subscribers": 90198, "created_utc": 1676812023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learn the differences between streaming databases, real-time OLAP databases, and stream processing", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_116273b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/m12X5bRyTnF43-JWIEJfJbtDedSSegMSyogc5hlyxtY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676783255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learn the differences between streaming databases, real-time OLAP databases, and stream processing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/stream-processing-vs-real-time-olap?r=46sqk&amp;utm_medium=ios&amp;utm_campaign=post", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?auto=webp&amp;v=enabled&amp;s=a7d0085c1936bbf1782ea420f65e41cfe6d80cdd", "width": 906, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01cc6aa5aab2ce0b5e9eec8e037aabea3ce231ce", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83e9ebbf277433dc7eed8d70e5f8d67d09ce8103", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db8dd20a1ef9ec3489697f6b91206f1fc602049", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8360069fd7707d9466c134b46d8389113481223", "width": 640, "height": 423}], "variants": {}, "id": "_VuVKTXnegN6nzjepsmw478Ywq3LWIdi_B29ArkOsZ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "116273b", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116273b/streaming_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/stream-processing-vs-real-time-olap?r=46sqk&amp;utm_medium=ios&amp;utm_campaign=post", "subreddit_subscribers": 90198, "created_utc": 1676783255.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "*Hoping for an interesting thread*", "author_fullname": "t2_27lz615w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the toughest DE problem you faced in your work career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_116a03p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676812430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hoping for an interesting thread&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116a03p", "is_robot_indexable": true, "report_reasons": null, "author": "priprocks", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/", "subreddit_subscribers": 90198, "created_utc": 1676812430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there an option for me here? Currently on Snowflake, and I know both our unit cost as well as our usage.\n\nWe\u2019re on AWS. I\u2019m trying to understand what the likely cost would be of running the same pipelines using dbt on Hudi over S3 or Databricks in comparison to the existing cost of dbt on Snowflake.\n\nIs there an easy (ish) way to go about this?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand likely costs without needing to PoC everything", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1163cc7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676787363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an option for me here? Currently on Snowflake, and I know both our unit cost as well as our usage.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re on AWS. I\u2019m trying to understand what the likely cost would be of running the same pipelines using dbt on Hudi over S3 or Databricks in comparison to the existing cost of dbt on Snowflake.&lt;/p&gt;\n\n&lt;p&gt;Is there an easy (ish) way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1163cc7", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1163cc7/trying_to_understand_likely_costs_without_needing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1163cc7/trying_to_understand_likely_costs_without_needing/", "subreddit_subscribers": 90198, "created_utc": 1676787363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've used Airflow since the early days and in fact made a few small contributions to the codebase back in \"the day\", so I'm extremely familiar with it. I'm about 3 years out of the loop though and I'm wondering if there's anything new as far as technologies or best practices.\n\nHere's some information about the requirements:\n\n1. ETLs will extract from local machine (at first) and send to S3.\n2. Looking to run it in containers so it can eventually be deployed to the cloud but will initially be run locally.\n\nI was able to get a basic situation rigged up and running with a few tweaks to [this docker-compose.yml](https://github.com/marclamberti/docker-airflow/blob/main/docker-compose.yml) file but I have a few questions:\n\n1. **Should I be using breeze?** Reading about breeze made me suspect it might be more of a tool that is used to develop airflow itself with an rarely used optional feature allowing the building of production images, but sometimes it's hard to tell these things just by reading on the internet so I'm looking for other feedback.\n2. **Are there any airflow-specific reasons to prefer podman over docker?**\n3. **Is it unwise to use docker-compose to orchestrate containers?** I was using AWS ECS to orchestrate airflow containers a bunch of years ago but then went back to a setup that just used an EC2 instance.\n4. **Any other advice?**\n\nThanks.", "author_fullname": "t2_96txdx2p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow: Any advice for someone with many years of Airflow experience but who has been out of the loop for a few years on the latest and greatest best practices? In particular I'm wondering about \"Airflow Breeze\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115x9q5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676790527.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676768320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used Airflow since the early days and in fact made a few small contributions to the codebase back in &amp;quot;the day&amp;quot;, so I&amp;#39;m extremely familiar with it. I&amp;#39;m about 3 years out of the loop though and I&amp;#39;m wondering if there&amp;#39;s anything new as far as technologies or best practices.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some information about the requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;ETLs will extract from local machine (at first) and send to S3.&lt;/li&gt;\n&lt;li&gt;Looking to run it in containers so it can eventually be deployed to the cloud but will initially be run locally.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I was able to get a basic situation rigged up and running with a few tweaks to &lt;a href=\"https://github.com/marclamberti/docker-airflow/blob/main/docker-compose.yml\"&gt;this docker-compose.yml&lt;/a&gt; file but I have a few questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Should I be using breeze?&lt;/strong&gt; Reading about breeze made me suspect it might be more of a tool that is used to develop airflow itself with an rarely used optional feature allowing the building of production images, but sometimes it&amp;#39;s hard to tell these things just by reading on the internet so I&amp;#39;m looking for other feedback.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Are there any airflow-specific reasons to prefer podman over docker?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Is it unwise to use docker-compose to orchestrate containers?&lt;/strong&gt; I was using AWS ECS to orchestrate airflow containers a bunch of years ago but then went back to a setup that just used an EC2 instance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Any other advice?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?auto=webp&amp;v=enabled&amp;s=b11713aa8d2f1f5b86d41094cd020ffcce5aa6a4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b016dc65fc63dee1e0e2848c31f9563b5b46262f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ab95253edc21b69d0cd67536435bf1b81ca8457", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08f592c95207484a6d9f09b117aaf638ecaecece", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72564aa423e78df38522940b3f2db630fb3c55b3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52ace1016ea0ceb3219aa6a5de947a572b3271b3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a5d9c3bdc226a6a36176c9b3ee19762e976cc68", "width": 1080, "height": 540}], "variants": {}, "id": "RLobfrRDx0KU8k0LkdNkGpVdk0R2NJlZ3FHqWaHw4E4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115x9q5", "is_robot_indexable": true, "report_reasons": null, "author": "thenextsymbol", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115x9q5/airflow_any_advice_for_someone_with_many_years_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115x9q5/airflow_any_advice_for_someone_with_many_years_of/", "subreddit_subscribers": 90198, "created_utc": 1676768320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote a post that you might enjoy if you're working with Delta tables and are struggling to keep them at max performance. It's a hands-on tutorial with a default dataset that shows how Delta's maintenance commands work at a lower level.\n\n[https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e](https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e)\n\nIf you like it, don't hesitate to drop a follow as I write often about Data and the Apache Spark ecosystem", "author_fullname": "t2_jxqw29xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake - Keeping it fast and clean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1169lak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676811193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a post that you might enjoy if you&amp;#39;re working with Delta tables and are struggling to keep them at max performance. It&amp;#39;s a hands-on tutorial with a default dataset that shows how Delta&amp;#39;s maintenance commands work at a lower level.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e\"&gt;https://medium.com/towards-data-science/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you like it, don&amp;#39;t hesitate to drop a follow as I write often about Data and the Apache Spark ecosystem&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?auto=webp&amp;v=enabled&amp;s=00b7fe2c8dc3d0d997638f61ae9d08eba78b699d", "width": 1063, "height": 777}, "resolutions": [{"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60633c68620342e9e456784cd2273bfb52644bca", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1da243eda34aa2295752f812e454828c560ac14", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2d16d68365e7c4bc1f7a1047f974d6c03299f55", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc8c922a41a754aa05dbd16081f985e3c2120aad", "width": 640, "height": 467}, {"url": "https://external-preview.redd.it/68Czml4E3N3IwSpgu9g32uRHu74kN_kcM4sqBj9rpcQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbd639b9a8ec5091e663fe3dcdce6ed195b7cef3", "width": 960, "height": 701}], "variants": {}, "id": "vyVHtKourTs-IBsK9lRJgvoNCNNc6PhgHIcVh1WBfuE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1169lak", "is_robot_indexable": true, "report_reasons": null, "author": "orpheuz24", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1169lak/delta_lake_keeping_it_fast_and_clean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1169lak/delta_lake_keeping_it_fast_and_clean/", "subreddit_subscribers": 90198, "created_utc": 1676811193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What would be the best way to setup RabbitMQ output to TimescaleDB? Is there any \u201cnative\u201d way without using any third application as a middleware? Or we need to setup some API endpoint for the DB?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RabbitMQ output to TimescaleDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1165n0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676796261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would be the best way to setup RabbitMQ output to TimescaleDB? Is there any \u201cnative\u201d way without using any third application as a middleware? Or we need to setup some API endpoint for the DB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1165n0e", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1165n0e/rabbitmq_output_to_timescaledb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1165n0e/rabbitmq_output_to_timescaledb/", "subreddit_subscribers": 90198, "created_utc": 1676796261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have moved to a new job that is a complete MS shop using PowerBI. Previously worked with AWS and very familiar with the S3 /glue/Athena using JSON and parquet to build a DW, fairly low cost compared to redshift.\n\nWhat would be the equivalent tools on Azure? Having trouble finding something where I can stick a load of text or parquet and build some formal schemas like in Glue and then connect the finished tables to PowerBI.\n\nEventually I would like to use DBT and Dagster for orchestration.", "author_fullname": "t2_clh5r1ln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure equivalent of S3/Glue/Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115quz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676753700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have moved to a new job that is a complete MS shop using PowerBI. Previously worked with AWS and very familiar with the S3 /glue/Athena using JSON and parquet to build a DW, fairly low cost compared to redshift.&lt;/p&gt;\n\n&lt;p&gt;What would be the equivalent tools on Azure? Having trouble finding something where I can stick a load of text or parquet and build some formal schemas like in Glue and then connect the finished tables to PowerBI.&lt;/p&gt;\n\n&lt;p&gt;Eventually I would like to use DBT and Dagster for orchestration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115quz6", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Restaurant-9691", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115quz6/azure_equivalent_of_s3glueathena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115quz6/azure_equivalent_of_s3glueathena/", "subreddit_subscribers": 90198, "created_utc": 1676753700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019ve been stuck with coming up with a plan to \u201cmodernize\u201d our data stack, but I\u2019m a bit fuzzy on a few details. Hoping some can provide insights. \n\nBoss wants to migrate towards the \u201cmodern\u201d stack (hello buzz words) and asked me to put a plan together. \n\nI have a general idea of the flow (load data from Airbyte into snowflake then run DBT models) but I\u2019m confused on the compute part. Where the heck does the orchestration call the compute resources. For DBT models I\u2019m assuming snowflake credits are used, but how does it work for ML workflows? Are you running jobs that pull/push directly into a warehouse? \n\nWe are a gov org so we support a ton of business units all doing god knows what at times, so they to centralize into our team to run the pipelines for everyone else. \n\nI\u2019m well familiar with python, and have been using DBT for jobs recently (in SQL server), so would love to stay in that ecosystem to start but I\u2019m fuzzy on what to recommend. Basically starting out with ELT jobs for now the later they want to do ML. \n\nAny suggestions, or books or guides to read up on? Total zoo that it\u2019s on me in an org our size , but I see it as a good opportunity to drive the data platform at our org.\n\nTLDR. Data leaves our systems and goes to a warehouse. Where does the compute happen. Where do ML pipelines get their compute. \n\nOther way to frame it. In a greenfield data org, what would you do. \n\nThanks!", "author_fullname": "t2_3sioksrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about data pipeline setup/structures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115xtyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676769980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019ve been stuck with coming up with a plan to \u201cmodernize\u201d our data stack, but I\u2019m a bit fuzzy on a few details. Hoping some can provide insights. &lt;/p&gt;\n\n&lt;p&gt;Boss wants to migrate towards the \u201cmodern\u201d stack (hello buzz words) and asked me to put a plan together. &lt;/p&gt;\n\n&lt;p&gt;I have a general idea of the flow (load data from Airbyte into snowflake then run DBT models) but I\u2019m confused on the compute part. Where the heck does the orchestration call the compute resources. For DBT models I\u2019m assuming snowflake credits are used, but how does it work for ML workflows? Are you running jobs that pull/push directly into a warehouse? &lt;/p&gt;\n\n&lt;p&gt;We are a gov org so we support a ton of business units all doing god knows what at times, so they to centralize into our team to run the pipelines for everyone else. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m well familiar with python, and have been using DBT for jobs recently (in SQL server), so would love to stay in that ecosystem to start but I\u2019m fuzzy on what to recommend. Basically starting out with ELT jobs for now the later they want to do ML. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions, or books or guides to read up on? Total zoo that it\u2019s on me in an org our size , but I see it as a good opportunity to drive the data platform at our org.&lt;/p&gt;\n\n&lt;p&gt;TLDR. Data leaves our systems and goes to a warehouse. Where does the compute happen. Where do ML pipelines get their compute. &lt;/p&gt;\n\n&lt;p&gt;Other way to frame it. In a greenfield data org, what would you do. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115xtyg", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Stay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115xtyg/questions_about_data_pipeline_setupstructures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115xtyg/questions_about_data_pipeline_setupstructures/", "subreddit_subscribers": 90198, "created_utc": 1676769980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a personal project that will end up with a R Shiny dashboard I want to deploy. My challenge is that the amount of data I'm working with is too cumbersome for my personal computer so I'd like to set up something remote to work off of. Unfortunately, I only have experience working as a data scientist and have no experience developing / deploying any of the back end infrastructure.\n\n&amp;#x200B;\n\nI need to be able to store data (downloaded as csvs from the internet) and then clean / manipulate that data (preferably using a spark API like sparkr or pyspark)  so that its ready for the shiny application. I was looking for help with recommendations for the best way to do this thats both cost effective and relatively simple given my limited data engineering experience (if any exists).", "author_fullname": "t2_a2s12j5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data storage and manipulation recs for personal project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115xhyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676768989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a personal project that will end up with a R Shiny dashboard I want to deploy. My challenge is that the amount of data I&amp;#39;m working with is too cumbersome for my personal computer so I&amp;#39;d like to set up something remote to work off of. Unfortunately, I only have experience working as a data scientist and have no experience developing / deploying any of the back end infrastructure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to be able to store data (downloaded as csvs from the internet) and then clean / manipulate that data (preferably using a spark API like sparkr or pyspark)  so that its ready for the shiny application. I was looking for help with recommendations for the best way to do this thats both cost effective and relatively simple given my limited data engineering experience (if any exists).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115xhyw", "is_robot_indexable": true, "report_reasons": null, "author": "ghtghtghtghtght", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115xhyw/data_storage_and_manipulation_recs_for_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115xhyw/data_storage_and_manipulation_recs_for_personal/", "subreddit_subscribers": 90198, "created_utc": 1676768989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How much do popular reverse ETL tools (e.g. Census, Hightouch) cost at the enterprise level (e.g. ability to push to 10 destinations, 5+ user seats etc.).   The pricing pages for Census and Hightouch don't list their enterprise level pricing.  My company is considering using one of these tools and we believe our use cases require us to get an enterprise plan, but for internal reasons we'd prefer not to share our info with them to get a price quote until we understand whether they are in our price range.", "author_fullname": "t2_42oggxjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much do reverse ETL tools cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115w0q7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676764706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How much do popular reverse ETL tools (e.g. Census, Hightouch) cost at the enterprise level (e.g. ability to push to 10 destinations, 5+ user seats etc.).   The pricing pages for Census and Hightouch don&amp;#39;t list their enterprise level pricing.  My company is considering using one of these tools and we believe our use cases require us to get an enterprise plan, but for internal reasons we&amp;#39;d prefer not to share our info with them to get a price quote until we understand whether they are in our price range.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115w0q7", "is_robot_indexable": true, "report_reasons": null, "author": "AlexSanders123", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115w0q7/how_much_do_reverse_etl_tools_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115w0q7/how_much_do_reverse_etl_tools_cost/", "subreddit_subscribers": 90198, "created_utc": 1676764706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use data from MS Dynamics CRM in Power BI via the API. However, the data size is very large and covers more than 10 years, which is causing long load times and poor report update efficiency in Power BI. Even if I use the Modeling feature in Power BI to filter the data, it still loads all the data before applying the filters, which is inefficient. I am seeking recommendations for tools and strategies to improve the speed and efficiency of my system. Any advice would be greatly appreciated.\n\nEdit: Do you think moving the data from CRM to Azure DW first and then connecting BI to DW would be helpful? or that is doesn't worth the cost? Is there any other tools that helps this process?", "author_fullname": "t2_oaaif8zw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS Dynamic CRM to Power BI slow loading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115tori", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676759964.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676758513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use data from MS Dynamics CRM in Power BI via the API. However, the data size is very large and covers more than 10 years, which is causing long load times and poor report update efficiency in Power BI. Even if I use the Modeling feature in Power BI to filter the data, it still loads all the data before applying the filters, which is inefficient. I am seeking recommendations for tools and strategies to improve the speed and efficiency of my system. Any advice would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Edit: Do you think moving the data from CRM to Azure DW first and then connecting BI to DW would be helpful? or that is doesn&amp;#39;t worth the cost? Is there any other tools that helps this process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115tori", "is_robot_indexable": true, "report_reasons": null, "author": "ugotbkidding", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115tori/ms_dynamic_crm_to_power_bi_slow_loading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115tori/ms_dynamic_crm_to_power_bi_slow_loading/", "subreddit_subscribers": 90198, "created_utc": 1676758513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am not sure I understand datalake folder structuring correctly:\n\nIf I get everyday new file for lets say sales for some region I would save this file like this:\n\nsales/region/yyyy/mm/dd/file\n\nor\n\nsales/file ?\n\n\nBecause if I understand it correctly - e.g. in Trino I pass the folder with the data (parquet files) and Trino itself loads the files. But if I would have each file in separate folder it would be a problem? I would have to create table for each file manually? Also if I go with \u201csales/region\u201d I can create table with partitions by date and Trino automatically would create these \u201cyyyy/mm/dd\u201d folders? Sorry not sure I understand it correctly.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake - partitions vs folder structure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_116iqu0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676827986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure I understand datalake folder structuring correctly:&lt;/p&gt;\n\n&lt;p&gt;If I get everyday new file for lets say sales for some region I would save this file like this:&lt;/p&gt;\n\n&lt;p&gt;sales/region/yyyy/mm/dd/file&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;sales/file ?&lt;/p&gt;\n\n&lt;p&gt;Because if I understand it correctly - e.g. in Trino I pass the folder with the data (parquet files) and Trino itself loads the files. But if I would have each file in separate folder it would be a problem? I would have to create table for each file manually? Also if I go with \u201csales/region\u201d I can create table with partitions by date and Trino automatically would create these \u201cyyyy/mm/dd\u201d folders? Sorry not sure I understand it correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116iqu0", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116iqu0/datalake_partitions_vs_folder_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116iqu0/datalake_partitions_vs_folder_structure/", "subreddit_subscribers": 90198, "created_utc": 1676827986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm in the final interview stage which should be scheduled for sometime next week. \n\nThey've told me there will be an hour allocated to a 'case study' with no extra details. I have asked for some extra info but if they don't reply soon what can I do to prepare for in the meantime? Also they said in the previous stage there's no live coding test.", "author_fullname": "t2_4ba5z1zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case study for a final interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11692c3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676809409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m in the final interview stage which should be scheduled for sometime next week. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;ve told me there will be an hour allocated to a &amp;#39;case study&amp;#39; with no extra details. I have asked for some extra info but if they don&amp;#39;t reply soon what can I do to prepare for in the meantime? Also they said in the previous stage there&amp;#39;s no live coding test.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11692c3", "is_robot_indexable": true, "report_reasons": null, "author": "dreamr49", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11692c3/case_study_for_a_final_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11692c3/case_study_for_a_final_interview/", "subreddit_subscribers": 90198, "created_utc": 1676809409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At the end of our pipeline there's an aggregate table `'agg_table'` serving user requests from a webpage in realtime. The user request consists of just 3 parameter values (`A, B, C`) identifying the row in the table, s.t. it can basically be interpreted as the query `SELECT * FROM agg_table WHERE key_1=A AND key_2=B AND key_3=C`. However, it is rarely the case that a row with all 3 (or even 2) values exists in the table, s.t. it then defaults (according to business/domain logic) to `WHERE key_1=A AND key_2=B`, if that doesn't exist to `WHERE key_1=A AND key_3=C`, if that doesn't exist to `WHERE key_2=B AND key_3=C`, if that doesn't exist to `WHERE key_1=A` etc.\n\nIs there a recommended way to model the analytics table, with respect to dimensional modelling?", "author_fullname": "t2_cgxiixth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with fallbacks for webpage-serving analytics table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1168siq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676808397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At the end of our pipeline there&amp;#39;s an aggregate table &lt;code&gt;&amp;#39;agg_table&amp;#39;&lt;/code&gt; serving user requests from a webpage in realtime. The user request consists of just 3 parameter values (&lt;code&gt;A, B, C&lt;/code&gt;) identifying the row in the table, s.t. it can basically be interpreted as the query &lt;code&gt;SELECT * FROM agg_table WHERE key_1=A AND key_2=B AND key_3=C&lt;/code&gt;. However, it is rarely the case that a row with all 3 (or even 2) values exists in the table, s.t. it then defaults (according to business/domain logic) to &lt;code&gt;WHERE key_1=A AND key_2=B&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_1=A AND key_3=C&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_2=B AND key_3=C&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_1=A&lt;/code&gt; etc.&lt;/p&gt;\n\n&lt;p&gt;Is there a recommended way to model the analytics table, with respect to dimensional modelling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1168siq", "is_robot_indexable": true, "report_reasons": null, "author": "BusyFture", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1168siq/how_to_deal_with_fallbacks_for_webpageserving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1168siq/how_to_deal_with_fallbacks_for_webpageserving/", "subreddit_subscribers": 90198, "created_utc": 1676808397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_puwuw2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenTelemetry and Jaeger backend integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_1166bc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fEXyZ_c3fZGe2j4rQPkjNteedWy9j3_-iLRCy7vAQFk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676798999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "sprkl.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://sprkl.dev/opentelemetry-and-jaeger-backend-integration/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?auto=webp&amp;v=enabled&amp;s=e259aa7b86787155a6797f091035f0f0462de52f", "width": 2400, "height": 1372}, "resolutions": [{"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6a75b7661a406d3316b092c671d75c39fe3ae4a", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdce88fb6d1189b4e8368b8401ed276ba8679862", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54b127893599f27dc179177c71ff1f367f744940", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1508f19437ab96c30328b48e5694c4f783ce537", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c9b32202a7a3a4668219731ae2b7999aa2d269e", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9118d8fe1a143ae1dde1d977b351e03a6b7687d2", "width": 1080, "height": 617}], "variants": {}, "id": "72XcLpUgjyfjcmTmotcL-rNH5PXjxPQwKx42jDMLYJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1166bc3", "is_robot_indexable": true, "report_reasons": null, "author": "observability_geek", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1166bc3/opentelemetry_and_jaeger_backend_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://sprkl.dev/opentelemetry-and-jaeger-backend-integration/", "subreddit_subscribers": 90198, "created_utc": 1676798999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Helo, a few days ago I posted if a CS degree is better than a DE degree,and by concensus the answer was yes for various of reasons,such as this being a new major it won't be well constructed,in addition to not seeing what a DE degree could offer more than a CS one.\n\nWhere I live, which is not the US or UK this major is becoming very will known,so don't worry about jop opportunities.\n\nThe point of this post is that you guys presented some true and interesting points that made me look into the syllabus of the degree,and since you are more knowledgeable than me you can tell me if these courses that this degree is not worth it.\n\nSome of the courses:\nSoftware engineering .\nIntroduction to CS.\nInfinitesimal calculus and algebra.\nComputer organisation and operating. \nGame theory and economic behaviour.\nData structure and algorithms.\nIntroduction to compubility and complexity.\nFundamentals of AI and its applications.\nAlgebraic methods in DE.\nDistributed information systems.\nComputational learning. \n\nTo my knowledge the first 2 years we learn the same as CS students.\n\n\nEdit to add:with this degree I can work as a SE,DS,DA,and finally a ML engineer. \n\nThank you for your Patience and help :)", "author_fullname": "t2_tzb9l6f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This is what a DE degree include!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11666si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676799878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676798465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Helo, a few days ago I posted if a CS degree is better than a DE degree,and by concensus the answer was yes for various of reasons,such as this being a new major it won&amp;#39;t be well constructed,in addition to not seeing what a DE degree could offer more than a CS one.&lt;/p&gt;\n\n&lt;p&gt;Where I live, which is not the US or UK this major is becoming very will known,so don&amp;#39;t worry about jop opportunities.&lt;/p&gt;\n\n&lt;p&gt;The point of this post is that you guys presented some true and interesting points that made me look into the syllabus of the degree,and since you are more knowledgeable than me you can tell me if these courses that this degree is not worth it.&lt;/p&gt;\n\n&lt;p&gt;Some of the courses:\nSoftware engineering .\nIntroduction to CS.\nInfinitesimal calculus and algebra.\nComputer organisation and operating. \nGame theory and economic behaviour.\nData structure and algorithms.\nIntroduction to compubility and complexity.\nFundamentals of AI and its applications.\nAlgebraic methods in DE.\nDistributed information systems.\nComputational learning. &lt;/p&gt;\n\n&lt;p&gt;To my knowledge the first 2 years we learn the same as CS students.&lt;/p&gt;\n\n&lt;p&gt;Edit to add:with this degree I can work as a SE,DS,DA,and finally a ML engineer. &lt;/p&gt;\n\n&lt;p&gt;Thank you for your Patience and help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11666si", "is_robot_indexable": true, "report_reasons": null, "author": "khtoto", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11666si/this_is_what_a_de_degree_include/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11666si/this_is_what_a_de_degree_include/", "subreddit_subscribers": 90198, "created_utc": 1676798465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am wondering if you couldn't get away with a relational database for vectorisation.\n\nA while back I got to play with vectorisation of terms using BERT. We then held the vectors within an instance of Facebook's Fais. One of the irritating things was having to initialise a Fais instance which took a lot of time.\n\nWhich got me thinking, vectorisation is basically breaking unstructured data into tokens (groups of terms, a term is a word) e.g.\n\n&gt; The cat in the hat sat on the mat and drank milk from a jug\n\nFirst we remove stop words \n\n&gt; Cat hat sat mat drank milk from jug\n\nNow we convert into tokens\n\n* Cat hat sat mat\n* Hat sat mat drank\n* Sat mat drank milk\n* mat drank milk jug\n\nA Machine Learning model like bert has been trained on billions of tokens and has given them positions (vectors) within an N dimensional array (called vectorspace).\n\nThe idea is \"cat\" should be located near \"pet\", \"hat\" is close to \"headgear\", etc.. \n\nSo you provide tokens and in return you get vectors indicating where in vectorspace your token resides.\n\nWhich got me thinking, in every example I can think of there is normally an easy way to keep the size of a corpus (dataset) small. For example with facebook each user would represent a corpus.\n\nFacebook Fais requires a lot of RAM and Storage and you have to preload data for it to the index everything and it can literally take days to start. \n\nSo my thinking is.\n\nCreate a relational database (e.g postgres) for each corpus. The database should contain a \"vectors\" table. \n\nThe vector table contains an integer field for each vector column. A vector can have a many to many relationship with the \"token\" table.\n\nThe token table contains each token generated within a corpus and some means to reference the document tokenised.\n\nSo when a user enters a search string, we tokenise it and generate a list of vectors. We then build a query for each vector allowing a range around each vector field.\n\nWe run these queries on each database collecting the responses to each query. \n\nWe then rank the tokens based on the number of matching vectors.\n\nBecause our corpus is bounded each database will likely be limited to thousands of tokens and a few million vectors. Which is within the abilities of something like postgres.\n\nThis would let us replicate the same capability as Facebook Fais or Word2Vec.\n\nThoughts?", "author_fullname": "t2_172g1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vectorisation and Querying", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1164fus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676792314.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676791562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering if you couldn&amp;#39;t get away with a relational database for vectorisation.&lt;/p&gt;\n\n&lt;p&gt;A while back I got to play with vectorisation of terms using BERT. We then held the vectors within an instance of Facebook&amp;#39;s Fais. One of the irritating things was having to initialise a Fais instance which took a lot of time.&lt;/p&gt;\n\n&lt;p&gt;Which got me thinking, vectorisation is basically breaking unstructured data into tokens (groups of terms, a term is a word) e.g.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The cat in the hat sat on the mat and drank milk from a jug&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;First we remove stop words &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Cat hat sat mat drank milk from jug&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Now we convert into tokens&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cat hat sat mat&lt;/li&gt;\n&lt;li&gt;Hat sat mat drank&lt;/li&gt;\n&lt;li&gt;Sat mat drank milk&lt;/li&gt;\n&lt;li&gt;mat drank milk jug&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A Machine Learning model like bert has been trained on billions of tokens and has given them positions (vectors) within an N dimensional array (called vectorspace).&lt;/p&gt;\n\n&lt;p&gt;The idea is &amp;quot;cat&amp;quot; should be located near &amp;quot;pet&amp;quot;, &amp;quot;hat&amp;quot; is close to &amp;quot;headgear&amp;quot;, etc.. &lt;/p&gt;\n\n&lt;p&gt;So you provide tokens and in return you get vectors indicating where in vectorspace your token resides.&lt;/p&gt;\n\n&lt;p&gt;Which got me thinking, in every example I can think of there is normally an easy way to keep the size of a corpus (dataset) small. For example with facebook each user would represent a corpus.&lt;/p&gt;\n\n&lt;p&gt;Facebook Fais requires a lot of RAM and Storage and you have to preload data for it to the index everything and it can literally take days to start. &lt;/p&gt;\n\n&lt;p&gt;So my thinking is.&lt;/p&gt;\n\n&lt;p&gt;Create a relational database (e.g postgres) for each corpus. The database should contain a &amp;quot;vectors&amp;quot; table. &lt;/p&gt;\n\n&lt;p&gt;The vector table contains an integer field for each vector column. A vector can have a many to many relationship with the &amp;quot;token&amp;quot; table.&lt;/p&gt;\n\n&lt;p&gt;The token table contains each token generated within a corpus and some means to reference the document tokenised.&lt;/p&gt;\n\n&lt;p&gt;So when a user enters a search string, we tokenise it and generate a list of vectors. We then build a query for each vector allowing a range around each vector field.&lt;/p&gt;\n\n&lt;p&gt;We run these queries on each database collecting the responses to each query. &lt;/p&gt;\n\n&lt;p&gt;We then rank the tokens based on the number of matching vectors.&lt;/p&gt;\n\n&lt;p&gt;Because our corpus is bounded each database will likely be limited to thousands of tokens and a few million vectors. Which is within the abilities of something like postgres.&lt;/p&gt;\n\n&lt;p&gt;This would let us replicate the same capability as Facebook Fais or Word2Vec.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1164fus", "is_robot_indexable": true, "report_reasons": null, "author": "stevecrox0914", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1164fus/vectorisation_and_querying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1164fus/vectorisation_and_querying/", "subreddit_subscribers": 90198, "created_utc": 1676791562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I downloaded my extended streams from Spotify and I created two dimension tables one with the Track Metadata which contains the Artist Uri which connects an Artist Metadata. When I created the Artist Dimension table, I discovered that they can have multiple genres and some cases many genres. I am not sure how I should finalize this table. This will be a csv going into cloud storage. \n\nCurrently, I have it as follows \n\n    Artist_ID              Genres                                                                                                                                    \n    00FQb4jTyendYWaN8pK0wa art pop|pop                                                                 00G1NTDAoU7rBpjG4KoYAM downtempo|electronica|japanese old school \n    00sAr10UTV1JZtHqxsLVn4 canadian psychedelic rock| space rock\n\nI have each one separated with pipes, so when I query, I guess I could do like operator or some kind of sperate, take the nth one. \n\nThe other option though is to have the Artist\\_ID repeat for each unique genre but I'm not a fan off repeating the Artist\\_ID. \n\nWhat do you guys think would be the best solution in this case?", "author_fullname": "t2_z321026", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spotify Genre Date: Debating how I should store this table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115zj00", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676774800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded my extended streams from Spotify and I created two dimension tables one with the Track Metadata which contains the Artist Uri which connects an Artist Metadata. When I created the Artist Dimension table, I discovered that they can have multiple genres and some cases many genres. I am not sure how I should finalize this table. This will be a csv going into cloud storage. &lt;/p&gt;\n\n&lt;p&gt;Currently, I have it as follows &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Artist_ID              Genres                                                                                                                                    \n00FQb4jTyendYWaN8pK0wa art pop|pop                                                                 00G1NTDAoU7rBpjG4KoYAM downtempo|electronica|japanese old school \n00sAr10UTV1JZtHqxsLVn4 canadian psychedelic rock| space rock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I have each one separated with pipes, so when I query, I guess I could do like operator or some kind of sperate, take the nth one. &lt;/p&gt;\n\n&lt;p&gt;The other option though is to have the Artist_ID repeat for each unique genre but I&amp;#39;m not a fan off repeating the Artist_ID. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think would be the best solution in this case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115zj00", "is_robot_indexable": true, "report_reasons": null, "author": "raz_the_kid0901", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115zj00/spotify_genre_date_debating_how_i_should_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115zj00/spotify_genre_date_debating_how_i_should_store/", "subreddit_subscribers": 90198, "created_utc": 1676774800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone shed some light on where the VMs for any serverless compute are deployed? Since those are managed by a cloud provider, my assumption is that VMs are deployed in a separate VPC and fetch the data from the storage account in the customer VPC.\n\nIf my assumption is correct then how is the data transfer between different VPCs managed? I believe both the VPCs will be in the same region and availability zone.", "author_fullname": "t2_etr435xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where are those serverless compute(VMs) deployed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115x4w8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676767910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone shed some light on where the VMs for any serverless compute are deployed? Since those are managed by a cloud provider, my assumption is that VMs are deployed in a separate VPC and fetch the data from the storage account in the customer VPC.&lt;/p&gt;\n\n&lt;p&gt;If my assumption is correct then how is the data transfer between different VPCs managed? I believe both the VPCs will be in the same region and availability zone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115x4w8", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Ranger806", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115x4w8/where_are_those_serverless_computevms_deployed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115x4w8/where_are_those_serverless_computevms_deployed/", "subreddit_subscribers": 90198, "created_utc": 1676767910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been told that I should be careful telling people that I\u2019m a \u2018data engineer\u2019 because I don\u2019t have an engineering degree, and only qualified engineers can call themselves engineers. I\u2019m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it\u2019s my job title. Is this something people care about?", "author_fullname": "t2_py81dzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I call myself a data engineer if I don\u2019t have an \u2018engineering\u2019 qualification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1162sjq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676785392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been told that I should be careful telling people that I\u2019m a \u2018data engineer\u2019 because I don\u2019t have an engineering degree, and only qualified engineers can call themselves engineers. I\u2019m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it\u2019s my job title. Is this something people care about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1162sjq", "is_robot_indexable": true, "report_reasons": null, "author": "CyclicDombo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1162sjq/can_i_call_myself_a_data_engineer_if_i_dont_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1162sjq/can_i_call_myself_a_data_engineer_if_i_dont_have/", "subreddit_subscribers": 90198, "created_utc": 1676785392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m getting myself a new laptop for work and will be maintaining a few databases for clients within. This would be mostly 3rd party platform data, in the 10\u2019s of millions of rows.\n\nI would have a cloud backup of course and consider migrating to a cloud based sql server. \n\nWould I see a real world impact data wise without ECC ram?", "author_fullname": "t2_5cwjpire", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ECC ram? Warehousing client reporting data\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11604np", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676776564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m getting myself a new laptop for work and will be maintaining a few databases for clients within. This would be mostly 3rd party platform data, in the 10\u2019s of millions of rows.&lt;/p&gt;\n\n&lt;p&gt;I would have a cloud backup of course and consider migrating to a cloud based sql server. &lt;/p&gt;\n\n&lt;p&gt;Would I see a real world impact data wise without ECC ram?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11604np", "is_robot_indexable": true, "report_reasons": null, "author": "JusticeSoup", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11604np/ecc_ram_warehousing_client_reporting_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11604np/ecc_ram_warehousing_client_reporting_data/", "subreddit_subscribers": 90198, "created_utc": 1676776564.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}