{"kind": "Listing", "data": {"after": "t3_115x4w8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My boss heard talk of \u201cdata meshes\u201d at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.\n\nOur conversation:\n\n\u201cSo we\u2019re giving ownership of data engineering and architecture to the business units?\u201d \u201cNo, there\u2019ll still be just the central data team continuing to fulfill requests in a round-robin fashion.\u201d\n\n\u201cWhy are we doing this?\u201d \u201cBecause it\u2019s better and we want to be forward thinking.\u201d\n\nAm I missing something here about the magic of data meshes if you\u2019re not actually transferring data ownership to the business units? Isn\u2019t that kinda the point?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boss wants a data mesh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115vbvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676762792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My boss heard talk of \u201cdata meshes\u201d at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.&lt;/p&gt;\n\n&lt;p&gt;Our conversation:&lt;/p&gt;\n\n&lt;p&gt;\u201cSo we\u2019re giving ownership of data engineering and architecture to the business units?\u201d \u201cNo, there\u2019ll still be just the central data team continuing to fulfill requests in a round-robin fashion.\u201d&lt;/p&gt;\n\n&lt;p&gt;\u201cWhy are we doing this?\u201d \u201cBecause it\u2019s better and we want to be forward thinking.\u201d&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here about the magic of data meshes if you\u2019re not actually transferring data ownership to the business units? Isn\u2019t that kinda the point?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115vbvp", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/", "subreddit_subscribers": 90165, "created_utc": 1676762792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn't paying well in DE job market. So would you advise anything which should be my best bet?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are the highest paying tech frameworks or programming languages in Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115lbja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676743691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn&amp;#39;t paying well in DE job market. So would you advise anything which should be my best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "115lbja", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115lbja/which_are_the_highest_paying_tech_frameworks_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115lbja/which_are_the_highest_paying_tech_frameworks_or/", "subreddit_subscribers": 90165, "created_utc": 1676743691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking to implement a simple data lake architecture in S3, based almost 1 to 1 on the example outlined by AWS here: [https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html](https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html)\n\nI have a few general data lake questions that I'm having a hard time googling, and would love more insight into before I start implementing the above.\n\n1. Most data lake organization plans I see include some sort of timestamp in the key (i.e. `s3:/12345-raw/[source]/[year]/[month]/[day]/example.xlsx`). I think that this works really well for incremental data extractions, but should I use the same logic for a full data extraction? I have plenty of cases where I'm forced to do a full extraction, in these cases, it's also highly likely that `s3:/12345-raw/[source]/2023/02/18/example.xlsx` will have the same value as `s3:/12345-raw/[source]/2023/02/19/example.xlsx`. Even though I think it's simpler to stick to the same general convention everywhere and just fully load this data downstream, is it bad to have so much data duplication in the raw zone? Or does it not matter when cloud storage is so cheap?\n2. How should I track data lineage between different zones of the data lake? Files in the stage zone are produced from files in the raw zone, but it's not clear to me whether the lineage should be explicitly in the files of the stage zone, or whether the lineage is more implicit? Somehow derived from the key's source &amp; date?\n3. Other groups in my company basically go right from the raw zone to RedShift tables, and I'm not sure when RedShift would be better than Athena/Trino on a data lake, and when it isn't? If I do integrate a lake w/ raw, stage, and analytic zones w/ RedShift, is the choice basically to load into either RedShift or the analytic zone from stage?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help understanding some general use questions about S3 data lakes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115iyzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676737370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to implement a simple data lake architecture in S3, based almost 1 to 1 on the example outlined by AWS here: &lt;a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html\"&gt;https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a few general data lake questions that I&amp;#39;m having a hard time googling, and would love more insight into before I start implementing the above.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Most data lake organization plans I see include some sort of timestamp in the key (i.e. &lt;code&gt;s3:/12345-raw/[source]/[year]/[month]/[day]/example.xlsx&lt;/code&gt;). I think that this works really well for incremental data extractions, but should I use the same logic for a full data extraction? I have plenty of cases where I&amp;#39;m forced to do a full extraction, in these cases, it&amp;#39;s also highly likely that &lt;code&gt;s3:/12345-raw/[source]/2023/02/18/example.xlsx&lt;/code&gt; will have the same value as &lt;code&gt;s3:/12345-raw/[source]/2023/02/19/example.xlsx&lt;/code&gt;. Even though I think it&amp;#39;s simpler to stick to the same general convention everywhere and just fully load this data downstream, is it bad to have so much data duplication in the raw zone? Or does it not matter when cloud storage is so cheap?&lt;/li&gt;\n&lt;li&gt;How should I track data lineage between different zones of the data lake? Files in the stage zone are produced from files in the raw zone, but it&amp;#39;s not clear to me whether the lineage should be explicitly in the files of the stage zone, or whether the lineage is more implicit? Somehow derived from the key&amp;#39;s source &amp;amp; date?&lt;/li&gt;\n&lt;li&gt;Other groups in my company basically go right from the raw zone to RedShift tables, and I&amp;#39;m not sure when RedShift would be better than Athena/Trino on a data lake, and when it isn&amp;#39;t? If I do integrate a lake w/ raw, stage, and analytic zones w/ RedShift, is the choice basically to load into either RedShift or the analytic zone from stage?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115iyzu", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115iyzu/need_help_understanding_some_general_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115iyzu/need_help_understanding_some_general_use/", "subreddit_subscribers": 90165, "created_utc": 1676737370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on the design of a data lake. Our application scrapes the web on a daily basis, resulting in a small daily data increment. My plan is to use the Python in combination with the Pandas for data ingestion, cleaning, and transformation. The goal is to save this data in Parquet format inside a storage. However, I am unsure what the best practice is for this scenario. Should I use a single Parquet file and append new data to it daily using Python, or create a new Parquet file every day? Alternatively, perhaps it would be best to use one Parquet file per month. What would be considered best practice in this situation?", "author_fullname": "t2_8ndxjs80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "design a data lake for a web scraping application with daily data increments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115j65j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676737920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on the design of a data lake. Our application scrapes the web on a daily basis, resulting in a small daily data increment. My plan is to use the Python in combination with the Pandas for data ingestion, cleaning, and transformation. The goal is to save this data in Parquet format inside a storage. However, I am unsure what the best practice is for this scenario. Should I use a single Parquet file and append new data to it daily using Python, or create a new Parquet file every day? Alternatively, perhaps it would be best to use one Parquet file per month. What would be considered best practice in this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115j65j", "is_robot_indexable": true, "report_reasons": null, "author": "xtrzx8", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115j65j/design_a_data_lake_for_a_web_scraping_application/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115j65j/design_a_data_lake_for_a_web_scraping_application/", "subreddit_subscribers": 90165, "created_utc": 1676737920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learn the differences between streaming databases, real-time OLAP databases, and stream processing", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_116273b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/m12X5bRyTnF43-JWIEJfJbtDedSSegMSyogc5hlyxtY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676783255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learn the differences between streaming databases, real-time OLAP databases, and stream processing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/stream-processing-vs-real-time-olap?r=46sqk&amp;utm_medium=ios&amp;utm_campaign=post", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?auto=webp&amp;v=enabled&amp;s=a7d0085c1936bbf1782ea420f65e41cfe6d80cdd", "width": 906, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01cc6aa5aab2ce0b5e9eec8e037aabea3ce231ce", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83e9ebbf277433dc7eed8d70e5f8d67d09ce8103", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db8dd20a1ef9ec3489697f6b91206f1fc602049", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8360069fd7707d9466c134b46d8389113481223", "width": 640, "height": 423}], "variants": {}, "id": "_VuVKTXnegN6nzjepsmw478Ywq3LWIdi_B29ArkOsZ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "116273b", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116273b/streaming_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/stream-processing-vs-real-time-olap?r=46sqk&amp;utm_medium=ios&amp;utm_campaign=post", "subreddit_subscribers": 90165, "created_utc": 1676783255.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zrj6c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema Registry Statistics Tool is a small utility that allows you to easily identify the usage of different schema versions within a topic. Using this tool, you can consume from a topic, while calculating the percentage of each schema version.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_115gxjw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qCu1P4R7dZOhFl9UatcYDADGrEJcGGo1H1hzfEnfnuk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676731723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/EladLeev/schema-registry-statistics", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?auto=webp&amp;v=enabled&amp;s=c1857dcbe17e4fa89df6e127e072092405f9186b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a9bbcf2992ce934031edb0dfb699771068a752", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbe8f2c74d3f67396f5d893f7fba851f49c4856d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ddb4e937c58fc738bc6697d22e6a9e0e260512", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10a918d4ebb7e47668b967a34937f5b3566bd573", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d4b88f8fa6209c77927faae2fa3386593fc06b6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f36e7af3dda60682a0ded9bf5fb0ea742ec42e7", "width": 1080, "height": 540}], "variants": {}, "id": "Q7gqhNR9BMw_rb7j2B5AkezSULT9JIkbxQDp0yDOZ5o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "115gxjw", "is_robot_indexable": true, "report_reasons": null, "author": "eladleev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115gxjw/schema_registry_statistics_tool_is_a_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/EladLeev/schema-registry-statistics", "subreddit_subscribers": 90165, "created_utc": 1676731723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anybody have a way to move data from mssql to s3 or snowflake without using paid 3rd party tools? My tools are airflow, pyspark, dbt ,snowflake and s3. Currently we use stitch and I\u2019m trying get away from it. I\u2019m sure we can do the spark jdbc connector. Wondering what other options are available.", "author_fullname": "t2_t27yz8m9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data from mssql to s3 or snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115kn04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676741860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody have a way to move data from mssql to s3 or snowflake without using paid 3rd party tools? My tools are airflow, pyspark, dbt ,snowflake and s3. Currently we use stitch and I\u2019m trying get away from it. I\u2019m sure we can do the spark jdbc connector. Wondering what other options are available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115kn04", "is_robot_indexable": true, "report_reasons": null, "author": "callmedivs", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115kn04/data_from_mssql_to_s3_or_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115kn04/data_from_mssql_to_s3_or_snowflake/", "subreddit_subscribers": 90165, "created_utc": 1676741860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've used Airflow since the early days and in fact made a few small contributions to the codebase back in \"the day\", so I'm extremely familiar with it. I'm about 3 years out of the loop though and I'm wondering if there's anything new as far as technologies or best practices.\n\nHere's some information about the requirements:\n\n1. ETLs will extract from local machine (at first) and send to S3.\n2. Looking to run it in containers so it can eventually be deployed to the cloud but will initially be run locally.\n\nI was able to get a basic situation rigged up and running with a few tweaks to [this docker-compose.yml](https://github.com/marclamberti/docker-airflow/blob/main/docker-compose.yml) file but I have a few questions:\n\n1. **Should I be using breeze?** Reading about breeze made me suspect it might be more of a tool that is used to develop airflow itself with an rarely used optional feature allowing the building of production images, but sometimes it's hard to tell these things just by reading on the internet so I'm looking for other feedback.\n2. **Are there any airflow-specific reasons to prefer podman over docker?**\n3. **Is it unwise to use docker-compose to orchestrate containers?** I was using AWS ECS to orchestrate airflow containers a bunch of years ago but then went back to a setup that just used an EC2 instance.\n4. **Any other advice?**\n\nThanks.", "author_fullname": "t2_96txdx2p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow: Any advice for someone with many years of Airflow experience but who has been out of the loop for a few years on the latest and greatest best practices? In particular I'm wondering about \"Airflow Breeze\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115x9q5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676790527.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676768320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used Airflow since the early days and in fact made a few small contributions to the codebase back in &amp;quot;the day&amp;quot;, so I&amp;#39;m extremely familiar with it. I&amp;#39;m about 3 years out of the loop though and I&amp;#39;m wondering if there&amp;#39;s anything new as far as technologies or best practices.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some information about the requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;ETLs will extract from local machine (at first) and send to S3.&lt;/li&gt;\n&lt;li&gt;Looking to run it in containers so it can eventually be deployed to the cloud but will initially be run locally.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I was able to get a basic situation rigged up and running with a few tweaks to &lt;a href=\"https://github.com/marclamberti/docker-airflow/blob/main/docker-compose.yml\"&gt;this docker-compose.yml&lt;/a&gt; file but I have a few questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Should I be using breeze?&lt;/strong&gt; Reading about breeze made me suspect it might be more of a tool that is used to develop airflow itself with an rarely used optional feature allowing the building of production images, but sometimes it&amp;#39;s hard to tell these things just by reading on the internet so I&amp;#39;m looking for other feedback.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Are there any airflow-specific reasons to prefer podman over docker?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Is it unwise to use docker-compose to orchestrate containers?&lt;/strong&gt; I was using AWS ECS to orchestrate airflow containers a bunch of years ago but then went back to a setup that just used an EC2 instance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Any other advice?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?auto=webp&amp;v=enabled&amp;s=b11713aa8d2f1f5b86d41094cd020ffcce5aa6a4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b016dc65fc63dee1e0e2848c31f9563b5b46262f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ab95253edc21b69d0cd67536435bf1b81ca8457", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08f592c95207484a6d9f09b117aaf638ecaecece", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72564aa423e78df38522940b3f2db630fb3c55b3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52ace1016ea0ceb3219aa6a5de947a572b3271b3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a5d9c3bdc226a6a36176c9b3ee19762e976cc68", "width": 1080, "height": 540}], "variants": {}, "id": "RLobfrRDx0KU8k0LkdNkGpVdk0R2NJlZ3FHqWaHw4E4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115x9q5", "is_robot_indexable": true, "report_reasons": null, "author": "thenextsymbol", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115x9q5/airflow_any_advice_for_someone_with_many_years_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115x9q5/airflow_any_advice_for_someone_with_many_years_of/", "subreddit_subscribers": 90165, "created_utc": 1676768320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "*Hoping for an interesting thread*", "author_fullname": "t2_27lz615w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the toughest DE problem you faced in your work career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_116a03p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676812430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hoping for an interesting thread&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "116a03p", "is_robot_indexable": true, "report_reasons": null, "author": "priprocks", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/", "subreddit_subscribers": 90165, "created_utc": 1676812430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will start.\n\nI would create a standardized framework to create airflow dags. Due to our inexperience with airflow, everyone created their own version of dags to achieve the same ETL tasks. This has led to unnecessary duplication of code and maintenance nightmare. And since all the jobs are prod critical and interdependent, we cannot phase these dags out. Hence we are perpetually stuck with them", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you could redo your company's data warehouse/data lake/data infrastructure, what would you do differently due to the benefit of hindsight?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1169v89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676812023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will start.&lt;/p&gt;\n\n&lt;p&gt;I would create a standardized framework to create airflow dags. Due to our inexperience with airflow, everyone created their own version of dags to achieve the same ETL tasks. This has led to unnecessary duplication of code and maintenance nightmare. And since all the jobs are prod critical and interdependent, we cannot phase these dags out. Hence we are perpetually stuck with them&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1169v89", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1169v89/if_you_could_redo_your_companys_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1169v89/if_you_could_redo_your_companys_data/", "subreddit_subscribers": 90165, "created_utc": 1676812023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there an option for me here? Currently on Snowflake, and I know both our unit cost as well as our usage.\n\nWe\u2019re on AWS. I\u2019m trying to understand what the likely cost would be of running the same pipelines using dbt on Hudi over S3 or Databricks in comparison to the existing cost of dbt on Snowflake.\n\nIs there an easy (ish) way to go about this?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand likely costs without needing to PoC everything", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1163cc7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676787363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an option for me here? Currently on Snowflake, and I know both our unit cost as well as our usage.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re on AWS. I\u2019m trying to understand what the likely cost would be of running the same pipelines using dbt on Hudi over S3 or Databricks in comparison to the existing cost of dbt on Snowflake.&lt;/p&gt;\n\n&lt;p&gt;Is there an easy (ish) way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1163cc7", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1163cc7/trying_to_understand_likely_costs_without_needing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1163cc7/trying_to_understand_likely_costs_without_needing/", "subreddit_subscribers": 90165, "created_utc": 1676787363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What would be the best way to setup RabbitMQ output to TimescaleDB? Is there any \u201cnative\u201d way without using any third application as a middleware? Or we need to setup some API endpoint for the DB?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RabbitMQ output to TimescaleDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1165n0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676796261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would be the best way to setup RabbitMQ output to TimescaleDB? Is there any \u201cnative\u201d way without using any third application as a middleware? Or we need to setup some API endpoint for the DB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1165n0e", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1165n0e/rabbitmq_output_to_timescaledb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1165n0e/rabbitmq_output_to_timescaledb/", "subreddit_subscribers": 90165, "created_utc": 1676796261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have moved to a new job that is a complete MS shop using PowerBI. Previously worked with AWS and very familiar with the S3 /glue/Athena using JSON and parquet to build a DW, fairly low cost compared to redshift.\n\nWhat would be the equivalent tools on Azure? Having trouble finding something where I can stick a load of text or parquet and build some formal schemas like in Glue and then connect the finished tables to PowerBI.\n\nEventually I would like to use DBT and Dagster for orchestration.", "author_fullname": "t2_clh5r1ln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure equivalent of S3/Glue/Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115quz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676753700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have moved to a new job that is a complete MS shop using PowerBI. Previously worked with AWS and very familiar with the S3 /glue/Athena using JSON and parquet to build a DW, fairly low cost compared to redshift.&lt;/p&gt;\n\n&lt;p&gt;What would be the equivalent tools on Azure? Having trouble finding something where I can stick a load of text or parquet and build some formal schemas like in Glue and then connect the finished tables to PowerBI.&lt;/p&gt;\n\n&lt;p&gt;Eventually I would like to use DBT and Dagster for orchestration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115quz6", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Restaurant-9691", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115quz6/azure_equivalent_of_s3glueathena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115quz6/azure_equivalent_of_s3glueathena/", "subreddit_subscribers": 90165, "created_utc": 1676753700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019ve been stuck with coming up with a plan to \u201cmodernize\u201d our data stack, but I\u2019m a bit fuzzy on a few details. Hoping some can provide insights. \n\nBoss wants to migrate towards the \u201cmodern\u201d stack (hello buzz words) and asked me to put a plan together. \n\nI have a general idea of the flow (load data from Airbyte into snowflake then run DBT models) but I\u2019m confused on the compute part. Where the heck does the orchestration call the compute resources. For DBT models I\u2019m assuming snowflake credits are used, but how does it work for ML workflows? Are you running jobs that pull/push directly into a warehouse? \n\nWe are a gov org so we support a ton of business units all doing god knows what at times, so they to centralize into our team to run the pipelines for everyone else. \n\nI\u2019m well familiar with python, and have been using DBT for jobs recently (in SQL server), so would love to stay in that ecosystem to start but I\u2019m fuzzy on what to recommend. Basically starting out with ELT jobs for now the later they want to do ML. \n\nAny suggestions, or books or guides to read up on? Total zoo that it\u2019s on me in an org our size , but I see it as a good opportunity to drive the data platform at our org.\n\nTLDR. Data leaves our systems and goes to a warehouse. Where does the compute happen. Where do ML pipelines get their compute. \n\nOther way to frame it. In a greenfield data org, what would you do. \n\nThanks!", "author_fullname": "t2_3sioksrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about data pipeline setup/structures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115xtyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676769980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019ve been stuck with coming up with a plan to \u201cmodernize\u201d our data stack, but I\u2019m a bit fuzzy on a few details. Hoping some can provide insights. &lt;/p&gt;\n\n&lt;p&gt;Boss wants to migrate towards the \u201cmodern\u201d stack (hello buzz words) and asked me to put a plan together. &lt;/p&gt;\n\n&lt;p&gt;I have a general idea of the flow (load data from Airbyte into snowflake then run DBT models) but I\u2019m confused on the compute part. Where the heck does the orchestration call the compute resources. For DBT models I\u2019m assuming snowflake credits are used, but how does it work for ML workflows? Are you running jobs that pull/push directly into a warehouse? &lt;/p&gt;\n\n&lt;p&gt;We are a gov org so we support a ton of business units all doing god knows what at times, so they to centralize into our team to run the pipelines for everyone else. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m well familiar with python, and have been using DBT for jobs recently (in SQL server), so would love to stay in that ecosystem to start but I\u2019m fuzzy on what to recommend. Basically starting out with ELT jobs for now the later they want to do ML. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions, or books or guides to read up on? Total zoo that it\u2019s on me in an org our size , but I see it as a good opportunity to drive the data platform at our org.&lt;/p&gt;\n\n&lt;p&gt;TLDR. Data leaves our systems and goes to a warehouse. Where does the compute happen. Where do ML pipelines get their compute. &lt;/p&gt;\n\n&lt;p&gt;Other way to frame it. In a greenfield data org, what would you do. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115xtyg", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Stay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115xtyg/questions_about_data_pipeline_setupstructures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115xtyg/questions_about_data_pipeline_setupstructures/", "subreddit_subscribers": 90165, "created_utc": 1676769980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a personal project that will end up with a R Shiny dashboard I want to deploy. My challenge is that the amount of data I'm working with is too cumbersome for my personal computer so I'd like to set up something remote to work off of. Unfortunately, I only have experience working as a data scientist and have no experience developing / deploying any of the back end infrastructure.\n\n&amp;#x200B;\n\nI need to be able to store data (downloaded as csvs from the internet) and then clean / manipulate that data (preferably using a spark API like sparkr or pyspark)  so that its ready for the shiny application. I was looking for help with recommendations for the best way to do this thats both cost effective and relatively simple given my limited data engineering experience (if any exists).", "author_fullname": "t2_a2s12j5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data storage and manipulation recs for personal project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115xhyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676768989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a personal project that will end up with a R Shiny dashboard I want to deploy. My challenge is that the amount of data I&amp;#39;m working with is too cumbersome for my personal computer so I&amp;#39;d like to set up something remote to work off of. Unfortunately, I only have experience working as a data scientist and have no experience developing / deploying any of the back end infrastructure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to be able to store data (downloaded as csvs from the internet) and then clean / manipulate that data (preferably using a spark API like sparkr or pyspark)  so that its ready for the shiny application. I was looking for help with recommendations for the best way to do this thats both cost effective and relatively simple given my limited data engineering experience (if any exists).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115xhyw", "is_robot_indexable": true, "report_reasons": null, "author": "ghtghtghtghtght", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115xhyw/data_storage_and_manipulation_recs_for_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115xhyw/data_storage_and_manipulation_recs_for_personal/", "subreddit_subscribers": 90165, "created_utc": 1676768989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use data from MS Dynamics CRM in Power BI via the API. However, the data size is very large and covers more than 10 years, which is causing long load times and poor report update efficiency in Power BI. Even if I use the Modeling feature in Power BI to filter the data, it still loads all the data before applying the filters, which is inefficient. I am seeking recommendations for tools and strategies to improve the speed and efficiency of my system. Any advice would be greatly appreciated.\n\nEdit: Do you think moving the data from CRM to Azure DW first and then connecting BI to DW would be helpful? or that is doesn't worth the cost? Is there any other tools that helps this process?", "author_fullname": "t2_oaaif8zw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS Dynamic CRM to Power BI slow loading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115tori", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676759964.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676758513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use data from MS Dynamics CRM in Power BI via the API. However, the data size is very large and covers more than 10 years, which is causing long load times and poor report update efficiency in Power BI. Even if I use the Modeling feature in Power BI to filter the data, it still loads all the data before applying the filters, which is inefficient. I am seeking recommendations for tools and strategies to improve the speed and efficiency of my system. Any advice would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Edit: Do you think moving the data from CRM to Azure DW first and then connecting BI to DW would be helpful? or that is doesn&amp;#39;t worth the cost? Is there any other tools that helps this process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115tori", "is_robot_indexable": true, "report_reasons": null, "author": "ugotbkidding", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115tori/ms_dynamic_crm_to_power_bi_slow_loading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115tori/ms_dynamic_crm_to_power_bi_slow_loading/", "subreddit_subscribers": 90165, "created_utc": 1676758513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What could be the best way to design a data lake layers for one particular data source considering we will be getting one small JSON file every day?\n\nI thought of getting this JSON file and \u201cinserting\u201d the content into the parquet file which would be partitioned e.g. by month? So I would have 1 parquet file per month? Creating one parquet file every day with little content seems weird to me (?)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake design with small files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115k5sx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676740575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What could be the best way to design a data lake layers for one particular data source considering we will be getting one small JSON file every day?&lt;/p&gt;\n\n&lt;p&gt;I thought of getting this JSON file and \u201cinserting\u201d the content into the parquet file which would be partitioned e.g. by month? So I would have 1 parquet file per month? Creating one parquet file every day with little content seems weird to me (?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115k5sx", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115k5sx/data_lake_design_with_small_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115k5sx/data_lake_design_with_small_files/", "subreddit_subscribers": 90165, "created_utc": 1676740575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm in the final interview stage which should be scheduled for sometime next week. \n\nThey've told me there will be an hour allocated to a 'case study' with no extra details. I have asked for some extra info but if they don't reply soon what can I do to prepare for in the meantime? Also they said in the previous stage there's no live coding test.", "author_fullname": "t2_4ba5z1zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case study for a final interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11692c3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676809409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m in the final interview stage which should be scheduled for sometime next week. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;ve told me there will be an hour allocated to a &amp;#39;case study&amp;#39; with no extra details. I have asked for some extra info but if they don&amp;#39;t reply soon what can I do to prepare for in the meantime? Also they said in the previous stage there&amp;#39;s no live coding test.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "11692c3", "is_robot_indexable": true, "report_reasons": null, "author": "dreamr49", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11692c3/case_study_for_a_final_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11692c3/case_study_for_a_final_interview/", "subreddit_subscribers": 90165, "created_utc": 1676809409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI recently applied for a DfE funded Date Science bootcamp with HyperionDev, here in the UK. Yesterday I came across many negative reviews of these particular camps, as found [here](https://www.reddit.com/r/learnprogramming/comments/xx90kb/thoughts_on_the_hyperiondev_bootcamp/) and [here](https://uk.trustpilot.com/review/www.hyperiondev.com). I\u2019m a little deflated now because, although I haven\u2019t received confirmation of a place, my mind had registered this as being a \u2018problem\u2019 potentially solved, so to speak.\n\n\nFurthermore, it was raised in the aforementioned link that the syllabus for the DfE funded camp omits some vital topics, which were found in HyperionDev\u2019s privately paid camps. In the case of Data Science the following topics are omitted from the DfE funded syllabus, found [here](https://docdro.id/suIflVe):\n\n\n* Project II (Put everything you\u2019ve learnt about files to the test in this comprehensive task.)\n* Applied Recursion (Explore the concepts of recursive programming and to \u2018think recursivley\u2019.)\n* Introduction to Oop 2 - Inheritance (Learn how you can improve the modularity and resume of code using inheritance and the critical role it plays in Python\u2019s object system.)\n* Project IV (Apply fundamentals of object-orientation to solve a simple problem.)\n* Introduction to NLP (Get acquainted with Natural Language Processing by learning about parts of speech, parsing, and how to install and start using spaCy.)\n* Semantic Similarity (NLP) (Learn about semantic similarity, a popular application of NLP widely used for social media analysis.)\n* Project V (Utilise your newly acquired knowledge of semantic similarity and natural language processing.)\n* Discrete Maths (Learn the basics of discrete maths to support your understanding of analytics and machine learning.)\n* Calculus (Learn the basics of calculus.)\n* Statistics (Learn the basics of statistics.)\n* Sources of Data (Learn how to extract and import data from different sources: JSON, XML, CSV.)\n* Design and Build a Relational Database (Design a database by applying normalisation principles. Create relational databases.)\nPython Packages for Data Science (Get introduced to some of the most popular Python packages for data science like pandas, NumPy and SciPcy.)\n* Exploring Neural Networks (Become familiar with the fundamental concepts and terminology used in neural networks. Understand backpropagation and learn how to validate your models.)\n\n\nMy first question is: might this bootcamp still be worth my time given:\n\n\n* I have zero experience and knowledge in this area and, in those terms, still stand to gain much,\n* It is fully funded,\n* I will ultimately have a certificate recognising my competence,\n* I am under no illusions about what\u2019s on offer, so will have realistic expectations\n* I work part-time and hypothetically have more time to dedicate to the camp than they suggest is necessary, which can help fill the gaps in their teaching\n\n\nMy second question is: are the omissions from the syllabus so egregious as to make the entire venture of a DfE bootcamp fundamentally flawed? Additionally, what would a comprehensive and well-constructed Data Science syllabus look like?\n\n\nSorry if this all a bit longwinded but I would really appreciate any of your considered  thoughts on these two questions.\n\n\nThank you in advance for your help.", "author_fullname": "t2_omf3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[UK] HyperionDev Department for Education funded Data Science Bootcamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1168v69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676814856.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676808670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently applied for a DfE funded Date Science bootcamp with HyperionDev, here in the UK. Yesterday I came across many negative reviews of these particular camps, as found &lt;a href=\"https://www.reddit.com/r/learnprogramming/comments/xx90kb/thoughts_on_the_hyperiondev_bootcamp/\"&gt;here&lt;/a&gt; and &lt;a href=\"https://uk.trustpilot.com/review/www.hyperiondev.com\"&gt;here&lt;/a&gt;. I\u2019m a little deflated now because, although I haven\u2019t received confirmation of a place, my mind had registered this as being a \u2018problem\u2019 potentially solved, so to speak.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, it was raised in the aforementioned link that the syllabus for the DfE funded camp omits some vital topics, which were found in HyperionDev\u2019s privately paid camps. In the case of Data Science the following topics are omitted from the DfE funded syllabus, found &lt;a href=\"https://docdro.id/suIflVe\"&gt;here&lt;/a&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Project II (Put everything you\u2019ve learnt about files to the test in this comprehensive task.)&lt;/li&gt;\n&lt;li&gt;Applied Recursion (Explore the concepts of recursive programming and to \u2018think recursivley\u2019.)&lt;/li&gt;\n&lt;li&gt;Introduction to Oop 2 - Inheritance (Learn how you can improve the modularity and resume of code using inheritance and the critical role it plays in Python\u2019s object system.)&lt;/li&gt;\n&lt;li&gt;Project IV (Apply fundamentals of object-orientation to solve a simple problem.)&lt;/li&gt;\n&lt;li&gt;Introduction to NLP (Get acquainted with Natural Language Processing by learning about parts of speech, parsing, and how to install and start using spaCy.)&lt;/li&gt;\n&lt;li&gt;Semantic Similarity (NLP) (Learn about semantic similarity, a popular application of NLP widely used for social media analysis.)&lt;/li&gt;\n&lt;li&gt;Project V (Utilise your newly acquired knowledge of semantic similarity and natural language processing.)&lt;/li&gt;\n&lt;li&gt;Discrete Maths (Learn the basics of discrete maths to support your understanding of analytics and machine learning.)&lt;/li&gt;\n&lt;li&gt;Calculus (Learn the basics of calculus.)&lt;/li&gt;\n&lt;li&gt;Statistics (Learn the basics of statistics.)&lt;/li&gt;\n&lt;li&gt;Sources of Data (Learn how to extract and import data from different sources: JSON, XML, CSV.)&lt;/li&gt;\n&lt;li&gt;Design and Build a Relational Database (Design a database by applying normalisation principles. Create relational databases.)\nPython Packages for Data Science (Get introduced to some of the most popular Python packages for data science like pandas, NumPy and SciPcy.)&lt;/li&gt;\n&lt;li&gt;Exploring Neural Networks (Become familiar with the fundamental concepts and terminology used in neural networks. Understand backpropagation and learn how to validate your models.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My first question is: might this bootcamp still be worth my time given:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I have zero experience and knowledge in this area and, in those terms, still stand to gain much,&lt;/li&gt;\n&lt;li&gt;It is fully funded,&lt;/li&gt;\n&lt;li&gt;I will ultimately have a certificate recognising my competence,&lt;/li&gt;\n&lt;li&gt;I am under no illusions about what\u2019s on offer, so will have realistic expectations&lt;/li&gt;\n&lt;li&gt;I work part-time and hypothetically have more time to dedicate to the camp than they suggest is necessary, which can help fill the gaps in their teaching&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My second question is: are the omissions from the syllabus so egregious as to make the entire venture of a DfE bootcamp fundamentally flawed? Additionally, what would a comprehensive and well-constructed Data Science syllabus look like?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this all a bit longwinded but I would really appreciate any of your considered  thoughts on these two questions.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?auto=webp&amp;v=enabled&amp;s=0a8751c7737c890e74eaef0be5ea1b1d8eeb17dc", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=885d83cc5b8e3281571f493ccfed61da0ffb7b8b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e3be79ad4253960dea2d5de73380ce8ef168172", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9981fd3b435d3346a6e67dcf0b8301b67f763b10", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eda7c93ce1f905b9ca2932bc2086ac5aae64a146", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1486ead40bc842565599d135ab25c8454c52453", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/cdTZ75-OBXLQsH5bXS5LtOwlbGP0AIxWuZi9bxtwVCw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f78ec6e6e3a6bf5714ab0b9cb3847201941079d0", "width": 1080, "height": 564}], "variants": {}, "id": "uCaRW9kFHqZpZN77FNb-pNz1yR65tjTl46EW2zax1dU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1168v69", "is_robot_indexable": true, "report_reasons": null, "author": "esonkcoc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1168v69/uk_hyperiondev_department_for_education_funded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1168v69/uk_hyperiondev_department_for_education_funded/", "subreddit_subscribers": 90165, "created_utc": 1676808670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At the end of our pipeline there's an aggregate table `'agg_table'` serving user requests from a webpage in realtime. The user request consists of just 3 parameter values (`A, B, C`) identifying the row in the table, s.t. it can basically be interpreted as the query `SELECT * FROM agg_table WHERE key_1=A AND key_2=B AND key_3=C`. However, it is rarely the case that a row with all 3 (or even 2) values exists in the table, s.t. it then defaults (according to business/domain logic) to `WHERE key_1=A AND key_2=B`, if that doesn't exist to `WHERE key_1=A AND key_3=C`, if that doesn't exist to `WHERE key_2=B AND key_3=C`, if that doesn't exist to `WHERE key_1=A` etc.\n\nIs there a recommended way to model the analytics table, with respect to dimensional modelling?", "author_fullname": "t2_cgxiixth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with fallbacks for webpage-serving analytics table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1168siq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676808397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At the end of our pipeline there&amp;#39;s an aggregate table &lt;code&gt;&amp;#39;agg_table&amp;#39;&lt;/code&gt; serving user requests from a webpage in realtime. The user request consists of just 3 parameter values (&lt;code&gt;A, B, C&lt;/code&gt;) identifying the row in the table, s.t. it can basically be interpreted as the query &lt;code&gt;SELECT * FROM agg_table WHERE key_1=A AND key_2=B AND key_3=C&lt;/code&gt;. However, it is rarely the case that a row with all 3 (or even 2) values exists in the table, s.t. it then defaults (according to business/domain logic) to &lt;code&gt;WHERE key_1=A AND key_2=B&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_1=A AND key_3=C&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_2=B AND key_3=C&lt;/code&gt;, if that doesn&amp;#39;t exist to &lt;code&gt;WHERE key_1=A&lt;/code&gt; etc.&lt;/p&gt;\n\n&lt;p&gt;Is there a recommended way to model the analytics table, with respect to dimensional modelling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1168siq", "is_robot_indexable": true, "report_reasons": null, "author": "BusyFture", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1168siq/how_to_deal_with_fallbacks_for_webpageserving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1168siq/how_to_deal_with_fallbacks_for_webpageserving/", "subreddit_subscribers": 90165, "created_utc": 1676808397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_puwuw2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenTelemetry and Jaeger backend integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_1166bc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fEXyZ_c3fZGe2j4rQPkjNteedWy9j3_-iLRCy7vAQFk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676798999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "sprkl.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://sprkl.dev/opentelemetry-and-jaeger-backend-integration/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?auto=webp&amp;v=enabled&amp;s=e259aa7b86787155a6797f091035f0f0462de52f", "width": 2400, "height": 1372}, "resolutions": [{"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6a75b7661a406d3316b092c671d75c39fe3ae4a", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdce88fb6d1189b4e8368b8401ed276ba8679862", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54b127893599f27dc179177c71ff1f367f744940", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1508f19437ab96c30328b48e5694c4f783ce537", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c9b32202a7a3a4668219731ae2b7999aa2d269e", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/SN2jvKBCIvUyGHfQED-TVEgM8INfDlcrp_mzaegQMms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9118d8fe1a143ae1dde1d977b351e03a6b7687d2", "width": 1080, "height": 617}], "variants": {}, "id": "72XcLpUgjyfjcmTmotcL-rNH5PXjxPQwKx42jDMLYJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1166bc3", "is_robot_indexable": true, "report_reasons": null, "author": "observability_geek", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1166bc3/opentelemetry_and_jaeger_backend_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://sprkl.dev/opentelemetry-and-jaeger-backend-integration/", "subreddit_subscribers": 90165, "created_utc": 1676798999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am wondering if you couldn't get away with a relational database for vectorisation.\n\nA while back I got to play with vectorisation of terms using BERT. We then held the vectors within an instance of Facebook's Fais. One of the irritating things was having to initialise a Fais instance which took a lot of time.\n\nWhich got me thinking, vectorisation is basically breaking unstructured data into tokens (groups of terms, a term is a word) e.g.\n\n&gt; The cat in the hat sat on the mat and drank milk from a jug\n\nFirst we remove stop words \n\n&gt; Cat hat sat mat drank milk from jug\n\nNow we convert into tokens\n\n* Cat hat sat mat\n* Hat sat mat drank\n* Sat mat drank milk\n* mat drank milk jug\n\nA Machine Learning model like bert has been trained on billions of tokens and has given them positions (vectors) within an N dimensional array (called vectorspace).\n\nThe idea is \"cat\" should be located near \"pet\", \"hat\" is close to \"headgear\", etc.. \n\nSo you provide tokens and in return you get vectors indicating where in vectorspace your token resides.\n\nWhich got me thinking, in every example I can think of there is normally an easy way to keep the size of a corpus (dataset) small. For example with facebook each user would represent a corpus.\n\nFacebook Fais requires a lot of RAM and Storage and you have to preload data for it to the index everything and it can literally take days to start. \n\nSo my thinking is.\n\nCreate a relational database (e.g postgres) for each corpus. The database should contain a \"vectors\" table. \n\nThe vector table contains an integer field for each vector column. A vector can have a many to many relationship with the \"token\" table.\n\nThe token table contains each token generated within a corpus and some means to reference the document tokenised.\n\nSo when a user enters a search string, we tokenise it and generate a list of vectors. We then build a query for each vector allowing a range around each vector field.\n\nWe run these queries on each database collecting the responses to each query. \n\nWe then rank the tokens based on the number of matching vectors.\n\nBecause our corpus is bounded each database will likely be limited to thousands of tokens and a few million vectors. Which is within the abilities of something like postgres.\n\nThis would let us replicate the same capability as Facebook Fais or Word2Vec.\n\nThoughts?", "author_fullname": "t2_172g1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vectorisation and Querying", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1164fus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676792314.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676791562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering if you couldn&amp;#39;t get away with a relational database for vectorisation.&lt;/p&gt;\n\n&lt;p&gt;A while back I got to play with vectorisation of terms using BERT. We then held the vectors within an instance of Facebook&amp;#39;s Fais. One of the irritating things was having to initialise a Fais instance which took a lot of time.&lt;/p&gt;\n\n&lt;p&gt;Which got me thinking, vectorisation is basically breaking unstructured data into tokens (groups of terms, a term is a word) e.g.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The cat in the hat sat on the mat and drank milk from a jug&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;First we remove stop words &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Cat hat sat mat drank milk from jug&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Now we convert into tokens&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cat hat sat mat&lt;/li&gt;\n&lt;li&gt;Hat sat mat drank&lt;/li&gt;\n&lt;li&gt;Sat mat drank milk&lt;/li&gt;\n&lt;li&gt;mat drank milk jug&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A Machine Learning model like bert has been trained on billions of tokens and has given them positions (vectors) within an N dimensional array (called vectorspace).&lt;/p&gt;\n\n&lt;p&gt;The idea is &amp;quot;cat&amp;quot; should be located near &amp;quot;pet&amp;quot;, &amp;quot;hat&amp;quot; is close to &amp;quot;headgear&amp;quot;, etc.. &lt;/p&gt;\n\n&lt;p&gt;So you provide tokens and in return you get vectors indicating where in vectorspace your token resides.&lt;/p&gt;\n\n&lt;p&gt;Which got me thinking, in every example I can think of there is normally an easy way to keep the size of a corpus (dataset) small. For example with facebook each user would represent a corpus.&lt;/p&gt;\n\n&lt;p&gt;Facebook Fais requires a lot of RAM and Storage and you have to preload data for it to the index everything and it can literally take days to start. &lt;/p&gt;\n\n&lt;p&gt;So my thinking is.&lt;/p&gt;\n\n&lt;p&gt;Create a relational database (e.g postgres) for each corpus. The database should contain a &amp;quot;vectors&amp;quot; table. &lt;/p&gt;\n\n&lt;p&gt;The vector table contains an integer field for each vector column. A vector can have a many to many relationship with the &amp;quot;token&amp;quot; table.&lt;/p&gt;\n\n&lt;p&gt;The token table contains each token generated within a corpus and some means to reference the document tokenised.&lt;/p&gt;\n\n&lt;p&gt;So when a user enters a search string, we tokenise it and generate a list of vectors. We then build a query for each vector allowing a range around each vector field.&lt;/p&gt;\n\n&lt;p&gt;We run these queries on each database collecting the responses to each query. &lt;/p&gt;\n\n&lt;p&gt;We then rank the tokens based on the number of matching vectors.&lt;/p&gt;\n\n&lt;p&gt;Because our corpus is bounded each database will likely be limited to thousands of tokens and a few million vectors. Which is within the abilities of something like postgres.&lt;/p&gt;\n\n&lt;p&gt;This would let us replicate the same capability as Facebook Fais or Word2Vec.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1164fus", "is_robot_indexable": true, "report_reasons": null, "author": "stevecrox0914", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1164fus/vectorisation_and_querying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1164fus/vectorisation_and_querying/", "subreddit_subscribers": 90165, "created_utc": 1676791562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been told that I should be careful telling people that I\u2019m a \u2018data engineer\u2019 because I don\u2019t have an engineering degree, and only qualified engineers can call themselves engineers. I\u2019m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it\u2019s my job title. Is this something people care about?", "author_fullname": "t2_py81dzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I call myself a data engineer if I don\u2019t have an \u2018engineering\u2019 qualification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1162sjq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676785392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been told that I should be careful telling people that I\u2019m a \u2018data engineer\u2019 because I don\u2019t have an engineering degree, and only qualified engineers can call themselves engineers. I\u2019m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it\u2019s my job title. Is this something people care about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1162sjq", "is_robot_indexable": true, "report_reasons": null, "author": "CyclicDombo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1162sjq/can_i_call_myself_a_data_engineer_if_i_dont_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1162sjq/can_i_call_myself_a_data_engineer_if_i_dont_have/", "subreddit_subscribers": 90165, "created_utc": 1676785392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I downloaded my extended streams from Spotify and I created two dimension tables one with the Track Metadata which contains the Artist Uri which connects an Artist Metadata. When I created the Artist Dimension table, I discovered that they can have multiple genres and some cases many genres. I am not sure how I should finalize this table. This will be a csv going into cloud storage. \n\nCurrently, I have it as follows \n\n    Artist_ID              Genres                                                                                                                                    \n    00FQb4jTyendYWaN8pK0wa art pop|pop                                                                 00G1NTDAoU7rBpjG4KoYAM downtempo|electronica|japanese old school \n    00sAr10UTV1JZtHqxsLVn4 canadian psychedelic rock| space rock\n\nI have each one separated with pipes, so when I query, I guess I could do like operator or some kind of sperate, take the nth one. \n\nThe other option though is to have the Artist\\_ID repeat for each unique genre but I'm not a fan off repeating the Artist\\_ID. \n\nWhat do you guys think would be the best solution in this case?", "author_fullname": "t2_z321026", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spotify Genre Date: Debating how I should store this table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115zj00", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676774800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded my extended streams from Spotify and I created two dimension tables one with the Track Metadata which contains the Artist Uri which connects an Artist Metadata. When I created the Artist Dimension table, I discovered that they can have multiple genres and some cases many genres. I am not sure how I should finalize this table. This will be a csv going into cloud storage. &lt;/p&gt;\n\n&lt;p&gt;Currently, I have it as follows &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Artist_ID              Genres                                                                                                                                    \n00FQb4jTyendYWaN8pK0wa art pop|pop                                                                 00G1NTDAoU7rBpjG4KoYAM downtempo|electronica|japanese old school \n00sAr10UTV1JZtHqxsLVn4 canadian psychedelic rock| space rock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I have each one separated with pipes, so when I query, I guess I could do like operator or some kind of sperate, take the nth one. &lt;/p&gt;\n\n&lt;p&gt;The other option though is to have the Artist_ID repeat for each unique genre but I&amp;#39;m not a fan off repeating the Artist_ID. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think would be the best solution in this case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115zj00", "is_robot_indexable": true, "report_reasons": null, "author": "raz_the_kid0901", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115zj00/spotify_genre_date_debating_how_i_should_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115zj00/spotify_genre_date_debating_how_i_should_store/", "subreddit_subscribers": 90165, "created_utc": 1676774800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone shed some light on where the VMs for any serverless compute are deployed? Since those are managed by a cloud provider, my assumption is that VMs are deployed in a separate VPC and fetch the data from the storage account in the customer VPC.\n\nIf my assumption is correct then how is the data transfer between different VPCs managed? I believe both the VPCs will be in the same region and availability zone.", "author_fullname": "t2_etr435xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where are those serverless compute(VMs) deployed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115x4w8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676767910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone shed some light on where the VMs for any serverless compute are deployed? Since those are managed by a cloud provider, my assumption is that VMs are deployed in a separate VPC and fetch the data from the storage account in the customer VPC.&lt;/p&gt;\n\n&lt;p&gt;If my assumption is correct then how is the data transfer between different VPCs managed? I believe both the VPCs will be in the same region and availability zone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115x4w8", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Ranger806", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115x4w8/where_are_those_serverless_computevms_deployed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115x4w8/where_are_those_serverless_computevms_deployed/", "subreddit_subscribers": 90165, "created_utc": 1676767910.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}