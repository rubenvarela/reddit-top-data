{"kind": "Listing", "data": {"after": "t3_115f49x", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My boss heard talk of \u201cdata meshes\u201d at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.\n\nOur conversation:\n\n\u201cSo we\u2019re giving ownership of data engineering and architecture to the business units?\u201d \u201cNo, there\u2019ll still be just the central data team continuing to fulfill requests in a round-robin fashion.\u201d\n\n\u201cWhy are we doing this?\u201d \u201cBecause it\u2019s better and we want to be forward thinking.\u201d\n\nAm I missing something here about the magic of data meshes if you\u2019re not actually transferring data ownership to the business units? Isn\u2019t that kinda the point?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boss wants a data mesh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115vbvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676762792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My boss heard talk of \u201cdata meshes\u201d at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.&lt;/p&gt;\n\n&lt;p&gt;Our conversation:&lt;/p&gt;\n\n&lt;p&gt;\u201cSo we\u2019re giving ownership of data engineering and architecture to the business units?\u201d \u201cNo, there\u2019ll still be just the central data team continuing to fulfill requests in a round-robin fashion.\u201d&lt;/p&gt;\n\n&lt;p&gt;\u201cWhy are we doing this?\u201d \u201cBecause it\u2019s better and we want to be forward thinking.\u201d&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here about the magic of data meshes if you\u2019re not actually transferring data ownership to the business units? Isn\u2019t that kinda the point?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115vbvp", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/", "subreddit_subscribers": 90128, "created_utc": 1676762792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn't paying well in DE job market. So would you advise anything which should be my best bet?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are the highest paying tech frameworks or programming languages in Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115lbja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676743691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn&amp;#39;t paying well in DE job market. So would you advise anything which should be my best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "115lbja", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115lbja/which_are_the_highest_paying_tech_frameworks_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115lbja/which_are_the_highest_paying_tech_frameworks_or/", "subreddit_subscribers": 90128, "created_utc": 1676743691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow data manglers \n\n  \nI currently work for a large railway company in Europe and provide internal consulting on our data engineering tech stack. Which is Snowflake, Snowpark for Python, Openshift, ADF and Argo Workflows.   \nOver the next two years the companies analytical applications should transition from the previous two techstack generations, which are PowerCenter or Hadoop/Spark, to the above-mentioned.   \nDue to the extreme shortage of data engineering profiles in the company, our goal is to enable full stack Java developers to create, migrate and maintain the plethora of analytical applications. (Teach a bear to dance, but don't wonder if your head gets bitten off)   \nTo achieve this, we provide sample applications, pilot migrations and a lot of documentation and tutorials on (best) practice with the formerly described stack. \n\n  \nSince half a year, I maintain a hefty discussion concerning the test concept. Since we're using Python for implementing our pipelines and some developers in our teams are former fullstack developers/application architects, a conflicting position around unit testing arose. According to my view and experience, the classical test pyramid, where unit testing forms the foundation, is not applicable, when implementing data pipelines. Automated measuring of data quality dimensions\\[1\\] and conducting user acceptance tests with personas who have the required insight, always deemed me much more significant. It's not even true unit testing, because the Snowpark API requires a Snowflake connection in order to generate the SQL statements, so it is violating isolation anyway. \n\n  \nI'd like to have your opinions and experience on testing in data engineering context, maybe I'm a little short-sighted/biased concerning the matter. Thanks!\n\n  \n\\[1\\][https://www.sciencedirect.com/topics/computer-science/data-quality-dimension](https://www.sciencedirect.com/topics/computer-science/data-quality-dimension)", "author_fullname": "t2_88g9czbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Your opinion on testing in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115akgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676708606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow data manglers &lt;/p&gt;\n\n&lt;p&gt;I currently work for a large railway company in Europe and provide internal consulting on our data engineering tech stack. Which is Snowflake, Snowpark for Python, Openshift, ADF and Argo Workflows.&lt;br/&gt;\nOver the next two years the companies analytical applications should transition from the previous two techstack generations, which are PowerCenter or Hadoop/Spark, to the above-mentioned.&lt;br/&gt;\nDue to the extreme shortage of data engineering profiles in the company, our goal is to enable full stack Java developers to create, migrate and maintain the plethora of analytical applications. (Teach a bear to dance, but don&amp;#39;t wonder if your head gets bitten off)&lt;br/&gt;\nTo achieve this, we provide sample applications, pilot migrations and a lot of documentation and tutorials on (best) practice with the formerly described stack. &lt;/p&gt;\n\n&lt;p&gt;Since half a year, I maintain a hefty discussion concerning the test concept. Since we&amp;#39;re using Python for implementing our pipelines and some developers in our teams are former fullstack developers/application architects, a conflicting position around unit testing arose. According to my view and experience, the classical test pyramid, where unit testing forms the foundation, is not applicable, when implementing data pipelines. Automated measuring of data quality dimensions[1] and conducting user acceptance tests with personas who have the required insight, always deemed me much more significant. It&amp;#39;s not even true unit testing, because the Snowpark API requires a Snowflake connection in order to generate the SQL statements, so it is violating isolation anyway. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have your opinions and experience on testing in data engineering context, maybe I&amp;#39;m a little short-sighted/biased concerning the matter. Thanks!&lt;/p&gt;\n\n&lt;p&gt;[1]&lt;a href=\"https://www.sciencedirect.com/topics/computer-science/data-quality-dimension\"&gt;https://www.sciencedirect.com/topics/computer-science/data-quality-dimension&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115akgk", "is_robot_indexable": true, "report_reasons": null, "author": "SuccessfulEar9225", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115akgk/your_opinion_on_testing_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115akgk/your_opinion_on_testing_in_data_engineering/", "subreddit_subscribers": 90128, "created_utc": 1676708606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking to implement a simple data lake architecture in S3, based almost 1 to 1 on the example outlined by AWS here: [https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html](https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html)\n\nI have a few general data lake questions that I'm having a hard time googling, and would love more insight into before I start implementing the above.\n\n1. Most data lake organization plans I see include some sort of timestamp in the key (i.e. `s3:/12345-raw/[source]/[year]/[month]/[day]/example.xlsx`). I think that this works really well for incremental data extractions, but should I use the same logic for a full data extraction? I have plenty of cases where I'm forced to do a full extraction, in these cases, it's also highly likely that `s3:/12345-raw/[source]/2023/02/18/example.xlsx` will have the same value as `s3:/12345-raw/[source]/2023/02/19/example.xlsx`. Even though I think it's simpler to stick to the same general convention everywhere and just fully load this data downstream, is it bad to have so much data duplication in the raw zone? Or does it not matter when cloud storage is so cheap?\n2. How should I track data lineage between different zones of the data lake? Files in the stage zone are produced from files in the raw zone, but it's not clear to me whether the lineage should be explicitly in the files of the stage zone, or whether the lineage is more implicit? Somehow derived from the key's source &amp; date?\n3. Other groups in my company basically go right from the raw zone to RedShift tables, and I'm not sure when RedShift would be better than Athena/Trino on a data lake, and when it isn't? If I do integrate a lake w/ raw, stage, and analytic zones w/ RedShift, is the choice basically to load into either RedShift or the analytic zone from stage?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help understanding some general use questions about S3 data lakes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115iyzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676737370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to implement a simple data lake architecture in S3, based almost 1 to 1 on the example outlined by AWS here: &lt;a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html\"&gt;https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/welcome.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a few general data lake questions that I&amp;#39;m having a hard time googling, and would love more insight into before I start implementing the above.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Most data lake organization plans I see include some sort of timestamp in the key (i.e. &lt;code&gt;s3:/12345-raw/[source]/[year]/[month]/[day]/example.xlsx&lt;/code&gt;). I think that this works really well for incremental data extractions, but should I use the same logic for a full data extraction? I have plenty of cases where I&amp;#39;m forced to do a full extraction, in these cases, it&amp;#39;s also highly likely that &lt;code&gt;s3:/12345-raw/[source]/2023/02/18/example.xlsx&lt;/code&gt; will have the same value as &lt;code&gt;s3:/12345-raw/[source]/2023/02/19/example.xlsx&lt;/code&gt;. Even though I think it&amp;#39;s simpler to stick to the same general convention everywhere and just fully load this data downstream, is it bad to have so much data duplication in the raw zone? Or does it not matter when cloud storage is so cheap?&lt;/li&gt;\n&lt;li&gt;How should I track data lineage between different zones of the data lake? Files in the stage zone are produced from files in the raw zone, but it&amp;#39;s not clear to me whether the lineage should be explicitly in the files of the stage zone, or whether the lineage is more implicit? Somehow derived from the key&amp;#39;s source &amp;amp; date?&lt;/li&gt;\n&lt;li&gt;Other groups in my company basically go right from the raw zone to RedShift tables, and I&amp;#39;m not sure when RedShift would be better than Athena/Trino on a data lake, and when it isn&amp;#39;t? If I do integrate a lake w/ raw, stage, and analytic zones w/ RedShift, is the choice basically to load into either RedShift or the analytic zone from stage?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115iyzu", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115iyzu/need_help_understanding_some_general_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115iyzu/need_help_understanding_some_general_use/", "subreddit_subscribers": 90128, "created_utc": 1676737370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on the design of a data lake. Our application scrapes the web on a daily basis, resulting in a small daily data increment. My plan is to use the Python in combination with the Pandas for data ingestion, cleaning, and transformation. The goal is to save this data in Parquet format inside a storage. However, I am unsure what the best practice is for this scenario. Should I use a single Parquet file and append new data to it daily using Python, or create a new Parquet file every day? Alternatively, perhaps it would be best to use one Parquet file per month. What would be considered best practice in this situation?", "author_fullname": "t2_8ndxjs80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "design a data lake for a web scraping application with daily data increments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115j65j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676737920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on the design of a data lake. Our application scrapes the web on a daily basis, resulting in a small daily data increment. My plan is to use the Python in combination with the Pandas for data ingestion, cleaning, and transformation. The goal is to save this data in Parquet format inside a storage. However, I am unsure what the best practice is for this scenario. Should I use a single Parquet file and append new data to it daily using Python, or create a new Parquet file every day? Alternatively, perhaps it would be best to use one Parquet file per month. What would be considered best practice in this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115j65j", "is_robot_indexable": true, "report_reasons": null, "author": "xtrzx8", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115j65j/design_a_data_lake_for_a_web_scraping_application/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115j65j/design_a_data_lake_for_a_web_scraping_application/", "subreddit_subscribers": 90128, "created_utc": 1676737920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zrj6c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema Registry Statistics Tool is a small utility that allows you to easily identify the usage of different schema versions within a topic. Using this tool, you can consume from a topic, while calculating the percentage of each schema version.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_115gxjw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qCu1P4R7dZOhFl9UatcYDADGrEJcGGo1H1hzfEnfnuk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676731723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/EladLeev/schema-registry-statistics", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?auto=webp&amp;v=enabled&amp;s=c1857dcbe17e4fa89df6e127e072092405f9186b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a9bbcf2992ce934031edb0dfb699771068a752", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbe8f2c74d3f67396f5d893f7fba851f49c4856d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ddb4e937c58fc738bc6697d22e6a9e0e260512", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10a918d4ebb7e47668b967a34937f5b3566bd573", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d4b88f8fa6209c77927faae2fa3386593fc06b6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/xKt48gcQ7lLkk8V_WqGVED2qi80-RTkuqc2JxAQ2_1I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f36e7af3dda60682a0ded9bf5fb0ea742ec42e7", "width": 1080, "height": 540}], "variants": {}, "id": "Q7gqhNR9BMw_rb7j2B5AkezSULT9JIkbxQDp0yDOZ5o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "115gxjw", "is_robot_indexable": true, "report_reasons": null, "author": "eladleev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115gxjw/schema_registry_statistics_tool_is_a_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/EladLeev/schema-registry-statistics", "subreddit_subscribers": 90128, "created_utc": 1676731723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anybody have a way to move data from mssql to s3 or snowflake without using paid 3rd party tools? My tools are airflow, pyspark, dbt ,snowflake and s3. Currently we use stitch and I\u2019m trying get away from it. I\u2019m sure we can do the spark jdbc connector. Wondering what other options are available.", "author_fullname": "t2_t27yz8m9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data from mssql to s3 or snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115kn04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676741860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody have a way to move data from mssql to s3 or snowflake without using paid 3rd party tools? My tools are airflow, pyspark, dbt ,snowflake and s3. Currently we use stitch and I\u2019m trying get away from it. I\u2019m sure we can do the spark jdbc connector. Wondering what other options are available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115kn04", "is_robot_indexable": true, "report_reasons": null, "author": "callmedivs", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115kn04/data_from_mssql_to_s3_or_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115kn04/data_from_mssql_to_s3_or_snowflake/", "subreddit_subscribers": 90128, "created_utc": 1676741860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've used Airflow since the early days and in fact made a few small contributions to the codebase back in \"the day\", so I'm extremely familiar with it. I'm about 3 years out of the loop though and I'm wondering if there's anything new as far as technologies or best practices.\n\nHere's some information about the requirements:\n\n1. ETLs will extract from local machine (at first) and send to S3.\n2. Looking to run it in containers so it can eventually be deployed to the cloud but will initially be run locally.\n\nI was able to get a basic situation rigged up and running with a few tweaks to [this docker-compose.yml](https://github.com/marclamberti/docker-airflow/blob/main/docker-compose.yml) file but I have a few questions:\n\n1. **Should I be using breeze?** Reading about breeze made me suspect it might be more of a tool that is used to develop airflow itself with an rarely used optional feature allowing the building of production images, but sometimes it's hard to tell these things just by reading on the internet so I'm looking for other feedback.\n2. **Are there any airflow-specific reasons to prefer podman over docker?**\n3. **Is it unwise to use docker-compose to orchestrate containers?** I was using AWS ECS to orchestrate airflow containers a bunch of years ago but then went back to a setup that just used an EC2 instance.\n4. **Any other advice?**\n\nThanks.", "author_fullname": "t2_96txdx2p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow: Any advice for someone with many years of Airflow experience but who has been out of the loop for a few years on the latest and greatest best practices? In particular I'm wondering about \"Airflow Breeze\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115x9q5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676790527.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676768320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used Airflow since the early days and in fact made a few small contributions to the codebase back in &amp;quot;the day&amp;quot;, so I&amp;#39;m extremely familiar with it. I&amp;#39;m about 3 years out of the loop though and I&amp;#39;m wondering if there&amp;#39;s anything new as far as technologies or best practices.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some information about the requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;ETLs will extract from local machine (at first) and send to S3.&lt;/li&gt;\n&lt;li&gt;Looking to run it in containers so it can eventually be deployed to the cloud but will initially be run locally.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I was able to get a basic situation rigged up and running with a few tweaks to &lt;a href=\"https://github.com/marclamberti/docker-airflow/blob/main/docker-compose.yml\"&gt;this docker-compose.yml&lt;/a&gt; file but I have a few questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Should I be using breeze?&lt;/strong&gt; Reading about breeze made me suspect it might be more of a tool that is used to develop airflow itself with an rarely used optional feature allowing the building of production images, but sometimes it&amp;#39;s hard to tell these things just by reading on the internet so I&amp;#39;m looking for other feedback.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Are there any airflow-specific reasons to prefer podman over docker?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Is it unwise to use docker-compose to orchestrate containers?&lt;/strong&gt; I was using AWS ECS to orchestrate airflow containers a bunch of years ago but then went back to a setup that just used an EC2 instance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Any other advice?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?auto=webp&amp;v=enabled&amp;s=b11713aa8d2f1f5b86d41094cd020ffcce5aa6a4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b016dc65fc63dee1e0e2848c31f9563b5b46262f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ab95253edc21b69d0cd67536435bf1b81ca8457", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08f592c95207484a6d9f09b117aaf638ecaecece", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72564aa423e78df38522940b3f2db630fb3c55b3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52ace1016ea0ceb3219aa6a5de947a572b3271b3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/I84Bz3vx6MiajRioaGgySYEBlGuHWuYu9Te-Gi4n75Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a5d9c3bdc226a6a36176c9b3ee19762e976cc68", "width": 1080, "height": 540}], "variants": {}, "id": "RLobfrRDx0KU8k0LkdNkGpVdk0R2NJlZ3FHqWaHw4E4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115x9q5", "is_robot_indexable": true, "report_reasons": null, "author": "thenextsymbol", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115x9q5/airflow_any_advice_for_someone_with_many_years_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115x9q5/airflow_any_advice_for_someone_with_many_years_of/", "subreddit_subscribers": 90128, "created_utc": 1676768320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have moved to a new job that is a complete MS shop using PowerBI. Previously worked with AWS and very familiar with the S3 /glue/Athena using JSON and parquet to build a DW, fairly low cost compared to redshift.\n\nWhat would be the equivalent tools on Azure? Having trouble finding something where I can stick a load of text or parquet and build some formal schemas like in Glue and then connect the finished tables to PowerBI.\n\nEventually I would like to use DBT and Dagster for orchestration.", "author_fullname": "t2_clh5r1ln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure equivalent of S3/Glue/Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115quz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676753700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have moved to a new job that is a complete MS shop using PowerBI. Previously worked with AWS and very familiar with the S3 /glue/Athena using JSON and parquet to build a DW, fairly low cost compared to redshift.&lt;/p&gt;\n\n&lt;p&gt;What would be the equivalent tools on Azure? Having trouble finding something where I can stick a load of text or parquet and build some formal schemas like in Glue and then connect the finished tables to PowerBI.&lt;/p&gt;\n\n&lt;p&gt;Eventually I would like to use DBT and Dagster for orchestration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115quz6", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Restaurant-9691", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115quz6/azure_equivalent_of_s3glueathena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115quz6/azure_equivalent_of_s3glueathena/", "subreddit_subscribers": 90128, "created_utc": 1676753700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm just beginning to learn the airflow and it's a little confusing to me. Spark has the driver that manages the workers and airflow has scheduler that also manages workers.\n\nSo right now at work we run both spark and airflow on one machine, 12 CPUs. Airflow is allowed to run 3 tasks at once, each of these tasks being a spark job (serializing data), and spark dynamically allocates resources to the executors.\n\nWhat does an airflow worker do here? If one worker takes one task at a time, does this worker talk to the spark driver, which then talks to spark executors? \n\nCan you please help me understand the relations between the airflow scheduler -&gt; a. workers -&gt; spark driver -&gt; s. workers in the context of airflow/spark pipeline?\n\nThank you", "author_fullname": "t2_gfx5s46h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relationship between airflow workers and spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115fgep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676727324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just beginning to learn the airflow and it&amp;#39;s a little confusing to me. Spark has the driver that manages the workers and airflow has scheduler that also manages workers.&lt;/p&gt;\n\n&lt;p&gt;So right now at work we run both spark and airflow on one machine, 12 CPUs. Airflow is allowed to run 3 tasks at once, each of these tasks being a spark job (serializing data), and spark dynamically allocates resources to the executors.&lt;/p&gt;\n\n&lt;p&gt;What does an airflow worker do here? If one worker takes one task at a time, does this worker talk to the spark driver, which then talks to spark executors? &lt;/p&gt;\n\n&lt;p&gt;Can you please help me understand the relations between the airflow scheduler -&amp;gt; a. workers -&amp;gt; spark driver -&amp;gt; s. workers in the context of airflow/spark pipeline?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115fgep", "is_robot_indexable": true, "report_reasons": null, "author": "LeftHelicopter5297", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115fgep/relationship_between_airflow_workers_and_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115fgep/relationship_between_airflow_workers_and_spark/", "subreddit_subscribers": 90128, "created_utc": 1676727324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have recently switched from Data Science to Data Engineering.\n\nBasically, I like what I'm doing now and I have nice colleagues. I have learned an extreme amount in the first few weeks and find it all exciting. But at the same time, I have the problem that I constantly feel overwhelmed and it's getting worse rather than better.\n\nThere are good times when I can get tickets done quickly, but there are also times when I scroll through code for hours and feel completely at a loss.\n\nRight now I'm doing a ticket on our infrastructure with Terraform. There is S3 storage to be created, permissions to be assigned, etc. It took me 5 hours to even find the right repositories to change (it's multi repo with all components in a single repository) and now it's turned into a total of 7 pull requests where I don't even know if that's it or if it will work in the end.\n\nThis uncertainty nags me a lot. I feel like I'm in the middle of a fog and basically everything I do takes way too much time. No one is pressuring me in this respect, but I am a rather impatient person and obviously not made for reading documentation and code for hours and then writing a line of code to maybe solve a problem.\n\nIs this something that gets better with time? Is there anything I can actively do to change this state? I currently feel like I am running into a burnout.\n\nThank you!", "author_fullname": "t2_5fudhnqra", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding a good way to deal with complex tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115c72p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676715229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have recently switched from Data Science to Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;Basically, I like what I&amp;#39;m doing now and I have nice colleagues. I have learned an extreme amount in the first few weeks and find it all exciting. But at the same time, I have the problem that I constantly feel overwhelmed and it&amp;#39;s getting worse rather than better.&lt;/p&gt;\n\n&lt;p&gt;There are good times when I can get tickets done quickly, but there are also times when I scroll through code for hours and feel completely at a loss.&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m doing a ticket on our infrastructure with Terraform. There is S3 storage to be created, permissions to be assigned, etc. It took me 5 hours to even find the right repositories to change (it&amp;#39;s multi repo with all components in a single repository) and now it&amp;#39;s turned into a total of 7 pull requests where I don&amp;#39;t even know if that&amp;#39;s it or if it will work in the end.&lt;/p&gt;\n\n&lt;p&gt;This uncertainty nags me a lot. I feel like I&amp;#39;m in the middle of a fog and basically everything I do takes way too much time. No one is pressuring me in this respect, but I am a rather impatient person and obviously not made for reading documentation and code for hours and then writing a line of code to maybe solve a problem.&lt;/p&gt;\n\n&lt;p&gt;Is this something that gets better with time? Is there anything I can actively do to change this state? I currently feel like I am running into a burnout.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "115c72p", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_Effect319", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115c72p/finding_a_good_way_to_deal_with_complex_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115c72p/finding_a_good_way_to_deal_with_complex_tasks/", "subreddit_subscribers": 90128, "created_utc": 1676715229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there an option for me here? Currently on Snowflake, and I know both our unit cost as well as our usage.\n\nWe\u2019re on AWS. I\u2019m trying to understand what the likely cost would be of running the same pipelines using dbt on Hudi over S3 or Databricks in comparison to the existing cost of dbt on Snowflake.\n\nIs there an easy (ish) way to go about this?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to understand likely costs without needing to PoC everything", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1163cc7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676787363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an option for me here? Currently on Snowflake, and I know both our unit cost as well as our usage.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re on AWS. I\u2019m trying to understand what the likely cost would be of running the same pipelines using dbt on Hudi over S3 or Databricks in comparison to the existing cost of dbt on Snowflake.&lt;/p&gt;\n\n&lt;p&gt;Is there an easy (ish) way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1163cc7", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1163cc7/trying_to_understand_likely_costs_without_needing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1163cc7/trying_to_understand_likely_costs_without_needing/", "subreddit_subscribers": 90128, "created_utc": 1676787363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learn the differences between streaming databases, real-time OLAP databases, and stream processing", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_116273b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/m12X5bRyTnF43-JWIEJfJbtDedSSegMSyogc5hlyxtY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676783255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learn the differences between streaming databases, real-time OLAP databases, and stream processing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/stream-processing-vs-real-time-olap?r=46sqk&amp;utm_medium=ios&amp;utm_campaign=post", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?auto=webp&amp;v=enabled&amp;s=a7d0085c1936bbf1782ea420f65e41cfe6d80cdd", "width": 906, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01cc6aa5aab2ce0b5e9eec8e037aabea3ce231ce", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83e9ebbf277433dc7eed8d70e5f8d67d09ce8103", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2db8dd20a1ef9ec3489697f6b91206f1fc602049", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/KX06YEAZo6UwEurj6f4qJNas1REF60deBNxl4N3bf5o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8360069fd7707d9466c134b46d8389113481223", "width": 640, "height": 423}], "variants": {}, "id": "_VuVKTXnegN6nzjepsmw478Ywq3LWIdi_B29ArkOsZ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "116273b", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/116273b/streaming_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/stream-processing-vs-real-time-olap?r=46sqk&amp;utm_medium=ios&amp;utm_campaign=post", "subreddit_subscribers": 90128, "created_utc": 1676783255.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How much do popular reverse ETL tools (e.g. Census, Hightouch) cost at the enterprise level (e.g. ability to push to 10 destinations, 5+ user seats etc.).   The pricing pages for Census and Hightouch don't list their enterprise level pricing.  My company is considering using one of these tools and we believe our use cases require us to get an enterprise plan, but for internal reasons we'd prefer not to share our info with them to get a price quote until we understand whether they are in our price range.", "author_fullname": "t2_42oggxjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much do reverse ETL tools cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115w0q7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676764706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How much do popular reverse ETL tools (e.g. Census, Hightouch) cost at the enterprise level (e.g. ability to push to 10 destinations, 5+ user seats etc.).   The pricing pages for Census and Hightouch don&amp;#39;t list their enterprise level pricing.  My company is considering using one of these tools and we believe our use cases require us to get an enterprise plan, but for internal reasons we&amp;#39;d prefer not to share our info with them to get a price quote until we understand whether they are in our price range.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115w0q7", "is_robot_indexable": true, "report_reasons": null, "author": "AlexSanders123", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115w0q7/how_much_do_reverse_etl_tools_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115w0q7/how_much_do_reverse_etl_tools_cost/", "subreddit_subscribers": 90128, "created_utc": 1676764706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use data from MS Dynamics CRM in Power BI via the API. However, the data size is very large and covers more than 10 years, which is causing long load times and poor report update efficiency in Power BI. Even if I use the Modeling feature in Power BI to filter the data, it still loads all the data before applying the filters, which is inefficient. I am seeking recommendations for tools and strategies to improve the speed and efficiency of my system. Any advice would be greatly appreciated.\n\nEdit: Do you think moving the data from CRM to Azure DW first and then connecting BI to DW would be helpful? or that is doesn't worth the cost? Is there any other tools that helps this process?", "author_fullname": "t2_oaaif8zw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS Dynamic CRM to Power BI slow loading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115tori", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676759964.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676758513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use data from MS Dynamics CRM in Power BI via the API. However, the data size is very large and covers more than 10 years, which is causing long load times and poor report update efficiency in Power BI. Even if I use the Modeling feature in Power BI to filter the data, it still loads all the data before applying the filters, which is inefficient. I am seeking recommendations for tools and strategies to improve the speed and efficiency of my system. Any advice would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Edit: Do you think moving the data from CRM to Azure DW first and then connecting BI to DW would be helpful? or that is doesn&amp;#39;t worth the cost? Is there any other tools that helps this process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115tori", "is_robot_indexable": true, "report_reasons": null, "author": "ugotbkidding", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115tori/ms_dynamic_crm_to_power_bi_slow_loading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115tori/ms_dynamic_crm_to_power_bi_slow_loading/", "subreddit_subscribers": 90128, "created_utc": 1676758513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What could be the best way to design a data lake layers for one particular data source considering we will be getting one small JSON file every day?\n\nI thought of getting this JSON file and \u201cinserting\u201d the content into the parquet file which would be partitioned e.g. by month? So I would have 1 parquet file per month? Creating one parquet file every day with little content seems weird to me (?)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake design with small files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115k5sx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676740575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What could be the best way to design a data lake layers for one particular data source considering we will be getting one small JSON file every day?&lt;/p&gt;\n\n&lt;p&gt;I thought of getting this JSON file and \u201cinserting\u201d the content into the parquet file which would be partitioned e.g. by month? So I would have 1 parquet file per month? Creating one parquet file every day with little content seems weird to me (?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115k5sx", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115k5sx/data_lake_design_with_small_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115k5sx/data_lake_design_with_small_files/", "subreddit_subscribers": 90128, "created_utc": 1676740575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Could anyone please tell if you are aware of Debezium CDC Usage keeping Production method in mind. Reading from Some DB and propogating to DW (say Azure ADLS Gen2/S3)", "author_fullname": "t2_sr3rc27q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium CDC Integration Blog/Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115em3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676724576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could anyone please tell if you are aware of Debezium CDC Usage keeping Production method in mind. Reading from Some DB and propogating to DW (say Azure ADLS Gen2/S3)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115em3z", "is_robot_indexable": true, "report_reasons": null, "author": "honey12123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/115em3z/debezium_cdc_integration_blogtutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115em3z/debezium_cdc_integration_blogtutorial/", "subreddit_subscribers": 90128, "created_utc": 1676724576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been told that I should be careful telling people that I\u2019m a \u2018data engineer\u2019 because I don\u2019t have an engineering degree, and only qualified engineers can call themselves engineers. I\u2019m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it\u2019s my job title. Is this something people care about?", "author_fullname": "t2_py81dzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I call myself a data engineer if I don\u2019t have an \u2018engineering\u2019 qualification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1162sjq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676785392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been told that I should be careful telling people that I\u2019m a \u2018data engineer\u2019 because I don\u2019t have an engineering degree, and only qualified engineers can call themselves engineers. I\u2019m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it\u2019s my job title. Is this something people care about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1162sjq", "is_robot_indexable": true, "report_reasons": null, "author": "CyclicDombo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1162sjq/can_i_call_myself_a_data_engineer_if_i_dont_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1162sjq/can_i_call_myself_a_data_engineer_if_i_dont_have/", "subreddit_subscribers": 90128, "created_utc": 1676785392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I downloaded my extended streams from Spotify and I created two dimension tables one with the Track Metadata which contains the Artist Uri which connects an Artist Metadata. When I created the Artist Dimension table, I discovered that they can have multiple genres and some cases many genres. I am not sure how I should finalize this table. This will be a csv going into cloud storage. \n\nCurrently, I have it as follows \n\n    Artist_ID              Genres                                                                                                                                    \n    00FQb4jTyendYWaN8pK0wa art pop|pop                                                                 00G1NTDAoU7rBpjG4KoYAM downtempo|electronica|japanese old school \n    00sAr10UTV1JZtHqxsLVn4 canadian psychedelic rock| space rock\n\nI have each one separated with pipes, so when I query, I guess I could do like operator or some kind of sperate, take the nth one. \n\nThe other option though is to have the Artist\\_ID repeat for each unique genre but I'm not a fan off repeating the Artist\\_ID. \n\nWhat do you guys think would be the best solution in this case?", "author_fullname": "t2_z321026", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spotify Genre Date: Debating how I should store this table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115zj00", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676774800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded my extended streams from Spotify and I created two dimension tables one with the Track Metadata which contains the Artist Uri which connects an Artist Metadata. When I created the Artist Dimension table, I discovered that they can have multiple genres and some cases many genres. I am not sure how I should finalize this table. This will be a csv going into cloud storage. &lt;/p&gt;\n\n&lt;p&gt;Currently, I have it as follows &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Artist_ID              Genres                                                                                                                                    \n00FQb4jTyendYWaN8pK0wa art pop|pop                                                                 00G1NTDAoU7rBpjG4KoYAM downtempo|electronica|japanese old school \n00sAr10UTV1JZtHqxsLVn4 canadian psychedelic rock| space rock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I have each one separated with pipes, so when I query, I guess I could do like operator or some kind of sperate, take the nth one. &lt;/p&gt;\n\n&lt;p&gt;The other option though is to have the Artist_ID repeat for each unique genre but I&amp;#39;m not a fan off repeating the Artist_ID. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think would be the best solution in this case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115zj00", "is_robot_indexable": true, "report_reasons": null, "author": "raz_the_kid0901", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115zj00/spotify_genre_date_debating_how_i_should_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115zj00/spotify_genre_date_debating_how_i_should_store/", "subreddit_subscribers": 90128, "created_utc": 1676774800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019ve been stuck with coming up with a plan to \u201cmodernize\u201d our data stack, but I\u2019m a bit fuzzy on a few details. Hoping some can provide insights. \n\nBoss wants to migrate towards the \u201cmodern\u201d stack (hello buzz words) and asked me to put a plan together. \n\nI have a general idea of the flow (load data from Airbyte into snowflake then run DBT models) but I\u2019m confused on the compute part. Where the heck does the orchestration call the compute resources. For DBT models I\u2019m assuming snowflake credits are used, but how does it work for ML workflows? Are you running jobs that pull/push directly into a warehouse? \n\nWe are a gov org so we support a ton of business units all doing god knows what at times, so they to centralize into our team to run the pipelines for everyone else. \n\nI\u2019m well familiar with python, and have been using DBT for jobs recently (in SQL server), so would love to stay in that ecosystem to start but I\u2019m fuzzy on what to recommend. Basically starting out with ELT jobs for now the later they want to do ML. \n\nAny suggestions, or books or guides to read up on? Total zoo that it\u2019s on me in an org our size , but I see it as a good opportunity to drive the data platform at our org.\n\nTLDR. Data leaves our systems and goes to a warehouse. Where does the compute happen. Where do ML pipelines get their compute. \n\nOther way to frame it. In a greenfield data org, what would you do. \n\nThanks!", "author_fullname": "t2_3sioksrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about data pipeline setup/structures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115xtyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676769980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019ve been stuck with coming up with a plan to \u201cmodernize\u201d our data stack, but I\u2019m a bit fuzzy on a few details. Hoping some can provide insights. &lt;/p&gt;\n\n&lt;p&gt;Boss wants to migrate towards the \u201cmodern\u201d stack (hello buzz words) and asked me to put a plan together. &lt;/p&gt;\n\n&lt;p&gt;I have a general idea of the flow (load data from Airbyte into snowflake then run DBT models) but I\u2019m confused on the compute part. Where the heck does the orchestration call the compute resources. For DBT models I\u2019m assuming snowflake credits are used, but how does it work for ML workflows? Are you running jobs that pull/push directly into a warehouse? &lt;/p&gt;\n\n&lt;p&gt;We are a gov org so we support a ton of business units all doing god knows what at times, so they to centralize into our team to run the pipelines for everyone else. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m well familiar with python, and have been using DBT for jobs recently (in SQL server), so would love to stay in that ecosystem to start but I\u2019m fuzzy on what to recommend. Basically starting out with ELT jobs for now the later they want to do ML. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions, or books or guides to read up on? Total zoo that it\u2019s on me in an org our size , but I see it as a good opportunity to drive the data platform at our org.&lt;/p&gt;\n\n&lt;p&gt;TLDR. Data leaves our systems and goes to a warehouse. Where does the compute happen. Where do ML pipelines get their compute. &lt;/p&gt;\n\n&lt;p&gt;Other way to frame it. In a greenfield data org, what would you do. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115xtyg", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Stay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115xtyg/questions_about_data_pipeline_setupstructures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115xtyg/questions_about_data_pipeline_setupstructures/", "subreddit_subscribers": 90128, "created_utc": 1676769980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a personal project that will end up with a R Shiny dashboard I want to deploy. My challenge is that the amount of data I'm working with is too cumbersome for my personal computer so I'd like to set up something remote to work off of. Unfortunately, I only have experience working as a data scientist and have no experience developing / deploying any of the back end infrastructure.\n\n&amp;#x200B;\n\nI need to be able to store data (downloaded as csvs from the internet) and then clean / manipulate that data (preferably using a spark API like sparkr or pyspark)  so that its ready for the shiny application. I was looking for help with recommendations for the best way to do this thats both cost effective and relatively simple given my limited data engineering experience (if any exists).", "author_fullname": "t2_a2s12j5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data storage and manipulation recs for personal project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115xhyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676768989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a personal project that will end up with a R Shiny dashboard I want to deploy. My challenge is that the amount of data I&amp;#39;m working with is too cumbersome for my personal computer so I&amp;#39;d like to set up something remote to work off of. Unfortunately, I only have experience working as a data scientist and have no experience developing / deploying any of the back end infrastructure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to be able to store data (downloaded as csvs from the internet) and then clean / manipulate that data (preferably using a spark API like sparkr or pyspark)  so that its ready for the shiny application. I was looking for help with recommendations for the best way to do this thats both cost effective and relatively simple given my limited data engineering experience (if any exists).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115xhyw", "is_robot_indexable": true, "report_reasons": null, "author": "ghtghtghtghtght", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115xhyw/data_storage_and_manipulation_recs_for_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115xhyw/data_storage_and_manipulation_recs_for_personal/", "subreddit_subscribers": 90128, "created_utc": 1676768989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone shed some light on where the VMs for any serverless compute are deployed? Since those are managed by a cloud provider, my assumption is that VMs are deployed in a separate VPC and fetch the data from the storage account in the customer VPC.\n\nIf my assumption is correct then how is the data transfer between different VPCs managed? I believe both the VPCs will be in the same region and availability zone.", "author_fullname": "t2_etr435xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where are those serverless compute(VMs) deployed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115x4w8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676767910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone shed some light on where the VMs for any serverless compute are deployed? Since those are managed by a cloud provider, my assumption is that VMs are deployed in a separate VPC and fetch the data from the storage account in the customer VPC.&lt;/p&gt;\n\n&lt;p&gt;If my assumption is correct then how is the data transfer between different VPCs managed? I believe both the VPCs will be in the same region and availability zone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "115x4w8", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Ranger806", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115x4w8/where_are_those_serverless_computevms_deployed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115x4w8/where_are_those_serverless_computevms_deployed/", "subreddit_subscribers": 90128, "created_utc": 1676767910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. I have a map reduce java code in which I am getting each character from a TEXT file.\n2. The output is correct, something like: \n\na 4\n\nb 5\n\nc 6\n\nd 6\n\n  3. Now I want to identify in a line that say \"language: Engish\" or any other language, I need the output to be for all my characteristics like:\n\nEnglish a 4\n\nEnglish b 5\n\nEnglish c 6\n\nEnglish d 6\n\n4. What approach can I use in my mapper/reducer.", "author_fullname": "t2_1m4a2741", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Challenging Question: Mapper/Reducer JAVA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115jnve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676739259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;I have a map reduce java code in which I am getting each character from a TEXT file.&lt;/li&gt;\n&lt;li&gt;The output is correct, something like: &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a 4&lt;/p&gt;\n\n&lt;p&gt;b 5&lt;/p&gt;\n\n&lt;p&gt;c 6&lt;/p&gt;\n\n&lt;p&gt;d 6&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Now I want to identify in a line that say &amp;quot;language: Engish&amp;quot; or any other language, I need the output to be for all my characteristics like:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;English a 4&lt;/p&gt;\n\n&lt;p&gt;English b 5&lt;/p&gt;\n\n&lt;p&gt;English c 6&lt;/p&gt;\n\n&lt;p&gt;English d 6&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What approach can I use in my mapper/reducer.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115jnve", "is_robot_indexable": true, "report_reasons": null, "author": "octergon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115jnve/small_challenging_question_mapperreducer_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115jnve/small_challenging_question_mapperreducer_java/", "subreddit_subscribers": 90128, "created_utc": 1676739259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m getting myself a new laptop for work and will be maintaining a few databases for clients within. This would be mostly 3rd party platform data, in the 10\u2019s of millions of rows.\n\nI would have a cloud backup of course and consider migrating to a cloud based sql server. \n\nWould I see a real world impact data wise without ECC ram?", "author_fullname": "t2_5cwjpire", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ECC ram? Warehousing client reporting data\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11604np", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676776564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m getting myself a new laptop for work and will be maintaining a few databases for clients within. This would be mostly 3rd party platform data, in the 10\u2019s of millions of rows.&lt;/p&gt;\n\n&lt;p&gt;I would have a cloud backup of course and consider migrating to a cloud based sql server. &lt;/p&gt;\n\n&lt;p&gt;Would I see a real world impact data wise without ECC ram?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11604np", "is_robot_indexable": true, "report_reasons": null, "author": "JusticeSoup", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11604np/ecc_ram_warehousing_client_reporting_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11604np/ecc_ram_warehousing_client_reporting_data/", "subreddit_subscribers": 90128, "created_utc": 1676776564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI have been solve a problem since 5 days. I\u2019ve never faced any problem like this. To clarify,\n\nI am running a gradient classifier using pandas udf. The function takes as input the spark DF that I got from oracle. I send the input to the function by partitioning the client number inside to 128 (input, single on the basis of client number).\nThe problem is, while examining the output, I noticed that some client numbers are duplicate and some clients are crushed and not restored.\n \nThe total number of lines of input and output count, but I don't understand why I'm facing such a problem. I don't understand if there is a problem with running parallel nodes.\n\nIs there anyone faced this problem. I'm also curious about your comments.\n\nThank you,\n\n\n\nExample:\n\nExpected output:\n\nClient 1\u2014-Score 1\n\nClient 2\u2014-Score 2\n\nClient 3\u2014-Score 3\n\nClient 4 \u2014-Score 4\n\nOutput ( its changing every run )\n\nClient 1\u2014-Score 1\n\nClient 1\u2014-Score 1\n\nClient 1\u2014-Score 1\n\nClient 4 \u2014-Score 4", "author_fullname": "t2_njq84xk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Duplication Problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_115f49x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676726253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have been solve a problem since 5 days. I\u2019ve never faced any problem like this. To clarify,&lt;/p&gt;\n\n&lt;p&gt;I am running a gradient classifier using pandas udf. The function takes as input the spark DF that I got from oracle. I send the input to the function by partitioning the client number inside to 128 (input, single on the basis of client number).\nThe problem is, while examining the output, I noticed that some client numbers are duplicate and some clients are crushed and not restored.&lt;/p&gt;\n\n&lt;p&gt;The total number of lines of input and output count, but I don&amp;#39;t understand why I&amp;#39;m facing such a problem. I don&amp;#39;t understand if there is a problem with running parallel nodes.&lt;/p&gt;\n\n&lt;p&gt;Is there anyone faced this problem. I&amp;#39;m also curious about your comments.&lt;/p&gt;\n\n&lt;p&gt;Thank you,&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;Expected output:&lt;/p&gt;\n\n&lt;p&gt;Client 1\u2014-Score 1&lt;/p&gt;\n\n&lt;p&gt;Client 2\u2014-Score 2&lt;/p&gt;\n\n&lt;p&gt;Client 3\u2014-Score 3&lt;/p&gt;\n\n&lt;p&gt;Client 4 \u2014-Score 4&lt;/p&gt;\n\n&lt;p&gt;Output ( its changing every run )&lt;/p&gt;\n\n&lt;p&gt;Client 1\u2014-Score 1&lt;/p&gt;\n\n&lt;p&gt;Client 1\u2014-Score 1&lt;/p&gt;\n\n&lt;p&gt;Client 1\u2014-Score 1&lt;/p&gt;\n\n&lt;p&gt;Client 4 \u2014-Score 4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "115f49x", "is_robot_indexable": true, "report_reasons": null, "author": "No_Rule871", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/115f49x/spark_duplication_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/115f49x/spark_duplication_problem/", "subreddit_subscribers": 90128, "created_utc": 1676726253.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}