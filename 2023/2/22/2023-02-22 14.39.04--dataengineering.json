{"kind": "Listing", "data": {"after": "t3_11895rk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you handle schema evolution in your data lake files?\n\nLet us assume, I have a csv file landing in my data lake everyday named 'emi.csv'\n\nMy pipeline reads this file using spark, converts it into a dataframe and stores it into an external hive table. The hive external table is partitioned using 'delivery_date' column. So the file arriving on June 1, 2023 will be stored in partition 'delivery_date=20230601'.\n\nNow suddenly, on June 10, the file has the following changes: 2 new columns were added, 1 column was renamed and 1 column was deleted. How do I handle this schema evolution?\n\nShould I retire the old table and create a new table with the new schema?\n\nShould I add the 2 new columns to existing hive table, delete the existing column which was deleted in new file and rename the existing column was renamed?\n\n\nHow do you handle schema evolution such as above scenario in your data lake? Do you manually handle it or is there some automated response to handle it?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle schema evolution in your data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11868u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676993683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you handle schema evolution in your data lake files?&lt;/p&gt;\n\n&lt;p&gt;Let us assume, I have a csv file landing in my data lake everyday named &amp;#39;emi.csv&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;My pipeline reads this file using spark, converts it into a dataframe and stores it into an external hive table. The hive external table is partitioned using &amp;#39;delivery_date&amp;#39; column. So the file arriving on June 1, 2023 will be stored in partition &amp;#39;delivery_date=20230601&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Now suddenly, on June 10, the file has the following changes: 2 new columns were added, 1 column was renamed and 1 column was deleted. How do I handle this schema evolution?&lt;/p&gt;\n\n&lt;p&gt;Should I retire the old table and create a new table with the new schema?&lt;/p&gt;\n\n&lt;p&gt;Should I add the 2 new columns to existing hive table, delete the existing column which was deleted in new file and rename the existing column was renamed?&lt;/p&gt;\n\n&lt;p&gt;How do you handle schema evolution such as above scenario in your data lake? Do you manually handle it or is there some automated response to handle it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11868u6", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11868u6/how_do_you_handle_schema_evolution_in_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11868u6/how_do_you_handle_schema_evolution_in_your_data/", "subreddit_subscribers": 90540, "created_utc": 1676993683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Primarily 15-445/645 - INTRO TO DATABASE SYSTEMS \n\nI came across it on Youtube and I guess it must be interesting for people who work with relational DBs a lot (esp. OLTP systems). The syllabus mostly focuses on internals of an abstract database like Buffer Pool, Query Optimizer, Concurrency Control, ACID Transactions etc.   \n\n\nAside from that they have an active community on Discord and all coding assignments are available to the general public for submission and auto grading.  \n   \nHas anyone taken this course recently? Would be great to hear your thoughts on it.", "author_fullname": "t2_ntpsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on MOOC CMU DB courses by Andy Pavlo?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118ccej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677004072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Primarily 15-445/645 - INTRO TO DATABASE SYSTEMS &lt;/p&gt;\n\n&lt;p&gt;I came across it on Youtube and I guess it must be interesting for people who work with relational DBs a lot (esp. OLTP systems). The syllabus mostly focuses on internals of an abstract database like Buffer Pool, Query Optimizer, Concurrency Control, ACID Transactions etc.   &lt;/p&gt;\n\n&lt;p&gt;Aside from that they have an active community on Discord and all coding assignments are available to the general public for submission and auto grading.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone taken this course recently? Would be great to hear your thoughts on it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118ccej", "is_robot_indexable": true, "report_reasons": null, "author": "DCman1993", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ccej/what_are_your_thoughts_on_mooc_cmu_db_courses_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118ccej/what_are_your_thoughts_on_mooc_cmu_db_courses_by/", "subreddit_subscribers": 90540, "created_utc": 1677004072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have multiple sources which we have no control over and they provide data in csv format.\n\nThis includes but not limited to:\n- files manually loaded into google cloud storage\n- apis returning data as csv\n- sftp server where we receive csv files\n- google sheets\n\nAll of them come with their own schema, number of columns etc. The data might be at most a few GBs I guess. Pipelines are going run hourly and some maybe daily.\n\nWe need to ingest all of them and we use cloud functions (python) to do this extraction part.\n\n\nSince I was setting this part from scratch. I was thinking if there are any best practices or automated setup that people are using.\n\nI plan to have:\n- all columns converted to strings and then doing transformation part in dwh.\n- have the logical part of parsing and loading to dwh separate and have schemas defined for each of these sources.\n\nMy questions are:\n- Should csv be directly consumed or do you think it is better to convert it into different format (i am seeing parquet a lot in csv related answers in this subreddit).\n- For json we were keeping json as a single column in our source table in dwh and then parsing it later in dwh using json_scalar function in bq. This helps us with changing schema etc. For csv this probably does not make sense at all or does it?\n- Csv are notorious with data quality and reliable data is our team OKR. what is the best way/tools to ensure data reliability in case on such unreliable data sources.\n\nAnd yes we use dbt with bigquery. And Airflow for orchestration.\n\n\nWould appreciate any help. Thanks.", "author_fullname": "t2_4x8s649h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices - Data Pipelines with CSV as Data Source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118njzh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677036078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have multiple sources which we have no control over and they provide data in csv format.&lt;/p&gt;\n\n&lt;p&gt;This includes but not limited to:\n- files manually loaded into google cloud storage\n- apis returning data as csv\n- sftp server where we receive csv files\n- google sheets&lt;/p&gt;\n\n&lt;p&gt;All of them come with their own schema, number of columns etc. The data might be at most a few GBs I guess. Pipelines are going run hourly and some maybe daily.&lt;/p&gt;\n\n&lt;p&gt;We need to ingest all of them and we use cloud functions (python) to do this extraction part.&lt;/p&gt;\n\n&lt;p&gt;Since I was setting this part from scratch. I was thinking if there are any best practices or automated setup that people are using.&lt;/p&gt;\n\n&lt;p&gt;I plan to have:\n- all columns converted to strings and then doing transformation part in dwh.\n- have the logical part of parsing and loading to dwh separate and have schemas defined for each of these sources.&lt;/p&gt;\n\n&lt;p&gt;My questions are:\n- Should csv be directly consumed or do you think it is better to convert it into different format (i am seeing parquet a lot in csv related answers in this subreddit).\n- For json we were keeping json as a single column in our source table in dwh and then parsing it later in dwh using json_scalar function in bq. This helps us with changing schema etc. For csv this probably does not make sense at all or does it?\n- Csv are notorious with data quality and reliable data is our team OKR. what is the best way/tools to ensure data reliability in case on such unreliable data sources.&lt;/p&gt;\n\n&lt;p&gt;And yes we use dbt with bigquery. And Airflow for orchestration.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118njzh", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Carob897", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118njzh/best_practices_data_pipelines_with_csv_as_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118njzh/best_practices_data_pipelines_with_csv_as_data/", "subreddit_subscribers": 90540, "created_utc": 1677036078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data source that's only accessible through rest API calls and I want to do some light transformation on the data before dumping into databricks table. One way I am thinking is to do this in python notebook and just schedule this notebook as part of the automation pipeline.\n\nIs this reasonable?  what downside is there to it?   I am asking because I haven't seen any tutorial suggesting loading data this way. All the info I see online is basically loading data from another storage infra like SQL table, kafka, or S3 for example.", "author_fullname": "t2_1qgl3bp9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to ingest data from API to databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118cb99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677003996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data source that&amp;#39;s only accessible through rest API calls and I want to do some light transformation on the data before dumping into databricks table. One way I am thinking is to do this in python notebook and just schedule this notebook as part of the automation pipeline.&lt;/p&gt;\n\n&lt;p&gt;Is this reasonable?  what downside is there to it?   I am asking because I haven&amp;#39;t seen any tutorial suggesting loading data this way. All the info I see online is basically loading data from another storage infra like SQL table, kafka, or S3 for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118cb99", "is_robot_indexable": true, "report_reasons": null, "author": "jakebigman", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118cb99/best_way_to_ingest_data_from_api_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118cb99/best_way_to_ingest_data_from_api_to_databricks/", "subreddit_subscribers": 90540, "created_utc": 1677003996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The project aims to demonstrate how to work with real-time data using Kafka, KSQL, Elasticsearch, and Flask. It shows how to perform joins on Kafka topics, ingest data into Elasticsearch using Kafka Connect, and build a REST API to provide real-time metrics to end-users. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3\n\n&amp;#x200B;\n\n[https://medium.com/@stefentaime\\_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78](https://medium.com/@stefentaime_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Data Processing and Analysis with Kafka, Connect, KSQL, Elasticsearch, and Flask", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"rqhxu6fggkja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8bb4d8d78959ebafa56a7d316cf8a10b643f3765"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efb1b74b01612aa5d3d5f62ff845b1a08fca4c51"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e603d2799cda843b1fa195212b9445b1952aff16"}], "s": {"y": 500, "x": 500, "u": "https://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3"}, "id": "rqhxu6fggkja1"}}, "name": "t3_1187yvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/45jT6O-2wt_Frt8Hjhhko35xPNFm-45S_M0URZeWFeY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1676996049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The project aims to demonstrate how to work with real-time data using Kafka, KSQL, Elasticsearch, and Flask. It shows how to perform joins on Kafka topics, ingest data into Elasticsearch using Kafka Connect, and build a REST API to provide real-time metrics to end-users. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3\"&gt;https://preview.redd.it/rqhxu6fggkja1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6175131e450fe67c5f331ed6d55ec2917fd208e3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78\"&gt;https://medium.com/@stefentaime_10958/real-time-data-processing-and-analysis-with-kafka-connect-ksql-elasticsearch-and-flask-f55366032d78&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?auto=webp&amp;v=enabled&amp;s=a9005179359307951a1e81d401ea5152ea3d1551", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61c9ac705a57f544b2d6d23928c46a99c340d9a1", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a25d5b750580dc67c058b057a7bb0f59ef9eb1e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/4TqLyRqENxzQfH8PNvjfkDIq7NKw8xngTrMXmVgCKx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eed4e3a9f5d7e40c79d6eff16e1bba7455f9d1c6", "width": 320, "height": 320}], "variants": {}, "id": "SaJ6-HMH0_rD4sVnfWReybYIyaTg716qwqx1Cd1dAYY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1187yvr", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1187yvr/realtime_data_processing_and_analysis_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1187yvr/realtime_data_processing_and_analysis_with_kafka/", "subreddit_subscribers": 90540, "created_utc": 1676996049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nMy org uses MySQL for a data warehouse and I asked for write permissions so that I could create tables and build out a semantic layer on top of the existing ones. I was told that this was a dubious ask and instead I can read from a follower db and write to a new db they are setting up for me.\n\nIs this good practice? Wouldn\u2019t general prod, dev, test, etc. schema design serve a similar purpose in protecting the data while with more cohesion and less infra?\n\nSorry if this is a dumb question, I\u2019m not much of a data engineer.", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would a MySQL follower db be used for permissioning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118ic8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677021517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;My org uses MySQL for a data warehouse and I asked for write permissions so that I could create tables and build out a semantic layer on top of the existing ones. I was told that this was a dubious ask and instead I can read from a follower db and write to a new db they are setting up for me.&lt;/p&gt;\n\n&lt;p&gt;Is this good practice? Wouldn\u2019t general prod, dev, test, etc. schema design serve a similar purpose in protecting the data while with more cohesion and less infra?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is a dumb question, I\u2019m not much of a data engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118ic8e", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ic8e/why_would_a_mysql_follower_db_be_used_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118ic8e/why_would_a_mysql_follower_db_be_used_for/", "subreddit_subscribers": 90540, "created_utc": 1677021517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m at the very beginning of my learning and have been trying to come up with a valid project or idea to have in my Github. Would building a marketplace type application from as close to scratch as possible be overkill compared to creating mock databases/pools and focusing on doing different types of those? Is it even a valid thought to think that I\u2019ll be able to create mock databases and pools as practice? If not, what other ways will I be able to practice?", "author_fullname": "t2_af6ad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would it be valuable to my learning to build a copy of something like Etsy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118nj7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677036010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m at the very beginning of my learning and have been trying to come up with a valid project or idea to have in my Github. Would building a marketplace type application from as close to scratch as possible be overkill compared to creating mock databases/pools and focusing on doing different types of those? Is it even a valid thought to think that I\u2019ll be able to create mock databases and pools as practice? If not, what other ways will I be able to practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118nj7n", "is_robot_indexable": true, "report_reasons": null, "author": "eugene_steelflex", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118nj7n/would_it_be_valuable_to_my_learning_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118nj7n/would_it_be_valuable_to_my_learning_to_build_a/", "subreddit_subscribers": 90540, "created_utc": 1677036010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mhylplef", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better Airflow with Metaflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_118i049", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/cQbCD2hcsqLToWLUCdZa0ooOTQ89pWVpCJ1M01TDYm0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677020683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "outerbounds.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://outerbounds.com/blog/better-airflow-with-metaflow/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?auto=webp&amp;v=enabled&amp;s=b336a20d4b970bc11e983fac38e90c4e48149eb0", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7aa8a73f0ff0905f0aae4756fa652e47cbed9e6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f1ed452d5ada02f7cc25ca046c83abf5db9abab", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba1d127b0455760c254b81039c348acc003f20df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bade790834149c64dc56ec15a579b4c5d9b6ce24", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a32b36e28940a7a6e42a17f6e08dfbc33faba347", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/80Z2io_9HJP-q_ETyJkhHyxfgUafgMzH34IjLXlr3kQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ab2cc4c15e7c69c8eec4c85394b25d921643192", "width": 1080, "height": 567}], "variants": {}, "id": "4EhDuTU6ls_hFqQh3dT7EFJt3w09q6N6aGQVNqKh1G4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "118i049", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Factor-51", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118i049/better_airflow_with_metaflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://outerbounds.com/blog/better-airflow-with-metaflow/", "subreddit_subscribers": 90540, "created_utc": 1677020683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, i go to school for DA but I'm keen on DE, insurance is a field I want to get in to. If you are an DE at an insurance company, what kind of project do you work on? If you can have advice for your younger self, what would that be? What kind of knowledge/courses do you recommend and what would be a school project that you think create a huge plus point to either get hired/prepare for future job? \n\nThank you so much.", "author_fullname": "t2_bsbuf8gj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calling Insurtech: what kind of project are you working on and what it's like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118qz94", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677047269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, i go to school for DA but I&amp;#39;m keen on DE, insurance is a field I want to get in to. If you are an DE at an insurance company, what kind of project do you work on? If you can have advice for your younger self, what would that be? What kind of knowledge/courses do you recommend and what would be a school project that you think create a huge plus point to either get hired/prepare for future job? &lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118qz94", "is_robot_indexable": true, "report_reasons": null, "author": "CockroachThink2070", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118qz94/calling_insurtech_what_kind_of_project_are_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118qz94/calling_insurtech_what_kind_of_project_are_you/", "subreddit_subscribers": 90540, "created_utc": 1677047269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Data Modelling Interview coming up. I was told this about it:  \n \"The technical exercise will be a live whiteboarding/pseudo-code conversation to understand how you think about architecting data systems. To prepare, think about a basic app or system you\u2019re familiar with - a messaging app, an e-commerce site, a news app, etc - and what sorts of tables are needed to support the app on the backend. We\u2019ll whiteboard out what those tables look like; talk through the types of analysis that the business might want to do on those tables; and pseudo-code how we would transform the data to support that analysis.\"   \n\n\nAny thoughts on how to prep for this? I have never done a data modelling interview before.  \n\n\nThanks!", "author_fullname": "t2_28kn9vuw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modelling Interview Prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118p4mg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677040975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Data Modelling Interview coming up. I was told this about it:&lt;br/&gt;\n &amp;quot;The technical exercise will be a live whiteboarding/pseudo-code conversation to understand how you think about architecting data systems. To prepare, think about a basic app or system you\u2019re familiar with - a messaging app, an e-commerce site, a news app, etc - and what sorts of tables are needed to support the app on the backend. We\u2019ll whiteboard out what those tables look like; talk through the types of analysis that the business might want to do on those tables; and pseudo-code how we would transform the data to support that analysis.&amp;quot;   &lt;/p&gt;\n\n&lt;p&gt;Any thoughts on how to prep for this? I have never done a data modelling interview before.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "118p4mg", "is_robot_indexable": true, "report_reasons": null, "author": "PythonDataEngineer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118p4mg/data_modelling_interview_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118p4mg/data_modelling_interview_prep/", "subreddit_subscribers": 90540, "created_utc": 1677040975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualize Streaming Data in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_118ndst", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UQMeohh2pxHn5ji1QmNx6Vwzp6KOaD2mxNdQhY0Os0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677035570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bytewax.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bytewax.io/blog/visualize-streaming-data-in-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?auto=webp&amp;v=enabled&amp;s=37a528a7612550bc82be48d4d478997d72be1558", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03247202e8466bf819d1b3ee42fec17247a41e34", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3bd711ec66db5611d6b0de37f4f4dec4eb202d8", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10a7f5446ed3fc25af7d29b96676adf58238e6bf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e250c5abbe743d967751dcc90847bdb4a0c0dad", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0cd35b80bb96804231f0d799445ba6c96e73da4", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e72f416371064af1d42c4426ab4b5ceace4a028d", "width": 1080, "height": 564}], "variants": {}, "id": "_GAQl_RTzqMiEFZm4ZiVzQqqGTrkugbg6LCg3kt5mlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "118ndst", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ndst/visualize_streaming_data_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bytewax.io/blog/visualize-streaming-data-in-python", "subreddit_subscribers": 90540, "created_utc": 1677035570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am attempting to perform a full scd type 2 merge on a delta table within Synapse.  I have most experience doing this in SQL Server and followed a general pattern seen [here](https://www.mssqltips.com/sqlservertip/2883/using-the-sql-server-merge-statement-to-process-type--slowly-changing-dimensions/) the syntax was always confusing to me, however and I never felt comfortable doing it.\n\nFor delta tables I was following along with [this tutorial](https://kontext.tech/article/1183/scd-type-2-implement-full-merge-with-delta-lake-table-via-pyspark) I found as well as [this description](https://www.databricks.com/blog/2023/01/25/loading-data-warehouse-slowly-changing-dimension-type-2-using-matillion.html) by DataBricks using Matillion.  Both use an intermediary table that joins source and target then labels each record as an Insert, Update, or Delete.  I do not have to worry about the delete as I am not getting a full table each time.  So I have implemented my scd type 2 by using the merge statement twice.  I have a hash column to compare any changes within the data and the resultant code I came up with is below.\n\n    merge = target.alias(\"tgt\").merge(source.alias(\"src\"), \"tgt.hash=src.hash\") \\\n        .whenMatchedUpdate(condition=\"tgt.is_current=1 AND tgt.end_date&lt;src.end_date\",\n            set={\n                \"end_date\":col('src.end_date'),\n                \"filename\":col('src.filename')\n                }) \\\n        .whenNotMatchedInsertAll()\n    merge.execute()\n    \n    merge = target.alias(\"tgt\").merge(source.alias(\"src\"), \"tgt.ApplicationId=src.ApplicationId AND tgt.Contact=src.Contact\") \\\n        .whenMatchedUpdate(condition=\"tgt.is_current=1 AND tgt.hash&lt;&gt;src.hash AND tgt.end_date&lt;src.end_date\",\n            set={\n                \"is_current\":lit(0)\n                }) \n    \n    \n    merge.execute()\n\nTests seem to work out well, but I do not know if performing 2 merges is more expensive than joining the tables into an intermediate table.  Both scenarios seem like they would need to perform a full table scan.  Is there any benefit to creating the intermediate table as opposed to the above double merge?  Or put another way, are there any pitfalls in performing the double merge vs an intermediate table?\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_bkzx0bcd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Full SCD2 merge on Delta Table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118if85", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677021728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am attempting to perform a full scd type 2 merge on a delta table within Synapse.  I have most experience doing this in SQL Server and followed a general pattern seen &lt;a href=\"https://www.mssqltips.com/sqlservertip/2883/using-the-sql-server-merge-statement-to-process-type--slowly-changing-dimensions/\"&gt;here&lt;/a&gt; the syntax was always confusing to me, however and I never felt comfortable doing it.&lt;/p&gt;\n\n&lt;p&gt;For delta tables I was following along with &lt;a href=\"https://kontext.tech/article/1183/scd-type-2-implement-full-merge-with-delta-lake-table-via-pyspark\"&gt;this tutorial&lt;/a&gt; I found as well as &lt;a href=\"https://www.databricks.com/blog/2023/01/25/loading-data-warehouse-slowly-changing-dimension-type-2-using-matillion.html\"&gt;this description&lt;/a&gt; by DataBricks using Matillion.  Both use an intermediary table that joins source and target then labels each record as an Insert, Update, or Delete.  I do not have to worry about the delete as I am not getting a full table each time.  So I have implemented my scd type 2 by using the merge statement twice.  I have a hash column to compare any changes within the data and the resultant code I came up with is below.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;merge = target.alias(&amp;quot;tgt&amp;quot;).merge(source.alias(&amp;quot;src&amp;quot;), &amp;quot;tgt.hash=src.hash&amp;quot;) \\\n    .whenMatchedUpdate(condition=&amp;quot;tgt.is_current=1 AND tgt.end_date&amp;lt;src.end_date&amp;quot;,\n        set={\n            &amp;quot;end_date&amp;quot;:col(&amp;#39;src.end_date&amp;#39;),\n            &amp;quot;filename&amp;quot;:col(&amp;#39;src.filename&amp;#39;)\n            }) \\\n    .whenNotMatchedInsertAll()\nmerge.execute()\n\nmerge = target.alias(&amp;quot;tgt&amp;quot;).merge(source.alias(&amp;quot;src&amp;quot;), &amp;quot;tgt.ApplicationId=src.ApplicationId AND tgt.Contact=src.Contact&amp;quot;) \\\n    .whenMatchedUpdate(condition=&amp;quot;tgt.is_current=1 AND tgt.hash&amp;lt;&amp;gt;src.hash AND tgt.end_date&amp;lt;src.end_date&amp;quot;,\n        set={\n            &amp;quot;is_current&amp;quot;:lit(0)\n            }) \n\n\nmerge.execute()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Tests seem to work out well, but I do not know if performing 2 merges is more expensive than joining the tables into an intermediate table.  Both scenarios seem like they would need to perform a full table scan.  Is there any benefit to creating the intermediate table as opposed to the above double merge?  Or put another way, are there any pitfalls in performing the double merge vs an intermediate table?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118if85", "is_robot_indexable": true, "report_reasons": null, "author": "SmothCerbrosoSimiae", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118if85/full_scd2_merge_on_delta_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118if85/full_scd2_merge_on_delta_table/", "subreddit_subscribers": 90540, "created_utc": 1677021728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Newbie here. Please be gentle.\n\nIn order to gain a understanding of the tables and their contents in our company, I have implemented one of the existing [data discovery platforms](https://github.com/opendatadiscovery/awesome-data-catalogs) (in my case [Amundsen](https://www.amundsen.io/)). Unfortunately, Amundsen can only display the tables it has access to.\n\nWhat I am looking for is a solution (similar to Amundsen or [Datahub](https://datahubproject.io/)) that also allows to add tables and their metadata manually.\n\nI would be glad for any kind of help.", "author_fullname": "t2_762zbgq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an \"offline\" data discovery platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118b8f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677001910.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677001374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newbie here. Please be gentle.&lt;/p&gt;\n\n&lt;p&gt;In order to gain a understanding of the tables and their contents in our company, I have implemented one of the existing &lt;a href=\"https://github.com/opendatadiscovery/awesome-data-catalogs\"&gt;data discovery platforms&lt;/a&gt; (in my case &lt;a href=\"https://www.amundsen.io/\"&gt;Amundsen&lt;/a&gt;). Unfortunately, Amundsen can only display the tables it has access to.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is a solution (similar to Amundsen or &lt;a href=\"https://datahubproject.io/\"&gt;Datahub&lt;/a&gt;) that also allows to add tables and their metadata manually.&lt;/p&gt;\n\n&lt;p&gt;I would be glad for any kind of help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?auto=webp&amp;v=enabled&amp;s=92b9653a9cf2ce71110179b3709e066b6cb0eefe", "width": 2560, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7fd913fec3e90e5089dbba711b2a6daef030c296", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d1f64f9bba16400bfd2ae22d7ba430051dcf64e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1360b393dd90be71a2aef6091457a269419d8932", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0174751cc19e56081b7b5bf498af3780218f962", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeacb5dc6f37022e5ba8e3396a20513f609fb209", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rn2AtjG9cyQEbPqihVKFQrJMWRbV0TxaVJfBKnYYSco.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8176f19e4228c9e5ce1add94e90d83919af5a556", "width": 1080, "height": 540}], "variants": {}, "id": "-EHj7XvhM7vQSeFel5uITngODMjFcKcpaC6j9DVqNZ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118b8f7", "is_robot_indexable": true, "report_reasons": null, "author": "rocket9001", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118b8f7/looking_for_an_offline_data_discovery_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118b8f7/looking_for_an_offline_data_discovery_platform/", "subreddit_subscribers": 90540, "created_utc": 1677001374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my current company, there is a data pipeline where we use Kafka Connect to write the change stream events (for a collection) to a Kafka stream, and then the Spark structured streaming application ingests the data from Kafka and processes it.\n\nMy question is, why can't we directly stream the data from MongoDB using the spark application? As per this article ([https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/](https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/)), change streams are responsible for passing on events to the client (in this case the spark application).\n\nIn both cases (Kafka Connect and the Spark application), change streams are created which notify the client of the changes. I don't really understand why we had to introduce Kafka in the middle of MongoDB and the Spark application.\n\nIt would be great if someone could help me understand this. Thanks for reading.\n\nEDIT:\n\nWhile writing the post, I realized that I am actually questioning the reason for using Kafka at all. I understood the following from this stack overflow answer\n\n[https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark](https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark): -\n\n&gt;Kafka offers persistence (till the set retention period). So if the spark application is down for maintenance, it will miss the change stream events from the MongoDB collection during the maintenance time. But if the events are stored in Kafka, they will be there for the duration of the retention period and ready for processing by the spark application.\n\nIf anyone else has anything to add to this, please feel free to do so. Thanks.\n\n&amp;#x200B;", "author_fullname": "t2_uqx8q0b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Streaming from MongoDB vs Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118stz7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677055432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677054235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my current company, there is a data pipeline where we use Kafka Connect to write the change stream events (for a collection) to a Kafka stream, and then the Spark structured streaming application ingests the data from Kafka and processes it.&lt;/p&gt;\n\n&lt;p&gt;My question is, why can&amp;#39;t we directly stream the data from MongoDB using the spark application? As per this article (&lt;a href=\"https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/\"&gt;https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/&lt;/a&gt;), change streams are responsible for passing on events to the client (in this case the spark application).&lt;/p&gt;\n\n&lt;p&gt;In both cases (Kafka Connect and the Spark application), change streams are created which notify the client of the changes. I don&amp;#39;t really understand why we had to introduce Kafka in the middle of MongoDB and the Spark application.&lt;/p&gt;\n\n&lt;p&gt;It would be great if someone could help me understand this. Thanks for reading.&lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;/p&gt;\n\n&lt;p&gt;While writing the post, I realized that I am actually questioning the reason for using Kafka at all. I understood the following from this stack overflow answer&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark\"&gt;https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark&lt;/a&gt;: -&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Kafka offers persistence (till the set retention period). So if the spark application is down for maintenance, it will miss the change stream events from the MongoDB collection during the maintenance time. But if the events are stored in Kafka, they will be there for the duration of the retention period and ready for processing by the spark application.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If anyone else has anything to add to this, please feel free to do so. Thanks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118stz7", "is_robot_indexable": true, "report_reasons": null, "author": "the-fake-me", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118stz7/spark_streaming_from_mongodb_vs_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118stz7/spark_streaming_from_mongodb_vs_kafka/", "subreddit_subscribers": 90540, "created_utc": 1677054235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Guys, \n\nHope you all are doing great.   \nI will be appearing in interview for Data Science Intern/Full-time (Later) roles. I wanted to ask what all resources you referred while preparing for the same? I understand companies sometimes use Data Analyst, Data Science Engineer, Data Engineer etc names interchangeably.   \nI had recently appeared for CVS Health Intern interview, and tbh it didn't go good. I had applied for analytics positions and in coding round only Pandas was asked. The interviewer told me it was understood that Pandas will be asked, but wtf in job description Python was written I thought it will be just normal Python questions.   \n\n\nAnyway, now I understood, I need to have pandas on my toes and MySQL, Is there anything else? Which resources to refer? There is abundance of material in online platform and it's confusing me.   \n\n\nPlease help!!", "author_fullname": "t2_8ut41pyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Preparation for Data Engineer/Data Analyst/Business Analyst roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118pdwn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677041814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Guys, &lt;/p&gt;\n\n&lt;p&gt;Hope you all are doing great.&lt;br/&gt;\nI will be appearing in interview for Data Science Intern/Full-time (Later) roles. I wanted to ask what all resources you referred while preparing for the same? I understand companies sometimes use Data Analyst, Data Science Engineer, Data Engineer etc names interchangeably.&lt;br/&gt;\nI had recently appeared for CVS Health Intern interview, and tbh it didn&amp;#39;t go good. I had applied for analytics positions and in coding round only Pandas was asked. The interviewer told me it was understood that Pandas will be asked, but wtf in job description Python was written I thought it will be just normal Python questions.   &lt;/p&gt;\n\n&lt;p&gt;Anyway, now I understood, I need to have pandas on my toes and MySQL, Is there anything else? Which resources to refer? There is abundance of material in online platform and it&amp;#39;s confusing me.   &lt;/p&gt;\n\n&lt;p&gt;Please help!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118pdwn", "is_robot_indexable": true, "report_reasons": null, "author": "Mystery_Shrey", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118pdwn/interview_preparation_for_data_engineerdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118pdwn/interview_preparation_for_data_engineerdata/", "subreddit_subscribers": 90540, "created_utc": 1677041814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to start a personal project that using ADF to ingest data from Excel files and perform some transformation before loading into Azure SQL database.\n\nI\u2019m thinking of using Pandas to perform all these transformation. What I meant by transformation is that I want to normalize the data before loading it into the database and would like to use Pandas to do all these because I have quite some knowledge on it. Will this be a good approach?", "author_fullname": "t2_imktgzwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118lx7u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677031297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to start a personal project that using ADF to ingest data from Excel files and perform some transformation before loading into Azure SQL database.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking of using Pandas to perform all these transformation. What I meant by transformation is that I want to normalize the data before loading it into the database and would like to use Pandas to do all these because I have quite some knowledge on it. Will this be a good approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118lx7u", "is_robot_indexable": true, "report_reasons": null, "author": "Acrobatic-Mobile-221", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118lx7u/azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118lx7u/azure_data_factory/", "subreddit_subscribers": 90540, "created_utc": 1677031297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like this is becoming more prevalent in my organization where some new vendor comes in and promises the moon to replace an on-premises system. \n\nThe problem is that although these new SaaS solutions offer a lot of flexibility, what appears to be secondary is their ability to provide access to the data. Most vendors I've spoken with almost appear a little surprised that it's even a requirement, and they show off their pie charts indicating that reporting and analytics can be done within platform just fine. This typically falls into one of three categories:\n\n&amp;#x200B;\n\n1. They can offer access to the system's data, but it's through a REST API. The problem with REST APIs is I feel like we don't have enough control over the data we receive (i.e. if a new attribute is added, or the hierarchy of an item changes, or if their pagination goes wonky.) \n2. They can offer access, but it's all NoSQL, which isn't as conducive as a relational database.\n3. They just can't offer access to the data.\n\nMy questions are:\n\n1. How do you convince business that having access to their own data is in their self interest.\n2. How important has it been to your organization's decision making on vendor selection.\n3. How much of a problem has this been for your team(s).", "author_fullname": "t2_ry4ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deal with SaaS solutions and getting access to data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118bj0p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677002104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like this is becoming more prevalent in my organization where some new vendor comes in and promises the moon to replace an on-premises system. &lt;/p&gt;\n\n&lt;p&gt;The problem is that although these new SaaS solutions offer a lot of flexibility, what appears to be secondary is their ability to provide access to the data. Most vendors I&amp;#39;ve spoken with almost appear a little surprised that it&amp;#39;s even a requirement, and they show off their pie charts indicating that reporting and analytics can be done within platform just fine. This typically falls into one of three categories:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;They can offer access to the system&amp;#39;s data, but it&amp;#39;s through a REST API. The problem with REST APIs is I feel like we don&amp;#39;t have enough control over the data we receive (i.e. if a new attribute is added, or the hierarchy of an item changes, or if their pagination goes wonky.) &lt;/li&gt;\n&lt;li&gt;They can offer access, but it&amp;#39;s all NoSQL, which isn&amp;#39;t as conducive as a relational database.&lt;/li&gt;\n&lt;li&gt;They just can&amp;#39;t offer access to the data.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you convince business that having access to their own data is in their self interest.&lt;/li&gt;\n&lt;li&gt;How important has it been to your organization&amp;#39;s decision making on vendor selection.&lt;/li&gt;\n&lt;li&gt;How much of a problem has this been for your team(s).&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118bj0p", "is_robot_indexable": true, "report_reasons": null, "author": "vincentx99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118bj0p/how_do_you_deal_with_saas_solutions_and_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118bj0p/how_do_you_deal_with_saas_solutions_and_getting/", "subreddit_subscribers": 90540, "created_utc": 1677002104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI\u2019ve been working on a side project that does some basic web scraping of fantasy basketball projections into a Postgres DB, using dbt for further modeling and airflow for orchestrating the whole thing. I currently have these services dockerized and since I don\u2019t have any cloud experience on my job, I thought it would be interesting to try messing around with AWS and see if I could get it deployed on a small EC2 instance.\n\nA friend of mine (software engineer) told me it would be way more worthwhile if I instead start up a raw Ubuntu server on a DigitalOcean droplet and set everything up from scratch on there, saying AWS would be a breeze to use once I am able to do that.\n\nI know he is a more traditional software engineer, but wanted to hear anyone\u2019s thoughts if this would be true since I technically don\u2019t need S3 or any additional AWS services for this project. I feel that it is important to get experience with AWS for future positions, so I guess I\u2019m curious to see if I should try deploying to DigitalOcean for practice and then scrap that/redeploy the project to AWS from there, or if I should just dive straight into AWS?", "author_fullname": "t2_bwp6e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DigitalOcean vs AWS for Side Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1185npa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676992269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working on a side project that does some basic web scraping of fantasy basketball projections into a Postgres DB, using dbt for further modeling and airflow for orchestrating the whole thing. I currently have these services dockerized and since I don\u2019t have any cloud experience on my job, I thought it would be interesting to try messing around with AWS and see if I could get it deployed on a small EC2 instance.&lt;/p&gt;\n\n&lt;p&gt;A friend of mine (software engineer) told me it would be way more worthwhile if I instead start up a raw Ubuntu server on a DigitalOcean droplet and set everything up from scratch on there, saying AWS would be a breeze to use once I am able to do that.&lt;/p&gt;\n\n&lt;p&gt;I know he is a more traditional software engineer, but wanted to hear anyone\u2019s thoughts if this would be true since I technically don\u2019t need S3 or any additional AWS services for this project. I feel that it is important to get experience with AWS for future positions, so I guess I\u2019m curious to see if I should try deploying to DigitalOcean for practice and then scrap that/redeploy the project to AWS from there, or if I should just dive straight into AWS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1185npa", "is_robot_indexable": true, "report_reasons": null, "author": "wild_bill34", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1185npa/digitalocean_vs_aws_for_side_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1185npa/digitalocean_vs_aws_for_side_project/", "subreddit_subscribers": 90540, "created_utc": 1676992269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi,\n\nI'm using Delta Live Tables to create a medallion architecture and am having trouble defining a parameterised function to upsert data from bronze into silver. I used the [CDC](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html) documentation but I also want my silver table to have expectations, schema hints and new columns, which I don't think is possible using streaming\\_live\\_table.\n\nAs a reference, I created the below function to take data from a bronze table and upsert into silver:\n\n    def generate_silver_streaming_live_table(env, schema_name, object_name, dedupe_key, seq):\n        dlt.create_streaming_live_table(\n            name = env + \"_\" + object_name + \"live_silver\",\n            comment = \"Silver data for \" + env + \"_\" + object_name,\n            path = \"abfss://datalakepath.dfs.core.windows.net/\" + env +\"/\" + schema_name + \"/\" + object_name + \"/silver\"\n        )\n        dlt.apply_changes(\n            target = env + \"_\" + object_name + \"live_silver\",\n            source = env + \"_\" + object_name + \"_bronze\",\n            keys = dedupe_key,\n            sequence_by = seq\n        )\n\nThis works well but like I mentioned is not compatible with expectations, schema hints and new columns. I then changed tactics to use the more traditional dlt.table decorator, but this is having a lot of trouble with upserts. ForEachBatch is not compatible with DLT - i've tried using dropDuplicates but this keeps the first update made rather than changes, i've also tried using merge statements like below but this isn't working for me either because the silver table has no columns yet so can't do the merge on silver.dedupe\\_key.\n\n    def generate_silver_table(env, object_name, dedupe_key):\n        @dlt.table(\n            name = env + \"_\" + object_name + \"static_silver\",\n            comment = \"Silver data for \" + env + \"_\" + object_name,\n        )\n        def silverincremental():\n                df = dlt.read_stream(\n                    env + \"_\" + object_name + \"_bronze\"\n                )\n                silver_table = DeltaTable.forPath(spark, \"dbfs:/pathtosilvertable\")\n                return(\n                silver_table.alias(\"silver\").merge(df.alias('source'), \"source.\" + dedupe_key + \"= silver.\" + dedupe_key)\n                    .whenMatchedUpdateAll()\n                    .whenNotMatchedInsertAll()\n                    .execute()\n                )\n\nI'm at a loose end here but there must be something i'm missing? This is a very simple and very common business problem using recommended steps in the medallion architecture recommended by Databricks and I can't believe it's so difficult to implement.", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Live Tables Upsert to Silver", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_118zrd4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677074324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Delta Live Tables to create a medallion architecture and am having trouble defining a parameterised function to upsert data from bronze into silver. I used the &lt;a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html\"&gt;CDC&lt;/a&gt; documentation but I also want my silver table to have expectations, schema hints and new columns, which I don&amp;#39;t think is possible using streaming_live_table.&lt;/p&gt;\n\n&lt;p&gt;As a reference, I created the below function to take data from a bronze table and upsert into silver:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def generate_silver_streaming_live_table(env, schema_name, object_name, dedupe_key, seq):\n    dlt.create_streaming_live_table(\n        name = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;live_silver&amp;quot;,\n        comment = &amp;quot;Silver data for &amp;quot; + env + &amp;quot;_&amp;quot; + object_name,\n        path = &amp;quot;abfss://datalakepath.dfs.core.windows.net/&amp;quot; + env +&amp;quot;/&amp;quot; + schema_name + &amp;quot;/&amp;quot; + object_name + &amp;quot;/silver&amp;quot;\n    )\n    dlt.apply_changes(\n        target = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;live_silver&amp;quot;,\n        source = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;_bronze&amp;quot;,\n        keys = dedupe_key,\n        sequence_by = seq\n    )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This works well but like I mentioned is not compatible with expectations, schema hints and new columns. I then changed tactics to use the more traditional dlt.table decorator, but this is having a lot of trouble with upserts. ForEachBatch is not compatible with DLT - i&amp;#39;ve tried using dropDuplicates but this keeps the first update made rather than changes, i&amp;#39;ve also tried using merge statements like below but this isn&amp;#39;t working for me either because the silver table has no columns yet so can&amp;#39;t do the merge on silver.dedupe_key.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def generate_silver_table(env, object_name, dedupe_key):\n    @dlt.table(\n        name = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;static_silver&amp;quot;,\n        comment = &amp;quot;Silver data for &amp;quot; + env + &amp;quot;_&amp;quot; + object_name,\n    )\n    def silverincremental():\n            df = dlt.read_stream(\n                env + &amp;quot;_&amp;quot; + object_name + &amp;quot;_bronze&amp;quot;\n            )\n            silver_table = DeltaTable.forPath(spark, &amp;quot;dbfs:/pathtosilvertable&amp;quot;)\n            return(\n            silver_table.alias(&amp;quot;silver&amp;quot;).merge(df.alias(&amp;#39;source&amp;#39;), &amp;quot;source.&amp;quot; + dedupe_key + &amp;quot;= silver.&amp;quot; + dedupe_key)\n                .whenMatchedUpdateAll()\n                .whenNotMatchedInsertAll()\n                .execute()\n            )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m at a loose end here but there must be something i&amp;#39;m missing? This is a very simple and very common business problem using recommended steps in the medallion architecture recommended by Databricks and I can&amp;#39;t believe it&amp;#39;s so difficult to implement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118zrd4", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118zrd4/delta_live_tables_upsert_to_silver/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118zrd4/delta_live_tables_upsert_to_silver/", "subreddit_subscribers": 90540, "created_utc": 1677074324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found the following job description and was wondering if this qualifies as a data engineering position\n\n&gt;\\- You will analyze, improve, and develop data architectures in Microsoft SQL Server to further develop our existing tool.   \n&gt;  \n&gt;\\- You will always consider the compatibility of the data and structure with AWS services against the backdrop of migration / replication capabilities.   \n&gt;  \n&gt;\\- You will replicate data where necessary and migrate data to the AWS Cloud where possible. In doing so, the end-to-end functionality of the applications based on this data is your top priority.   \n&gt;  \n&gt;\\- You will collaborate on the data structure concept, the semantics model and the basic data collection, processing and usage concept with a focus on orientation towards the cloud.   \n&gt;  \n&gt;\\- You will advise and support on all questions and decisions regarding data structure.\n\nAny opinions? :)", "author_fullname": "t2_idpai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a data engineering position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118wer7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677067326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found the following job description and was wondering if this qualifies as a data engineering position&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;- You will analyze, improve, and develop data architectures in Microsoft SQL Server to further develop our existing tool.   &lt;/p&gt;\n\n&lt;p&gt;- You will always consider the compatibility of the data and structure with AWS services against the backdrop of migration / replication capabilities.   &lt;/p&gt;\n\n&lt;p&gt;- You will replicate data where necessary and migrate data to the AWS Cloud where possible. In doing so, the end-to-end functionality of the applications based on this data is your top priority.   &lt;/p&gt;\n\n&lt;p&gt;- You will collaborate on the data structure concept, the semantics model and the basic data collection, processing and usage concept with a focus on orientation towards the cloud.   &lt;/p&gt;\n\n&lt;p&gt;- You will advise and support on all questions and decisions regarding data structure.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Any opinions? :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "118wer7", "is_robot_indexable": true, "report_reasons": null, "author": "MoBoo138", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118wer7/is_this_a_data_engineering_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118wer7/is_this_a_data_engineering_position/", "subreddit_subscribers": 90540, "created_utc": 1677067326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our company is B2B2C so we collect user emails but we have limited information about our users since we handle a service for our users. We want to understand our users better and collect data attributes to help us understand our users better. \n\nWhat's the best way to do this?", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Enrichment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118osuu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677039947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our company is B2B2C so we collect user emails but we have limited information about our users since we handle a service for our users. We want to understand our users better and collect data attributes to help us understand our users better. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118osuu", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118osuu/data_enrichment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118osuu/data_enrichment/", "subreddit_subscribers": 90540, "created_utc": 1677039947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to copy about 100 resources in azure data factory and applying some minor changes like name and folder. I have downloaded the JSON files from ADF and edited them all at one using a Powershell script and pushed them again to the development branch. The problem now is that ADF is telling me that JSON structure is wrong and didn't recognize the new JSON files. \nDoes anyone have an idea how to fix it or we have always to edit resources in ADF? \nThanks in advance for your feedback\n\nEdit: I was able to fix it using Python to manipulate the JSON instead of Powershell. I don't know why but I used the same steps with Python as with Powershell.", "author_fullname": "t2_pp0zirow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure ADF using VS Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118h8or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677072361.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677018831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to copy about 100 resources in azure data factory and applying some minor changes like name and folder. I have downloaded the JSON files from ADF and edited them all at one using a Powershell script and pushed them again to the development branch. The problem now is that ADF is telling me that JSON structure is wrong and didn&amp;#39;t recognize the new JSON files. \nDoes anyone have an idea how to fix it or we have always to edit resources in ADF? \nThanks in advance for your feedback&lt;/p&gt;\n\n&lt;p&gt;Edit: I was able to fix it using Python to manipulate the JSON instead of Powershell. I don&amp;#39;t know why but I used the same steps with Python as with Powershell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118h8or", "is_robot_indexable": true, "report_reasons": null, "author": "These_Rip_9327", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118h8or/azure_adf_using_vs_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118h8or/azure_adf_using_vs_code/", "subreddit_subscribers": 90540, "created_utc": 1677018831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some common challenges in scaling machine learning systems?", "author_fullname": "t2_9u34jt5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some common challenges in scaling machine learning systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118c3li", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677003474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common challenges in scaling machine learning systems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118c3li", "is_robot_indexable": true, "report_reasons": null, "author": "Nice-Tomorrow2926", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118c3li/what_are_some_common_challenges_in_scaling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118c3li/what_are_some_common_challenges_in_scaling/", "subreddit_subscribers": 90540, "created_utc": 1677003474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everybody, \ud83d\udc4b\ud83d\udc4b\n\nWe are really excited to open source YoBulk today!\n\nYoBulk is an **open-source CSV importer** for any SaaS application - It's a free alternative to [https://flatfile.com/](https://flatfile.com/)\n\nWe realized that more than 70% of business data is shared in CSV and Excel formats, and only a small percentage use API integrations for data exchange. As developers and product managers, we have experienced the difficulties of building a scalable CSV importer, and we know that many others face the same challenges. Our goal is to solve this problem by taking an open-source AI and developer-centric approach\n\n**Who can use YoBulk:**\n\nYoBulk is a highly beneficial tool for a variety of professionals, such as Developers, Product Managers, Customer Success teams, and Marketers. It simplifies the process of onboarding and verifying customer data, making it an indispensable asset for those who deal with frequent CSV data uploads to a system with a predetermined schema or template.\n\nThis tool is particularly valuable for updating sales CRM or product catalog data, and it effectively solves the initial challenge of customer data ingestion.\n\n&amp;#x200B;\n\n# The Problem:\n\n**Importing a CSV is a really hard problem to solve. Some of the key problems are:**\n\n**1. Missing Collaboration and Automation in CSV importing workflow:** In a usual situation, the customer success team responsible for receiving CSV data has to engage in extensive back-and-forth communication with the customer to address unintentional manual errors present in a CSV.\n\n**2. Scale:** CRM CSV files can sometimes reach sizes as large as 4 GB, making it nearly impossible to open them on a standalone machine for data correction. This presents a significant challenge for small businesses that cannot afford to invest in big data technologies such as EMR, Databrick, and ETL tools to address CSV import scaling problems.\n\n**3. Countless complex validation Types:** single date format can have as many as 100 different variations, such as dd-mm-yyyy, mm-dd-yyyy, and dd.mm.yyyy. Manually setting validation rules for each of these formats is almost impossible, and correcting errors manually will be difficult.\n\n**4. Data mapping issues**: In a typical scenario, the recipient of CSV data provides a template to the data donor and creates a CSV column for template mapping before importing\n\n**5. Data Security and Privacy**: It is always risky to share your customer data with third-party companies for data cleaning purposes.\n\n**6. Non-availability of low code/No code tool:** Product managers and customer success teams, who are typically no-code users, often rely on data analysts to create a programmed CSV template with validation rules, which must be shared with customers to receive CSV data in a specific format.\n\n**7.** **Vague error messages:** Unclear error messages do not provide users with enough context to confidently resolve their issues before uploading their data.\n\n&amp;#x200B;\n\n# How YoBulk helps address the above issues :\n\n\ud83d\ude80 **Smart Spreadsheet View:** Designed to be a data exchange hub for any business that utilizes CSV files, YoBulk makes it easy to import and transform any CSV into a smart spreadsheet interface. This user-friendly interface highlights errors in a clear, concise manner, simplifying the task of cleaning data.\n\n[Spreadsheet view](https://preview.redd.it/mifmavo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8cfd6f44d3372f019e550c0547d3d7d11c0361d2)\n\n\ud83d\ude80 **Bring your validation function:** YoBulk offers a platform for Developers to create a custom CSV importer that includes personalized **validation rules based on JSON schema.** With this functionality, developers can design an importer that meets their specific needs and preferences.\n\n[JSON Validator](https://preview.redd.it/42dh3wo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e0b34c17390cc1b4fc9fe6124608f2ad1c035d9b)\n\n\ud83d\ude80 **AI first:** YoBulk harnesses the power of OpenAI to provide advanced column matching, data cleaning, and JSON schema generation features.\n\n[Power of AI](https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=cd76a30304fe90365aa0c6b3fde38c1abb40f592)\n\n\ud83d\ude80 **Build for Scale:** YoBulk is designed for large-scale CSV validation, with the ability to process files in the gigabyte range without any glitches or errors.\n\n&amp;#x200B;\n\n\ud83d\ude80 **Embeddable:** Take advantage of YoBulk's customizable import button feature, which can be embedded in any SaaS or App. This allows you to receive CSV data in the exact format you require, streamlining your workflows.\n\n[Template builder](https://preview.redd.it/8bexoro5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0ce1e1b95cd5374551b110962b4e58d98c5636f3)\n\n[Embed with ease](https://preview.redd.it/he2d1vo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ee65ea7b06c5e59c751168980096cf7fa650ba16)\n\n&amp;#x200B;\n\n# Quick Demo\n\n[String validation](https://reddit.com/link/118ait5/video/d9xo4n5frkja1/player)\n\n[GPT](https://reddit.com/link/118ait5/video/xwtzbi5frkja1/player)\n\n&amp;#x200B;\n\n**Hosting and Deployment:**\n\nYoBulk can be self-hosted and currently running on Mongo.\n\nGithub: git clone [git@github.com](mailto:git@github.com):yobulkdev/yobulkdev.git\n\n**Getting started is really simple :**\n\nPlease refer [https://doc.yobulk.dev/GetStarted/Installation](https://doc.yobulk.dev/GetStarted/Installation)\n\nDocker command:\n\n&gt;git clone [https://github.com/yobulkdev/yobulkdev.git](https://github.com/yobulkdev/yobulkdev.git)  \ncd yobulkdev  \ndocker-compose up -d\n\nOr\n\n&gt;docker run --rm -it -p 5050:5050/tcp yobulk/yobulk\n\nOr\n\n&gt;git clone [https://github.com/yobulkdev/yobulkdev](https://github.com/yobulkdev/yobulkdev)  \ncd yobulkdev  \nyarn install  \nyarn run dev\n\n&amp;#x200B;\n\n**Also please join our community at :**\n\n**\ud83d\udce3** Github: [https://github.com/yobulkdev/yobulkdev](https://github.com/yobulkdev/yobulkdev)\n\n**\ud83d\udce3** Slack: [https://join.slack.com/t/yobulkdev/signup](https://join.slack.com/t/yobulkdev/signup).\n\n**\ud83d\udce3** Twitter: [https://twitter.com/YoBulkDev](https://twitter.com/YoBulkDev)\n\n**\ud83d\udce3** Reditt: [https://reddit.com/r/YoBulk](https://reddit.com/r/YoBulk)\n\nWould love to hear your feedback &amp; how we can make this better.\n\nThank you,\n\n**Team YoBulk**", "author_fullname": "t2_e5nsso8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YoBulk: Open Source CSV importer powered by GPT3 (Free flatfile.com alternative)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zmrmpuo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ada2ab8b2e6d085ababc8828403e887bb18bae7b"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f3b690eea82cabdbb5d794c2793a8b7fb3dfa61"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e94190fc78c28583766c6584d3372ad7b03ec34"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=309adf6955fbe3ace7552c99f4b9a613ccbd4e3b"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2953550dcbe89fa6e27636983f3f8fb3142f7d4"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47850b45caf01a142637bd9ad00350f3b37c017b"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=cd76a30304fe90365aa0c6b3fde38c1abb40f592"}, "id": "zmrmpuo5rkja1"}, "d9xo4n5frkja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/118ait5/asset/d9xo4n5frkja1/DASHPlaylist.mpd?a=1679668744%2CY2UxMGMyYTY4ODk2YTgwM2M4ODk4YzhiNDRlYWFmNmUzMmQ3MGI2MWM5NGU2MDE5ZjAzNWEwNTdkZTNmMWEwNg%3D%3D&amp;v=1&amp;f=sd", "x": 1728, "y": 1080, "hlsUrl": "https://v.redd.it/link/118ait5/asset/d9xo4n5frkja1/HLSPlaylist.m3u8?a=1679668744%2CNDNhZjg1M2FiY2VlOTBmMGVlYzliNjI1NGE3NmY1OWU4M2Q5Zjg4N2E3NWRlNGI1Zjg0YTBmMjViNjYwMWQ1Nw%3D%3D&amp;v=1&amp;f=sd", "id": "d9xo4n5frkja1", "isGif": false}, "42dh3wo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0af9167ec55bf0baad399a747a0d465d8283115a"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e60d32fa66e854044bfff093f9731809a22c30d4"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38a91c124a622d0e4621247d298a7ba0dee8cf85"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c33b70f2d81dd3c8cbbaf26312a96a9117299132"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0b501331d1219e12ae1920b9e21757d2ea0f895"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd0d3e37b6f67186d1083a0c452276007b059408"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/42dh3wo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e0b34c17390cc1b4fc9fe6124608f2ad1c035d9b"}, "id": "42dh3wo5rkja1"}, "xwtzbi5frkja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/118ait5/asset/xwtzbi5frkja1/DASHPlaylist.mpd?a=1679668744%2CMmNmNzZiNzc5NjUxZTIxNmJjYTFlNjU5ZWVjMDVkZWFhYWM1NjNlOTM0N2VlMTM3NWI2MzAxMmY0NTAyNjc5Mw%3D%3D&amp;v=1&amp;f=sd", "x": 1728, "y": 1080, "hlsUrl": "https://v.redd.it/link/118ait5/asset/xwtzbi5frkja1/HLSPlaylist.m3u8?a=1679668744%2CNDM2MzI2NDE3ZDljZTU5OTkyZDM5NWY5NmMzMzViM2EyMTE1MzA4YmNiNThkY2Q3MjNlNTZhYjk4MDRhZjc0Zg%3D%3D&amp;v=1&amp;f=sd", "id": "xwtzbi5frkja1", "isGif": false}, "8bexoro5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6583ec18f72a4fe01169498cd426e662cc8adb9"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a39e8977e7e8408d6ba7ab9da312df2e3f443bc1"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5e23fc922a349c4056fc34cbdd3d0ff36f3abea"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5417eba5b12dd839173efc659626be255bb19fc"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5b41dee24fae39aa49388080ad10ee9dfefd351"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95b0bb98d03d577a3c547768c0cc5c821ee99753"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/8bexoro5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0ce1e1b95cd5374551b110962b4e58d98c5636f3"}, "id": "8bexoro5rkja1"}, "he2d1vo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58ef5edbd3e2b87ed54fec3db7939037be4e1fc3"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb96c9e9322a19b2852b0d8ff5e00118f9fb7ddf"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f79ec7aa6f0ab4cec02043c2bbfb66c2ac110c0"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d94cea3b2010a42479a7d7190c1a465407cb936"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=100c7fce3e59826e596e38f99c01b04f84c2aee2"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac6b580c10b22f983a865565d6c285d79091b155"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/he2d1vo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ee65ea7b06c5e59c751168980096cf7fa650ba16"}, "id": "he2d1vo5rkja1"}, "mifmavo5rkja1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb00214ed2915a5c84e002b77d5b3871a6438317"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=165de6dbe0e08cef82fe0e8d7acd918826a75924"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5afe85e8962c35347ba8b088f5b33228a94356e7"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d855503bd8046c45520eeed710bbd1a73a5f647f"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6045f17645f8d666c44166a0224f204265c9657"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3585ce13cc1c47d7ae0ddf9b3180c10141f17821"}], "s": {"y": 720, "x": 1280, "u": "https://preview.redd.it/mifmavo5rkja1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8cfd6f44d3372f019e550c0547d3d7d11c0361d2"}, "id": "mifmavo5rkja1"}}, "name": "t3_118ait5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/k50OptU2Vj9lv7XzmTuRSwWc6hnyQKNXFIhjf2uGr2U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1676999778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everybody, \ud83d\udc4b\ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;We are really excited to open source YoBulk today!&lt;/p&gt;\n\n&lt;p&gt;YoBulk is an &lt;strong&gt;open-source CSV importer&lt;/strong&gt; for any SaaS application - It&amp;#39;s a free alternative to &lt;a href=\"https://flatfile.com/\"&gt;https://flatfile.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We realized that more than 70% of business data is shared in CSV and Excel formats, and only a small percentage use API integrations for data exchange. As developers and product managers, we have experienced the difficulties of building a scalable CSV importer, and we know that many others face the same challenges. Our goal is to solve this problem by taking an open-source AI and developer-centric approach&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Who can use YoBulk:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;YoBulk is a highly beneficial tool for a variety of professionals, such as Developers, Product Managers, Customer Success teams, and Marketers. It simplifies the process of onboarding and verifying customer data, making it an indispensable asset for those who deal with frequent CSV data uploads to a system with a predetermined schema or template.&lt;/p&gt;\n\n&lt;p&gt;This tool is particularly valuable for updating sales CRM or product catalog data, and it effectively solves the initial challenge of customer data ingestion.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;The Problem:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Importing a CSV is a really hard problem to solve. Some of the key problems are:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Missing Collaboration and Automation in CSV importing workflow:&lt;/strong&gt; In a usual situation, the customer success team responsible for receiving CSV data has to engage in extensive back-and-forth communication with the customer to address unintentional manual errors present in a CSV.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Scale:&lt;/strong&gt; CRM CSV files can sometimes reach sizes as large as 4 GB, making it nearly impossible to open them on a standalone machine for data correction. This presents a significant challenge for small businesses that cannot afford to invest in big data technologies such as EMR, Databrick, and ETL tools to address CSV import scaling problems.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Countless complex validation Types:&lt;/strong&gt; single date format can have as many as 100 different variations, such as dd-mm-yyyy, mm-dd-yyyy, and dd.mm.yyyy. Manually setting validation rules for each of these formats is almost impossible, and correcting errors manually will be difficult.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Data mapping issues&lt;/strong&gt;: In a typical scenario, the recipient of CSV data provides a template to the data donor and creates a CSV column for template mapping before importing&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Data Security and Privacy&lt;/strong&gt;: It is always risky to share your customer data with third-party companies for data cleaning purposes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6. Non-availability of low code/No code tool:&lt;/strong&gt; Product managers and customer success teams, who are typically no-code users, often rely on data analysts to create a programmed CSV template with validation rules, which must be shared with customers to receive CSV data in a specific format.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; &lt;strong&gt;Vague error messages:&lt;/strong&gt; Unclear error messages do not provide users with enough context to confidently resolve their issues before uploading their data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;How YoBulk helps address the above issues :&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Smart Spreadsheet View:&lt;/strong&gt; Designed to be a data exchange hub for any business that utilizes CSV files, YoBulk makes it easy to import and transform any CSV into a smart spreadsheet interface. This user-friendly interface highlights errors in a clear, concise manner, simplifying the task of cleaning data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mifmavo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8cfd6f44d3372f019e550c0547d3d7d11c0361d2\"&gt;Spreadsheet view&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Bring your validation function:&lt;/strong&gt; YoBulk offers a platform for Developers to create a custom CSV importer that includes personalized &lt;strong&gt;validation rules based on JSON schema.&lt;/strong&gt; With this functionality, developers can design an importer that meets their specific needs and preferences.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/42dh3wo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e0b34c17390cc1b4fc9fe6124608f2ad1c035d9b\"&gt;JSON Validator&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;AI first:&lt;/strong&gt; YoBulk harnesses the power of OpenAI to provide advanced column matching, data cleaning, and JSON schema generation features.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zmrmpuo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cd76a30304fe90365aa0c6b3fde38c1abb40f592\"&gt;Power of AI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Build for Scale:&lt;/strong&gt; YoBulk is designed for large-scale CSV validation, with the ability to process files in the gigabyte range without any glitches or errors.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 &lt;strong&gt;Embeddable:&lt;/strong&gt; Take advantage of YoBulk&amp;#39;s customizable import button feature, which can be embedded in any SaaS or App. This allows you to receive CSV data in the exact format you require, streamlining your workflows.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8bexoro5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0ce1e1b95cd5374551b110962b4e58d98c5636f3\"&gt;Template builder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/he2d1vo5rkja1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ee65ea7b06c5e59c751168980096cf7fa650ba16\"&gt;Embed with ease&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;Quick Demo&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/118ait5/video/d9xo4n5frkja1/player\"&gt;String validation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/118ait5/video/xwtzbi5frkja1/player\"&gt;GPT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hosting and Deployment:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;YoBulk can be self-hosted and currently running on Mongo.&lt;/p&gt;\n\n&lt;p&gt;Github: git clone [&lt;a href=\"mailto:git@github.com\"&gt;git@github.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:git@github.com\"&gt;git@github.com&lt;/a&gt;):yobulkdev/yobulkdev.git&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Getting started is really simple :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Please refer &lt;a href=\"https://doc.yobulk.dev/GetStarted/Installation\"&gt;https://doc.yobulk.dev/GetStarted/Installation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Docker command:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;git clone &lt;a href=\"https://github.com/yobulkdev/yobulkdev.git\"&gt;https://github.com/yobulkdev/yobulkdev.git&lt;/a&gt;&lt;br/&gt;\ncd yobulkdev&lt;br/&gt;\ndocker-compose up -d&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;docker run --rm -it -p 5050:5050/tcp yobulk/yobulk&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;git clone &lt;a href=\"https://github.com/yobulkdev/yobulkdev\"&gt;https://github.com/yobulkdev/yobulkdev&lt;/a&gt;&lt;br/&gt;\ncd yobulkdev&lt;br/&gt;\nyarn install&lt;br/&gt;\nyarn run dev&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Also please join our community at :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Github: &lt;a href=\"https://github.com/yobulkdev/yobulkdev\"&gt;https://github.com/yobulkdev/yobulkdev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Slack: &lt;a href=\"https://join.slack.com/t/yobulkdev/signup\"&gt;https://join.slack.com/t/yobulkdev/signup&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Twitter: &lt;a href=\"https://twitter.com/YoBulkDev\"&gt;https://twitter.com/YoBulkDev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\ud83d\udce3&lt;/strong&gt; Reditt: &lt;a href=\"https://reddit.com/r/YoBulk\"&gt;https://reddit.com/r/YoBulk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your feedback &amp;amp; how we can make this better.&lt;/p&gt;\n\n&lt;p&gt;Thank you,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Team YoBulk&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?auto=webp&amp;v=enabled&amp;s=37744d5163d376765ca256576a6e8dc8b5b5b0a3", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a265b5f1ef8b75d86212284540a796f627e0cb7e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be2a8811203462e9b854ea10eae0c3386197c6e0", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c303167c7987abf079d24bb474f344e98b9dbc1", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=105c05bb236804bed5cc2c8679ac7a623e51ce9b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e10c99c20fdabf0612f26c5d81a3f253f32e0ba", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/lwWxafYkKFdu37BLNfyAVX28WpRMo6w1s9VhoeDa_Iw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4675912b97773b7899a78db7d1119eabc9ac6e1b", "width": 1080, "height": 564}], "variants": {}, "id": "Ok4CUIAgW7uB_hl0dnfVegWrTM9wJo9ASZ1FvnRrJE8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "118ait5", "is_robot_indexable": true, "report_reasons": null, "author": "dstala", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ait5/yobulk_open_source_csv_importer_powered_by_gpt3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118ait5/yobulk_open_source_csv_importer_powered_by_gpt3/", "subreddit_subscribers": 90540, "created_utc": 1676999778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am currently in the process of designing a new ETL framework from scratch using Apache Airflow. For the purpose of this post, I will only talk about a specific type of process which does extract. In general the sequence is:\n\n1. Execute Query against MySQL Database\n2. Save Query Results to S3\n3. Drop and Create Athena Table, Perform Maintenance on Table\n\nThis sequence would have to be repeated for X extract jobs with different queries. The idea is to have several of these sequences running in parallel.\n\n**Option 1**: Design a DAG template which performs the sequences where each step is a different task. Have one centralized DAG which calls each DAG in parallel and manages the overall orchestration of the other dags.\n\n**Option 2**: Design a DAG generator which places all processes in a single DAG, together with Operators for parallelizing the different tasks, resulting in one big DAG.\n\nI am currently not sure which option to go for, and I was wondering if the community has any ideas. Feel free to offer pros/cons, suggestions or alternatives.", "author_fullname": "t2_j3gqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Discussion: Several DAGs vs Several Tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11895rk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676997699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently in the process of designing a new ETL framework from scratch using Apache Airflow. For the purpose of this post, I will only talk about a specific type of process which does extract. In general the sequence is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Execute Query against MySQL Database&lt;/li&gt;\n&lt;li&gt;Save Query Results to S3&lt;/li&gt;\n&lt;li&gt;Drop and Create Athena Table, Perform Maintenance on Table&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This sequence would have to be repeated for X extract jobs with different queries. The idea is to have several of these sequences running in parallel.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;: Design a DAG template which performs the sequences where each step is a different task. Have one centralized DAG which calls each DAG in parallel and manages the overall orchestration of the other dags.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt;: Design a DAG generator which places all processes in a single DAG, together with Operators for parallelizing the different tasks, resulting in one big DAG.&lt;/p&gt;\n\n&lt;p&gt;I am currently not sure which option to go for, and I was wondering if the community has any ideas. Feel free to offer pros/cons, suggestions or alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11895rk", "is_robot_indexable": true, "report_reasons": null, "author": "exact-approximate", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11895rk/airflow_discussion_several_dags_vs_several_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11895rk/airflow_discussion_several_dags_vs_several_tasks/", "subreddit_subscribers": 90540, "created_utc": 1676997699.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}