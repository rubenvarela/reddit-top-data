{"kind": "Listing", "data": {"after": "t3_119weiv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "dbt is getting pretty popular recently, but is it really that \u201cnecessary\u201d? I mean what are the added benefit of introducing new tool when you can do all transformations using python (polars, duckDB\u2026) + in python you can also do the \u201cextract\u201d step so basically you are able to cover entire ETL lifecycle with one tool? Also you can unit test your code better. As python disadvantage I see the dependency management. The only advantage of dbt I can see is you do not have to explicitly create tables as it creates it for you.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt really necessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119s7yv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677139328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;dbt is getting pretty popular recently, but is it really that \u201cnecessary\u201d? I mean what are the added benefit of introducing new tool when you can do all transformations using python (polars, duckDB\u2026) + in python you can also do the \u201cextract\u201d step so basically you are able to cover entire ETL lifecycle with one tool? Also you can unit test your code better. As python disadvantage I see the dependency management. The only advantage of dbt I can see is you do not have to explicitly create tables as it creates it for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119s7yv", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119s7yv/is_dbt_really_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119s7yv/is_dbt_really_necessary/", "subreddit_subscribers": 90853, "created_utc": 1677139328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))\n\nI love VS Code and other local IDEs, but they don\u2019t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick [demo](https://www.loom.com/share/8db10268612d4769893123b00500ad35) below).\n\nNext, we\u2019re planning to explore column-level lineage and code/yaml autocomplete using AI. I\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!\n\n[https://www.loom.com/share/8db10268612d4769893123b00500ad35](https://www.loom.com/share/8db10268612d4769893123b00500ad35)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a better local dbt experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119oxil", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677130632.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677128125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable (&lt;a href=\"https://www.turntable.so/\"&gt;https://www.turntable.so/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;I love VS Code and other local IDEs, but they don\u2019t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick &lt;a href=\"https://www.loom.com/share/8db10268612d4769893123b00500ad35\"&gt;demo&lt;/a&gt; below).&lt;/p&gt;\n\n&lt;p&gt;Next, we\u2019re planning to explore column-level lineage and code/yaml autocomplete using AI. I\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.loom.com/share/8db10268612d4769893123b00500ad35\"&gt;https://www.loom.com/share/8db10268612d4769893123b00500ad35&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "119oxil", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/", "subreddit_subscribers": 90853, "created_utc": 1677128125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_em2yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_119g5nc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QZH3QHnUFoosVA85lVKFbP13HNR91vebAHaKr_M4qCU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677105257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "rcpassos.me", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://rcpassos.me/post/apache-arrow-future-of-data-engineering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?auto=webp&amp;v=enabled&amp;s=0dbc90e7190608d0a5c7f3b79b3cc14dcf40e005", "width": 2048, "height": 1170}, "resolutions": [{"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9346544144f2d70bd9adf0c2d91e5037f1e1bb50", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45759a1b653546d00c5c5eace4bbf344355bcc14", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c4e1c29f639231505e892e2d07b2e35afb9b80f", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58b9cc2cf6daa644cca89de9c250f43bb88b6b73", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143764eb76d7d1dcd28d1a82935d3f9ffdc93209", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d6dcdb61c28ea1a4275ccc32506478a736e4833", "width": 1080, "height": 616}], "variants": {}, "id": "aaEDVnIXM_-RMbLWzDpeO6CMM6TnSdzuUj2_DeBa8lk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "119g5nc", "is_robot_indexable": true, "report_reasons": null, "author": "auyer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119g5nc/how_i_decreased_etl_cost_by_leveraging_the_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://rcpassos.me/post/apache-arrow-future-of-data-engineering", "subreddit_subscribers": 90853, "created_utc": 1677105257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pandas 2.0 and the Arrow revolution (part I)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119ig9h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1677110639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datapythonista.me", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "119ig9h", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119ig9h/pandas_20_and_the_arrow_revolution_part_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i", "subreddit_subscribers": 90853, "created_utc": 1677110639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently researching Unity Catalog for my organization. We have 3 storage account data lakes, one for dev test and prod. We currently mount the appropriate account to our Databricks workspace and so all the paths to tables are constant in our code, and we don\u2019t have to specify which to use when our pipelines run. \n\nWith the switch to Unity Catalog, I am wondering how people specify which catalog to use? Do you have a variable that gets set in the the release? Do you specify a \u201cuse catalog name\u201d somewhere before running a pipeline? Any advice is appreciated!", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unity Catalog and CI/CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119et7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677102545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently researching Unity Catalog for my organization. We have 3 storage account data lakes, one for dev test and prod. We currently mount the appropriate account to our Databricks workspace and so all the paths to tables are constant in our code, and we don\u2019t have to specify which to use when our pipelines run. &lt;/p&gt;\n\n&lt;p&gt;With the switch to Unity Catalog, I am wondering how people specify which catalog to use? Do you have a variable that gets set in the the release? Do you specify a \u201cuse catalog name\u201d somewhere before running a pipeline? Any advice is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119et7b", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119et7b/unity_catalog_and_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119et7b/unity_catalog_and_cicd/", "subreddit_subscribers": 90853, "created_utc": 1677102545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just joined a small B2C company that produces written content similar to spark notes. They have no data warehouse and is using Postgres for housing content to deliver via their website. Separately they also have data scientists scraping web data to find which books are popular.\n\nI need to setup a data warehouse and a database for the company. The first purpose would be to automate the scraping process - it is currently manual and the datasets are stored on someone\u2019s computer lol.\n\nI\u2019ll considering using all of Google Cloud\u2019s suite of tools. What are the risks here? Google Cloud seems to have everything - BigQuery as the warehouse, SQL database, data studio as BI tool, and Cloud Run to automate python scrapers.\n\nWhat are the pros and cons of using every tool from one provider (aka Google here)? \n\nFor context, I am more familiar with Snowflake and Looker setup from my last job.", "author_fullname": "t2_gfy8k6mh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting DataOps function from Scratch at small B2C company.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119emfw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677102222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just joined a small B2C company that produces written content similar to spark notes. They have no data warehouse and is using Postgres for housing content to deliver via their website. Separately they also have data scientists scraping web data to find which books are popular.&lt;/p&gt;\n\n&lt;p&gt;I need to setup a data warehouse and a database for the company. The first purpose would be to automate the scraping process - it is currently manual and the datasets are stored on someone\u2019s computer lol.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll considering using all of Google Cloud\u2019s suite of tools. What are the risks here? Google Cloud seems to have everything - BigQuery as the warehouse, SQL database, data studio as BI tool, and Cloud Run to automate python scrapers.&lt;/p&gt;\n\n&lt;p&gt;What are the pros and cons of using every tool from one provider (aka Google here)? &lt;/p&gt;\n\n&lt;p&gt;For context, I am more familiar with Snowflake and Looker setup from my last job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119emfw", "is_robot_indexable": true, "report_reasons": null, "author": "FivePointyChickens", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119emfw/starting_dataops_function_from_scratch_at_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119emfw/starting_dataops_function_from_scratch_at_small/", "subreddit_subscribers": 90853, "created_utc": 1677102222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently joined a company who is doing the majority of data transformation in lookml.  Seems like there are 2 reasons for this from what I'm told:  \n\n1. Every end user wants a different level of aggregation and that would require creating duplicative tables in Snowflake (think something like marketing wants product level, finance wants order level, but user specific not business specific)\n\n2. If we change the data structure of a table that would require a lot of maintenance in Snowflake \n\n\nI'm very self taught and still very much a beginner so I'm looking for a sanity check. My thought is having a solid groundwork laid out in Snowflake is going to solve the majority of those concerns. Define the aggregate levels by the business needs and if a single user has a specific request evaluate the business need for it. That way the data warehouse is our source of truth.\n\n\nIdeally, what is the right way to use Looker?", "author_fullname": "t2_2q171de9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does Looker fit into the data stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119ailf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677096297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently joined a company who is doing the majority of data transformation in lookml.  Seems like there are 2 reasons for this from what I&amp;#39;m told:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Every end user wants a different level of aggregation and that would require creating duplicative tables in Snowflake (think something like marketing wants product level, finance wants order level, but user specific not business specific)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If we change the data structure of a table that would require a lot of maintenance in Snowflake &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m very self taught and still very much a beginner so I&amp;#39;m looking for a sanity check. My thought is having a solid groundwork laid out in Snowflake is going to solve the majority of those concerns. Define the aggregate levels by the business needs and if a single user has a specific request evaluate the business need for it. That way the data warehouse is our source of truth.&lt;/p&gt;\n\n&lt;p&gt;Ideally, what is the right way to use Looker?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119ailf", "is_robot_indexable": true, "report_reasons": null, "author": "lahma_mama", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119ailf/where_does_looker_fit_into_the_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119ailf/where_does_looker_fit_into_the_data_stack/", "subreddit_subscribers": 90853, "created_utc": 1677096297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create \\*actual\\* streaming pipelines. \n\nBut no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)\n\nAs such, we\u2019d love to have both your **warm** and **snarky** feedback on our baby (beta platform).\n\nThe most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)\n\n**The Pitch:**  \nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. \n\nA free account up to 25gb/mo in data movement can be had here: [www.estuary.dev](https://www.estuary.dev/?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=reddit_feedback&amp;utm_id=18681982783)\n\n**The Details:**\n\nEstuary Flow is built on top of an open-source streaming framework ([Gazette](http://gazette.dev/)) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.\n\nBeyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:\n\n**\\*Collections instead of Buffers.** When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history *and* ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.\n\n**\\*Continuous Views instead of Sinks.** Materialized views update *in-place.* Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make *any* database a \u201creal time\u201d database.  \n\n\n**\\*Completely Incremental, Exactly-Once.** Flow uses a continuous processing model, which propagates transactional data *changes* through your processing graph. This helps keep costs low while maintaining exact copies across different systems.\n\n\\***Turnkey batch and streaming connectors.** Both real-time data as well as historical data supported through one tool and access to pre-built connectors to \\~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.\n\n**\\*Transformations.** We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.\n\n**Managed CDC.**  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  \n\nWe have thick skin and welcome all feedback on our newborn.\n\nSo thick a phlebotomist uses a hammer and nail to take our blood :)  \nBut we also love hugs if that is what you have for us!  \n\n\n[a quick video of our baby, Fivka \\(Estuary Flow\\)](https://reddit.com/link/11a3vga/video/t566u66qlyja1/player)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If Fivetran and Kafka had a baby...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t566u66qlyja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/DASHPlaylist.mpd?a=1679776796%2CMWM0NjhlNmVmMTNhYTRlNTgwYmVmY2ViYTI2YzE3MjE1MDI4Y2E4OTU5MzJiYTIxOTk4ZTMwOWU4NjQ3YWMzNg%3D%3D&amp;v=1&amp;f=sd", "x": 1080, "y": 720, "hlsUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/HLSPlaylist.m3u8?a=1679776796%2CMzBlZmRjMDBkYmM4MTZmZWVkNTY1ZmZjZThjODkwZmRlM2JlMTZlYmQwOWNhOTM4NzdhNTA3MWQwNjc0MTUzZQ%3D%3D&amp;v=1&amp;f=sd", "id": "t566u66qlyja1", "isGif": false}}, "name": "t3_11a3vga", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hcqEDMEJ6eCMTtHlsTHEcAGVk3iLOY4l6Pt3zg-23AQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677174561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create *actual* streaming pipelines. &lt;/p&gt;\n\n&lt;p&gt;But no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)&lt;/p&gt;\n\n&lt;p&gt;As such, we\u2019d love to have both your &lt;strong&gt;warm&lt;/strong&gt; and &lt;strong&gt;snarky&lt;/strong&gt; feedback on our baby (beta platform).&lt;/p&gt;\n\n&lt;p&gt;The most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Pitch:&lt;/strong&gt;&lt;br/&gt;\nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. &lt;/p&gt;\n\n&lt;p&gt;A free account up to 25gb/mo in data movement can be had here: &lt;a href=\"https://www.estuary.dev/?utm_source=social&amp;amp;utm_medium=reddit&amp;amp;utm_campaign=reddit_feedback&amp;amp;utm_id=18681982783\"&gt;www.estuary.dev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Estuary Flow is built on top of an open-source streaming framework (&lt;a href=\"http://gazette.dev/\"&gt;Gazette&lt;/a&gt;) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.&lt;/p&gt;\n\n&lt;p&gt;Beyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Collections instead of Buffers.&lt;/strong&gt; When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history &lt;em&gt;and&lt;/em&gt; ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Continuous Views instead of Sinks.&lt;/strong&gt; Materialized views update &lt;em&gt;in-place.&lt;/em&gt; Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make &lt;em&gt;any&lt;/em&gt; database a \u201creal time\u201d database.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Completely Incremental, Exactly-Once.&lt;/strong&gt; Flow uses a continuous processing model, which propagates transactional data &lt;em&gt;changes&lt;/em&gt; through your processing graph. This helps keep costs low while maintaining exact copies across different systems.&lt;/p&gt;\n\n&lt;p&gt;*&lt;strong&gt;Turnkey batch and streaming connectors.&lt;/strong&gt; Both real-time data as well as historical data supported through one tool and access to pre-built connectors to ~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Transformations.&lt;/strong&gt; We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed CDC.&lt;/strong&gt;  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  &lt;/p&gt;\n\n&lt;p&gt;We have thick skin and welcome all feedback on our newborn.&lt;/p&gt;\n\n&lt;p&gt;So thick a phlebotomist uses a hammer and nail to take our blood :)&lt;br/&gt;\nBut we also love hugs if that is what you have for us!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/11a3vga/video/t566u66qlyja1/player\"&gt;a quick video of our baby, Fivka (Estuary Flow)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?auto=webp&amp;v=enabled&amp;s=70ac2549cbcee50babf14c4348696590af422bb9", "width": 1024, "height": 542}, "resolutions": [{"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d4aa52afef2755d5d67cba98872d25b5fa321a6", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22d039df909086dd8040909466aec6b242932759", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67d17a7aa8da8d0faa193b11b00fce364340cf6d", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7effc9be0656e11886240c621f136698e2b36fda", "width": 640, "height": 338}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=237b5d6191866d30915c9b83efa0184390c18c6f", "width": 960, "height": 508}], "variants": {}, "id": "gYr1fOXcV-SPELTio4np6ONxmyr0lk2IGzDEuzijt1A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11a3vga", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "subreddit_subscribers": 90853, "created_utc": 1677174561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have spend the better part of the past two weeks reading about data engineering (and MLOps). The space seems to be dominated by Python-based workflows and tools like Apache Airflow, Dagster, MLFlow etc", "author_fullname": "t2_srfmey5u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should data engineering be done in a non-Python organization that is dominated by C#/.Net developers and data science people who prefer R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119hqbf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677108845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have spend the better part of the past two weeks reading about data engineering (and MLOps). The space seems to be dominated by Python-based workflows and tools like Apache Airflow, Dagster, MLFlow etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119hqbf", "is_robot_indexable": true, "report_reasons": null, "author": "gheex", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119hqbf/how_should_data_engineering_be_done_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119hqbf/how_should_data_engineering_be_done_in_a/", "subreddit_subscribers": 90853, "created_utc": 1677108845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A day in the life of centralized IT...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11a3ecv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PGEguyXn6VqV0TOrBaPj1_BRDZvB_Ym2V_GhwQ2t8CQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677173420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1puqfv734zja1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1puqfv734zja1.jpg?auto=webp&amp;v=enabled&amp;s=de267645d8420ae9b14da400b8d3bd42fc98c44d", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8163c8aeb9edc556e46b2977b7e05a4f8053618", "width": 108, "height": 108}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93991da2ca07bfdcd3ced3ec2e13f072a4e1214a", "width": 216, "height": 216}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d8390e6ca8bd81cee8512c97f5479f6692d185", "width": 320, "height": 320}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e45703f97977f05a0ddf779a9785e13fc440726", "width": 640, "height": 640}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=568f2a2dcd0cf1d4df6215cab06a129ea7c3aa1f", "width": 960, "height": 960}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ab649d8539e8ffda47f1ff8dfd94b3468b3f657", "width": 1080, "height": 1080}], "variants": {}, "id": "p1GSJSfAihfxQGzlZqxswANtTs13sdu2gw0ZCmq9rDg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11a3ecv", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3ecv/a_day_in_the_life_of_centralized_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1puqfv734zja1.jpg", "subreddit_subscribers": 90853, "created_utc": 1677173420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table with complex type such as array of dictionaries . I would need to flatten the table  in such a way that every dictionary item value in a array shows as a separate column. How do I achieve this in spark or hive without exploding(and thereby increasing the rows) to multiple rows.\n\nTldr: sample data is A,B,[dict(key1=\"Col3\",value1=\"col3_value\"),dict(key1=\"Col4\",value1=\"col4_value\")].\n\nFinal expected result:\nCol1,col2,col3,col4\nA,B,col3_value,col4_value", "author_fullname": "t2_9fhhwjm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to solve this with hive or spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119xrc4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677159046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table with complex type such as array of dictionaries . I would need to flatten the table  in such a way that every dictionary item value in a array shows as a separate column. How do I achieve this in spark or hive without exploding(and thereby increasing the rows) to multiple rows.&lt;/p&gt;\n\n&lt;p&gt;Tldr: sample data is A,B,[dict(key1=&amp;quot;Col3&amp;quot;,value1=&amp;quot;col3_value&amp;quot;),dict(key1=&amp;quot;Col4&amp;quot;,value1=&amp;quot;col4_value&amp;quot;)].&lt;/p&gt;\n\n&lt;p&gt;Final expected result:\nCol1,col2,col3,col4\nA,B,col3_value,col4_value&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119xrc4", "is_robot_indexable": true, "report_reasons": null, "author": "cieloskyg", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119xrc4/how_to_solve_this_with_hive_or_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119xrc4/how_to_solve_this_with_hive_or_spark/", "subreddit_subscribers": 90853, "created_utc": 1677159046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My new favorite website -https://status.snowflake.com :(", "author_fullname": "t2_6l3ghhxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new favorite website -&lt;a href=\"https://status.snowflake.com\"&gt;https://status.snowflake.com&lt;/a&gt; :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mhv", "is_robot_indexable": true, "report_reasons": null, "author": "MRWH35", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mhv/snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mhv/snowflake/", "subreddit_subscribers": 90853, "created_utc": 1677176324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\n I got might get a scholarship from [Turing College's Data Engineering course](https://www.turingcollege.com/data-engineering) \\- which comes with an obligation to accept a DE job from their partner companies. If no offer given within 1 month after finishing the course (around 8 months duration), you can apply to other companies as well. Fair, but though. I would need to accept a Data Engineering position with the pay levels they advertise (50k\u20ac), after graduation.\n\nWould you rather\u2026\n\n* ...stick to [self-curated curriculum](https://binchentso.notion.site/My-learning-path-516014e8d523457cbb277c9e2f00e409) aligned with a DE mentor, using the slack time at my current role to work on portfolio projects?\n* ...or follow up with the Turing College course, for 6\u20138 months and afterward have the chance to land a job with their hiring partners?", "author_fullname": "t2_9v9dakww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-curated curriculum vs. Turing College | Data Engineer in training", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a13ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677167900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I got might get a scholarship from &lt;a href=\"https://www.turingcollege.com/data-engineering\"&gt;Turing College&amp;#39;s Data Engineering course&lt;/a&gt; - which comes with an obligation to accept a DE job from their partner companies. If no offer given within 1 month after finishing the course (around 8 months duration), you can apply to other companies as well. Fair, but though. I would need to accept a Data Engineering position with the pay levels they advertise (50k\u20ac), after graduation.&lt;/p&gt;\n\n&lt;p&gt;Would you rather\u2026&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;...stick to &lt;a href=\"https://binchentso.notion.site/My-learning-path-516014e8d523457cbb277c9e2f00e409\"&gt;self-curated curriculum&lt;/a&gt; aligned with a DE mentor, using the slack time at my current role to work on portfolio projects?&lt;/li&gt;\n&lt;li&gt;...or follow up with the Turing College course, for 6\u20138 months and afterward have the chance to land a job with their hiring partners?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?auto=webp&amp;v=enabled&amp;s=df68ae1816dc7afdf03ee42537ebb0b71aacabcb", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bb2164dd8b8974933cbe4b231a8477df4f32097", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccd665a3aea84b67300a43e71c0a49fb8b49a715", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=beffcdca2b7995bee20a409769a64d46f43ff5ce", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11ea4960f04d10d36ba6bd388fada31e58ba1825", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31bfc7b9868fcd83a8c8798e05af9af474a2f682", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82c9ac111fe09001c54f00087a00654e84a211e8", "width": 1080, "height": 564}], "variants": {}, "id": "XqCbOvusBXQkcj77ZU8r51MvvlCHrNaKfTmnqOakYNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data &amp; Analytics Engineer in training", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11a13ge", "is_robot_indexable": true, "report_reasons": null, "author": "binchentso", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11a13ge/selfcurated_curriculum_vs_turing_college_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a13ge/selfcurated_curriculum_vs_turing_college_data/", "subreddit_subscribers": 90853, "created_utc": 1677167900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!   \n\n\nI work on a logistics startup that currently relies on Superset and Power BI as viz toolings. Superset works as self-service BI, while Power BI has an extra layer of validation by the BI team. \n\nI would like to know if you use tools like **Redash**, **Superset** or **Metabase**, what do you do to extract the most from them? Do you have a routine in place to keep the tools clean? Or somehow certify Dashboards official?", "author_fullname": "t2_ijp90vxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How your company uses Superset? Is it on a big scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119wc2e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677154698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!   &lt;/p&gt;\n\n&lt;p&gt;I work on a logistics startup that currently relies on Superset and Power BI as viz toolings. Superset works as self-service BI, while Power BI has an extra layer of validation by the BI team. &lt;/p&gt;\n\n&lt;p&gt;I would like to know if you use tools like &lt;strong&gt;Redash&lt;/strong&gt;, &lt;strong&gt;Superset&lt;/strong&gt; or &lt;strong&gt;Metabase&lt;/strong&gt;, what do you do to extract the most from them? Do you have a routine in place to keep the tools clean? Or somehow certify Dashboards official?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119wc2e", "is_robot_indexable": true, "report_reasons": null, "author": "CzarSantos98", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119wc2e/how_your_company_uses_superset_is_it_on_a_big/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119wc2e/how_your_company_uses_superset_is_it_on_a_big/", "subreddit_subscribers": 90853, "created_utc": 1677154698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I've been a DBA for about 8 years now (currently an MSSQL DBA), and I'm looking to learn more about data engineering and other, non-RDBMS systems. \n\nFor a first project, I want to do some basic extraction of data from various apis/web scraping, load it into a data warehouse (either directly into a traditional DW or use parquet files for a data lake). I put together the first steps of such on GCS using python scripts, duckdb, cloud storage and BigQuery. However, I'm not sure if given my background on the Microsoft side of things. This makes me think that I might want to focus on learning Azure, but looking over Synapse, I'm not sure how I feel about it, as it doesn't seem as simple as BigQuery did to me at first, as ADLS gen2 seems a bit odd, as my current parquet files aren't split down into many files, which seems to be the way to go for ADLS?\n\nI thought about using Databricks, but I'm hesitant, as I can't find much about how much it'll cost me to use as a small DW for learning, and I don't have a free tier to use anymore (used it a few years back for basic learning).\n\nI'm thinking of maybe trying out a different DW, something like ClickHouse, and running that on an Azure VM, but I'm not sure if doing that is a good idea for learning, since it isn't an \"official\" Azure tool. Also considered SingleStore, but a VM large enough to self host it might be too much (the docs recommend 4 CPUs/4GB RAM.\n\n&amp;#x200B;\n\nI guess to give TLDR, I currently work as a DBA in the Microsoft Stack, and when trying to figure out where to start learning DE, I feel like Azure is a natural starting point, but I'm not sure if I'm a fan of Synapse at first, compared to my first impressions of BigQuery.\n\nThanks!", "author_fullname": "t2_ssh4888", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What stack to focus on for learning for my background?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119qxna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677134613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;ve been a DBA for about 8 years now (currently an MSSQL DBA), and I&amp;#39;m looking to learn more about data engineering and other, non-RDBMS systems. &lt;/p&gt;\n\n&lt;p&gt;For a first project, I want to do some basic extraction of data from various apis/web scraping, load it into a data warehouse (either directly into a traditional DW or use parquet files for a data lake). I put together the first steps of such on GCS using python scripts, duckdb, cloud storage and BigQuery. However, I&amp;#39;m not sure if given my background on the Microsoft side of things. This makes me think that I might want to focus on learning Azure, but looking over Synapse, I&amp;#39;m not sure how I feel about it, as it doesn&amp;#39;t seem as simple as BigQuery did to me at first, as ADLS gen2 seems a bit odd, as my current parquet files aren&amp;#39;t split down into many files, which seems to be the way to go for ADLS?&lt;/p&gt;\n\n&lt;p&gt;I thought about using Databricks, but I&amp;#39;m hesitant, as I can&amp;#39;t find much about how much it&amp;#39;ll cost me to use as a small DW for learning, and I don&amp;#39;t have a free tier to use anymore (used it a few years back for basic learning).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of maybe trying out a different DW, something like ClickHouse, and running that on an Azure VM, but I&amp;#39;m not sure if doing that is a good idea for learning, since it isn&amp;#39;t an &amp;quot;official&amp;quot; Azure tool. Also considered SingleStore, but a VM large enough to self host it might be too much (the docs recommend 4 CPUs/4GB RAM.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I guess to give TLDR, I currently work as a DBA in the Microsoft Stack, and when trying to figure out where to start learning DE, I feel like Azure is a natural starting point, but I&amp;#39;m not sure if I&amp;#39;m a fan of Synapse at first, compared to my first impressions of BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119qxna", "is_robot_indexable": true, "report_reasons": null, "author": "dontmakemeplaypool", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119qxna/what_stack_to_focus_on_for_learning_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119qxna/what_stack_to_focus_on_for_learning_for_my/", "subreddit_subscribers": 90853, "created_utc": 1677134613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In VSCode,the colorizer feature,\n\n ([@id](https://github.com/id):editor.bracketPairColorization.enabled [@id](https://github.com/id):editor.guides.bracketPairs) \n\ndoes not work for brackets inside strings for ex: scores in (), greatest (), cast() etc. The example provided below is very basic, but we've run into nested functions that span &gt; 10 lines which makes debugging difficult. This would be very helpful to a lot of engineers who use sql/jinja. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2\n\nI created an issue for this feature on the official VSCode Github , \n\n[https://github.com/microsoft/vscode/issues/169649](https://github.com/microsoft/vscode/issues/169649)\n\n&amp;#x200B;\n\nIt requires 20 votes for it to move it to their backlog. If you find this would be helpful, please upvote it on github  \n(more on upvoting here: [https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request](https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request))", "author_fullname": "t2_5rikt61xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bracket Pair Colorizer for SQL - VSCode issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bdl9glzcyuja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=414f2f2e2a175ed9f5f488dac842aa05835d0df1"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=888f9acb467f86766ec4bd54d06812bff16cb0da"}, {"y": 221, "x": 320, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9eedc1f55b572f5dec02d33a88172d47fb407a5"}, {"y": 442, "x": 640, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0266046deeea13439e0e6d664f2fa31f9c00bcb0"}], "s": {"y": 507, "x": 733, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2"}, "id": "bdl9glzcyuja1"}}, "name": "t3_119na3a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b32GTmNvzTSoMHbew6yaoKUffq22YJ4yrtm6BLCeMHk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677123247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In VSCode,the colorizer feature,&lt;/p&gt;\n\n&lt;p&gt;(&lt;a href=\"https://github.com/id\"&gt;@id&lt;/a&gt;:editor.bracketPairColorization.enabled &lt;a href=\"https://github.com/id\"&gt;@id&lt;/a&gt;:editor.guides.bracketPairs) &lt;/p&gt;\n\n&lt;p&gt;does not work for brackets inside strings for ex: scores in (), greatest (), cast() etc. The example provided below is very basic, but we&amp;#39;ve run into nested functions that span &amp;gt; 10 lines which makes debugging difficult. This would be very helpful to a lot of engineers who use sql/jinja. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2\"&gt;https://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I created an issue for this feature on the official VSCode Github , &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/microsoft/vscode/issues/169649\"&gt;https://github.com/microsoft/vscode/issues/169649&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It requires 20 votes for it to move it to their backlog. If you find this would be helpful, please upvote it on github&lt;br/&gt;\n(more on upvoting here: &lt;a href=\"https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request\"&gt;https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?auto=webp&amp;v=enabled&amp;s=6dd45d060d304a95d84693ac89a3463494910530", "width": 336, "height": 336}, "resolutions": [{"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd83866d1749754918daa4b1d42d8da603983899", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0c884a660b38a41899fb3b5e3e3308d51e7e9ec", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f19d55525044730593b95c0af01f089482efa199", "width": 320, "height": 320}], "variants": {}, "id": "wWaXLNqFaTps7pGrJM9prEG2Htb0x9UlwSG11gTLPCc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "119na3a", "is_robot_indexable": true, "report_reasons": null, "author": "KangarOOCase", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119na3a/bracket_pair_colorizer_for_sql_vscode_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119na3a/bracket_pair_colorizer_for_sql_vscode_issue/", "subreddit_subscribers": 90853, "created_utc": 1677123247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently finished my internship as a juco student at an MNC and have tapped on a bit of data engineering work in terms of data wrangling, activation, preparation, migration. Not so much about building pipelines.\n\nPrior to that, I didn't have much experience and this internship opportunity has allowed me to discover an interest into this field. As I have a short break before the next steps into my career, I want to get more involved into the aspects of data engineering (build pipelines etc) and embark on some basic projects.\n\nI am mainly proficient in Python. \n\nWhat are the resources that I can seek for pertaining to data engineering?", "author_fullname": "t2_czigz4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner Side Project Ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119hdb5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677107994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently finished my internship as a juco student at an MNC and have tapped on a bit of data engineering work in terms of data wrangling, activation, preparation, migration. Not so much about building pipelines.&lt;/p&gt;\n\n&lt;p&gt;Prior to that, I didn&amp;#39;t have much experience and this internship opportunity has allowed me to discover an interest into this field. As I have a short break before the next steps into my career, I want to get more involved into the aspects of data engineering (build pipelines etc) and embark on some basic projects.&lt;/p&gt;\n\n&lt;p&gt;I am mainly proficient in Python. &lt;/p&gt;\n\n&lt;p&gt;What are the resources that I can seek for pertaining to data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119hdb5", "is_robot_indexable": true, "report_reasons": null, "author": "NotYule", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119hdb5/beginner_side_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119hdb5/beginner_side_project_ideas/", "subreddit_subscribers": 90853, "created_utc": 1677107994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some things that would make it better?  \nWhat would be nice to have? What's missing?", "author_fullname": "t2_8t7dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Those who use dbt Cloud...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11a588a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677177787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some things that would make it better?&lt;br/&gt;\nWhat would be nice to have? What&amp;#39;s missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a588a", "is_robot_indexable": true, "report_reasons": null, "author": "yummypoutine", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a588a/those_who_use_dbt_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a588a/those_who_use_dbt_cloud/", "subreddit_subscribers": 90853, "created_utc": 1677177787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?", "author_fullname": "t2_bigv1te1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way of merging tables in snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4zio", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677177188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4zio", "is_robot_indexable": true, "report_reasons": null, "author": "SD_strange", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "subreddit_subscribers": 90853, "created_utc": 1677177188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&gt;The **Bronze layer** is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n\n[Databricks Guide on Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n\n[View Poll](https://www.reddit.com/poll/11a0cmf)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you follow Medallion Architecture and utilize Bronze layer storage for ingested data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a0cmf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677166004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The &lt;strong&gt;Bronze layer&lt;/strong&gt; is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures &amp;quot;as-is,&amp;quot; along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/glossary/medallion-architecture\"&gt;Databricks Guide on Medallion Architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/11a0cmf\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a0cmf", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1677425204031, "options": [{"text": "Yes, but stored without schemas (JSON blobs of each row, CSVs, etc)", "id": "21744110"}, {"text": "Yes, but typed (Parquet/Avro/Table/etc)", "id": "21744111"}, {"text": "No", "id": "21744112"}, {"text": "Other (Add comment)", "id": "21744113"}, {"text": "See Results", "id": "21744114"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 36, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11a0cmf/do_you_follow_medallion_architecture_and_utilize/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/11a0cmf/do_you_follow_medallion_architecture_and_utilize/", "subreddit_subscribers": 90853, "created_utc": 1677166004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After having worked at different companies, at different sizes and needs. The larger companies seem to keep building pipeline, with lots of duplicated code, deployment scripts, versioning scripts, CI pipelines etc for each process in their business.\n\nThere is a common theme in places where I have worked, where people tend to think of things as either a batch or a stream. Now, given Kafka is embraced at your company and event streaming is implemented heavily. Why not just treat the batch data sources as a stream? By this I mean, just model the pipeline as a producer to a topic and then go on about your transformations. (Confluent's kSQL fb and Connectors help a lot with this).\n\nTime and time again, I keep seeing a lot of re inventing because of the decision to separate the two. It seems much easier, (perhaps with hindsight) to treat data as a first-class citizen and write less code to just produce and consume messages with the data itself, at a record level and use fan-out accordingly. This way the workflow or orchestration is embedded within the services and how they operate. This requires no need for DAGs or step function configurations. Granted this is usually best for one domain where it is appropriate.\n\nWhat is your take? There are of course edge cases and different approaches based on you business needs. But I think simply embracing the architecture of kafka and allowing services to easily plug in to a topic and do a certain transformation at the record level, makes things much easier to reason about.   \n\n\nFor example, SFTP files to an athena table can be simplified greatly.\n\nSpecifically from:\n\n\\-&gt; Poll (x source) (Python Application) -&gt; S3 -&gt; Spark (EMR) Hudi Table -&gt; Athena -&gt; Consumption\n\nExtra configuration (Check state of source, Metrics, Obersvability, Workflow Code (Airflow DAGs, Deployment Code etc)\n\nto:\n\n\\-&gt; Kafka Connector (source) -&gt; Topic -&gt; kSQL DB/Consumer -&gt; Sink\n\nPerhaps I am missing some details but for simpler integrations it seems easier to follow this logic. Where your data is pushed through and is masked (PII). Instead of pushing references to data and then orchestrating each workflow and for each case.", "author_fullname": "t2_7jhnfjx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on thinking of everything as a stream and not a batch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119xan6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After having worked at different companies, at different sizes and needs. The larger companies seem to keep building pipeline, with lots of duplicated code, deployment scripts, versioning scripts, CI pipelines etc for each process in their business.&lt;/p&gt;\n\n&lt;p&gt;There is a common theme in places where I have worked, where people tend to think of things as either a batch or a stream. Now, given Kafka is embraced at your company and event streaming is implemented heavily. Why not just treat the batch data sources as a stream? By this I mean, just model the pipeline as a producer to a topic and then go on about your transformations. (Confluent&amp;#39;s kSQL fb and Connectors help a lot with this).&lt;/p&gt;\n\n&lt;p&gt;Time and time again, I keep seeing a lot of re inventing because of the decision to separate the two. It seems much easier, (perhaps with hindsight) to treat data as a first-class citizen and write less code to just produce and consume messages with the data itself, at a record level and use fan-out accordingly. This way the workflow or orchestration is embedded within the services and how they operate. This requires no need for DAGs or step function configurations. Granted this is usually best for one domain where it is appropriate.&lt;/p&gt;\n\n&lt;p&gt;What is your take? There are of course edge cases and different approaches based on you business needs. But I think simply embracing the architecture of kafka and allowing services to easily plug in to a topic and do a certain transformation at the record level, makes things much easier to reason about.   &lt;/p&gt;\n\n&lt;p&gt;For example, SFTP files to an athena table can be simplified greatly.&lt;/p&gt;\n\n&lt;p&gt;Specifically from:&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; Poll (x source) (Python Application) -&amp;gt; S3 -&amp;gt; Spark (EMR) Hudi Table -&amp;gt; Athena -&amp;gt; Consumption&lt;/p&gt;\n\n&lt;p&gt;Extra configuration (Check state of source, Metrics, Obersvability, Workflow Code (Airflow DAGs, Deployment Code etc)&lt;/p&gt;\n\n&lt;p&gt;to:&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; Kafka Connector (source) -&amp;gt; Topic -&amp;gt; kSQL DB/Consumer -&amp;gt; Sink&lt;/p&gt;\n\n&lt;p&gt;Perhaps I am missing some details but for simpler integrations it seems easier to follow this logic. Where your data is pushed through and is masked (PII). Instead of pushing references to data and then orchestrating each workflow and for each case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119xan6", "is_robot_indexable": true, "report_reasons": null, "author": "the_real_tobo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119xan6/what_are_your_thoughts_on_thinking_of_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119xan6/what_are_your_thoughts_on_thinking_of_everything/", "subreddit_subscribers": 90853, "created_utc": 1677157717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious if there\u2019s roles out there that are a hybrid of both.", "author_fullname": "t2_l1vnoo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering roles that involve both technical and managerial skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119x4bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious if there\u2019s roles out there that are a hybrid of both.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119x4bq", "is_robot_indexable": true, "report_reasons": null, "author": "Paulythress", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119x4bq/data_engineering_roles_that_involve_both/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119x4bq/data_engineering_roles_that_involve_both/", "subreddit_subscribers": 90853, "created_utc": 1677157209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[This article](https://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7) is a nice summary of different patterns for dealing with DQ issues. It got me to wondering: what (or even *if*) people tend to do with data quality errors? \n\nDo you use Write-Audit-Publish? Just ignore the errors? Not even check for DQ and wait until users start to scream?\u2026", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do *you* implement data quality in your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119x43k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7\"&gt;This article&lt;/a&gt; is a nice summary of different patterns for dealing with DQ issues. It got me to wondering: what (or even &lt;em&gt;if&lt;/em&gt;) people tend to do with data quality errors? &lt;/p&gt;\n\n&lt;p&gt;Do you use Write-Audit-Publish? Just ignore the errors? Not even check for DQ and wait until users start to scream?\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?auto=webp&amp;v=enabled&amp;s=39aec5dfc16d1e4ae17045f2d7d70d73355a13af", "width": 785, "height": 728}, "resolutions": [{"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7294aa8fb39ec073993e20336f03ccb896c909ad", "width": 108, "height": 100}, {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d81961ca0665d92b74e24fc5026f076840b0233d", "width": 216, "height": 200}, {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=639b9bb187d5aee4fc8cfeb506ce07ec645c6d95", "width": 320, "height": 296}, {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f96b3288b28b4a13363a07f45117094b4671108", "width": 640, "height": 593}], "variants": {}, "id": "EtMVakfqU2sDntG3RUUPf0qPToPTaAd3rB7HkaZsRIE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119x43k", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119x43k/how_do_you_implement_data_quality_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119x43k/how_do_you_implement_data_quality_in_your/", "subreddit_subscribers": 90853, "created_utc": 1677157190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have little experience myself with DAGs so please correct me if I'm wrong, but I get the feeling that there's a good amount of DE practitioners that doesn't like DAGs.\n\nThe feeling is reinforced by my current experience with Prefect, which seems to make the fact that it doesn't use DAGs one of their selling points (e.g. https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/)\n\nWhat's the deal? What am I missing?", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's wrong with DAGs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119wyu1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677156735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have little experience myself with DAGs so please correct me if I&amp;#39;m wrong, but I get the feeling that there&amp;#39;s a good amount of DE practitioners that doesn&amp;#39;t like DAGs.&lt;/p&gt;\n\n&lt;p&gt;The feeling is reinforced by my current experience with Prefect, which seems to make the fact that it doesn&amp;#39;t use DAGs one of their selling points (e.g. &lt;a href=\"https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/\"&gt;https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the deal? What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?auto=webp&amp;v=enabled&amp;s=547594bb7ad1461e2bf376971cdf95a74f1db4d2", "width": 6930, "height": 4220}, "resolutions": [{"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a957033059ab0398722232920683d08838f596a", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84decadd5dfee4b9d747e77e10a678f0bfab0d9d", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad3d118771a2f5516054663ffee041741d83b000", "width": 320, "height": 194}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=285b88fd76ca6cb55d0eaee34d6d8326d1eeba32", "width": 640, "height": 389}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=119a69e1370db6e473c0f3edbbbea4890b69e281", "width": 960, "height": 584}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521052318c5d7fd560d892a96b70506c103b8440", "width": 1080, "height": 657}], "variants": {}, "id": "i8z4NJBB0EQA17Rwbx0Uu43Y6kWtvPxTeve2tIChgrI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119wyu1", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119wyu1/whats_wrong_with_dags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119wyu1/whats_wrong_with_dags/", "subreddit_subscribers": 90853, "created_utc": 1677156735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vd2d51zd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\udce2For those interested in Polars, have a look to this new awesome list about Polars !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_119weiv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/F-JhJNcmVOZf-_wMQSTDaOZInhghXyI4NN6Y79hr53c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677154908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/ddotta/awesome-polars", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?auto=webp&amp;v=enabled&amp;s=fa50e5725fc522b6f89c59ea850db115822da451", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=444503196ae850daf677abf6ad6a2169cfe1cefb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d853d713b8b85ff995cb7e9d44f99ccb380cecb1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a3f77f1bebe98a06e6586747e07c73764eedf29", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=464cd75cec88fe4b4cf69ff1beaa3a9af61b74a7", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f514c304d122ea12ca787b525114ac94c345808e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Y05cn4iXMeVH3SyN-2N-o9WqnXDiccn9Oy4KClPKqzs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8eb61827ae7e32037456b388b3cda0699e433a9", "width": 1080, "height": 540}], "variants": {}, "id": "sx-5QeR_HmQOzd-a-D_hNeQpACQUM3bpl8k_FouLD30"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "119weiv", "is_robot_indexable": true, "report_reasons": null, "author": "damiendotta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119weiv/for_those_interested_in_polars_have_a_look_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/ddotta/awesome-polars", "subreddit_subscribers": 90853, "created_utc": 1677154908.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}