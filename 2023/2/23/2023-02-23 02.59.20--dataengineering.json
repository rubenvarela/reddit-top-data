{"kind": "Listing", "data": {"after": "t3_1197xzc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently on Azure datafactory (orcestration)  + Azure SQL database (ETL done using procedures + presentation layer). We tested databricks and liked the functionality so are utilizing that for newer ETL development. The company has decided to go to AWS so now we are exploring options there. \n\nSo my question to you would be which orcestration tools, databases/data warehouses, CICD tools are you using and why?", "author_fullname": "t2_4doyx62l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what does your company's current data landscape look like? Which tools and technologies did you go for and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1191ia8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677077301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently on Azure datafactory (orcestration)  + Azure SQL database (ETL done using procedures + presentation layer). We tested databricks and liked the functionality so are utilizing that for newer ETL development. The company has decided to go to AWS so now we are exploring options there. &lt;/p&gt;\n\n&lt;p&gt;So my question to you would be which orcestration tools, databases/data warehouses, CICD tools are you using and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1191ia8", "is_robot_indexable": true, "report_reasons": null, "author": "fancyshamancy", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1191ia8/what_does_your_companys_current_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1191ia8/what_does_your_companys_current_data_landscape/", "subreddit_subscribers": 90611, "created_utc": 1677077301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have multiple sources which we have no control over and they provide data in csv format.\n\nThis includes but not limited to:\n- files manually loaded into google cloud storage\n- apis returning data as csv\n- sftp server where we receive csv files\n- google sheets\n\nAll of them come with their own schema, number of columns etc. The data might be at most a few GBs I guess. Pipelines are going run hourly and some maybe daily.\n\nWe need to ingest all of them and we use cloud functions (python) to do this extraction part.\n\n\nSince I was setting this part from scratch. I was thinking if there are any best practices or automated setup that people are using.\n\nI plan to have:\n- all columns converted to strings and then doing transformation part in dwh.\n- have the logical part of parsing and loading to dwh separate and have schemas defined for each of these sources.\n\nMy questions are:\n- Should csv be directly consumed or do you think it is better to convert it into different format (i am seeing parquet a lot in csv related answers in this subreddit).\n- For json we were keeping json as a single column in our source table in dwh and then parsing it later in dwh using json_scalar function in bq. This helps us with changing schema etc. For csv this probably does not make sense at all or does it?\n- Csv are notorious with data quality and reliable data is our team OKR. what is the best way/tools to ensure data reliability in case on such unreliable data sources.\n\nAnd yes we use dbt with bigquery. And Airflow for orchestration.\n\n\nWould appreciate any help. Thanks.", "author_fullname": "t2_4x8s649h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices - Data Pipelines with CSV as Data Source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118njzh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677036078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have multiple sources which we have no control over and they provide data in csv format.&lt;/p&gt;\n\n&lt;p&gt;This includes but not limited to:\n- files manually loaded into google cloud storage\n- apis returning data as csv\n- sftp server where we receive csv files\n- google sheets&lt;/p&gt;\n\n&lt;p&gt;All of them come with their own schema, number of columns etc. The data might be at most a few GBs I guess. Pipelines are going run hourly and some maybe daily.&lt;/p&gt;\n\n&lt;p&gt;We need to ingest all of them and we use cloud functions (python) to do this extraction part.&lt;/p&gt;\n\n&lt;p&gt;Since I was setting this part from scratch. I was thinking if there are any best practices or automated setup that people are using.&lt;/p&gt;\n\n&lt;p&gt;I plan to have:\n- all columns converted to strings and then doing transformation part in dwh.\n- have the logical part of parsing and loading to dwh separate and have schemas defined for each of these sources.&lt;/p&gt;\n\n&lt;p&gt;My questions are:\n- Should csv be directly consumed or do you think it is better to convert it into different format (i am seeing parquet a lot in csv related answers in this subreddit).\n- For json we were keeping json as a single column in our source table in dwh and then parsing it later in dwh using json_scalar function in bq. This helps us with changing schema etc. For csv this probably does not make sense at all or does it?\n- Csv are notorious with data quality and reliable data is our team OKR. what is the best way/tools to ensure data reliability in case on such unreliable data sources.&lt;/p&gt;\n\n&lt;p&gt;And yes we use dbt with bigquery. And Airflow for orchestration.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118njzh", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Carob897", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118njzh/best_practices_data_pipelines_with_csv_as_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118njzh/best_practices_data_pipelines_with_csv_as_data/", "subreddit_subscribers": 90611, "created_utc": 1677036078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What tips &amp; tricks would you recommend to the beginner pandas users? What to definitely avoid?\n\n(I would like to provide a pandas workshop to our business users so collecting ideas)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your best pandas tips&amp;tricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1199zm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677095572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tips &amp;amp; tricks would you recommend to the beginner pandas users? What to definitely avoid?&lt;/p&gt;\n\n&lt;p&gt;(I would like to provide a pandas workshop to our business users so collecting ideas)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1199zm1", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1199zm1/what_are_your_best_pandas_tipstricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1199zm1/what_are_your_best_pandas_tipstricks/", "subreddit_subscribers": 90611, "created_utc": 1677095572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Polars is getting pretty popular recently but I would like to know what pandas can do and polars still can not.\n\nE. g. I found polars cannot work efficiently with json (missing e.g. json_normalize function).", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What pandas can do and polars can\u2019t?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119a3vs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677095736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Polars is getting pretty popular recently but I would like to know what pandas can do and polars still can not.&lt;/p&gt;\n\n&lt;p&gt;E. g. I found polars cannot work efficiently with json (missing e.g. json_normalize function).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119a3vs", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119a3vs/what_pandas_can_do_and_polars_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119a3vs/what_pandas_can_do_and_polars_cant/", "subreddit_subscribers": 90611, "created_utc": 1677095736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building the modern data stack but curious if anyone uses Kafka. What use cases have you utilized it for and what stack have you integrated it with to get the most value out of this data that you're collecting?", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Kafka?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1194ppe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677084762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building the modern data stack but curious if anyone uses Kafka. What use cases have you utilized it for and what stack have you integrated it with to get the most value out of this data that you&amp;#39;re collecting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1194ppe", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1194ppe/anyone_using_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1194ppe/anyone_using_kafka/", "subreddit_subscribers": 90611, "created_utc": 1677084762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All --\n\nI am not a data engineer -- I'm an analyst, but I use a very robust and convoluted data infrastructure built by a team of data engineers. The data infrastructure that I use on a day-to-day basis has almost no documentation regarding how the ETLs map to higher abstractions of unstructured data, nor really any good descriptions of source data systems underlying the ETL process.\n\n&amp;#x200B;\n\nDoes anyone know if there are any documentation frameworks for documenting this sort of thing? My goal is to reduce tribal knowledge of our data engineering team to make things more accessible for our analysts.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_5b1wzyfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to build documentation for a data infrastructure? any existing tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1195ts7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677087389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All --&lt;/p&gt;\n\n&lt;p&gt;I am not a data engineer -- I&amp;#39;m an analyst, but I use a very robust and convoluted data infrastructure built by a team of data engineers. The data infrastructure that I use on a day-to-day basis has almost no documentation regarding how the ETLs map to higher abstractions of unstructured data, nor really any good descriptions of source data systems underlying the ETL process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone know if there are any documentation frameworks for documenting this sort of thing? My goal is to reduce tribal knowledge of our data engineering team to make things more accessible for our analysts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1195ts7", "is_robot_indexable": true, "report_reasons": null, "author": "MrNezzer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1195ts7/whats_the_best_way_to_build_documentation_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1195ts7/whats_the_best_way_to_build_documentation_for_a/", "subreddit_subscribers": 90611, "created_utc": 1677087389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just joined a small B2C company that produces written content similar to spark notes. They have no data warehouse and is using Postgres for housing content to deliver via their website. Separately they also have data scientists scraping web data to find which books are popular.\n\nI need to setup a data warehouse and a database for the company. The first purpose would be to automate the scraping process - it is currently manual and the datasets are stored on someone\u2019s computer lol.\n\nI\u2019ll considering using all of Google Cloud\u2019s suite of tools. What are the risks here? Google Cloud seems to have everything - BigQuery as the warehouse, SQL database, data studio as BI tool, and Cloud Run to automate python scrapers.\n\nWhat are the pros and cons of using every tool from one provider (aka Google here)? \n\nFor context, I am more familiar with Snowflake and Looker setup from my last job.", "author_fullname": "t2_gfy8k6mh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting DataOps function from Scratch at small B2C company.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119emfw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677102222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just joined a small B2C company that produces written content similar to spark notes. They have no data warehouse and is using Postgres for housing content to deliver via their website. Separately they also have data scientists scraping web data to find which books are popular.&lt;/p&gt;\n\n&lt;p&gt;I need to setup a data warehouse and a database for the company. The first purpose would be to automate the scraping process - it is currently manual and the datasets are stored on someone\u2019s computer lol.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll considering using all of Google Cloud\u2019s suite of tools. What are the risks here? Google Cloud seems to have everything - BigQuery as the warehouse, SQL database, data studio as BI tool, and Cloud Run to automate python scrapers.&lt;/p&gt;\n\n&lt;p&gt;What are the pros and cons of using every tool from one provider (aka Google here)? &lt;/p&gt;\n\n&lt;p&gt;For context, I am more familiar with Snowflake and Looker setup from my last job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119emfw", "is_robot_indexable": true, "report_reasons": null, "author": "FivePointyChickens", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119emfw/starting_dataops_function_from_scratch_at_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119emfw/starting_dataops_function_from_scratch_at_small/", "subreddit_subscribers": 90611, "created_utc": 1677102222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I enjoyed listening to this 30-minute [Data Engineering Podcast](https://www.dataengineeringpodcast.com/six-year-retrospective-episode-361?t=57) episode with its summary of the ecosystem today. The commentary on how we got here too was interesting. \n\nIf you've listened to it do you agree with the summary, or did he miss anything important out?", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Podcast] \ud83c\udfa7 A useful summary of the data engineering ecosystem today", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1195xdc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677087629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I enjoyed listening to this 30-minute &lt;a href=\"https://www.dataengineeringpodcast.com/six-year-retrospective-episode-361?t=57\"&gt;Data Engineering Podcast&lt;/a&gt; episode with its summary of the ecosystem today. The commentary on how we got here too was interesting. &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve listened to it do you agree with the summary, or did he miss anything important out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?auto=webp&amp;v=enabled&amp;s=e23e22e97e3722d0f8f9a61615e50ebcb3fa1826", "width": 1400, "height": 1400}, "resolutions": [{"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9da709a3840cf93e0a7b0b851bac03e773481d2b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=358185c1b969237f260a000561002ac68e71b2dd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=142a7305c615c834abda063fa9659d4312a4db7f", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8482cb9ec4aa7be58a4defea83030c21e651314f", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1672edce28c8d7b5be11836f2e364a11b78dacb7", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/yqRqF_wh_bUOV1-09CKYUysdUkhID2Ds1TevGDSoVV4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d6af140426178eebb01eca44ec153b253ef8cd1", "width": 1080, "height": 1080}], "variants": {}, "id": "T69lnyzJjP6GC8oxXuuJPHNQnXgo_9t63cJnvtec6xw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1195xdc", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1195xdc/podcast_a_useful_summary_of_the_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1195xdc/podcast_a_useful_summary_of_the_data_engineering/", "subreddit_subscribers": 90611, "created_utc": 1677087629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "March 7 at 12 pm ET (17:00 UTC), join Zhamak Dehghani, founder and CEO of Nextdata and founder of the concept of Data Mesh, for the [ACM TechTalk](https://acm-org.zoom.us/webinar/register/9116770839344/WN_oqLHsa1GTnmtqafja30RNQ) \"**State of Data Mesh.**\"\n\nIn this talk Zhamak tells a short story of why we are here and what has happened before the inflection point of Data Mesh. What does the destination of an organization toward Data Mesh look like, after the inflection point? What is anchoring organizations to move forward and move fast? She leaves the audience with some practical steps to rewire an organizational brain\u2014behavior and technology\u2014to make atomic changes toward Data Mesh and move to new heights.\n\n[Register](https://acm-org.zoom.us/webinar/register/9116770839344/WN_oqLHsa1GTnmtqafja30RNQ) to attend this talk live or on demand.", "author_fullname": "t2_cd4qjhv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "March 7, Free Talk with Data Mesh Founder Zhamak Dehghani, Founder and CEO of Nextdata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1194fln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677084092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;March 7 at 12 pm ET (17:00 UTC), join Zhamak Dehghani, founder and CEO of Nextdata and founder of the concept of Data Mesh, for the &lt;a href=\"https://acm-org.zoom.us/webinar/register/9116770839344/WN_oqLHsa1GTnmtqafja30RNQ\"&gt;ACM TechTalk&lt;/a&gt; &amp;quot;&lt;strong&gt;State of Data Mesh.&lt;/strong&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;In this talk Zhamak tells a short story of why we are here and what has happened before the inflection point of Data Mesh. What does the destination of an organization toward Data Mesh look like, after the inflection point? What is anchoring organizations to move forward and move fast? She leaves the audience with some practical steps to rewire an organizational brain\u2014behavior and technology\u2014to make atomic changes toward Data Mesh and move to new heights.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://acm-org.zoom.us/webinar/register/9116770839344/WN_oqLHsa1GTnmtqafja30RNQ\"&gt;Register&lt;/a&gt; to attend this talk live or on demand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-TdXzS9iAsjf4dUR8tBzLdampPsR6fNnRPpBR3xMsmE.jpg?auto=webp&amp;v=enabled&amp;s=244c59d4d7c0afb6b0df199d693b7efda7162db6", "width": 60, "height": 60}, "resolutions": [], "variants": {}, "id": "Ow6yc0WkWVXCRxHUhhRq4t7_TN18fLOlAO-3jycS2Z4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1194fln", "is_robot_indexable": true, "report_reasons": null, "author": "ACMLearning", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1194fln/march_7_free_talk_with_data_mesh_founder_zhamak/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1194fln/march_7_free_talk_with_data_mesh_founder_zhamak/", "subreddit_subscribers": 90611, "created_utc": 1677084092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m at the very beginning of my learning and have been trying to come up with a valid project or idea to have in my Github. Would building a marketplace type application from as close to scratch as possible be overkill compared to creating mock databases/pools and focusing on doing different types of those? Is it even a valid thought to think that I\u2019ll be able to create mock databases and pools as practice? If not, what other ways will I be able to practice?", "author_fullname": "t2_af6ad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would it be valuable to my learning to build a copy of something like Etsy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118nj7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677036010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m at the very beginning of my learning and have been trying to come up with a valid project or idea to have in my Github. Would building a marketplace type application from as close to scratch as possible be overkill compared to creating mock databases/pools and focusing on doing different types of those? Is it even a valid thought to think that I\u2019ll be able to create mock databases and pools as practice? If not, what other ways will I be able to practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118nj7n", "is_robot_indexable": true, "report_reasons": null, "author": "eugene_steelflex", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118nj7n/would_it_be_valuable_to_my_learning_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118nj7n/would_it_be_valuable_to_my_learning_to_build_a/", "subreddit_subscribers": 90611, "created_utc": 1677036010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualize Streaming Data in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_118ndst", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UQMeohh2pxHn5ji1QmNx6Vwzp6KOaD2mxNdQhY0Os0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677035570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bytewax.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bytewax.io/blog/visualize-streaming-data-in-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?auto=webp&amp;v=enabled&amp;s=37a528a7612550bc82be48d4d478997d72be1558", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03247202e8466bf819d1b3ee42fec17247a41e34", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3bd711ec66db5611d6b0de37f4f4dec4eb202d8", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10a7f5446ed3fc25af7d29b96676adf58238e6bf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e250c5abbe743d967751dcc90847bdb4a0c0dad", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0cd35b80bb96804231f0d799445ba6c96e73da4", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e72f416371064af1d42c4426ab4b5ceace4a028d", "width": 1080, "height": 564}], "variants": {}, "id": "_GAQl_RTzqMiEFZm4ZiVzQqqGTrkugbg6LCg3kt5mlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "118ndst", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118ndst/visualize_streaming_data_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bytewax.io/blog/visualize-streaming-data-in-python", "subreddit_subscribers": 90611, "created_utc": 1677035570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am building a no-code data platform that will hookup to various data sources to move data from one source to another. Would like some feedback on the concept if anyone has time. I may be open to open sourcing some of the connector logic that I build.\n\nThe pain point that I am trying to solve is having to spend time building custom connectors and core logic to move data from various sources to your data warehouse for further processing or to other core business systems that you and your team use.\n\nWould appreciate any feedback on the idea / concept. Currently in a conceptual / ideation phase.\n\n[decouple.ai](https://decouple-ai.webflow.io/)", "author_fullname": "t2_tboe0h8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "feedback on data platform concept", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1197sp1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677091933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a no-code data platform that will hookup to various data sources to move data from one source to another. Would like some feedback on the concept if anyone has time. I may be open to open sourcing some of the connector logic that I build.&lt;/p&gt;\n\n&lt;p&gt;The pain point that I am trying to solve is having to spend time building custom connectors and core logic to move data from various sources to your data warehouse for further processing or to other core business systems that you and your team use.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any feedback on the idea / concept. Currently in a conceptual / ideation phase.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://decouple-ai.webflow.io/\"&gt;decouple.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?auto=webp&amp;v=enabled&amp;s=bcdaf7024adf75785ab27ccbb7781293f2663b62", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d63f88fc7621545a599bdc28869d81c6f7075a28", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7d099f9ff0132f8c58729912b675a633d2784b4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9bb2c9f14c6733db0c7b7ea997487b60339ba97", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2925489094c89ce9f52ca3bc20a562fbf2894513", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cc0f379e132c42b374c704b5007f1459a17301a", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qNjewdgx32ue_xkhs376YFgcEltnmy2cSZrGFKZ4kao.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9872dc5fa45242e80a7314c079ba7d5428862057", "width": 1080, "height": 567}], "variants": {}, "id": "N_GVmsM00cAQGHLporpzUSZ1pQHBjxv2qEICAWCoMrc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1197sp1", "is_robot_indexable": true, "report_reasons": null, "author": "gardendotplace", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1197sp1/feedback_on_data_platform_concept/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1197sp1/feedback_on_data_platform_concept/", "subreddit_subscribers": 90611, "created_utc": 1677091933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I\u2019m currently a BI dev but have worked loosely with data engineering concepts like warehousing. I know the theoretical side of it but lack practical experience.\n\nDoes anyone know where I could find any data warehousing projects to improve my skillset and put the theory to practical", "author_fullname": "t2_bnbu4t02", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a data warehousing project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1193qmi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677082446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I\u2019m currently a BI dev but have worked loosely with data engineering concepts like warehousing. I know the theoretical side of it but lack practical experience.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know where I could find any data warehousing projects to improve my skillset and put the theory to practical&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1193qmi", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Respect39", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1193qmi/looking_for_a_data_warehousing_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1193qmi/looking_for_a_data_warehousing_project/", "subreddit_subscribers": 90611, "created_utc": 1677082446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my current company, there is a data pipeline where we use Kafka Connect to write the change stream events (for a collection) to a Kafka stream, and then the Spark structured streaming application ingests the data from Kafka and processes it.\n\nMy question is, why can't we directly stream the data from MongoDB using the spark application? As per this article ([https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/](https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/)), change streams are responsible for passing on events to the client (in this case the spark application).\n\nIn both cases (Kafka Connect and the Spark application), change streams are created which notify the client of the changes. I don't really understand why we had to introduce Kafka in the middle of MongoDB and the Spark application.\n\nIt would be great if someone could help me understand this. Thanks for reading.\n\nEDIT:\n\nWhile writing the post, I realized that I am actually questioning the reason for using Kafka at all. I understood the following from this stack overflow answer\n\n[https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark](https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark): -\n\n&gt;Kafka offers persistence (till the set retention period). So if the spark application is down for maintenance, it will miss the change stream events from the MongoDB collection during the maintenance time. But if the events are stored in Kafka, they will be there for the duration of the retention period and ready for processing by the spark application.\n\nIf anyone else has anything to add to this, please feel free to do so. Thanks.\n\n&amp;#x200B;", "author_fullname": "t2_uqx8q0b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Streaming from MongoDB vs Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118stz7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677055432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677054235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my current company, there is a data pipeline where we use Kafka Connect to write the change stream events (for a collection) to a Kafka stream, and then the Spark structured streaming application ingests the data from Kafka and processes it.&lt;/p&gt;\n\n&lt;p&gt;My question is, why can&amp;#39;t we directly stream the data from MongoDB using the spark application? As per this article (&lt;a href=\"https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/\"&gt;https://www.mongodb.com/developer/languages/python/streaming-data-apache-spark-mongodb/&lt;/a&gt;), change streams are responsible for passing on events to the client (in this case the spark application).&lt;/p&gt;\n\n&lt;p&gt;In both cases (Kafka Connect and the Spark application), change streams are created which notify the client of the changes. I don&amp;#39;t really understand why we had to introduce Kafka in the middle of MongoDB and the Spark application.&lt;/p&gt;\n\n&lt;p&gt;It would be great if someone could help me understand this. Thanks for reading.&lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;/p&gt;\n\n&lt;p&gt;While writing the post, I realized that I am actually questioning the reason for using Kafka at all. I understood the following from this stack overflow answer&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark\"&gt;https://stackoverflow.com/questions/42673343/why-do-we-need-kafka-to-feed-data-to-apache-spark&lt;/a&gt;: -&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Kafka offers persistence (till the set retention period). So if the spark application is down for maintenance, it will miss the change stream events from the MongoDB collection during the maintenance time. But if the events are stored in Kafka, they will be there for the duration of the retention period and ready for processing by the spark application.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If anyone else has anything to add to this, please feel free to do so. Thanks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118stz7", "is_robot_indexable": true, "report_reasons": null, "author": "the-fake-me", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118stz7/spark_streaming_from_mongodb_vs_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118stz7/spark_streaming_from_mongodb_vs_kafka/", "subreddit_subscribers": 90611, "created_utc": 1677054235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have spend the better part of the past two weeks reading about data engineering (and MLOps). The space seems to be dominated by Python-based workflows and tools like Apache Airflow, Dagster, MLFlow etc", "author_fullname": "t2_srfmey5u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should data engineering be done in a non-Python organization that is dominated by C#/.Net developers and data science people who prefer R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119hqbf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677108845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have spend the better part of the past two weeks reading about data engineering (and MLOps). The space seems to be dominated by Python-based workflows and tools like Apache Airflow, Dagster, MLFlow etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119hqbf", "is_robot_indexable": true, "report_reasons": null, "author": "gheex", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119hqbf/how_should_data_engineering_be_done_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119hqbf/how_should_data_engineering_be_done_in_a/", "subreddit_subscribers": 90611, "created_utc": 1677108845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_em2yi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_119g5nc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QZH3QHnUFoosVA85lVKFbP13HNR91vebAHaKr_M4qCU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677105257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "rcpassos.me", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://rcpassos.me/post/apache-arrow-future-of-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?auto=webp&amp;v=enabled&amp;s=0dbc90e7190608d0a5c7f3b79b3cc14dcf40e005", "width": 2048, "height": 1170}, "resolutions": [{"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9346544144f2d70bd9adf0c2d91e5037f1e1bb50", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45759a1b653546d00c5c5eace4bbf344355bcc14", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c4e1c29f639231505e892e2d07b2e35afb9b80f", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58b9cc2cf6daa644cca89de9c250f43bb88b6b73", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143764eb76d7d1dcd28d1a82935d3f9ffdc93209", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/hiYMqBu6iaHtrs89QTU3Z0cevu_mdcbV6u5wa9WXVs4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d6dcdb61c28ea1a4275ccc32506478a736e4833", "width": 1080, "height": 616}], "variants": {}, "id": "aaEDVnIXM_-RMbLWzDpeO6CMM6TnSdzuUj2_DeBa8lk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "119g5nc", "is_robot_indexable": true, "report_reasons": null, "author": "auyer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119g5nc/how_i_decreased_etl_cost_by_leveraging_the_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://rcpassos.me/post/apache-arrow-future-of-data-engineering", "subreddit_subscribers": 90611, "created_utc": 1677105257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently joined a company who is doing the majority of data transformation in lookml.  Seems like there are 2 reasons for this from what I'm told:  \n\n1. Every end user wants a different level of aggregation and that would require creating duplicative tables in Snowflake (think something like marketing wants product level, finance wants order level, but user specific not business specific)\n\n2. If we change the data structure of a table that would require a lot of maintenance in Snowflake \n\n\nI'm very self taught and still very much a beginner so I'm looking for a sanity check. My thought is having a solid groundwork laid out in Snowflake is going to solve the majority of those concerns. Define the aggregate levels by the business needs and if a single user has a specific request evaluate the business need for it. That way the data warehouse is our source of truth.\n\n\nIdeally, what is the right way to use Looker?", "author_fullname": "t2_2q171de9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does Looker fit into the data stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119ailf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677096297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently joined a company who is doing the majority of data transformation in lookml.  Seems like there are 2 reasons for this from what I&amp;#39;m told:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Every end user wants a different level of aggregation and that would require creating duplicative tables in Snowflake (think something like marketing wants product level, finance wants order level, but user specific not business specific)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If we change the data structure of a table that would require a lot of maintenance in Snowflake &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m very self taught and still very much a beginner so I&amp;#39;m looking for a sanity check. My thought is having a solid groundwork laid out in Snowflake is going to solve the majority of those concerns. Define the aggregate levels by the business needs and if a single user has a specific request evaluate the business need for it. That way the data warehouse is our source of truth.&lt;/p&gt;\n\n&lt;p&gt;Ideally, what is the right way to use Looker?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119ailf", "is_robot_indexable": true, "report_reasons": null, "author": "lahma_mama", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119ailf/where_does_looker_fit_into_the_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119ailf/where_does_looker_fit_into_the_data_stack/", "subreddit_subscribers": 90611, "created_utc": 1677096297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently working for a mid size tech company. The Data Science team I\u2019m a part of, is in its infancy and there is not a lot of direction from my team lead. The current senior data engineer is not very helpful in most situations as he is usually swamped with requests and I have a feeling the person is planning to leave soon. This would put a lot more pressure on my shoulders. I have Business Intelligence and Software Development experience (about 8 years overall) but I\u2019ve recently internally moved into the Data Engineer position. This team heavily uses Spark, Scala and Kafka. I haven\u2019t worked on these technologies before. With the lack of clear leadership in my team, I\u2019m struggling to learn anything. What resources in terms of blogs/YouTube/tutorials/books can I use to be ready for a senior role if it comes to that? Thank you for any information you guys can share.", "author_fullname": "t2_1reibdu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources/Books that help transition into a Senior Data Engineering role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1199ilk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677094879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently working for a mid size tech company. The Data Science team I\u2019m a part of, is in its infancy and there is not a lot of direction from my team lead. The current senior data engineer is not very helpful in most situations as he is usually swamped with requests and I have a feeling the person is planning to leave soon. This would put a lot more pressure on my shoulders. I have Business Intelligence and Software Development experience (about 8 years overall) but I\u2019ve recently internally moved into the Data Engineer position. This team heavily uses Spark, Scala and Kafka. I haven\u2019t worked on these technologies before. With the lack of clear leadership in my team, I\u2019m struggling to learn anything. What resources in terms of blogs/YouTube/tutorials/books can I use to be ready for a senior role if it comes to that? Thank you for any information you guys can share.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1199ilk", "is_robot_indexable": true, "report_reasons": null, "author": "luckykanwar", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1199ilk/resourcesbooks_that_help_transition_into_a_senior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1199ilk/resourcesbooks_that_help_transition_into_a_senior/", "subreddit_subscribers": 90611, "created_utc": 1677094879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have defined some SLOs on your data? Meaning for example you guarantee your users dataset XYZ will have no more than 5% NULL values in column ABC? If so, how do you present it to the user? Is it e.g. part of the data catalog?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data products SLO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11960zu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677087868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have defined some SLOs on your data? Meaning for example you guarantee your users dataset XYZ will have no more than 5% NULL values in column ABC? If so, how do you present it to the user? Is it e.g. part of the data catalog?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11960zu", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11960zu/data_products_slo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11960zu/data_products_slo/", "subreddit_subscribers": 90611, "created_utc": 1677087868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \nhow does your bronze silver gold layers look like? do you use different storage accounts for each layer?   \ndo you have one metastore for multiple workspaces? (prod, dev, sandbox) or one metastore for each workspace?    \ndo you use managed tables or managed tables with external locations so that you can have visibility of the underlying files?   \nin the 3 level namespace, how do you name your ***catalog.schema.tables***? do you use Prod.bronze.salesforce\\_table1 or something like prod\\_bronze.salesforce.customers (just an example to get the discussion going).", "author_fullname": "t2_4doyx62l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "people who are using databricks and dataricks unity catalog, how does your setup look like? have you come across any downsides or limitations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1193j3m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677081962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how does your bronze silver gold layers look like? do you use different storage accounts for each layer?&lt;br/&gt;\ndo you have one metastore for multiple workspaces? (prod, dev, sandbox) or one metastore for each workspace?&lt;br/&gt;\ndo you use managed tables or managed tables with external locations so that you can have visibility of the underlying files?&lt;br/&gt;\nin the 3 level namespace, how do you name your &lt;strong&gt;&lt;em&gt;catalog.schema.tables&lt;/em&gt;&lt;/strong&gt;? do you use Prod.bronze.salesforce_table1 or something like prod_bronze.salesforce.customers (just an example to get the discussion going).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1193j3m", "is_robot_indexable": true, "report_reasons": null, "author": "fancyshamancy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1193j3m/people_who_are_using_databricks_and_dataricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1193j3m/people_who_are_using_databricks_and_dataricks/", "subreddit_subscribers": 90611, "created_utc": 1677081962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi,\n\nI'm using Delta Live Tables to create a medallion architecture and am having trouble defining a parameterised function to upsert data from bronze into silver. I used the [CDC](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html) documentation but I also want my silver table to have expectations, schema hints and new columns, which I don't think is possible using streaming\\_live\\_table.\n\nAs a reference, I created the below function to take data from a bronze table and upsert into silver:\n\n    def generate_silver_streaming_live_table(env, schema_name, object_name, dedupe_key, seq):\n        dlt.create_streaming_live_table(\n            name = env + \"_\" + object_name + \"live_silver\",\n            comment = \"Silver data for \" + env + \"_\" + object_name,\n            path = \"abfss://datalakepath.dfs.core.windows.net/\" + env +\"/\" + schema_name + \"/\" + object_name + \"/silver\"\n        )\n        dlt.apply_changes(\n            target = env + \"_\" + object_name + \"live_silver\",\n            source = env + \"_\" + object_name + \"_bronze\",\n            keys = dedupe_key,\n            sequence_by = seq\n        )\n\nThis works well but like I mentioned is not compatible with expectations, schema hints and new columns. I then changed tactics to use the more traditional dlt.table decorator, but this is having a lot of trouble with upserts. ForEachBatch is not compatible with DLT - i've tried using dropDuplicates but this keeps the first update made rather than changes, i've also tried using merge statements like below but this isn't working for me either because the silver table has no columns yet so can't do the merge on silver.dedupe\\_key.\n\n    def generate_silver_table(env, object_name, dedupe_key):\n        @dlt.table(\n            name = env + \"_\" + object_name + \"static_silver\",\n            comment = \"Silver data for \" + env + \"_\" + object_name,\n        )\n        def silverincremental():\n                df = dlt.read_stream(\n                    env + \"_\" + object_name + \"_bronze\"\n                )\n                silver_table = DeltaTable.forPath(spark, \"dbfs:/pathtosilvertable\")\n                return(\n                silver_table.alias(\"silver\").merge(df.alias('source'), \"source.\" + dedupe_key + \"= silver.\" + dedupe_key)\n                    .whenMatchedUpdateAll()\n                    .whenNotMatchedInsertAll()\n                    .execute()\n                )\n\nI'm at a loose end here but there must be something i'm missing? This is a very simple and very common business problem using recommended steps in the medallion architecture recommended by Databricks and I can't believe it's so difficult to implement.", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Live Tables Upsert to Silver", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118zrd4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677074324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Delta Live Tables to create a medallion architecture and am having trouble defining a parameterised function to upsert data from bronze into silver. I used the &lt;a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html\"&gt;CDC&lt;/a&gt; documentation but I also want my silver table to have expectations, schema hints and new columns, which I don&amp;#39;t think is possible using streaming_live_table.&lt;/p&gt;\n\n&lt;p&gt;As a reference, I created the below function to take data from a bronze table and upsert into silver:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def generate_silver_streaming_live_table(env, schema_name, object_name, dedupe_key, seq):\n    dlt.create_streaming_live_table(\n        name = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;live_silver&amp;quot;,\n        comment = &amp;quot;Silver data for &amp;quot; + env + &amp;quot;_&amp;quot; + object_name,\n        path = &amp;quot;abfss://datalakepath.dfs.core.windows.net/&amp;quot; + env +&amp;quot;/&amp;quot; + schema_name + &amp;quot;/&amp;quot; + object_name + &amp;quot;/silver&amp;quot;\n    )\n    dlt.apply_changes(\n        target = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;live_silver&amp;quot;,\n        source = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;_bronze&amp;quot;,\n        keys = dedupe_key,\n        sequence_by = seq\n    )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This works well but like I mentioned is not compatible with expectations, schema hints and new columns. I then changed tactics to use the more traditional dlt.table decorator, but this is having a lot of trouble with upserts. ForEachBatch is not compatible with DLT - i&amp;#39;ve tried using dropDuplicates but this keeps the first update made rather than changes, i&amp;#39;ve also tried using merge statements like below but this isn&amp;#39;t working for me either because the silver table has no columns yet so can&amp;#39;t do the merge on silver.dedupe_key.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def generate_silver_table(env, object_name, dedupe_key):\n    @dlt.table(\n        name = env + &amp;quot;_&amp;quot; + object_name + &amp;quot;static_silver&amp;quot;,\n        comment = &amp;quot;Silver data for &amp;quot; + env + &amp;quot;_&amp;quot; + object_name,\n    )\n    def silverincremental():\n            df = dlt.read_stream(\n                env + &amp;quot;_&amp;quot; + object_name + &amp;quot;_bronze&amp;quot;\n            )\n            silver_table = DeltaTable.forPath(spark, &amp;quot;dbfs:/pathtosilvertable&amp;quot;)\n            return(\n            silver_table.alias(&amp;quot;silver&amp;quot;).merge(df.alias(&amp;#39;source&amp;#39;), &amp;quot;source.&amp;quot; + dedupe_key + &amp;quot;= silver.&amp;quot; + dedupe_key)\n                .whenMatchedUpdateAll()\n                .whenNotMatchedInsertAll()\n                .execute()\n            )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m at a loose end here but there must be something i&amp;#39;m missing? This is a very simple and very common business problem using recommended steps in the medallion architecture recommended by Databricks and I can&amp;#39;t believe it&amp;#39;s so difficult to implement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "118zrd4", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118zrd4/delta_live_tables_upsert_to_silver/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118zrd4/delta_live_tables_upsert_to_silver/", "subreddit_subscribers": 90611, "created_utc": 1677074324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, i go to school for DA but I'm keen on DE, insurance is a field I want to get in to. If you are an DE at an insurance company, what kind of project do you work on? If you can have advice for your younger self, what would that be? What kind of knowledge/courses do you recommend and what would be a school project that you think create a huge plus point to either get hired/prepare for future job? \n\nThank you so much.", "author_fullname": "t2_bsbuf8gj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calling Insurtech: what kind of project are you working on and what it's like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118qz94", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677047269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, i go to school for DA but I&amp;#39;m keen on DE, insurance is a field I want to get in to. If you are an DE at an insurance company, what kind of project do you work on? If you can have advice for your younger self, what would that be? What kind of knowledge/courses do you recommend and what would be a school project that you think create a huge plus point to either get hired/prepare for future job? &lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "118qz94", "is_robot_indexable": true, "report_reasons": null, "author": "CockroachThink2070", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118qz94/calling_insurtech_what_kind_of_project_are_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118qz94/calling_insurtech_what_kind_of_project_are_you/", "subreddit_subscribers": 90611, "created_utc": 1677047269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Data Modelling Interview coming up. I was told this about it:  \n \"The technical exercise will be a live whiteboarding/pseudo-code conversation to understand how you think about architecting data systems. To prepare, think about a basic app or system you\u2019re familiar with - a messaging app, an e-commerce site, a news app, etc - and what sorts of tables are needed to support the app on the backend. We\u2019ll whiteboard out what those tables look like; talk through the types of analysis that the business might want to do on those tables; and pseudo-code how we would transform the data to support that analysis.\"   \n\n\nAny thoughts on how to prep for this? I have never done a data modelling interview before.  \n\n\nThanks!", "author_fullname": "t2_28kn9vuw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modelling Interview Prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_118p4mg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677040975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Data Modelling Interview coming up. I was told this about it:&lt;br/&gt;\n &amp;quot;The technical exercise will be a live whiteboarding/pseudo-code conversation to understand how you think about architecting data systems. To prepare, think about a basic app or system you\u2019re familiar with - a messaging app, an e-commerce site, a news app, etc - and what sorts of tables are needed to support the app on the backend. We\u2019ll whiteboard out what those tables look like; talk through the types of analysis that the business might want to do on those tables; and pseudo-code how we would transform the data to support that analysis.&amp;quot;   &lt;/p&gt;\n\n&lt;p&gt;Any thoughts on how to prep for this? I have never done a data modelling interview before.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "118p4mg", "is_robot_indexable": true, "report_reasons": null, "author": "PythonDataEngineer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/118p4mg/data_modelling_interview_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/118p4mg/data_modelling_interview_prep/", "subreddit_subscribers": 90611, "created_utc": 1677040975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently researching Unity Catalog for my organization. We have 3 storage account data lakes, one for dev test and prod. We currently mount the appropriate account to our Databricks workspace and so all the paths to tables are constant in our code, and we don\u2019t have to specify which to use when our pipelines run. \n\nWith the switch to Unity Catalog, I am wondering how people specify which catalog to use? Do you have a variable that gets set in the the release? Do you specify a \u201cuse catalog name\u201d somewhere before running a pipeline? Any advice is appreciated!", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unity Catalog and CI/CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119et7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677102545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently researching Unity Catalog for my organization. We have 3 storage account data lakes, one for dev test and prod. We currently mount the appropriate account to our Databricks workspace and so all the paths to tables are constant in our code, and we don\u2019t have to specify which to use when our pipelines run. &lt;/p&gt;\n\n&lt;p&gt;With the switch to Unity Catalog, I am wondering how people specify which catalog to use? Do you have a variable that gets set in the the release? Do you specify a \u201cuse catalog name\u201d somewhere before running a pipeline? Any advice is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119et7b", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119et7b/unity_catalog_and_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119et7b/unity_catalog_and_cicd/", "subreddit_subscribers": 90611, "created_utc": 1677102545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "different indicators can have different disaggregations. Disaggregations can be nested also. I have identified about 40-50 disaggregations do i need to create create different dimension table for each disaggregation and reference key in fact table. Also number of disaggregations can increase further. How I can handle this issue. Is there any way to create one table which could include all dissagregations and reference that table to fact table", "author_fullname": "t2_8biz17e7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am designing fact table to measure performance of indicators.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1197xzc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677092273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;different indicators can have different disaggregations. Disaggregations can be nested also. I have identified about 40-50 disaggregations do i need to create create different dimension table for each disaggregation and reference key in fact table. Also number of disaggregations can increase further. How I can handle this issue. Is there any way to create one table which could include all dissagregations and reference that table to fact table&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1197xzc", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic-Fall-4319", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1197xzc/i_am_designing_fact_table_to_measure_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1197xzc/i_am_designing_fact_table_to_measure_performance/", "subreddit_subscribers": 90611, "created_utc": 1677092273.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}