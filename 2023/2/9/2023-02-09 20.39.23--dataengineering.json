{"kind": "Listing", "data": {"after": "t3_10xeil2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a **technical** interview coming up with a big chocolate manufacturer for an Azure data engineer role, and I'm wondering if the traditional advice of practicing leetcode questions still applies. I'm really good at solving SQL problems, but my Python algos are sort of lacking.\n\nShould I just focus on preparing for questions on things like data modeling, data warehouses, migrations, transformations, storage options, pipelines, etc? Or should I mostly focus on grinding easy/mid leetcode python questions?\n\nAppreciate any advice, I'm kind of stressing over what to study...", "author_fullname": "t2_115k4e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do data engineer interviews look like at non-technical companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xn42u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675922725.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675922506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a &lt;strong&gt;technical&lt;/strong&gt; interview coming up with a big chocolate manufacturer for an Azure data engineer role, and I&amp;#39;m wondering if the traditional advice of practicing leetcode questions still applies. I&amp;#39;m really good at solving SQL problems, but my Python algos are sort of lacking.&lt;/p&gt;\n\n&lt;p&gt;Should I just focus on preparing for questions on things like data modeling, data warehouses, migrations, transformations, storage options, pipelines, etc? Or should I mostly focus on grinding easy/mid leetcode python questions?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any advice, I&amp;#39;m kind of stressing over what to study...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer @ Startup", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10xn42u", "is_robot_indexable": true, "report_reasons": null, "author": "-5677-", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10xn42u/how_do_data_engineer_interviews_look_like_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xn42u/how_do_data_engineer_interviews_look_like_at/", "subreddit_subscribers": 89001, "created_utc": 1675922506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I became a data engineer, most of the data engineers were coming from BI/ETL, usually after having spent a few years doing BI.\n\nHow do people get to engineering now? I saw many get to it via data science for a couple of years before they specialised\n\n[View Poll](https://www.reddit.com/poll/10xus1c)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New data engineers (0-3y in the job): How did you get into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xus1c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675949395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I became a data engineer, most of the data engineers were coming from BI/ETL, usually after having spent a few years doing BI.&lt;/p&gt;\n\n&lt;p&gt;How do people get to engineering now? I saw many get to it via data science for a couple of years before they specialised&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10xus1c\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10xus1c", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1676208595272, "options": [{"text": "I started as a data scientist and later moved to DE", "id": "21513133"}, {"text": "I started as a software developer and later moved to DE", "id": "21513134"}, {"text": "I started as a BI/DWH/business analyst and moved to DE", "id": "21513135"}, {"text": "I started my career directly as DE", "id": "21513136"}, {"text": "I have more than 3y experience, so I am out of scope of the question.", "id": "21513137"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 800, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xus1c/new_data_engineers_03y_in_the_job_how_did_you_get/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10xus1c/new_data_engineers_03y_in_the_job_how_did_you_get/", "subreddit_subscribers": 89001, "created_utc": 1675949395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI need to build an Azure data infrastructure, which ingests data from different on-prem source systems (but mainly from relational databases and from text files), loads the data to some sort of data lake (it could be something else too, like delta lake, all that matters is to have a platform from which various departments can access the data for various reasons), makes it possible to do transformations on this data (we are possibly talking about big amounts), then load the data to some kind of SQL Database / DWH System. This final platform needs to be something that provides seamless integration with cloud Dynamics 365 and Power BI.\n\nMy question for you is what kind of architecture would you use in this situation?\n\nSo far, I have explored the following options:\n\n- Data Factory / Azure Synapse Pipelines for copying data from on-prem to the data lake, and copying data from data lake to SQL Database. Also a possible option for starting scheduled Databricks jobs\n- Azure Data Lake Gen 2 as a data lake -&gt; this would be where raw, ingested data is stored (mainly text files, parquet files)\n- Azure Databricks  -&gt; For doing transformations on the ingested data. I have also explored the possibility of using Azure Delta Lake options. I am not sure what I the best practice in this situation. Is it worth to maintain a Data Lake AND a Delta Lake?\n- Azure Synapse Serverless / Dedicated SQL Pool OR SQL database / managed instance -&gt; this would be the platform which is the backend of the Dynamics 365 system. I'm really not sure if I should use Synapse for this whole process. It might be overkill for my needs.\n\nI understand this is probably a bit chaotic, Azure architecture is new to me. I'd appreciate any advice, any tips on best practices which help me decide between my options. \n\nTL;DR: Need to set up an Azure infrastructure for integrating data from on-prem DB and text files, storing raw data available for multiple use cases, enabling scheduled transformations, and providing some sort of SQL DB which can easily be integrated with Dynamics 365.", "author_fullname": "t2_vun99h9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for bringing data to Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvxpw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675952523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I need to build an Azure data infrastructure, which ingests data from different on-prem source systems (but mainly from relational databases and from text files), loads the data to some sort of data lake (it could be something else too, like delta lake, all that matters is to have a platform from which various departments can access the data for various reasons), makes it possible to do transformations on this data (we are possibly talking about big amounts), then load the data to some kind of SQL Database / DWH System. This final platform needs to be something that provides seamless integration with cloud Dynamics 365 and Power BI.&lt;/p&gt;\n\n&lt;p&gt;My question for you is what kind of architecture would you use in this situation?&lt;/p&gt;\n\n&lt;p&gt;So far, I have explored the following options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Factory / Azure Synapse Pipelines for copying data from on-prem to the data lake, and copying data from data lake to SQL Database. Also a possible option for starting scheduled Databricks jobs&lt;/li&gt;\n&lt;li&gt;Azure Data Lake Gen 2 as a data lake -&amp;gt; this would be where raw, ingested data is stored (mainly text files, parquet files)&lt;/li&gt;\n&lt;li&gt;Azure Databricks  -&amp;gt; For doing transformations on the ingested data. I have also explored the possibility of using Azure Delta Lake options. I am not sure what I the best practice in this situation. Is it worth to maintain a Data Lake AND a Delta Lake?&lt;/li&gt;\n&lt;li&gt;Azure Synapse Serverless / Dedicated SQL Pool OR SQL database / managed instance -&amp;gt; this would be the platform which is the backend of the Dynamics 365 system. I&amp;#39;m really not sure if I should use Synapse for this whole process. It might be overkill for my needs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I understand this is probably a bit chaotic, Azure architecture is new to me. I&amp;#39;d appreciate any advice, any tips on best practices which help me decide between my options. &lt;/p&gt;\n\n&lt;p&gt;TL;DR: Need to set up an Azure infrastructure for integrating data from on-prem DB and text files, storing raw data available for multiple use cases, enabling scheduled transformations, and providing some sort of SQL DB which can easily be integrated with Dynamics 365.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xvxpw", "is_robot_indexable": true, "report_reasons": null, "author": "justadataengineer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvxpw/best_practices_for_bringing_data_to_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvxpw/best_practices_for_bringing_data_to_azure/", "subreddit_subscribers": 89001, "created_utc": 1675952523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New here, what would be the best one to learn and why? \n\nPrefect or Airflow?\n\nI seen Zoomcamp changed their tool from Airflow to Prefect for the 2023 cohort and I was just wondering what would be the tool of choice in 2023 for you guys.", "author_fullname": "t2_j68xrjb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10x9uq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675888218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New here, what would be the best one to learn and why? &lt;/p&gt;\n\n&lt;p&gt;Prefect or Airflow?&lt;/p&gt;\n\n&lt;p&gt;I seen Zoomcamp changed their tool from Airflow to Prefect for the 2023 cohort and I was just wondering what would be the tool of choice in 2023 for you guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10x9uq0", "is_robot_indexable": true, "report_reasons": null, "author": "Yimmy_90", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10x9uq0/orchestration_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10x9uq0/orchestration_tools/", "subreddit_subscribers": 89001, "created_utc": 1675888218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey people,\n\n&amp;#x200B;\n\nok so, \"data as code\" is a simple idea: *put all your data (especially all that is newly created) into code, and treat it like any other software component. Put it in version control (in SOME way), test it before \"deploying it\" to production, run CD over it.*\n\n&amp;#x200B;\n\nI've been writing about this for some time and technology has moved quite a bit to make data as code real easy to do... So I'm still confused that it hasn't caught on.\n\n&amp;#x200B;\n\nThe benefit of having data in VC is pretty simple, it allows a level of reproducibility that is insane, and we don't have at all right now. It is also more secure, allows for more robust data stacks, and more...\n\n**So what is your take on \"data as code\"?** \n\n&amp;#x200B;\n\n**Notes:**\n\n\\- \"having it in version control\" for data obviously doesn't mean having huge amounts of data in VC. Instead, all reasonable solutions like lakeFS, DVC use metadata to make it fast and easy to switch between different \"versions\". \n\n\\- A simple implementation for reporting could look like this:\n\n   1. Your CI Pipeline (e.g. GitHub Actions or GitLab) triggers your Data Ingestion. The data gets ingested  and gets stored inbetween inside S3 with a new $COMMITHASH identifier. \n\n   2. Your CI Pipeline then (best in parallel!) pushes the data into your snowflake schema snowflake schema called \"cicd\\_$COMMITHASH\"; \n\n   3. Your CI Pipeline then triggers the dbt model runs, on \"cicd\\_$COMMITHASH\".\n\n4. You then run all your tests over your snowflake schema. if you're happy, you do a \"swap\" operation inside snowflake replacing the production schema with your new data.\n\n5. should something break, you trigger a \"roll-back to last version\" button inside your CI Pipeline that simply does a \"swap back\"\n\n6. Should you need to reproduce way older results, you simply trigger your CI Pipeline starting at step 2 with a different DVC data input set. \n\n\\- data as code works just fine for all the major kinds of data. For analytical data (reporting, dashboarding), for machine learning system and even for operational data (although this is way down the line in terms of importance).\n\n\\- yes in all applications (as you can see in the implementation) you do have to be careful to always use metadata like data movement operations (that are super fast) as not to slow down your pipelines, but there's enough technology available now to make that possible.", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data as code - why doesn't it catch on? What's your take?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xob67", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675926523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey people,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ok so, &amp;quot;data as code&amp;quot; is a simple idea: &lt;em&gt;put all your data (especially all that is newly created) into code, and treat it like any other software component. Put it in version control (in SOME way), test it before &amp;quot;deploying it&amp;quot; to production, run CD over it.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been writing about this for some time and technology has moved quite a bit to make data as code real easy to do... So I&amp;#39;m still confused that it hasn&amp;#39;t caught on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The benefit of having data in VC is pretty simple, it allows a level of reproducibility that is insane, and we don&amp;#39;t have at all right now. It is also more secure, allows for more robust data stacks, and more...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So what is your take on &amp;quot;data as code&amp;quot;?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;having it in version control&amp;quot; for data obviously doesn&amp;#39;t mean having huge amounts of data in VC. Instead, all reasonable solutions like lakeFS, DVC use metadata to make it fast and easy to switch between different &amp;quot;versions&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;- A simple implementation for reporting could look like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Your CI Pipeline (e.g. GitHub Actions or GitLab) triggers your Data Ingestion. The data gets ingested  and gets stored inbetween inside S3 with a new $COMMITHASH identifier. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your CI Pipeline then (best in parallel!) pushes the data into your snowflake schema snowflake schema called &amp;quot;cicd_$COMMITHASH&amp;quot;; &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your CI Pipeline then triggers the dbt model runs, on &amp;quot;cicd_$COMMITHASH&amp;quot;.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You then run all your tests over your snowflake schema. if you&amp;#39;re happy, you do a &amp;quot;swap&amp;quot; operation inside snowflake replacing the production schema with your new data.&lt;/li&gt;\n&lt;li&gt;should something break, you trigger a &amp;quot;roll-back to last version&amp;quot; button inside your CI Pipeline that simply does a &amp;quot;swap back&amp;quot;&lt;/li&gt;\n&lt;li&gt;Should you need to reproduce way older results, you simply trigger your CI Pipeline starting at step 2 with a different DVC data input set. &lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- data as code works just fine for all the major kinds of data. For analytical data (reporting, dashboarding), for machine learning system and even for operational data (although this is way down the line in terms of importance).&lt;/p&gt;\n\n&lt;p&gt;- yes in all applications (as you can see in the implementation) you do have to be careful to always use metadata like data movement operations (that are super fast) as not to slow down your pipelines, but there&amp;#39;s enough technology available now to make that possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xob67", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xob67/data_as_code_why_doesnt_it_catch_on_whats_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xob67/data_as_code_why_doesnt_it_catch_on_whats_your/", "subreddit_subscribers": 89001, "created_utc": 1675926523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working on a private Python package with some useful stuff in it that I use across various bits of our pipeline. \n\nI'm trying to figure out how best to host and use it. I had this idea that I could just install directly from the package GitHub repo using a pip install in the Docker images built in CICD. However, I've recently read a bit about hosting packages on AWS Codeartifact and wondered if I should actually be doing a proper package build and storing it there? I'm not sure what benefits I'd get from that versus installing direct from source on GitHub, but instinctively I like it more. Annoyingly (and surprisingly) GitHub Packages doesn't support Python.\n\nIf anyone has done something similar I'd really appreciate some advice!", "author_fullname": "t2_1znkakv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Private Python packages in pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xttw4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675946578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a private Python package with some useful stuff in it that I use across various bits of our pipeline. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out how best to host and use it. I had this idea that I could just install directly from the package GitHub repo using a pip install in the Docker images built in CICD. However, I&amp;#39;ve recently read a bit about hosting packages on AWS Codeartifact and wondered if I should actually be doing a proper package build and storing it there? I&amp;#39;m not sure what benefits I&amp;#39;d get from that versus installing direct from source on GitHub, but instinctively I like it more. Annoyingly (and surprisingly) GitHub Packages doesn&amp;#39;t support Python.&lt;/p&gt;\n\n&lt;p&gt;If anyone has done something similar I&amp;#39;d really appreciate some advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xttw4", "is_robot_indexable": true, "report_reasons": null, "author": "Psychological-Suit-5", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xttw4/private_python_packages_in_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xttw4/private_python_packages_in_pipelines/", "subreddit_subscribers": 89001, "created_utc": 1675946578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, y\u2019all \n\n\nI landed a technical interview for an MLE position at Toyota. \n\nWould yall happen to know what should I expect? I haven\u2019t found any resources online. It is a 30min online interview. \n\n\nThanks!", "author_fullname": "t2_3zbbn8s2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Toyota Machine Learning Engineer Technical interview; what to expect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xiwu8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675910073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, y\u2019all &lt;/p&gt;\n\n&lt;p&gt;I landed a technical interview for an MLE position at Toyota. &lt;/p&gt;\n\n&lt;p&gt;Would yall happen to know what should I expect? I haven\u2019t found any resources online. It is a 30min online interview. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10xiwu8", "is_robot_indexable": true, "report_reasons": null, "author": "RaunchyAppleSauce", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xiwu8/toyota_machine_learning_engineer_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xiwu8/toyota_machine_learning_engineer_technical/", "subreddit_subscribers": 89001, "created_utc": 1675910073.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nAfter evaluating needs of our team, we are looking to implement Delta on EMR for incremental data processing. I am wondering if anyone can share their experiences dealing with OSS Delta on EMR.\n\nHad decent success with PoC but looking to understand any issues to be known at scale. Be it operational, scale etc.", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Success with Delta on AWS EMR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xhiau", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675906387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;After evaluating needs of our team, we are looking to implement Delta on EMR for incremental data processing. I am wondering if anyone can share their experiences dealing with OSS Delta on EMR.&lt;/p&gt;\n\n&lt;p&gt;Had decent success with PoC but looking to understand any issues to be known at scale. Be it operational, scale etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xhiau", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xhiau/success_with_delta_on_aws_emr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xhiau/success_with_delta_on_aws_emr/", "subreddit_subscribers": 89001, "created_utc": 1675906387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel like there was a lot of noise and they showed some impressive benchmarks a while back. But from people I've spoken to and heard from at conferences I'm not aware of anyone using it. Is the project waning or just still super early? It's barely mentioned on this sub.", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use Redpanda?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xn9l4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675923013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like there was a lot of noise and they showed some impressive benchmarks a while back. But from people I&amp;#39;ve spoken to and heard from at conferences I&amp;#39;m not aware of anyone using it. Is the project waning or just still super early? It&amp;#39;s barely mentioned on this sub.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xn9l4", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xn9l4/anyone_use_redpanda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xn9l4/anyone_use_redpanda/", "subreddit_subscribers": 89001, "created_utc": 1675923013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello team,\nWe\u2019ve got an ETL pipeline that draws data from PostgreSQL, and sends it to Redshift. Yes, we\u2019re in AWS entirely. Something about the setup irks me. I\u2019ll elaborate.\n\nWe use GLUE to read from RDS Postgres. We do NOT use the Crawler because our Glue jobs use jdbc connections to read from Postgres. Now, toward the destination, we also use jdbc to write the data to Redshift. Problem(?): The target table must already exist, and its structure must be perfectly matched to the source.\n\nTo me, I was hoping that the structure would be inferred, but most importantly, created on the target (Redshift). Instead, we have to meticulously update the schema on Redshift, if the source tables change.\n\nThis raises a big questions for me - Is this normal or Best practice? Some people at my company claim that this level of awareness of the data is good. Some say its too much maintenance and error prone. What do you think?\n\nNote: You may wonder why we don\u2019t use Crawlers. I\u2019ll justify that here. For the source db, we didn\u2019t need it. We needed a jdbc connection so that we could specify custom jobBookmarkKeys. When you do that, the Glue Catalogue table isn\u2019t actually used. Superfluous. For the target, there is also no point. In the case where the table doesn\u2019t exist, the Crawler will not work. In the case that the table did exist, you wouldn\u2019t need a Crawler anyway.\n\nAny advice? Please and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manual schema updates in Data Warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xk9to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675913829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello team,\nWe\u2019ve got an ETL pipeline that draws data from PostgreSQL, and sends it to Redshift. Yes, we\u2019re in AWS entirely. Something about the setup irks me. I\u2019ll elaborate.&lt;/p&gt;\n\n&lt;p&gt;We use GLUE to read from RDS Postgres. We do NOT use the Crawler because our Glue jobs use jdbc connections to read from Postgres. Now, toward the destination, we also use jdbc to write the data to Redshift. Problem(?): The target table must already exist, and its structure must be perfectly matched to the source.&lt;/p&gt;\n\n&lt;p&gt;To me, I was hoping that the structure would be inferred, but most importantly, created on the target (Redshift). Instead, we have to meticulously update the schema on Redshift, if the source tables change.&lt;/p&gt;\n\n&lt;p&gt;This raises a big questions for me - Is this normal or Best practice? Some people at my company claim that this level of awareness of the data is good. Some say its too much maintenance and error prone. What do you think?&lt;/p&gt;\n\n&lt;p&gt;Note: You may wonder why we don\u2019t use Crawlers. I\u2019ll justify that here. For the source db, we didn\u2019t need it. We needed a jdbc connection so that we could specify custom jobBookmarkKeys. When you do that, the Glue Catalogue table isn\u2019t actually used. Superfluous. For the target, there is also no point. In the case where the table doesn\u2019t exist, the Crawler will not work. In the case that the table did exist, you wouldn\u2019t need a Crawler anyway.&lt;/p&gt;\n\n&lt;p&gt;Any advice? Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xk9to", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xk9to/manual_schema_updates_in_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xk9to/manual_schema_updates_in_data_warehouse/", "subreddit_subscribers": 89001, "created_utc": 1675913829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nI am a data science professional, currently unemployed and looking for work.  A contact told me about a potential project her company might need, and I wanted to get your input on how much work the project might take.  They need to migrate text data from 400 PDFs into a database for use by some software system.  It really does not seem that complicated, but I wanted to gauge how much work this might take to set a reasonable price and give them a realistic estimation for how long it will take to complete.\n\nAppreciate your thoughts!\n\nEdit: Thanks so much for these thoughts.  I'll get a sample PDF and see what I can do with 1 first, that's a great suggestion.  I might make a follow-up post after I've taken a look and given things an initial rundown on how to best go about doing the work.  Thanks!", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consulting Project Viability - Migrate Text from 400 PDFs Into Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xhwqp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675963240.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675907432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I am a data science professional, currently unemployed and looking for work.  A contact told me about a potential project her company might need, and I wanted to get your input on how much work the project might take.  They need to migrate text data from 400 PDFs into a database for use by some software system.  It really does not seem that complicated, but I wanted to gauge how much work this might take to set a reasonable price and give them a realistic estimation for how long it will take to complete.&lt;/p&gt;\n\n&lt;p&gt;Appreciate your thoughts!&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks so much for these thoughts.  I&amp;#39;ll get a sample PDF and see what I can do with 1 first, that&amp;#39;s a great suggestion.  I might make a follow-up post after I&amp;#39;ve taken a look and given things an initial rundown on how to best go about doing the work.  Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10xhwqp", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xhwqp/consulting_project_viability_migrate_text_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xhwqp/consulting_project_viability_migrate_text_from/", "subreddit_subscribers": 89001, "created_utc": 1675907432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Current situation: \n\n* large government contact for an agency, best job security in the country without giving away more info\n* Salary $88,500 with a 7% match to 401K, 3 weeks vacation that accrues fast, 4 weeks paternity leave, frequent raises (pay went up 14% in the past 2 years I've been here), and a 9/80 schedule. COMPLETE WFH\n* Using outdated tools (Talend, Denodo, and newer qlik replicate), we will never use the cloud, and bringing in newer technologies is a fucking nightmare that takes months if not years. \n* They are currently offering a retention program. if you sign a 2 year agreeement to stay you will receive a 15K bonus immediately (pay back if you leave of course) 10k after taxes\n* Very good work-life balance and flexibility. I never feel stressed about work and have a well managed work load with an awesome manager and a good team.\n\nFuture opportunity: \n\n* Data engineer at a large freight company that does several things in the industry. \n* Pay would be 105-125k with an annual bonus ( not sure how much) , 4% 401k match I believe, this might be wrong. Fully paid medical benefits (not sure how good coverage is), and not sure how vacation is. Haven't received the full offer yet but they are sending it over soon.\n* They use the latest and greatest tools (aws, airflow, terraform, dbt, python ) just to name a few. They move fast and can whip up new POCs very quickly. I would like to have this skill set as I grow in my carerr\n* Might not have a good work-life balance, not sure how it is there. Boss wants people in most of the time but is okay with some remote work. \n\nMy situation: We are currently doing well financially, my wife has a great job although her company is doing another round of layoffs she has been identified as a top performer and we don't really have concerns of her being effected. We recently just had a baby and she is 7 weeks old WITHOUT DAYCARE. We had our daycare lady leave the country so we are planning to WFH and take care of baby until October until she can get in. Really torn on what to do.\n\nTLDR: Have a cushy job now with a shitty toolset but offering more money, getting a offer for more money and a better toolset but would have to go into the office more with a baby at home.", "author_fullname": "t2_18qay50v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Really need help with a Career decision.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xfbh3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675900835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current situation: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;large government contact for an agency, best job security in the country without giving away more info&lt;/li&gt;\n&lt;li&gt;Salary $88,500 with a 7% match to 401K, 3 weeks vacation that accrues fast, 4 weeks paternity leave, frequent raises (pay went up 14% in the past 2 years I&amp;#39;ve been here), and a 9/80 schedule. COMPLETE WFH&lt;/li&gt;\n&lt;li&gt;Using outdated tools (Talend, Denodo, and newer qlik replicate), we will never use the cloud, and bringing in newer technologies is a fucking nightmare that takes months if not years. &lt;/li&gt;\n&lt;li&gt;They are currently offering a retention program. if you sign a 2 year agreeement to stay you will receive a 15K bonus immediately (pay back if you leave of course) 10k after taxes&lt;/li&gt;\n&lt;li&gt;Very good work-life balance and flexibility. I never feel stressed about work and have a well managed work load with an awesome manager and a good team.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Future opportunity: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data engineer at a large freight company that does several things in the industry. &lt;/li&gt;\n&lt;li&gt;Pay would be 105-125k with an annual bonus ( not sure how much) , 4% 401k match I believe, this might be wrong. Fully paid medical benefits (not sure how good coverage is), and not sure how vacation is. Haven&amp;#39;t received the full offer yet but they are sending it over soon.&lt;/li&gt;\n&lt;li&gt;They use the latest and greatest tools (aws, airflow, terraform, dbt, python ) just to name a few. They move fast and can whip up new POCs very quickly. I would like to have this skill set as I grow in my carerr&lt;/li&gt;\n&lt;li&gt;Might not have a good work-life balance, not sure how it is there. Boss wants people in most of the time but is okay with some remote work. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My situation: We are currently doing well financially, my wife has a great job although her company is doing another round of layoffs she has been identified as a top performer and we don&amp;#39;t really have concerns of her being effected. We recently just had a baby and she is 7 weeks old WITHOUT DAYCARE. We had our daycare lady leave the country so we are planning to WFH and take care of baby until October until she can get in. Really torn on what to do.&lt;/p&gt;\n\n&lt;p&gt;TLDR: Have a cushy job now with a shitty toolset but offering more money, getting a offer for more money and a better toolset but would have to go into the office more with a baby at home.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10xfbh3", "is_robot_indexable": true, "report_reasons": null, "author": "Doyale_royale", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xfbh3/really_need_help_with_a_career_decision/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xfbh3/really_need_help_with_a_career_decision/", "subreddit_subscribers": 89001, "created_utc": 1675900835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Azure data factory doesn't seem to provide direct copy activity from SAP to Databricks. One way I found was to get it in adls staging and auto create the table in databricks then send to Databricks. Are there better ways to do it.", "author_fullname": "t2_m1qiwmwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to migrate data from SAP to Databricks using ADF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xug50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675948412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Azure data factory doesn&amp;#39;t seem to provide direct copy activity from SAP to Databricks. One way I found was to get it in adls staging and auto create the table in databricks then send to Databricks. Are there better ways to do it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xug50", "is_robot_indexable": true, "report_reasons": null, "author": "uncertainBoi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xug50/what_is_the_best_way_to_migrate_data_from_sap_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xug50/what_is_the_best_way_to_migrate_data_from_sap_to/", "subreddit_subscribers": 89001, "created_utc": 1675948412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The nurse can be called to a specific room , now what is the cardinality between the nurse and the room ? ( There are multiple examination rooms and Nurse)\n\nMy logic is that:\n\n it's m:n  because any nurse can be called to any room \n\nAm I right ?", "author_fullname": "t2_2ah0kkor", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Er diagram question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xp7l7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675929752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The nurse can be called to a specific room , now what is the cardinality between the nurse and the room ? ( There are multiple examination rooms and Nurse)&lt;/p&gt;\n\n&lt;p&gt;My logic is that:&lt;/p&gt;\n\n&lt;p&gt;it&amp;#39;s m:n  because any nurse can be called to any room &lt;/p&gt;\n\n&lt;p&gt;Am I right ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xp7l7", "is_robot_indexable": true, "report_reasons": null, "author": "omidhhh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xp7l7/er_diagram_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xp7l7/er_diagram_question/", "subreddit_subscribers": 89001, "created_utc": 1675929752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I really love R, and want to use it as much as I can for analysis and visualizations.  (Although I am open to criticism)\n\nMy understanding of the two Synapse pools:\n\n1. Dedicated SQL pool: I think I should be able to fetch data with R using RODBC and the like, to work on the data in RStudio locally, in memory (assuming I pull something down that fits in local memory).  Is this true??\n\n2. Serverless SQL pool (parquet files in data lake):   Serverless natively \u201csupports R\u201d now in Azure with notebooks, but it seems like it\u2019s watered down, and not at all like running R in the RStudio IDE with the plethora of packages and features.  I may be wrong. Someone please tell me I\u2019m dumb and wrong.\n\nCan I point RStudio/RODBC to the Serverless pool?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to use R for data analysis/viz - do I have a choice between Synapse Dedicated SQL pool vs Serverless (parquet)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xndv6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675923383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really love R, and want to use it as much as I can for analysis and visualizations.  (Although I am open to criticism)&lt;/p&gt;\n\n&lt;p&gt;My understanding of the two Synapse pools:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Dedicated SQL pool: I think I should be able to fetch data with R using RODBC and the like, to work on the data in RStudio locally, in memory (assuming I pull something down that fits in local memory).  Is this true??&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Serverless SQL pool (parquet files in data lake):   Serverless natively \u201csupports R\u201d now in Azure with notebooks, but it seems like it\u2019s watered down, and not at all like running R in the RStudio IDE with the plethora of packages and features.  I may be wrong. Someone please tell me I\u2019m dumb and wrong.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can I point RStudio/RODBC to the Serverless pool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xndv6", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xndv6/i_want_to_use_r_for_data_analysisviz_do_i_have_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xndv6/i_want_to_use_r_for_data_analysisviz_do_i_have_a/", "subreddit_subscribers": 89001, "created_utc": 1675923383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Gurus,\n\nMy question may be very basic so apologies in advance.\n\nI want to implement a pyspark code that will read data from BigQuery and perform a simple spark task. I cannot install spark on my local machine so was looking for ways on how to do this on a IDE where I can use spark directly from the cluster.\n\nI do have the .json file for service account authentication.\n\nThanks!", "author_fullname": "t2_6it6xybd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connect IDE to Big Query and use Dataproc cluster\u2019s spark environment.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xfux3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675902141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Gurus,&lt;/p&gt;\n\n&lt;p&gt;My question may be very basic so apologies in advance.&lt;/p&gt;\n\n&lt;p&gt;I want to implement a pyspark code that will read data from BigQuery and perform a simple spark task. I cannot install spark on my local machine so was looking for ways on how to do this on a IDE where I can use spark directly from the cluster.&lt;/p&gt;\n\n&lt;p&gt;I do have the .json file for service account authentication.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xfux3", "is_robot_indexable": true, "report_reasons": null, "author": "bobasucks", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xfux3/connect_ide_to_big_query_and_use_dataproc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xfux3/connect_ide_to_big_query_and_use_dataproc/", "subreddit_subscribers": 89001, "created_utc": 1675902141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everybody! \n\nI am analyzing the data infrastructure and reliability for data collection in e-commerce marketplace where you can book appointments for services both from website and app. \n\nThe structure is as follows: \n\nSources: \n\n1. Google Tag Manager: in which there are tags for GA3 (UA), GA4 and Advertising Platforms (let's say acquisitions via Meta ads, Google and so on). Focusing solely on GA4, this data collection flow is considered **not reliable**. Consider that the company operates in Europe so I guess that the tracking via GA is highly biased by GDPR/Privacy problems.\n2. Snowplow.io : collects behavioral data, **reliable**.\n3. App collecting CRM data, considered **reliable.**\n\nhttps://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555\n\nMoreover, the data from:\n\n1. Advertising Platforms \n2. Snowplow events \n3. Product (CRM) data from the App \n\nare joined and used directly into Looker (apparently, without an intermediate data warehouse!)\n\nMy questions are: \n\nA. How would you run an assessment of this data structure? I would imagine to start with understanding why GA4 transactional data are wrongly collected and why they are \\*not\\* used into the Looker data. There might be consistent differences between the data in the adv APIs vs. data isn GA4.\n\n...Maybe looking in a server-side tracking solution could make this sources more reliable (for sure)?\n\n&amp;#x200B;\n\nB. Knowing that one can only trust behavioral data / CRM (so snowplow events and CRM data) how can you establish a unified source of truth?\n\nIn this case for me the idea would be to combine these data sources in a dwh/datalake first. \n\n&amp;#x200B;\n\nThanks for any hint in this analysis!", "author_fullname": "t2_bwe0zlu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evaluate reliability for a data infrastructure (e-commerce marketplace scenario)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cgk9xbon45ha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ec1f473c85355102d1bac9bd5308abf07941d69"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92a6f8205ef2ba66a37da2d57f5d9a848ab53fd2"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c58949813fc1be1b0423e8c5f01becbbb8fa50"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51835c982a9cb64b254fcee59c43beb7fe07ac3d"}, {"y": 567, "x": 960, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41137a70ba5563891a15b45611bf43414b36144b"}, {"y": 638, "x": 1080, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c96a228d3b4890c7d61249bb725db120dbb7305d"}], "s": {"y": 1136, "x": 1922, "u": "https://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555"}, "id": "cgk9xbon45ha1"}}, "name": "t3_10xrnof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DnpvPe1PyyxdDzwsPVhCV87CeCccw8OpJxT2nTvy4rs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675939097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody! &lt;/p&gt;\n\n&lt;p&gt;I am analyzing the data infrastructure and reliability for data collection in e-commerce marketplace where you can book appointments for services both from website and app. &lt;/p&gt;\n\n&lt;p&gt;The structure is as follows: &lt;/p&gt;\n\n&lt;p&gt;Sources: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Google Tag Manager: in which there are tags for GA3 (UA), GA4 and Advertising Platforms (let&amp;#39;s say acquisitions via Meta ads, Google and so on). Focusing solely on GA4, this data collection flow is considered &lt;strong&gt;not reliable&lt;/strong&gt;. Consider that the company operates in Europe so I guess that the tracking via GA is highly biased by GDPR/Privacy problems.&lt;/li&gt;\n&lt;li&gt;Snowplow.io : collects behavioral data, &lt;strong&gt;reliable&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;App collecting CRM data, considered &lt;strong&gt;reliable.&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555\"&gt;https://preview.redd.it/cgk9xbon45ha1.png?width=1922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=92a44904c3a4af43dc0c53c89d66f931b4691555&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Moreover, the data from:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Advertising Platforms &lt;/li&gt;\n&lt;li&gt;Snowplow events &lt;/li&gt;\n&lt;li&gt;Product (CRM) data from the App &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;are joined and used directly into Looker (apparently, without an intermediate data warehouse!)&lt;/p&gt;\n\n&lt;p&gt;My questions are: &lt;/p&gt;\n\n&lt;p&gt;A. How would you run an assessment of this data structure? I would imagine to start with understanding why GA4 transactional data are wrongly collected and why they are *not* used into the Looker data. There might be consistent differences between the data in the adv APIs vs. data isn GA4.&lt;/p&gt;\n\n&lt;p&gt;...Maybe looking in a server-side tracking solution could make this sources more reliable (for sure)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;B. Knowing that one can only trust behavioral data / CRM (so snowplow events and CRM data) how can you establish a unified source of truth?&lt;/p&gt;\n\n&lt;p&gt;In this case for me the idea would be to combine these data sources in a dwh/datalake first. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for any hint in this analysis!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xrnof", "is_robot_indexable": true, "report_reasons": null, "author": "xvinc666x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xrnof/evaluate_reliability_for_a_data_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xrnof/evaluate_reliability_for_a_data_infrastructure/", "subreddit_subscribers": 89001, "created_utc": 1675939097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been recently asked to look into optimizing some notebooks written using spark sql and those notebooks sometime have 4 levels of nesting in the from clause. Does this effect performance? vs writing the logic in different data frames considering Spark optimizes the query plan anyway. If so how?", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "sub queries in spark - bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1kwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675966154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been recently asked to look into optimizing some notebooks written using spark sql and those notebooks sometime have 4 levels of nesting in the from clause. Does this effect performance? vs writing the logic in different data frames considering Spark optimizes the query plan anyway. If so how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10y1kwq", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y1kwq/sub_queries_in_spark_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y1kwq/sub_queries_in_spark_bad/", "subreddit_subscribers": 89001, "created_utc": 1675966154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you ever wished you could ask any question to your data and get a fast, contextual answer?\n\nWe have some exciting news for you. Today we're launching a private beta\u00a0for \u2728 Secoda AI: the first AI suite of search, catalog, lineage, and documentation solutions, to enhance your data workspace.\n\nOur new AI Assistant \ud83e\udd16\u00a0will save you hours of manual labor by:\n\n\u2705\u00a0Surfacing insights through natural language search  \n\u2705\u00a0Automatically generating documentation  \n\u2705\u00a0Turning text into SQL\n\nLearn more about Secoda AI and how to get access to the private beta in the comments blow.\n\nTry it out today and see how it can improve your data discovery experience! \n\nLearn more about Secoda AI: [https://www.secoda.co/blog/learn-about-secoda-ai](https://www.secoda.co/blog/learn-about-secoda-ai)", "author_fullname": "t2_aiinah9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing: Secoda AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10y1eba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675965732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever wished you could ask any question to your data and get a fast, contextual answer?&lt;/p&gt;\n\n&lt;p&gt;We have some exciting news for you. Today we&amp;#39;re launching a private beta\u00a0for \u2728 Secoda AI: the first AI suite of search, catalog, lineage, and documentation solutions, to enhance your data workspace.&lt;/p&gt;\n\n&lt;p&gt;Our new AI Assistant \ud83e\udd16\u00a0will save you hours of manual labor by:&lt;/p&gt;\n\n&lt;p&gt;\u2705\u00a0Surfacing insights through natural language search&lt;br/&gt;\n\u2705\u00a0Automatically generating documentation&lt;br/&gt;\n\u2705\u00a0Turning text into SQL&lt;/p&gt;\n\n&lt;p&gt;Learn more about Secoda AI and how to get access to the private beta in the comments blow.&lt;/p&gt;\n\n&lt;p&gt;Try it out today and see how it can improve your data discovery experience! &lt;/p&gt;\n\n&lt;p&gt;Learn more about Secoda AI: &lt;a href=\"https://www.secoda.co/blog/learn-about-secoda-ai\"&gt;https://www.secoda.co/blog/learn-about-secoda-ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?auto=webp&amp;v=enabled&amp;s=8c5b8cb57e5526d06edcfa692d557d8de635a1c7", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f609b1dff87c4361f4eb611b4e0516ee8be17d67", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab583607a5d69a1fb66ed01b404ad79fd1cb4ab8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93e1fe995115293cf4c58f9f966aae2e1a97dbf1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b68e67e7d00848f42033d8553d997ae83372e1e6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6fd0043d6d5b3a3c2a7a189aaee5cd2c9291a81", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/wroBjEaz7VFRE1PTx0VDnbYR0fjawdOIwMeYAPP1pjE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc0ff6d857747ec0e900d773fbb565ce45012064", "width": 1080, "height": 607}], "variants": {}, "id": "7WY8fl_h7RsynzfO-QX-YTUd7xXR4M4ad5IO3mvCPLI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10y1eba", "is_robot_indexable": true, "report_reasons": null, "author": "secodaHQ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10y1eba/introducing_secoda_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10y1eba/introducing_secoda_ai/", "subreddit_subscribers": 89001, "created_utc": 1675965732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I regularly receive files from 3rd parties that I import into our database as temporary tables.  I'm able to join most of the records with queries, matching on people's names and other fields.  But a small percentage need to be done manually.  Is there a tool out there for filtering/searching two data sets and using the mouse to drag and drop to match the remaining records together?  I'm getting tired of copying and pasting keys into an update statement to match these records.\n\nWhen I google this problem, there are lots of fuzzy-logic tools, but I'm looking for human-operated manual matching.", "author_fullname": "t2_71qwor1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a GUI tool for manually matching and setting foreign keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvucj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675955029.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675952277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I regularly receive files from 3rd parties that I import into our database as temporary tables.  I&amp;#39;m able to join most of the records with queries, matching on people&amp;#39;s names and other fields.  But a small percentage need to be done manually.  Is there a tool out there for filtering/searching two data sets and using the mouse to drag and drop to match the remaining records together?  I&amp;#39;m getting tired of copying and pasting keys into an update statement to match these records.&lt;/p&gt;\n\n&lt;p&gt;When I google this problem, there are lots of fuzzy-logic tools, but I&amp;#39;m looking for human-operated manual matching.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xvucj", "is_robot_indexable": true, "report_reasons": null, "author": "Jz7t6ak5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvucj/is_there_a_gui_tool_for_manually_matching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvucj/is_there_a_gui_tool_for_manually_matching_and/", "subreddit_subscribers": 89001, "created_utc": 1675952277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Situation: I have a pyspark job that needs to be deployed on a cluster that has SparkonK8s support. A pipeline is in place that builds an image and pushes an image to azure container registry and now there is Airflow job that refers to that image having a DAG that runs this pyspark job periodically. \nI have secrets that are being utilised by this job that need to be populated to the environment as environment varibales. These secrets are placed in azure ado library as a variable group connected to a keyvault and have been authorised for pipeline usage.\n\nProblem: Now I have an issue populating the environment variables with these secrets in the environment variables.\n\nMy approach:\n1.  I have tried populating the secrets through the pipeline itself using Bash @3 task/Powershell@2 task. I try to print them in the pipeline and they are successfully printed as well. But when i run kubectl apply to my spark config file to check if the environment has variables I don't see them.\n2. Created a shell script that would login through the azure service principal, connect to the key vault and export the secrets using export command in bash(This approach works really well in local). I trigger this script through python file it populates the environment but my job which is in a spark session isn't able to get the environment variables.\n\nQuestions\n1.Where I should be assigning the secrets as an environment variables?\n2. What should the ideal flow.\n3. If I am triggering the python script why is the pyspark job not able to get the secrets?\n\nDue to this issue the entire deployment of jobs is kind of stuck.\nPlease help, any kind of discussions or help is greatly appreciated?\n\n#pyspark #airflow #kubernetes #sparkonk8s #secrets", "author_fullname": "t2_389zs7ih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "But it works in my local\ud83d\ude29.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xvk5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675951526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Situation: I have a pyspark job that needs to be deployed on a cluster that has SparkonK8s support. A pipeline is in place that builds an image and pushes an image to azure container registry and now there is Airflow job that refers to that image having a DAG that runs this pyspark job periodically. \nI have secrets that are being utilised by this job that need to be populated to the environment as environment varibales. These secrets are placed in azure ado library as a variable group connected to a keyvault and have been authorised for pipeline usage.&lt;/p&gt;\n\n&lt;p&gt;Problem: Now I have an issue populating the environment variables with these secrets in the environment variables.&lt;/p&gt;\n\n&lt;p&gt;My approach:\n1.  I have tried populating the secrets through the pipeline itself using Bash @3 task/Powershell@2 task. I try to print them in the pipeline and they are successfully printed as well. But when i run kubectl apply to my spark config file to check if the environment has variables I don&amp;#39;t see them.\n2. Created a shell script that would login through the azure service principal, connect to the key vault and export the secrets using export command in bash(This approach works really well in local). I trigger this script through python file it populates the environment but my job which is in a spark session isn&amp;#39;t able to get the environment variables.&lt;/p&gt;\n\n&lt;p&gt;Questions\n1.Where I should be assigning the secrets as an environment variables?\n2. What should the ideal flow.\n3. If I am triggering the python script why is the pyspark job not able to get the secrets?&lt;/p&gt;\n\n&lt;p&gt;Due to this issue the entire deployment of jobs is kind of stuck.\nPlease help, any kind of discussions or help is greatly appreciated?&lt;/p&gt;\n\n&lt;h1&gt;pyspark #airflow #kubernetes #sparkonk8s #secrets&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xvk5g", "is_robot_indexable": true, "report_reasons": null, "author": "shrey2204", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xvk5g/but_it_works_in_my_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xvk5g/but_it_works_in_my_local/", "subreddit_subscribers": 89001, "created_utc": 1675951526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We don't have any data dictionary and governance over data sets. So want to start with, business glossary(someone to take ownership), data dictionary and a data discovery product.\n\nOther things to consider are data quality and metadata management without any additional tooling.\n\nAny guidance on open source vs SaaS, want to get better insights before POC?", "author_fullname": "t2_7lah4x4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data discovery product?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xjy88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675912927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We don&amp;#39;t have any data dictionary and governance over data sets. So want to start with, business glossary(someone to take ownership), data dictionary and a data discovery product.&lt;/p&gt;\n\n&lt;p&gt;Other things to consider are data quality and metadata management without any additional tooling.&lt;/p&gt;\n\n&lt;p&gt;Any guidance on open source vs SaaS, want to get better insights before POC?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xjy88", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Whereas_3564", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xjy88/data_discovery_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xjy88/data_discovery_product/", "subreddit_subscribers": 89001, "created_utc": 1675912927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, it's been a while since I've developed data pipelines with any tool, but I recently created a tutorial for calling REST API's with Azure Data Factory. Wanted to see if anyone can provide any feedback on it (good bad). My actual project is really the API for zip codes, but I'm trying to prove the value of adding this type of REST API calls to data pipelines.  (if this has good value i can create tutorials for other tools as well). \n\n[Dynamically Invoking REST API with Data Factory](https://www.metadapi.com/Blog/dynamically-invoking-rest-api-with-data-factory)", "author_fullname": "t2_imia197u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting Feedback on Blog Article - Azure Data Factory and REST API's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xhf9s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675906174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, it&amp;#39;s been a while since I&amp;#39;ve developed data pipelines with any tool, but I recently created a tutorial for calling REST API&amp;#39;s with Azure Data Factory. Wanted to see if anyone can provide any feedback on it (good bad). My actual project is really the API for zip codes, but I&amp;#39;m trying to prove the value of adding this type of REST API calls to data pipelines.  (if this has good value i can create tutorials for other tools as well). &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.metadapi.com/Blog/dynamically-invoking-rest-api-with-data-factory\"&gt;Dynamically Invoking REST API with Data Factory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Tt-agP-Vy8DNIi2WI_713PoDe0gq_Kg-EGzu5-5h3z4.jpg?auto=webp&amp;v=enabled&amp;s=9e86b4097bad780f796974bb2cfd735e687223a5", "width": 526, "height": 393}, "resolutions": [{"url": "https://external-preview.redd.it/Tt-agP-Vy8DNIi2WI_713PoDe0gq_Kg-EGzu5-5h3z4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca4716770c63b16a49758884bdd5e7a03f09fa68", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/Tt-agP-Vy8DNIi2WI_713PoDe0gq_Kg-EGzu5-5h3z4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=019721352e560d4e39a4fd6cbe2cafee46b48936", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/Tt-agP-Vy8DNIi2WI_713PoDe0gq_Kg-EGzu5-5h3z4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a16406c29c2fd3e9a2fd25afc0258a0b2665831", "width": 320, "height": 239}], "variants": {}, "id": "MzFOhk0OqBGy1Jd_wFSfYDBfBZ2SDIiuB8uWxFPpmVg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10xhf9s", "is_robot_indexable": true, "report_reasons": null, "author": "Pty_Rick", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xhf9s/requesting_feedback_on_blog_article_azure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xhf9s/requesting_feedback_on_blog_article_azure_data/", "subreddit_subscribers": 89001, "created_utc": 1675906174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, this Will be my First post ever on reddit and  I'm a New aspirante to data enginner.\n\nMy Company have a database on aws. To connect to this database i need to use a VPN. So, my question is: I want to build a serverless etl process, using aws lambda, airflow, databricks or other methods. What is the best way to accomplish that knowing that i'll have to connect to a vpn in order to access the database.\n\nEddit: Adding context\n\nI have some ETL Scripts in python that i would like to automate their execution. I want to automate It on a cloud service like aws, heroku, azure etc, but How can i accomplish that If its necessary to be connected to a vpn ir order to access the database?", "author_fullname": "t2_17f19lgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline and vpn connection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xgbig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675906161.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675903300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, this Will be my First post ever on reddit and  I&amp;#39;m a New aspirante to data enginner.&lt;/p&gt;\n\n&lt;p&gt;My Company have a database on aws. To connect to this database i need to use a VPN. So, my question is: I want to build a serverless etl process, using aws lambda, airflow, databricks or other methods. What is the best way to accomplish that knowing that i&amp;#39;ll have to connect to a vpn in order to access the database.&lt;/p&gt;\n\n&lt;p&gt;Eddit: Adding context&lt;/p&gt;\n\n&lt;p&gt;I have some ETL Scripts in python that i would like to automate their execution. I want to automate It on a cloud service like aws, heroku, azure etc, but How can i accomplish that If its necessary to be connected to a vpn ir order to access the database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10xgbig", "is_robot_indexable": true, "report_reasons": null, "author": "Seyrenz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xgbig/pipeline_and_vpn_connection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xgbig/pipeline_and_vpn_connection/", "subreddit_subscribers": 89001, "created_utc": 1675903300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, \n\nI'm a junior data engineer and still has a lot of learning to do about how to design a model which can encapsulate both ERP and Big Data, so this has always been in the back of my mind: how do you handle time series/big data for an ERP system database design? \n\n&amp;#x200B;\n\nFor instance, if we were a manufacturing company and wanted to have traceability on what we make, sell and buy, but also manufacturing details on how the products were made (time series measurements) for data science &amp; BI.  \n\n&amp;#x200B;\n\n\\- Traceability: a relational database is preferred as you need consistency and integrity (SQL) \n\n&amp;#x200B;\n\n\\- Time series: the data can become big very quickly so something horizontally scalable is preferred (NoSQL)  \n\n&amp;#x200B;\n\nHow do you link these two databases together with a \"foreign key\" while keeping data integrity? What are the good practices for situations like this? How would you do it differently?", "author_fullname": "t2_bu40k4xv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ERP system and Big Data applications? (manufacturing)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10xeil2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675898908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a junior data engineer and still has a lot of learning to do about how to design a model which can encapsulate both ERP and Big Data, so this has always been in the back of my mind: how do you handle time series/big data for an ERP system database design? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For instance, if we were a manufacturing company and wanted to have traceability on what we make, sell and buy, but also manufacturing details on how the products were made (time series measurements) for data science &amp;amp; BI.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Traceability: a relational database is preferred as you need consistency and integrity (SQL) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Time series: the data can become big very quickly so something horizontally scalable is preferred (NoSQL)  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do you link these two databases together with a &amp;quot;foreign key&amp;quot; while keeping data integrity? What are the good practices for situations like this? How would you do it differently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10xeil2", "is_robot_indexable": true, "report_reasons": null, "author": "NodeJsUser", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10xeil2/erp_system_and_big_data_applications_manufacturing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10xeil2/erp_system_and_big_data_applications_manufacturing/", "subreddit_subscribers": 89001, "created_utc": 1675898908.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}