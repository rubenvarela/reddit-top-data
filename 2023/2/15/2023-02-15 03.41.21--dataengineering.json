{"kind": "Listing", "data": {"after": "t3_112f685", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the past two years I switched jobs two times, I find it hard in the long run to still be stimulated by what I do on a daily basis, or even to keep learning. But switching also has cons because it means getting to discover a whole other environment and starting over again from scratch in terms of knowledge of the industry you work on. \n\nI am aware that I can try and learn new things on the side of work to keep growing technically, but I find it very hard to learn alone about a subject if it doesn\u2019t apply to challenges I face while trying to do my tasks, I can\u2019t establish a clear roadmap of what I should learn or read, it feels too abstract, while if I have a challenge related to my job I know specifically what to look for and what to document myself on.\n\nSo my question is are you still able to find stimulating tasks in your daily activities as months go by, if so how? Do you learn new things on the side? Experience more routine in your job?", "author_fullname": "t2_6jles2jd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you find your daily tasks interesting / stimulating?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1124c0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676378086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past two years I switched jobs two times, I find it hard in the long run to still be stimulated by what I do on a daily basis, or even to keep learning. But switching also has cons because it means getting to discover a whole other environment and starting over again from scratch in terms of knowledge of the industry you work on. &lt;/p&gt;\n\n&lt;p&gt;I am aware that I can try and learn new things on the side of work to keep growing technically, but I find it very hard to learn alone about a subject if it doesn\u2019t apply to challenges I face while trying to do my tasks, I can\u2019t establish a clear roadmap of what I should learn or read, it feels too abstract, while if I have a challenge related to my job I know specifically what to look for and what to document myself on.&lt;/p&gt;\n\n&lt;p&gt;So my question is are you still able to find stimulating tasks in your daily activities as months go by, if so how? Do you learn new things on the side? Experience more routine in your job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1124c0r", "is_robot_indexable": true, "report_reasons": null, "author": "barbapapalone", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1124c0r/do_you_find_your_daily_tasks_interesting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1124c0r/do_you_find_your_daily_tasks_interesting/", "subreddit_subscribers": 89605, "created_utc": 1676378086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering\n\nI wrote a [short tutorial on how to run PySpark in a Jupyter notebook](https://www.datain30.com/p/run-spark-with-jupyter-using-docker). Let me know if you try it out and are able to finish in under 30 minutes.", "author_fullname": "t2_vri7kka6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A short tutorial on running Spark with Jupyter using Docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112dz5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676403219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wrote a &lt;a href=\"https://www.datain30.com/p/run-spark-with-jupyter-using-docker\"&gt;short tutorial on how to run PySpark in a Jupyter notebook&lt;/a&gt;. Let me know if you try it out and are able to finish in under 30 minutes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E8FxIvXsoENn6VaVaJnNu-X9kY6ROc1nWGKZquDZqpY.jpg?auto=webp&amp;v=enabled&amp;s=d57acccd37cd2dc1ed1aeda26688d7d7b051a00a", "width": 256, "height": 256}, "resolutions": [{"url": "https://external-preview.redd.it/E8FxIvXsoENn6VaVaJnNu-X9kY6ROc1nWGKZquDZqpY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d03520cfe2eaabc83c52d699ad2f7dd25e528a42", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/E8FxIvXsoENn6VaVaJnNu-X9kY6ROc1nWGKZquDZqpY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc5d9d0177622d49dab11896441e5572bad8637a", "width": 216, "height": 216}], "variants": {}, "id": "fZc830sC60fmrpFMh1DzkGdJurbm_CKNAvt6rJyk86E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "112dz5s", "is_robot_indexable": true, "report_reasons": null, "author": "datain30", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112dz5s/a_short_tutorial_on_running_spark_with_jupyter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112dz5s/a_short_tutorial_on_running_spark_with_jupyter/", "subreddit_subscribers": 89605, "created_utc": 1676403219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Anyone relate ? \ud83d\ude02](https://preview.redd.it/0a7oanyld5ia1.png?width=794&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aeef29539ee5f243953cb1d27e4bffd7042405f2)", "author_fullname": "t2_txucj9ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science model goes to production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0a7oanyld5ia1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 120, "x": 108, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=313c8d88fe5ec1f8e9eb56350cfe9b541c6f50ba"}, {"y": 241, "x": 216, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bcbb0fc719c0e46cda772897d4c69df356cd97c"}, {"y": 357, "x": 320, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fda46028a6593c9fdfc98526785ebb579280ca85"}, {"y": 714, "x": 640, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=462a3838d03a60160336f4b3f76df0ba240434ae"}], "s": {"y": 886, "x": 794, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=794&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aeef29539ee5f243953cb1d27e4bffd7042405f2"}, "id": "0a7oanyld5ia1"}}, "name": "t3_11247gg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/IsrGi0XcqxJ93lSqk_wtestGqaNQMk1QmaK53B06GE0.jpg", "edited": 1676390783.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676377700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0a7oanyld5ia1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=aeef29539ee5f243953cb1d27e4bffd7042405f2\"&gt;Anyone relate ? \ud83d\ude02&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11247gg", "is_robot_indexable": true, "report_reasons": null, "author": "nxt-engineering", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11247gg/data_science_model_goes_to_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11247gg/data_science_model_goes_to_production/", "subreddit_subscribers": 89605, "created_utc": 1676377700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've just been offered this DE position, and as it will be my first ever, I've wanted to get your opinion on the tech stack they use:\n\nOracle, Hadoop (Cloudera), Nifi, Kafka, Airflow, Flink, MLFlow, GCS, BigQuery\n\nWill working with these technologies give me an edge when looking for roles in other companies? Thanks", "author_fullname": "t2_777e0jj2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this tech stack good for my career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122wme", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676373228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve just been offered this DE position, and as it will be my first ever, I&amp;#39;ve wanted to get your opinion on the tech stack they use:&lt;/p&gt;\n\n&lt;p&gt;Oracle, Hadoop (Cloudera), Nifi, Kafka, Airflow, Flink, MLFlow, GCS, BigQuery&lt;/p&gt;\n\n&lt;p&gt;Will working with these technologies give me an edge when looking for roles in other companies? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1122wme", "is_robot_indexable": true, "report_reasons": null, "author": "lil_colon_69", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1122wme/is_this_tech_stack_good_for_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1122wme/is_this_tech_stack_good_for_my_career/", "subreddit_subscribers": 89605, "created_utc": 1676373228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a string ID per each customer table which we use to join the tables, with 30+ characters per string. I created a new column using farm_fingerprint to hash the string to int. However, in my testing, the joins are now slower on some tables. I can't figure out why this behaviour would be happening. Would anybody have any insight?", "author_fullname": "t2_2gzsok4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hashed strings slower on joins than strings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111x9jg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676361542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a string ID per each customer table which we use to join the tables, with 30+ characters per string. I created a new column using farm_fingerprint to hash the string to int. However, in my testing, the joins are now slower on some tables. I can&amp;#39;t figure out why this behaviour would be happening. Would anybody have any insight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111x9jg", "is_robot_indexable": true, "report_reasons": null, "author": "ciarandeceol1", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111x9jg/hashed_strings_slower_on_joins_than_strings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111x9jg/hashed_strings_slower_on_joins_than_strings/", "subreddit_subscribers": 89605, "created_utc": 1676361542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel more connected to DE, it is more suitable for me, but I keep hearing from all around me and especially in this subreddit that DE is like taking the easier route,and is not guaranteed as CS.\n\nDISCLAIMER: the major exists,*it is real*.\n\nThought's?", "author_fullname": "t2_tzb9l6f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is having a CS degree better than a DE degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111tz3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676356238.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676349616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel more connected to DE, it is more suitable for me, but I keep hearing from all around me and especially in this subreddit that DE is like taking the easier route,and is not guaranteed as CS.&lt;/p&gt;\n\n&lt;p&gt;DISCLAIMER: the major exists,&lt;em&gt;it is real&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;Thought&amp;#39;s?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111tz3j", "is_robot_indexable": true, "report_reasons": null, "author": "khtoto", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111tz3j/is_having_a_cs_degree_better_than_a_de_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111tz3j/is_having_a_cs_degree_better_than_a_de_degree/", "subreddit_subscribers": 89605, "created_utc": 1676349616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some help with Lyft/Uber sftp integration, from folks who have a successfully running integration with these sources. \n\nThe challenge we are facing is that we get multiple files with a different set of columns, which also keep changing. Also the files do not land at a consistent time every day.\n\nWould love to know how you are doing this. \n\nThanks.", "author_fullname": "t2_26djbmc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lyft/Uber sftp integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1126942", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676390244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676383770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some help with Lyft/Uber sftp integration, from folks who have a successfully running integration with these sources. &lt;/p&gt;\n\n&lt;p&gt;The challenge we are facing is that we get multiple files with a different set of columns, which also keep changing. Also the files do not land at a consistent time every day.&lt;/p&gt;\n\n&lt;p&gt;Would love to know how you are doing this. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1126942", "is_robot_indexable": true, "report_reasons": null, "author": "sanimesa", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1126942/lyftuber_sftp_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1126942/lyftuber_sftp_integration/", "subreddit_subscribers": 89605, "created_utc": 1676383770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, corporate finance worker who\u2019s been considering moving over to Data Engineering for a couple of months. \n\nI\u2019ve been following AI for a while now, and it seems like we\u2019re at the bottom of some sort of exponential curve in the applications in practical settings, and I\u2019m getting the feeling that a lot of things are going to change in the white collar space.\n\nI have been trying to think ahead of the curve as far as my career goes, and I had a thought today, and wanted some feedback from people who are experienced in the field.\n\nI\u2019m thinking that AI will lead to a boon in DE jobs, as in the future, tech companies will begin packaging SAAS AI programs that are trained on a specific company\u2019s data (financials, operational data, correspondence, emails) to eventually replace or augment many white collar roles. However most of us know very well that company data is disjointed and often garbage, hence the demand for engineers to bridge the gap.\n\nDoes this theory sound plausible? Forgive me if this post is too outlandish.", "author_fullname": "t2_11jujz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering+AI automation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111w62d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676358061.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676357229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, corporate finance worker who\u2019s been considering moving over to Data Engineering for a couple of months. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been following AI for a while now, and it seems like we\u2019re at the bottom of some sort of exponential curve in the applications in practical settings, and I\u2019m getting the feeling that a lot of things are going to change in the white collar space.&lt;/p&gt;\n\n&lt;p&gt;I have been trying to think ahead of the curve as far as my career goes, and I had a thought today, and wanted some feedback from people who are experienced in the field.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking that AI will lead to a boon in DE jobs, as in the future, tech companies will begin packaging SAAS AI programs that are trained on a specific company\u2019s data (financials, operational data, correspondence, emails) to eventually replace or augment many white collar roles. However most of us know very well that company data is disjointed and often garbage, hence the demand for engineers to bridge the gap.&lt;/p&gt;\n\n&lt;p&gt;Does this theory sound plausible? Forgive me if this post is too outlandish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111w62d", "is_robot_indexable": true, "report_reasons": null, "author": "RadiantVessel", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111w62d/data_engineeringai_automation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111w62d/data_engineeringai_automation/", "subreddit_subscribers": 89605, "created_utc": 1676357229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on \"Databricks \u2764\ufe0f IDEs\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_112l0h5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/SOrJwgEmcji99qBB7f8cs3zClfI4s7tDtiqx1eSlcEw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676421566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/blog/2023/02/14/announcing-a-native-visual-studio-code-experience-for-databricks.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?auto=webp&amp;v=enabled&amp;s=542ffe72699f182145030ab57fceb0690e556eae", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce9048c75face835b64732dc192c4e59b1a1b4ea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ef884a4cb10bb33fd097dd0207db4e0e56368e3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99ee41eb1c1482daac0735c971f8c1a20d8506c4", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7f569924f0a22439680dbf1be5e394df5ab2027", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b42ac75475681132a1c658a834f6d449b4ed4c16", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143fe5376bacaa2bf617398cfa8585d1b2ea2773", "width": 1080, "height": 565}], "variants": {}, "id": "GtRADiMcQfJdZjMrWJwz2KJVAao1Bb5LSMuNacp0rVI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "112l0h5", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112l0h5/thoughts_on_databricks_ides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/blog/2023/02/14/announcing-a-native-visual-studio-code-experience-for-databricks.html", "subreddit_subscribers": 89605, "created_utc": 1676421566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've been working out of the same workspace for the past year slowly accumulating tech debt, etc. \n\nWe're looking to onboard more parts of the company and use unity catalog for the data lineage and setting up proper dev/staging/prod environments and by business unit (DE/DS/Reporting).\n\nI plan on starting out with just the DE work, creating a DE dev environment and porting/updating our current production workflow.  Once I'm finished with that and all QA checks pass, I plan on cloning that workspace to serve as the production.\n\nThe production and development workspace will utilize the same bitbucket repo, but the dev workspace will be defaulting to the development branch whereas production will be synced to the main branch. I'll then use CI/CD to automate the workflow so that any accepted pull that merges to dev updates the Databricks Dev workspace. And any accepted pull that merges from dev to prod updates the Databricks Prod workspace.\n\nOne thing I'm fuzzy on is how the object storage paths work in this scenario. Is everything inside of the root metastore (unity catalog) folder and it creates a folder for each workspace you create? So if I have two scripts writing the same table, but in different workspaces, will unity catalog automatically organize the file paths into  &lt;unity catalog&gt;/ &lt;workspace&gt;/&lt;schema&gt;/&lt;table&gt;/?\n\nI also don't want to drown in setting up credentials. So for now, I plan on doing one IAM role per workspace and utilizing Unity Catalog's roles to manage access.\n\nDoes that sound like a good path? Any gotcha's I may encounter or things I should look out for that may come bite me in the ass later?", "author_fullname": "t2_rdm9w1f0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About to jump into Databrick's Unity Catalog. Anyone have any \"gotchas\" they'd like to share?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1129oat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676392446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working out of the same workspace for the past year slowly accumulating tech debt, etc. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking to onboard more parts of the company and use unity catalog for the data lineage and setting up proper dev/staging/prod environments and by business unit (DE/DS/Reporting).&lt;/p&gt;\n\n&lt;p&gt;I plan on starting out with just the DE work, creating a DE dev environment and porting/updating our current production workflow.  Once I&amp;#39;m finished with that and all QA checks pass, I plan on cloning that workspace to serve as the production.&lt;/p&gt;\n\n&lt;p&gt;The production and development workspace will utilize the same bitbucket repo, but the dev workspace will be defaulting to the development branch whereas production will be synced to the main branch. I&amp;#39;ll then use CI/CD to automate the workflow so that any accepted pull that merges to dev updates the Databricks Dev workspace. And any accepted pull that merges from dev to prod updates the Databricks Prod workspace.&lt;/p&gt;\n\n&lt;p&gt;One thing I&amp;#39;m fuzzy on is how the object storage paths work in this scenario. Is everything inside of the root metastore (unity catalog) folder and it creates a folder for each workspace you create? So if I have two scripts writing the same table, but in different workspaces, will unity catalog automatically organize the file paths into  &amp;lt;unity catalog&amp;gt;/ &amp;lt;workspace&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;table&amp;gt;/?&lt;/p&gt;\n\n&lt;p&gt;I also don&amp;#39;t want to drown in setting up credentials. So for now, I plan on doing one IAM role per workspace and utilizing Unity Catalog&amp;#39;s roles to manage access.&lt;/p&gt;\n\n&lt;p&gt;Does that sound like a good path? Any gotcha&amp;#39;s I may encounter or things I should look out for that may come bite me in the ass later?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1129oat", "is_robot_indexable": true, "report_reasons": null, "author": "keeney_arcadia", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1129oat/about_to_jump_into_databricks_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1129oat/about_to_jump_into_databricks_unity_catalog/", "subreddit_subscribers": 89605, "created_utc": 1676392446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, this is a question to Polars users. How confident do you feel about Polars being a Pandas replacement? Have you already or are thinking of switching your production code to Polars? How is/was the experience so far?\n\n\\---\n\nA little background.\n\nI am experimenting with Polars and was trying to replace reading parquet files from Pandas to it because I really enjoyed the speed bump. However there are some nuances that seems very strange. For example, those files are written with PySpark, usually they have one partition. So the dir structure is like this:\n\n    .\n    \u2514\u2500\u2500 master_data.snappy.parquet\n        \u251c\u2500\u2500 _SUCCESS\n        \u251c\u2500\u2500 country=Albania\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n        \u251c\u2500\u2500 country=Algeria\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n        ...\n\nFrom the [documentation](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html#polars-read-parquet) I see that it should be very straightforward to switch Pandas to Polars in this regard:\n\n    import pandas as pd\n    import polars as pl\n    \n    # This works\n    dd = pd.read_parquet(\"cm_mbs_spend_v3.snappy.parquet\")\n    # This doesn't\n    dd = pd.read_parquet(\"cm_mbs_spend_v3.snappy.parquet\")\n\nThe Polars version throws this error:\n\n&gt;IsADirectoryError: Expected a file path; 'cm\\_mbs\\_spend\\_v3.snappy.parquet' is a directory\n\nPolars should be able to understand partitioned dataset. However from my experiment I realized the dataset cannot have any extra meta files like `_SUCCESS`. So if you do this, it works:\n\n    dd = pl.read_parquet(\"master_data.snappy.parquet/**/*.parquet\")", "author_fullname": "t2_12lkky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replace Pandas with Parquet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11234oo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676382074.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676374040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, this is a question to Polars users. How confident do you feel about Polars being a Pandas replacement? Have you already or are thinking of switching your production code to Polars? How is/was the experience so far?&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;A little background.&lt;/p&gt;\n\n&lt;p&gt;I am experimenting with Polars and was trying to replace reading parquet files from Pandas to it because I really enjoyed the speed bump. However there are some nuances that seems very strange. For example, those files are written with PySpark, usually they have one partition. So the dir structure is like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;.\n\u2514\u2500\u2500 master_data.snappy.parquet\n    \u251c\u2500\u2500 _SUCCESS\n    \u251c\u2500\u2500 country=Albania\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n    \u251c\u2500\u2500 country=Algeria\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n    ...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;From the &lt;a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html#polars-read-parquet\"&gt;documentation&lt;/a&gt; I see that it should be very straightforward to switch Pandas to Polars in this regard:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pandas as pd\nimport polars as pl\n\n# This works\ndd = pd.read_parquet(&amp;quot;cm_mbs_spend_v3.snappy.parquet&amp;quot;)\n# This doesn&amp;#39;t\ndd = pd.read_parquet(&amp;quot;cm_mbs_spend_v3.snappy.parquet&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The Polars version throws this error:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;IsADirectoryError: Expected a file path; &amp;#39;cm_mbs_spend_v3.snappy.parquet&amp;#39; is a directory&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Polars should be able to understand partitioned dataset. However from my experiment I realized the dataset cannot have any extra meta files like &lt;code&gt;_SUCCESS&lt;/code&gt;. So if you do this, it works:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dd = pl.read_parquet(&amp;quot;master_data.snappy.parquet/**/*.parquet&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11234oo", "is_robot_indexable": true, "report_reasons": null, "author": "ratulotron", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11234oo/replace_pandas_with_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11234oo/replace_pandas_with_parquet/", "subreddit_subscribers": 89605, "created_utc": 1676374040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I thought this was a useful article [**Unit Testing for Data Engineers**](https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers). It covers the variety of reasons why people don't unit test, and suggests some tips for writing testable code. \n\nI wonder though, do DEs really not write unit tests? What's your experience? would love to know.", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit Testing for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11217n7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676369531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought this was a useful article &lt;a href=\"https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers\"&gt;&lt;strong&gt;Unit Testing for Data Engineers&lt;/strong&gt;&lt;/a&gt;. It covers the variety of reasons why people don&amp;#39;t unit test, and suggests some tips for writing testable code. &lt;/p&gt;\n\n&lt;p&gt;I wonder though, do DEs really not write unit tests? What&amp;#39;s your experience? would love to know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?auto=webp&amp;v=enabled&amp;s=1b3b7c5418d793ecc90f7af41dd70d237c1587c0", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dc65a9ff168e3d268e62fc66ca617575d19f7a8", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a80233610d688ad8682934ed511c36e0858ebe5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c26c7c6829f1d139b4476d6ba2153609e3833a4", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62eaba84e361dd7899286abe3071118182c7beda", "width": 640, "height": 480}], "variants": {}, "id": "1GzRNniO0GB5cJxUYNRFNIyvjlQ3ogl487sx3wSKEhA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11217n7", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11217n7/unit_testing_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11217n7/unit_testing_for_data_engineers/", "subreddit_subscribers": 89605, "created_utc": 1676369531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a data engineer for the past 3 years, and Iam looking to change jobs because I feel like my growth has stalled. I don\u2019t come from a cs background, I moved from a data analyst to data engineer. Because of this I feel major imposter syndrome when interviewing. When interviewing I feel so overwhelmed and I\u2019m wondering if I should stop looking for data engineering roles. I use python, sql, docker, git, and cloud technologies but I can\u2019t help but feel less of a data engineer because I don\u2019t have the formal knowledge like data structures and algorithms or time complexity. Should I look for data analyst roles instead?", "author_fullname": "t2_6let0rxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feeling stuck", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112by7k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676398160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a data engineer for the past 3 years, and Iam looking to change jobs because I feel like my growth has stalled. I don\u2019t come from a cs background, I moved from a data analyst to data engineer. Because of this I feel major imposter syndrome when interviewing. When interviewing I feel so overwhelmed and I\u2019m wondering if I should stop looking for data engineering roles. I use python, sql, docker, git, and cloud technologies but I can\u2019t help but feel less of a data engineer because I don\u2019t have the formal knowledge like data structures and algorithms or time complexity. Should I look for data analyst roles instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "112by7k", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Luck40", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112by7k/feeling_stuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112by7k/feeling_stuck/", "subreddit_subscribers": 89605, "created_utc": 1676398160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As mentioned in the title, I am the only \"data engineer\" in my current company. Everyone else is either a business user or web developer. Due to multiple reasons, I have become the go to data engineering guy who gets all the oddball data engineering tickets. While I am always able to get the job done, there is no senior guy with experience in data engineering to teach me best practices. I want to switch my job in next 1 year but don't know where I stand currently. Here is my current skill level description \n\nYears of experience: 1 year \n\nData orchestration: Currently, I am able to use airflow. I can write my own simple dags and debug issues with existing dags. \n\nSpark: I am able to use Spark API in both Scala and Python to create dataframes of source data and transform it as per my requirements using spark data frame functions \n\nPython: I am fairly proficient in Python and I have been able to debug any issues using basic googling. I regularly leverage Python libraries like Pandas, Cxoracle, requests, Beautiful soup, dbt. \n\nShell: While, I have not written any new code in bash, I can figure out most bash scripts quickly by going through them and make any bug fixes as required. \n\nAWS: I am an AWS certified developer and I am currently preparing for the AWS certified solutions architect certification \n\nAzure: I am an Azure certified data engineer.\n\n\nTableau and Power BI: I am able to make simple dashboards in Tableau and Power BI\n\nSQL: I am able to write SQL Query using concepts like joins, window functions and CTE.\nI have worked with both relational databases like Microsoft sql, oracle and no sql databases like mongodb\n\nCan any experienced data engineer review my skill set and let me know how much I need to improve to become employable as a real data engineer?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am the only \"data engineer\" in my company. I need an honest assessment of my current skill level. Currently, I have no guidance and frame of reference. As a result, I am afraid to switch. Can experienced data engineers please review my skills and suggest where I need to improve.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112anao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676394874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As mentioned in the title, I am the only &amp;quot;data engineer&amp;quot; in my current company. Everyone else is either a business user or web developer. Due to multiple reasons, I have become the go to data engineering guy who gets all the oddball data engineering tickets. While I am always able to get the job done, there is no senior guy with experience in data engineering to teach me best practices. I want to switch my job in next 1 year but don&amp;#39;t know where I stand currently. Here is my current skill level description &lt;/p&gt;\n\n&lt;p&gt;Years of experience: 1 year &lt;/p&gt;\n\n&lt;p&gt;Data orchestration: Currently, I am able to use airflow. I can write my own simple dags and debug issues with existing dags. &lt;/p&gt;\n\n&lt;p&gt;Spark: I am able to use Spark API in both Scala and Python to create dataframes of source data and transform it as per my requirements using spark data frame functions &lt;/p&gt;\n\n&lt;p&gt;Python: I am fairly proficient in Python and I have been able to debug any issues using basic googling. I regularly leverage Python libraries like Pandas, Cxoracle, requests, Beautiful soup, dbt. &lt;/p&gt;\n\n&lt;p&gt;Shell: While, I have not written any new code in bash, I can figure out most bash scripts quickly by going through them and make any bug fixes as required. &lt;/p&gt;\n\n&lt;p&gt;AWS: I am an AWS certified developer and I am currently preparing for the AWS certified solutions architect certification &lt;/p&gt;\n\n&lt;p&gt;Azure: I am an Azure certified data engineer.&lt;/p&gt;\n\n&lt;p&gt;Tableau and Power BI: I am able to make simple dashboards in Tableau and Power BI&lt;/p&gt;\n\n&lt;p&gt;SQL: I am able to write SQL Query using concepts like joins, window functions and CTE.\nI have worked with both relational databases like Microsoft sql, oracle and no sql databases like mongodb&lt;/p&gt;\n\n&lt;p&gt;Can any experienced data engineer review my skill set and let me know how much I need to improve to become employable as a real data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112anao", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112anao/i_am_the_only_data_engineer_in_my_company_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112anao/i_am_the_only_data_engineer_in_my_company_i_need/", "subreddit_subscribers": 89605, "created_utc": 1676394874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, I am new to this data engineering world and was wondering how does a large company which doesn't have any infrastructure for data can make a transition to a more structured environment?\n\nI mean, how do you standarize things in order to get to know the data you have?\n\nI used to work for an automotive company which annual revenue was really high, still they didn't have a database administrator and every single department was working by his own. This was a extremely big problem when requiring specific data, I remember spending two or three days just to find out who had certain information, lol.", "author_fullname": "t2_15xhup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How large companies handle their internal data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1129ene", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676391768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I am new to this data engineering world and was wondering how does a large company which doesn&amp;#39;t have any infrastructure for data can make a transition to a more structured environment?&lt;/p&gt;\n\n&lt;p&gt;I mean, how do you standarize things in order to get to know the data you have?&lt;/p&gt;\n\n&lt;p&gt;I used to work for an automotive company which annual revenue was really high, still they didn&amp;#39;t have a database administrator and every single department was working by his own. This was a extremely big problem when requiring specific data, I remember spending two or three days just to find out who had certain information, lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1129ene", "is_robot_indexable": true, "report_reasons": null, "author": "Hapzek", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1129ene/how_large_companies_handle_their_internal_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1129ene/how_large_companies_handle_their_internal_data/", "subreddit_subscribers": 89605, "created_utc": 1676391768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A friend of mine wants to embark on a DE journey. I am a software engineer myself. He is a Microsoft fan so he wants to do Azure. \n\nI recommended that he gets on the AWS track, since there are more jobs for AWS than there are for Azure. \n\nHe said there are fewer people that are Azure-qualified than there are AWS so it balances itself out, but when I did my research I think I concluded that there are infact more qualified Azure people than there are AWS, which supports going the AWS way even more. \n\nHe is interested in pursuing jobs in the U.S and Europe - for context. And he is not going to do both. \n\nLet me know what do you guys think ?", "author_fullname": "t2_uo9gwrfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure vs AWS DE tracks.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122ou8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676372452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend of mine wants to embark on a DE journey. I am a software engineer myself. He is a Microsoft fan so he wants to do Azure. &lt;/p&gt;\n\n&lt;p&gt;I recommended that he gets on the AWS track, since there are more jobs for AWS than there are for Azure. &lt;/p&gt;\n\n&lt;p&gt;He said there are fewer people that are Azure-qualified than there are AWS so it balances itself out, but when I did my research I think I concluded that there are infact more qualified Azure people than there are AWS, which supports going the AWS way even more. &lt;/p&gt;\n\n&lt;p&gt;He is interested in pursuing jobs in the U.S and Europe - for context. And he is not going to do both. &lt;/p&gt;\n\n&lt;p&gt;Let me know what do you guys think ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1122ou8", "is_robot_indexable": true, "report_reasons": null, "author": "Rami_zaki", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1122ou8/azure_vs_aws_de_tracks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1122ou8/azure_vs_aws_de_tracks/", "subreddit_subscribers": 89605, "created_utc": 1676372452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a media company  client that has DW for internal and client reporting (integrating data for multiple properties) and part of the data is Google Analytics, they wish to move reporting from being based on UA to GA4, I'm not too familiar with the changes in GA4, but if any of you worked on anything, similar - **how much work was it**?\n\nMy assumption going in, is that the final data model does not change, I just plug out the old and plug in the new. I've noticed that certain metrics do not have one to one correspondance, so there is going to be some research involved and with some slack for unexpected things and testing I'm thinking **3-4 weeks?** \n\nWould this be reasonable or am I way over optimistic/pessimistic?\n\n\nAny pitfalls or issues you ran into I should be aware of before starting?\n\nEdit: \n\nThe setup:\n\n * EL is done via Stitch into Snowflake\n\n * T is done via dbt", "author_fullname": "t2_9d1jjuxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating from UA to GA4 for the DW? How much work is it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1123i3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676375350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a media company  client that has DW for internal and client reporting (integrating data for multiple properties) and part of the data is Google Analytics, they wish to move reporting from being based on UA to GA4, I&amp;#39;m not too familiar with the changes in GA4, but if any of you worked on anything, similar - &lt;strong&gt;how much work was it&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;My assumption going in, is that the final data model does not change, I just plug out the old and plug in the new. I&amp;#39;ve noticed that certain metrics do not have one to one correspondance, so there is going to be some research involved and with some slack for unexpected things and testing I&amp;#39;m thinking &lt;strong&gt;3-4 weeks?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Would this be reasonable or am I way over optimistic/pessimistic?&lt;/p&gt;\n\n&lt;p&gt;Any pitfalls or issues you ran into I should be aware of before starting?&lt;/p&gt;\n\n&lt;p&gt;Edit: &lt;/p&gt;\n\n&lt;p&gt;The setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;EL is done via Stitch into Snowflake&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;T is done via dbt&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1123i3u", "is_robot_indexable": true, "report_reasons": null, "author": "boggle_thy_mind", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1123i3u/migrating_from_ua_to_ga4_for_the_dw_how_much_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1123i3u/migrating_from_ua_to_ga4_for_the_dw_how_much_work/", "subreddit_subscribers": 89605, "created_utc": 1676375350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9dz8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostgreSQL DATE, TIMESTAMP, and INTERVAL cheat sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_112c51w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/78IUn7DYPCP_utnaofMqRZAQTsc9PqguG_s2Ge3dT3Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676398633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "gist.github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://gist.github.com/henryivesjones/ebd653acbf61cb408380a49659e2be97", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;v=enabled&amp;s=71a0c0d89dff6da89bceba27fe1c91bcf534185e", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861a79178e2b1526b1d25e6f6024914f88255496", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4bb6ee491fe5bcd46e3f3931d54224ce794c00f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f3a516b3685246445a086721a600beada5d2696", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dba5835846d436768bb69d07e885dd9a8857cf16", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d36a205248a1e4685635ac78ef3c46e2dfcba5e7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc0368c25d57483c435e40bc8a848d6c3fa75df3", "width": 1080, "height": 540}], "variants": {}, "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "112c51w", "is_robot_indexable": true, "report_reasons": null, "author": "MidgetDufus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112c51w/postgresql_date_timestamp_and_interval_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://gist.github.com/henryivesjones/ebd653acbf61cb408380a49659e2be97", "subreddit_subscribers": 89605, "created_utc": 1676398633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. Having a bit of an issue at work in that we have an excel file that one of our billing departments uses to track billing on various projects we have going on. They edit in the file and they have formatting in the file (which is what i think causes a lot of issues)\n\nWe keep running into an issue where there will be blank strings or errant spaces in numerical and date columns that will cause our pipeline to fail when SQL Server tries to convert the values to dates or floats. \n\nMy style is that the best way to solve for errors is to see if they can be fixed at the source first and then go on down the pipeline. Is there any formatting or rules we can make in the excel file to make sure this is not an issue,  is there a better choice of file format they can use instead, or is this just better handled upstream?\n\nI know how to handle it upstream within ADF using a dataflow or using databricks.", "author_fullname": "t2_4bsgo8fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling Exel File as a Source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1129y0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676393131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. Having a bit of an issue at work in that we have an excel file that one of our billing departments uses to track billing on various projects we have going on. They edit in the file and they have formatting in the file (which is what i think causes a lot of issues)&lt;/p&gt;\n\n&lt;p&gt;We keep running into an issue where there will be blank strings or errant spaces in numerical and date columns that will cause our pipeline to fail when SQL Server tries to convert the values to dates or floats. &lt;/p&gt;\n\n&lt;p&gt;My style is that the best way to solve for errors is to see if they can be fixed at the source first and then go on down the pipeline. Is there any formatting or rules we can make in the excel file to make sure this is not an issue,  is there a better choice of file format they can use instead, or is this just better handled upstream?&lt;/p&gt;\n\n&lt;p&gt;I know how to handle it upstream within ADF using a dataflow or using databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1129y0d", "is_robot_indexable": true, "report_reasons": null, "author": "Hexboy3", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1129y0d/handling_exel_file_as_a_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1129y0d/handling_exel_file_as_a_source/", "subreddit_subscribers": 89605, "created_utc": 1676393131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\nIm building a ML stack using Clickhouse and Sagemaker. Both are kinda invariant at this point, although I may arrange Azure ML these days.\n\nClickhouse will contained labeled data - it is actually where our labeling team will save bounding boxes, etc.\n\nIm trying to figure out the other pieces of the pipeline - to move data to Sagemaker and kick off a training+inference run.\n\nIm considering Airflow + dbt + Airbyte based on my research. I haven't done this before, so im grateful for any suggestions.", "author_fullname": "t2_3auav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "building a ML training stack with Clickhouse &amp; Sagemaker. What should be the other pieces?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122l1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676372090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nIm building a ML stack using Clickhouse and Sagemaker. Both are kinda invariant at this point, although I may arrange Azure ML these days.&lt;/p&gt;\n\n&lt;p&gt;Clickhouse will contained labeled data - it is actually where our labeling team will save bounding boxes, etc.&lt;/p&gt;\n\n&lt;p&gt;Im trying to figure out the other pieces of the pipeline - to move data to Sagemaker and kick off a training+inference run.&lt;/p&gt;\n\n&lt;p&gt;Im considering Airflow + dbt + Airbyte based on my research. I haven&amp;#39;t done this before, so im grateful for any suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1122l1k", "is_robot_indexable": true, "report_reasons": null, "author": "sandys1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1122l1k/building_a_ml_training_stack_with_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1122l1k/building_a_ml_training_stack_with_clickhouse/", "subreddit_subscribers": 89605, "created_utc": 1676372090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say I have a realtime data producer that produces data with some schema. Producer publishes the data into messaging a from there data get to the relational database. \n\nNow, I would like to change schema of the produced data. That means consumers subscribed as well as table in the DB will break. \n\nHow do you solve this issue? Obviously I can go and fix all the consumers and db table but that would mean stopped services until they got fixed. Is there any library/technology/approach that solves this issue?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realtime data schema change", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112anz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676394920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have a realtime data producer that produces data with some schema. Producer publishes the data into messaging a from there data get to the relational database. &lt;/p&gt;\n\n&lt;p&gt;Now, I would like to change schema of the produced data. That means consumers subscribed as well as table in the DB will break. &lt;/p&gt;\n\n&lt;p&gt;How do you solve this issue? Obviously I can go and fix all the consumers and db table but that would mean stopped services until they got fixed. Is there any library/technology/approach that solves this issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "112anz8", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112anz8/realtime_data_schema_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112anz8/realtime_data_schema_change/", "subreddit_subscribers": 89605, "created_utc": 1676394920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have made an API call script to get some data from one online store.\n\nEach store requires unique store\\_id and api key, so I have to change these field in the scriptHowever, there are 13 stores in total, so I am reluctant to duplicate 13 almost identical scripts to get all the data from different stores.\n\n1. How can I make this query more efficient and maintainable\n2. Actually, I am using AWS step function to orchestrate lambda functions, so ideally, the marketID and api key has to pass to the variables in the scripts of functions.\n3. I am aware of setting up environment variables in aws lambda, but I have to pass the one env variable each by each. Would it be a loop to solve this task ?\n4. The script will generate a some kind id and it's paired with marketplace id and api key. So I have to wait one script to finish before passing a new environment variables  \n", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to loop all the environment variables and pass them to a script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1128ci8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676389740.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676389104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have made an API call script to get some data from one online store.&lt;/p&gt;\n\n&lt;p&gt;Each store requires unique store_id and api key, so I have to change these field in the scriptHowever, there are 13 stores in total, so I am reluctant to duplicate 13 almost identical scripts to get all the data from different stores.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How can I make this query more efficient and maintainable&lt;/li&gt;\n&lt;li&gt;Actually, I am using AWS step function to orchestrate lambda functions, so ideally, the marketID and api key has to pass to the variables in the scripts of functions.&lt;/li&gt;\n&lt;li&gt;I am aware of setting up environment variables in aws lambda, but I have to pass the one env variable each by each. Would it be a loop to solve this task ?&lt;/li&gt;\n&lt;li&gt;The script will generate a some kind id and it&amp;#39;s paired with marketplace id and api key. So I have to wait one script to finish before passing a new environment variables&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1128ci8", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1128ci8/how_to_loop_all_the_environment_variables_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1128ci8/how_to_loop_all_the_environment_variables_and/", "subreddit_subscribers": 89605, "created_utc": 1676389104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're attempting to create scheduled tasks that involve:\n\n1. Initiating a Lambda function to extract tar.gz files to a disk(temporary).\n\n1. Running AWS Glue to read the extracted files from the disk(temporary).\n\n1. Saving the output as a Parquet file to S3.\n\nHowever, I'm uncertain if this is the best approach or if there are alternative methods.", "author_fullname": "t2_eckrjl60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If we have some tar.gz file in s3, how to make use of aws Glue to parse it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111sim5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676345024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re attempting to create scheduled tasks that involve:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Initiating a Lambda function to extract tar.gz files to a disk(temporary).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Running AWS Glue to read the extracted files from the disk(temporary).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Saving the output as a Parquet file to S3.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;However, I&amp;#39;m uncertain if this is the best approach or if there are alternative methods.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111sim5", "is_robot_indexable": true, "report_reasons": null, "author": "b-y-f", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111sim5/if_we_have_some_targz_file_in_s3_how_to_make_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111sim5/if_we_have_some_targz_file_in_s3_how_to_make_use/", "subreddit_subscribers": 89605, "created_utc": 1676345024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi I'm in a new python class learning DB Scan. Can anyone tell me how I'm doing on this practice problem? I feel like I am messing this up. Can a core point also have yes under border? SOS \n\nhttps://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6", "author_fullname": "t2_lp2rsb27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB Scan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "media_metadata": {"99tif9npv7ia1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98b182a299a261ff4d4fd30ad7163a3cf2ca9201"}, {"y": 124, "x": 216, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e15ea5f19b5398bd7434ec22c45808ed369171b"}, {"y": 184, "x": 320, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6325b8ccf6bf934f76d6c31d97f6fed053bd4883"}, {"y": 368, "x": 640, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae3efb131157d6373f492dd3cb018c4dd0dd91bb"}, {"y": 553, "x": 960, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c930027bc98469806422e479a43294007cdf88a4"}, {"y": 622, "x": 1080, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7fa939d48d0f5064f130694bcb0f55a36ee2172"}], "s": {"y": 965, "x": 1675, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6"}, "id": "99tif9npv7ia1"}}, "name": "t3_112fsrx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ke5qE7LN7fxxFih-_c_lMl7AeEqjxUz29RaruR1qKfA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676407866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m in a new python class learning DB Scan. Can anyone tell me how I&amp;#39;m doing on this practice problem? I feel like I am messing this up. Can a core point also have yes under border? SOS &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6\"&gt;https://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112fsrx", "is_robot_indexable": true, "report_reasons": null, "author": "Quirky-Effect-2479", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112fsrx/db_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112fsrx/db_scan/", "subreddit_subscribers": 89605, "created_utc": 1676407866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This was making the rounds - thought it was funny: [https://youtu.be/kN8Yv5plkFk](https://youtu.be/kN8Yv5plkFk)\n\nDisclaimer: I work at Databricks", "author_fullname": "t2_vwkemc26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse Blues song", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112f685", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676406267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This was making the rounds - thought it was funny: &lt;a href=\"https://youtu.be/kN8Yv5plkFk\"&gt;https://youtu.be/kN8Yv5plkFk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I work at Databricks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?auto=webp&amp;v=enabled&amp;s=79b7575b6bc7e15a0d7688f971417dd25a9fb6b7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbb8db4f18d2b0b8fb41481018b6d933c84f48b2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f552eaf9959d49e7f9dcc2a79b46ca0db42d386", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aa4263e9030e37d1ce74e5f8a90fabf7b83db0e", "width": 320, "height": 240}], "variants": {}, "id": "sP3IbMcyHdG2cmTvBoBnBod-9ZpaX3XFSSsQr3hMdsQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "112f685", "is_robot_indexable": true, "report_reasons": null, "author": "databricks-employee", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112f685/data_warehouse_blues_song/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112f685/data_warehouse_blues_song/", "subreddit_subscribers": 89605, "created_utc": 1676406267.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}