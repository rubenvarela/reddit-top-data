{"kind": "Listing", "data": {"after": "t3_112opzv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on \"Databricks \u2764\ufe0f IDEs\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_112l0h5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/SOrJwgEmcji99qBB7f8cs3zClfI4s7tDtiqx1eSlcEw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676421566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/blog/2023/02/14/announcing-a-native-visual-studio-code-experience-for-databricks.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?auto=webp&amp;v=enabled&amp;s=542ffe72699f182145030ab57fceb0690e556eae", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce9048c75face835b64732dc192c4e59b1a1b4ea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ef884a4cb10bb33fd097dd0207db4e0e56368e3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99ee41eb1c1482daac0735c971f8c1a20d8506c4", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7f569924f0a22439680dbf1be5e394df5ab2027", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b42ac75475681132a1c658a834f6d449b4ed4c16", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/lRXw7aSkwr2vDc87Jertcty43PF6Vt5WQZNJVGJpTTM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143fe5376bacaa2bf617398cfa8585d1b2ea2773", "width": 1080, "height": 565}], "variants": {}, "id": "GtRADiMcQfJdZjMrWJwz2KJVAao1Bb5LSMuNacp0rVI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "112l0h5", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112l0h5/thoughts_on_databricks_ides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/blog/2023/02/14/announcing-a-native-visual-studio-code-experience-for-databricks.html", "subreddit_subscribers": 89663, "created_utc": 1676421566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering\n\nI wrote a [short tutorial on how to run PySpark in a Jupyter notebook](https://www.datain30.com/p/run-spark-with-jupyter-using-docker). Let me know if you try it out and are able to finish in under 30 minutes.", "author_fullname": "t2_vri7kka6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A short tutorial on running Spark with Jupyter using Docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112dz5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676403219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wrote a &lt;a href=\"https://www.datain30.com/p/run-spark-with-jupyter-using-docker\"&gt;short tutorial on how to run PySpark in a Jupyter notebook&lt;/a&gt;. Let me know if you try it out and are able to finish in under 30 minutes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E8FxIvXsoENn6VaVaJnNu-X9kY6ROc1nWGKZquDZqpY.jpg?auto=webp&amp;v=enabled&amp;s=d57acccd37cd2dc1ed1aeda26688d7d7b051a00a", "width": 256, "height": 256}, "resolutions": [{"url": "https://external-preview.redd.it/E8FxIvXsoENn6VaVaJnNu-X9kY6ROc1nWGKZquDZqpY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d03520cfe2eaabc83c52d699ad2f7dd25e528a42", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/E8FxIvXsoENn6VaVaJnNu-X9kY6ROc1nWGKZquDZqpY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc5d9d0177622d49dab11896441e5572bad8637a", "width": 216, "height": 216}], "variants": {}, "id": "fZc830sC60fmrpFMh1DzkGdJurbm_CKNAvt6rJyk86E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "112dz5s", "is_robot_indexable": true, "report_reasons": null, "author": "datain30", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112dz5s/a_short_tutorial_on_running_spark_with_jupyter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112dz5s/a_short_tutorial_on_running_spark_with_jupyter/", "subreddit_subscribers": 89663, "created_utc": 1676403219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a data engineer for the past 3 years, and Iam looking to change jobs because I feel like my growth has stalled. I don\u2019t come from a cs background, I moved from a data analyst to data engineer. Because of this I feel major imposter syndrome when interviewing. When interviewing I feel so overwhelmed and I\u2019m wondering if I should stop looking for data engineering roles. I use python, sql, docker, git, and cloud technologies but I can\u2019t help but feel less of a data engineer because I don\u2019t have the formal knowledge like data structures and algorithms or time complexity. Should I look for data analyst roles instead?", "author_fullname": "t2_6let0rxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feeling stuck", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112by7k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676398160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a data engineer for the past 3 years, and Iam looking to change jobs because I feel like my growth has stalled. I don\u2019t come from a cs background, I moved from a data analyst to data engineer. Because of this I feel major imposter syndrome when interviewing. When interviewing I feel so overwhelmed and I\u2019m wondering if I should stop looking for data engineering roles. I use python, sql, docker, git, and cloud technologies but I can\u2019t help but feel less of a data engineer because I don\u2019t have the formal knowledge like data structures and algorithms or time complexity. Should I look for data analyst roles instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "112by7k", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Luck40", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112by7k/feeling_stuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112by7k/feeling_stuck/", "subreddit_subscribers": 89663, "created_utc": 1676398160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've been working out of the same workspace for the past year slowly accumulating tech debt, etc. \n\nWe're looking to onboard more parts of the company and use unity catalog for the data lineage and setting up proper dev/staging/prod environments and by business unit (DE/DS/Reporting).\n\nI plan on starting out with just the DE work, creating a DE dev environment and porting/updating our current production workflow.  Once I'm finished with that and all QA checks pass, I plan on cloning that workspace to serve as the production.\n\nThe production and development workspace will utilize the same bitbucket repo, but the dev workspace will be defaulting to the development branch whereas production will be synced to the main branch. I'll then use CI/CD to automate the workflow so that any accepted pull that merges to dev updates the Databricks Dev workspace. And any accepted pull that merges from dev to prod updates the Databricks Prod workspace.\n\nOne thing I'm fuzzy on is how the object storage paths work in this scenario. Is everything inside of the root metastore (unity catalog) folder and it creates a folder for each workspace you create? So if I have two scripts writing the same table, but in different workspaces, will unity catalog automatically organize the file paths into  &lt;unity catalog&gt;/ &lt;workspace&gt;/&lt;schema&gt;/&lt;table&gt;/?\n\nI also don't want to drown in setting up credentials. So for now, I plan on doing one IAM role per workspace and utilizing Unity Catalog's roles to manage access.\n\nDoes that sound like a good path? Any gotcha's I may encounter or things I should look out for that may come bite me in the ass later?", "author_fullname": "t2_rdm9w1f0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About to jump into Databrick's Unity Catalog. Anyone have any \"gotchas\" they'd like to share?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1129oat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676392446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working out of the same workspace for the past year slowly accumulating tech debt, etc. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking to onboard more parts of the company and use unity catalog for the data lineage and setting up proper dev/staging/prod environments and by business unit (DE/DS/Reporting).&lt;/p&gt;\n\n&lt;p&gt;I plan on starting out with just the DE work, creating a DE dev environment and porting/updating our current production workflow.  Once I&amp;#39;m finished with that and all QA checks pass, I plan on cloning that workspace to serve as the production.&lt;/p&gt;\n\n&lt;p&gt;The production and development workspace will utilize the same bitbucket repo, but the dev workspace will be defaulting to the development branch whereas production will be synced to the main branch. I&amp;#39;ll then use CI/CD to automate the workflow so that any accepted pull that merges to dev updates the Databricks Dev workspace. And any accepted pull that merges from dev to prod updates the Databricks Prod workspace.&lt;/p&gt;\n\n&lt;p&gt;One thing I&amp;#39;m fuzzy on is how the object storage paths work in this scenario. Is everything inside of the root metastore (unity catalog) folder and it creates a folder for each workspace you create? So if I have two scripts writing the same table, but in different workspaces, will unity catalog automatically organize the file paths into  &amp;lt;unity catalog&amp;gt;/ &amp;lt;workspace&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;table&amp;gt;/?&lt;/p&gt;\n\n&lt;p&gt;I also don&amp;#39;t want to drown in setting up credentials. So for now, I plan on doing one IAM role per workspace and utilizing Unity Catalog&amp;#39;s roles to manage access.&lt;/p&gt;\n\n&lt;p&gt;Does that sound like a good path? Any gotcha&amp;#39;s I may encounter or things I should look out for that may come bite me in the ass later?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1129oat", "is_robot_indexable": true, "report_reasons": null, "author": "keeney_arcadia", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1129oat/about_to_jump_into_databricks_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1129oat/about_to_jump_into_databricks_unity_catalog/", "subreddit_subscribers": 89663, "created_utc": 1676392446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As mentioned in the title, I am the only \"data engineer\" in my current company. Everyone else is either a business user or web developer. Due to multiple reasons, I have become the go to data engineering guy who gets all the oddball data engineering tickets. While I am always able to get the job done, there is no senior guy with experience in data engineering to teach me best practices. I want to switch my job in next 1 year but don't know where I stand currently. Here is my current skill level description \n\nYears of experience: 1 year \n\nData orchestration: Currently, I am able to use airflow. I can write my own simple dags and debug issues with existing dags. \n\nSpark: I am able to use Spark API in both Scala and Python to create dataframes of source data and transform it as per my requirements using spark data frame functions \n\nPython: I am fairly proficient in Python and I have been able to debug any issues using basic googling. I regularly leverage Python libraries like Pandas, Cxoracle, requests, Beautiful soup, dbt. \n\nShell: While, I have not written any new code in bash, I can figure out most bash scripts quickly by going through them and make any bug fixes as required. \n\nAWS: I am an AWS certified developer and I am currently preparing for the AWS certified solutions architect certification \n\nAzure: I am an Azure certified data engineer.\n\n\nTableau and Power BI: I am able to make simple dashboards in Tableau and Power BI\n\nSQL: I am able to write SQL Query using concepts like joins, window functions and CTE.\nI have worked with both relational databases like Microsoft sql, oracle and no sql databases like mongodb\n\nCan any experienced data engineer review my skill set and let me know how much I need to improve to become employable as a real data engineer?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am the only \"data engineer\" in my company. I need an honest assessment of my current skill level. Currently, I have no guidance and frame of reference. As a result, I am afraid to switch. Can experienced data engineers please review my skills and suggest where I need to improve.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112anao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676394874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As mentioned in the title, I am the only &amp;quot;data engineer&amp;quot; in my current company. Everyone else is either a business user or web developer. Due to multiple reasons, I have become the go to data engineering guy who gets all the oddball data engineering tickets. While I am always able to get the job done, there is no senior guy with experience in data engineering to teach me best practices. I want to switch my job in next 1 year but don&amp;#39;t know where I stand currently. Here is my current skill level description &lt;/p&gt;\n\n&lt;p&gt;Years of experience: 1 year &lt;/p&gt;\n\n&lt;p&gt;Data orchestration: Currently, I am able to use airflow. I can write my own simple dags and debug issues with existing dags. &lt;/p&gt;\n\n&lt;p&gt;Spark: I am able to use Spark API in both Scala and Python to create dataframes of source data and transform it as per my requirements using spark data frame functions &lt;/p&gt;\n\n&lt;p&gt;Python: I am fairly proficient in Python and I have been able to debug any issues using basic googling. I regularly leverage Python libraries like Pandas, Cxoracle, requests, Beautiful soup, dbt. &lt;/p&gt;\n\n&lt;p&gt;Shell: While, I have not written any new code in bash, I can figure out most bash scripts quickly by going through them and make any bug fixes as required. &lt;/p&gt;\n\n&lt;p&gt;AWS: I am an AWS certified developer and I am currently preparing for the AWS certified solutions architect certification &lt;/p&gt;\n\n&lt;p&gt;Azure: I am an Azure certified data engineer.&lt;/p&gt;\n\n&lt;p&gt;Tableau and Power BI: I am able to make simple dashboards in Tableau and Power BI&lt;/p&gt;\n\n&lt;p&gt;SQL: I am able to write SQL Query using concepts like joins, window functions and CTE.\nI have worked with both relational databases like Microsoft sql, oracle and no sql databases like mongodb&lt;/p&gt;\n\n&lt;p&gt;Can any experienced data engineer review my skill set and let me know how much I need to improve to become employable as a real data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112anao", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112anao/i_am_the_only_data_engineer_in_my_company_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112anao/i_am_the_only_data_engineer_in_my_company_i_need/", "subreddit_subscribers": 89663, "created_utc": 1676394874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, question to all of you working in MS ecosystem - are you using only PowerShell in your job or is it a mix of bash/PS?\n\nI'm asking because working rn with Hadoop stack, I decided to get more proficient with linux/bash and am finding it pretty horrible ([this rant](http://xahlee.info/comp/why_bash_sucks.html) sounds about right), I'm not a linux noob but I still have to google hard to do anything beyond very basic. I heard good things about PS and from cursory research it seems much more well thought-out, unlike bash with its decades of hacky baggage.\n\nIf you think that I'm misguided and bash is awesome, please let me know, however my main point is: can one as DE with Azure avoid bash completely or you still need to interact with Linux, which I suppose to be the case? And how much will limiting myself to MS ecosystem limit my career options?\n\nThanks!", "author_fullname": "t2_470sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE with 100% Windows/Powershell possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112uq66", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676455985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, question to all of you working in MS ecosystem - are you using only PowerShell in your job or is it a mix of bash/PS?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m asking because working rn with Hadoop stack, I decided to get more proficient with linux/bash and am finding it pretty horrible (&lt;a href=\"http://xahlee.info/comp/why_bash_sucks.html\"&gt;this rant&lt;/a&gt; sounds about right), I&amp;#39;m not a linux noob but I still have to google hard to do anything beyond very basic. I heard good things about PS and from cursory research it seems much more well thought-out, unlike bash with its decades of hacky baggage.&lt;/p&gt;\n\n&lt;p&gt;If you think that I&amp;#39;m misguided and bash is awesome, please let me know, however my main point is: can one as DE with Azure avoid bash completely or you still need to interact with Linux, which I suppose to be the case? And how much will limiting myself to MS ecosystem limit my career options?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "112uq66", "is_robot_indexable": true, "report_reasons": null, "author": "Zosimas", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112uq66/de_with_100_windowspowershell_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112uq66/de_with_100_windowspowershell_possible/", "subreddit_subscribers": 89663, "created_utc": 1676455985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I am trying to stand up a self managed DataHub instance for my organization. I believe for a data catalog to truly provide business value, the respective business teams need to \"buy in\" and document their domain knowledge into the Data Catalog. One of my fears is that after putting in a ton of work standing up a self managed DataHub platform is that it will be dead in the water due to lack of adoption by  business teams and their users. To people who have implemented a Data Catalog solution before my question is this - how did you gain \"buy in\" from the business and what data catalog features were important to provide actual business value to the end users (i.e. data lineage, business glossary, data profiling, etc.)?", "author_fullname": "t2_6fq4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get business user buy in for a Data Catalog?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112pxwi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676437150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I am trying to stand up a self managed DataHub instance for my organization. I believe for a data catalog to truly provide business value, the respective business teams need to &amp;quot;buy in&amp;quot; and document their domain knowledge into the Data Catalog. One of my fears is that after putting in a ton of work standing up a self managed DataHub platform is that it will be dead in the water due to lack of adoption by  business teams and their users. To people who have implemented a Data Catalog solution before my question is this - how did you gain &amp;quot;buy in&amp;quot; from the business and what data catalog features were important to provide actual business value to the end users (i.e. data lineage, business glossary, data profiling, etc.)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "112pxwi", "is_robot_indexable": true, "report_reasons": null, "author": "Ghostflake", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112pxwi/how_to_get_business_user_buy_in_for_a_data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112pxwi/how_to_get_business_user_buy_in_for_a_data_catalog/", "subreddit_subscribers": 89663, "created_utc": 1676437150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just to give some context, I started this week at a new job working for about a 100 person startup. I was brought on as the first data hire, however, my boss (the cto) had already build out a MySQL database functioning as a data warehouse, being fed by a custom rails platform. As it is now, there is very little structure or documentation and I can\u2019t access the db directly bc of the firewall. I guess the assumption was that I could explore the data and build out dashboards using the metbase platform that was in place. He also told me he can ingest new data sources like hubspot directly into MySQL through the rails platform. \n\nMy questions are vague:\n\n-Does any of this seem like bad practice?\n\n-Is it possible that no additional modeling would need to be done and I could rely on say, the initial sql in tableau to bring the data together?\n\n-Are there any general indications to look out for that necessitate modeling beyond what is currently in place?\n\n-Is using MySQL in this fashion kosher or could it be problematic beyond just for performance issues (The tables are all under 1 mill rows but growing steadily)?\n\n\n\nI was told that the top priority is to start establishing metrics to calculate ROI but this seems like it could be a bit premature before confirming we have everything in place from a data perspective. Perhaps throwing together some quick wins in the beginning with dashboards is the best approach.\n\nThanks for any advice here! Just looking for some guidance from more seasoned folks.", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could use some advice for my new job.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112ro91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676443898.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676443556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just to give some context, I started this week at a new job working for about a 100 person startup. I was brought on as the first data hire, however, my boss (the cto) had already build out a MySQL database functioning as a data warehouse, being fed by a custom rails platform. As it is now, there is very little structure or documentation and I can\u2019t access the db directly bc of the firewall. I guess the assumption was that I could explore the data and build out dashboards using the metbase platform that was in place. He also told me he can ingest new data sources like hubspot directly into MySQL through the rails platform. &lt;/p&gt;\n\n&lt;p&gt;My questions are vague:&lt;/p&gt;\n\n&lt;p&gt;-Does any of this seem like bad practice?&lt;/p&gt;\n\n&lt;p&gt;-Is it possible that no additional modeling would need to be done and I could rely on say, the initial sql in tableau to bring the data together?&lt;/p&gt;\n\n&lt;p&gt;-Are there any general indications to look out for that necessitate modeling beyond what is currently in place?&lt;/p&gt;\n\n&lt;p&gt;-Is using MySQL in this fashion kosher or could it be problematic beyond just for performance issues (The tables are all under 1 mill rows but growing steadily)?&lt;/p&gt;\n\n&lt;p&gt;I was told that the top priority is to start establishing metrics to calculate ROI but this seems like it could be a bit premature before confirming we have everything in place from a data perspective. Perhaps throwing together some quick wins in the beginning with dashboards is the best approach.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice here! Just looking for some guidance from more seasoned folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112ro91", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112ro91/could_use_some_advice_for_my_new_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112ro91/could_use_some_advice_for_my_new_job/", "subreddit_subscribers": 89663, "created_utc": 1676443556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, I am new to this data engineering world and was wondering how does a large company which doesn't have any infrastructure for data can make a transition to a more structured environment?\n\nI mean, how do you standarize things in order to get to know the data you have?\n\nI used to work for an automotive company which annual revenue was really high, still they didn't have a database administrator and every single department was working by his own. This was a extremely big problem when requiring specific data, I remember spending two or three days just to find out who had certain information, lol.", "author_fullname": "t2_15xhup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How large companies handle their internal data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1129ene", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676391768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I am new to this data engineering world and was wondering how does a large company which doesn&amp;#39;t have any infrastructure for data can make a transition to a more structured environment?&lt;/p&gt;\n\n&lt;p&gt;I mean, how do you standarize things in order to get to know the data you have?&lt;/p&gt;\n\n&lt;p&gt;I used to work for an automotive company which annual revenue was really high, still they didn&amp;#39;t have a database administrator and every single department was working by his own. This was a extremely big problem when requiring specific data, I remember spending two or three days just to find out who had certain information, lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1129ene", "is_robot_indexable": true, "report_reasons": null, "author": "Hapzek", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1129ene/how_large_companies_handle_their_internal_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1129ene/how_large_companies_handle_their_internal_data/", "subreddit_subscribers": 89663, "created_utc": 1676391768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am trying to work on some personal projects to supplement my learning as I take the Data Talks Data Engineer camp.\n\nRecently, I've received my extended streams data from Spotify. As a way to have an extra layer to my project I've created a separate dataset with Unique Spotify tracks with metadata\n\nTable One: Extended Steaming Data. All my listens from Spotify between three years\nTable Two: Dimension table. Has metadata for each unique track.\n \nThese two tables are related through a Track URI ID.\n\nMy next step is to push these two tables to Google Cloud Services where I can create some views to query off if and visualize with Big Query.\n\nThis is really one first time pushing a dataset for my own personal project to the cloud. I guess I just wanted to post to see if there was any advice or suggestions.", "author_fullname": "t2_z321026", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Project: Beginner seeking advice for Spotify streams project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112ok1b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676432433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am trying to work on some personal projects to supplement my learning as I take the Data Talks Data Engineer camp.&lt;/p&gt;\n\n&lt;p&gt;Recently, I&amp;#39;ve received my extended streams data from Spotify. As a way to have an extra layer to my project I&amp;#39;ve created a separate dataset with Unique Spotify tracks with metadata&lt;/p&gt;\n\n&lt;p&gt;Table One: Extended Steaming Data. All my listens from Spotify between three years\nTable Two: Dimension table. Has metadata for each unique track.&lt;/p&gt;\n\n&lt;p&gt;These two tables are related through a Track URI ID.&lt;/p&gt;\n\n&lt;p&gt;My next step is to push these two tables to Google Cloud Services where I can create some views to query off if and visualize with Big Query.&lt;/p&gt;\n\n&lt;p&gt;This is really one first time pushing a dataset for my own personal project to the cloud. I guess I just wanted to post to see if there was any advice or suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112ok1b", "is_robot_indexable": true, "report_reasons": null, "author": "raz_the_kid0901", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112ok1b/personal_project_beginner_seeking_advice_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112ok1b/personal_project_beginner_seeking_advice_for/", "subreddit_subscribers": 89663, "created_utc": 1676432433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This was making the rounds - thought it was funny: [https://youtu.be/kN8Yv5plkFk](https://youtu.be/kN8Yv5plkFk)\n\nDisclaimer: I work at Databricks", "author_fullname": "t2_vwkemc26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse Blues song", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112f685", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676406267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This was making the rounds - thought it was funny: &lt;a href=\"https://youtu.be/kN8Yv5plkFk\"&gt;https://youtu.be/kN8Yv5plkFk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I work at Databricks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?auto=webp&amp;v=enabled&amp;s=79b7575b6bc7e15a0d7688f971417dd25a9fb6b7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbb8db4f18d2b0b8fb41481018b6d933c84f48b2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f552eaf9959d49e7f9dcc2a79b46ca0db42d386", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/FUBQgxy-1hhe-t0eKuN1Xd-A-JchhIq5whJUk_syD0w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aa4263e9030e37d1ce74e5f8a90fabf7b83db0e", "width": 320, "height": 240}], "variants": {}, "id": "sP3IbMcyHdG2cmTvBoBnBod-9ZpaX3XFSSsQr3hMdsQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "112f685", "is_robot_indexable": true, "report_reasons": null, "author": "databricks-employee", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112f685/data_warehouse_blues_song/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112f685/data_warehouse_blues_song/", "subreddit_subscribers": 89663, "created_utc": 1676406267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9dz8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostgreSQL DATE, TIMESTAMP, and INTERVAL cheat sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_112c51w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/78IUn7DYPCP_utnaofMqRZAQTsc9PqguG_s2Ge3dT3Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676398633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "gist.github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://gist.github.com/henryivesjones/ebd653acbf61cb408380a49659e2be97", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;v=enabled&amp;s=71a0c0d89dff6da89bceba27fe1c91bcf534185e", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861a79178e2b1526b1d25e6f6024914f88255496", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4bb6ee491fe5bcd46e3f3931d54224ce794c00f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f3a516b3685246445a086721a600beada5d2696", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dba5835846d436768bb69d07e885dd9a8857cf16", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d36a205248a1e4685635ac78ef3c46e2dfcba5e7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc0368c25d57483c435e40bc8a848d6c3fa75df3", "width": 1080, "height": 540}], "variants": {}, "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "112c51w", "is_robot_indexable": true, "report_reasons": null, "author": "MidgetDufus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112c51w/postgresql_date_timestamp_and_interval_cheat_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://gist.github.com/henryivesjones/ebd653acbf61cb408380a49659e2be97", "subreddit_subscribers": 89663, "created_utc": 1676398633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lets say I have a realtime data producer that produces data with some schema. Producer publishes the data into messaging a from there data get to the relational database. \n\nNow, I would like to change schema of the produced data. That means consumers subscribed as well as table in the DB will break. \n\nHow do you solve this issue? Obviously I can go and fix all the consumers and db table but that would mean stopped services until they got fixed. Is there any library/technology/approach that solves this issue?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realtime data schema change", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112anz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676394920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have a realtime data producer that produces data with some schema. Producer publishes the data into messaging a from there data get to the relational database. &lt;/p&gt;\n\n&lt;p&gt;Now, I would like to change schema of the produced data. That means consumers subscribed as well as table in the DB will break. &lt;/p&gt;\n\n&lt;p&gt;How do you solve this issue? Obviously I can go and fix all the consumers and db table but that would mean stopped services until they got fixed. Is there any library/technology/approach that solves this issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "112anz8", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112anz8/realtime_data_schema_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112anz8/realtime_data_schema_change/", "subreddit_subscribers": 89663, "created_utc": 1676394920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. Having a bit of an issue at work in that we have an excel file that one of our billing departments uses to track billing on various projects we have going on. They edit in the file and they have formatting in the file (which is what i think causes a lot of issues)\n\nWe keep running into an issue where there will be blank strings or errant spaces in numerical and date columns that will cause our pipeline to fail when SQL Server tries to convert the values to dates or floats. \n\nMy style is that the best way to solve for errors is to see if they can be fixed at the source first and then go on down the pipeline. Is there any formatting or rules we can make in the excel file to make sure this is not an issue,  is there a better choice of file format they can use instead, or is this just better handled upstream?\n\nI know how to handle it upstream within ADF using a dataflow or using databricks.", "author_fullname": "t2_4bsgo8fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling Exel File as a Source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1129y0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676393131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. Having a bit of an issue at work in that we have an excel file that one of our billing departments uses to track billing on various projects we have going on. They edit in the file and they have formatting in the file (which is what i think causes a lot of issues)&lt;/p&gt;\n\n&lt;p&gt;We keep running into an issue where there will be blank strings or errant spaces in numerical and date columns that will cause our pipeline to fail when SQL Server tries to convert the values to dates or floats. &lt;/p&gt;\n\n&lt;p&gt;My style is that the best way to solve for errors is to see if they can be fixed at the source first and then go on down the pipeline. Is there any formatting or rules we can make in the excel file to make sure this is not an issue,  is there a better choice of file format they can use instead, or is this just better handled upstream?&lt;/p&gt;\n\n&lt;p&gt;I know how to handle it upstream within ADF using a dataflow or using databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1129y0d", "is_robot_indexable": true, "report_reasons": null, "author": "Hexboy3", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1129y0d/handling_exel_file_as_a_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1129y0d/handling_exel_file_as_a_source/", "subreddit_subscribers": 89663, "created_utc": 1676393131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Snowflake to store all of our data and while I'm investigating issues, I notice certain columns haven't been updated for weeks which I'd like to flag and triage. \n\nI use dbt and unsure if dbt can help with this easily. I use dbt tests and can program a macro across certain columns to catch if data isn't populating as I'd expect....but I was curious...\n\n**What approaches or tools have made your lives easier with data profiling and catching data issues?**", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data profiling tools / approaches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1130jx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676474767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Snowflake to store all of our data and while I&amp;#39;m investigating issues, I notice certain columns haven&amp;#39;t been updated for weeks which I&amp;#39;d like to flag and triage. &lt;/p&gt;\n\n&lt;p&gt;I use dbt and unsure if dbt can help with this easily. I use dbt tests and can program a macro across certain columns to catch if data isn&amp;#39;t populating as I&amp;#39;d expect....but I was curious...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What approaches or tools have made your lives easier with data profiling and catching data issues?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1130jx5", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130jx5/data_profiling_tools_approaches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130jx5/data_profiling_tools_approaches/", "subreddit_subscribers": 89663, "created_utc": 1676474767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi I'm in a new python class learning DB Scan. Can anyone tell me how I'm doing on this practice problem? I feel like I am messing this up. Can a core point also have yes under border? SOS \n\nhttps://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6", "author_fullname": "t2_lp2rsb27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB Scan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "media_metadata": {"99tif9npv7ia1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98b182a299a261ff4d4fd30ad7163a3cf2ca9201"}, {"y": 124, "x": 216, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e15ea5f19b5398bd7434ec22c45808ed369171b"}, {"y": 184, "x": 320, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6325b8ccf6bf934f76d6c31d97f6fed053bd4883"}, {"y": 368, "x": 640, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae3efb131157d6373f492dd3cb018c4dd0dd91bb"}, {"y": 553, "x": 960, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c930027bc98469806422e479a43294007cdf88a4"}, {"y": 622, "x": 1080, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7fa939d48d0f5064f130694bcb0f55a36ee2172"}], "s": {"y": 965, "x": 1675, "u": "https://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6"}, "id": "99tif9npv7ia1"}}, "name": "t3_112fsrx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ke5qE7LN7fxxFih-_c_lMl7AeEqjxUz29RaruR1qKfA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676407866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m in a new python class learning DB Scan. Can anyone tell me how I&amp;#39;m doing on this practice problem? I feel like I am messing this up. Can a core point also have yes under border? SOS &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6\"&gt;https://preview.redd.it/99tif9npv7ia1.png?width=1675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2898e7f16b68847c209157c378937fd4e4d55bb6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112fsrx", "is_robot_indexable": true, "report_reasons": null, "author": "Quirky-Effect-2479", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112fsrx/db_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112fsrx/db_scan/", "subreddit_subscribers": 89663, "created_utc": 1676407866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this post [https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking\\_for\\_data\\_analytics\\_engine\\_for\\_lowlatency/](https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/) I was guided towards Headless BI, specifically [Cube.dev](https://Cube.dev).\n\nWe looked into it, seemed to work for us and went with it. Nice!\n\nExcept... we've run into various edge cases where we're now questioning our choice heavily.\n\nBrowsing around, I've found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?\n\nAnd while at it, how a tool like that would be actually called? Headless BI doesn't turn up too much results. Semantic Layer? Maybe something else?", "author_fullname": "t2_4gt5e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Looker/LookML &amp; Cube.dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1131e3a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676477013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/\"&gt;https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/&lt;/a&gt; I was guided towards Headless BI, specifically &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We looked into it, seemed to work for us and went with it. Nice!&lt;/p&gt;\n\n&lt;p&gt;Except... we&amp;#39;ve run into various edge cases where we&amp;#39;re now questioning our choice heavily.&lt;/p&gt;\n\n&lt;p&gt;Browsing around, I&amp;#39;ve found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?&lt;/p&gt;\n\n&lt;p&gt;And while at it, how a tool like that would be actually called? Headless BI doesn&amp;#39;t turn up too much results. Semantic Layer? Maybe something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1131e3a", "is_robot_indexable": true, "report_reasons": null, "author": "psycketom", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "subreddit_subscribers": 89663, "created_utc": 1676477013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's been quite a while since I dealt with managing a type 2 SCD so I wanted to get confirmation on what the typical strategies are. I've done hash columns that will hash all of the columns of a dimension table and compare that to the hash column of the incoming data. This worked pretty well but I remember hearing that there's always a chance that hash values will be the same. Is that concern still valid, and how often does that happen? \n\nWhat are other strategies/data models/queries that will identify a changed row in a type 2 SCD? I came across [this Stackoverflow answer](https://stackoverflow.com/a/35171620/1175788) for essentially the same question but the query is pretty obtuse. I want to avoid a query where it's doing comparisons across all the columns like: `WHERE source.id = incoming.id AND (source.quantity &lt;&gt; incoming.quantity OR source.date &lt;&gt; incoming.date)` because that's obviously terribly inefficient", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Type 2 slowly changing dimension. What are the common SQL queries to identify updated rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1130zot", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676475958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been quite a while since I dealt with managing a type 2 SCD so I wanted to get confirmation on what the typical strategies are. I&amp;#39;ve done hash columns that will hash all of the columns of a dimension table and compare that to the hash column of the incoming data. This worked pretty well but I remember hearing that there&amp;#39;s always a chance that hash values will be the same. Is that concern still valid, and how often does that happen? &lt;/p&gt;\n\n&lt;p&gt;What are other strategies/data models/queries that will identify a changed row in a type 2 SCD? I came across &lt;a href=\"https://stackoverflow.com/a/35171620/1175788\"&gt;this Stackoverflow answer&lt;/a&gt; for essentially the same question but the query is pretty obtuse. I want to avoid a query where it&amp;#39;s doing comparisons across all the columns like: &lt;code&gt;WHERE source.id = incoming.id AND (source.quantity &amp;lt;&amp;gt; incoming.quantity OR source.date &amp;lt;&amp;gt; incoming.date)&lt;/code&gt; because that&amp;#39;s obviously terribly inefficient&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1130zot", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130zot/type_2_slowly_changing_dimension_what_are_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130zot/type_2_slowly_changing_dimension_what_are_the/", "subreddit_subscribers": 89663, "created_utc": 1676475958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you mask the data? Have full access rights? Follow just guidance? How does it work where you are?", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your data engineering team handle ingesting PII and classified data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1130z35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676475915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you mask the data? Have full access rights? Follow just guidance? How does it work where you are?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1130z35", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130z35/how_does_your_data_engineering_team_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130z35/how_does_your_data_engineering_team_handle/", "subreddit_subscribers": 89663, "created_utc": 1676475915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d0gtdepx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake CHANGE DATE FEED \u2014 How to read CDC without using Delta Log", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 57, "top_awarded_type": null, "hide_score": true, "name": "t3_1130wj8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gcQT_X76jiYTXNkvzEX7-zIUUj14WcY6xa5Jb9So8io.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676475716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@joydeep.roy/change-date-feed-how-to-read-cdc-without-using-delta-log-17e0ceb99a8e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?auto=webp&amp;v=enabled&amp;s=f97d42dbbd8e27dc2b44056b321fe4b91e3d7d9d", "width": 1002, "height": 414}, "resolutions": [{"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=716712296ffbdb9044c829dcc7a73014376063e1", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63f7f69089c1f264df96a662f8ec1bb1f6c1fe40", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=294b02119766f1b2ecd4dacada768322d17868d1", "width": 320, "height": 132}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32d70acf82831566b93ab0124a35cd18d16ad1fb", "width": 640, "height": 264}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef28f0508c14093813f44b172a31b8a160ca28b4", "width": 960, "height": 396}], "variants": {}, "id": "gVVkQGOOB1Nv48mh-zEITYt_WkTJdgoTJ9edGrmyA_I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1130wj8", "is_robot_indexable": true, "report_reasons": null, "author": "JBR_Codepen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1130wj8/delta_lake_change_date_feed_how_to_read_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@joydeep.roy/change-date-feed-how-to-read-cdc-without-using-delta-log-17e0ceb99a8e", "subreddit_subscribers": 89663, "created_utc": 1676475716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This article has a useful *very* high level summary tl;dr of Fundamentals of Data Engineering\n\n[https://siliconangle.com/2023/02/10/evolving-role-data-engineer/](https://siliconangle.com/2023/02/10/evolving-role-data-engineer/)\n\n&amp;#x200B;\n\n(although the cover image overlaid with some python code for 3D rendering instead of DE loses it some geek points ;)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A useful summary/overview of FoDE: The evolving role of the data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112vn1i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676459535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article has a useful &lt;em&gt;very&lt;/em&gt; high level summary tl;dr of Fundamentals of Data Engineering&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://siliconangle.com/2023/02/10/evolving-role-data-engineer/\"&gt;https://siliconangle.com/2023/02/10/evolving-role-data-engineer/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(although the cover image overlaid with some python code for 3D rendering instead of DE loses it some geek points ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?auto=webp&amp;v=enabled&amp;s=f6151291d14854b67add19949d5e08b2a632bcc9", "width": 1920, "height": 1281}, "resolutions": [{"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e97b9d6fcf855ca862789412fcc89068b09ba8d", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac22a21d79279d31bbdbc673ca391c002b2b0279", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=829ba013e1977ce0c4e2b902689ab893e1238b45", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59226715d7bf94bed8a70fd87e1f5b3a4b885fcb", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd4c98658f09df2ffc40dcdd9a369fd6bbb40b1b", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/H9O296jDd5ja0wrDNbARLYfxkPUiPLwhif6aKR3sY9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65eb2be5ade82bbb5cc4ac84394ccf65315ee3c0", "width": 1080, "height": 720}], "variants": {}, "id": "RtziY08pvnHZqgYEZJKNNk3sCzE45Sd9eIPgowb1wP8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "112vn1i", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112vn1i/a_useful_summaryoverview_of_fode_the_evolving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112vn1i/a_useful_summaryoverview_of_fode_the_evolving/", "subreddit_subscribers": 89663, "created_utc": 1676459535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, it seems you cannot create a view over a federated datasource?\n\nthats a great shame :(\n\nWe use Superset to access our datalake, and it's nice to push query complexity into Athena rather than doing it in Superset!  (In fact, i'm not even sure superset sees the federated sources)\n\nHas anyone seen if this restriction is documented?\n\nIt's not terrible tho to use a virtual dataset, just inconsistent in the architecture we have right now.", "author_fullname": "t2_30v538jt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena - no views over federated data sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112v475", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676458421.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676457513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, it seems you cannot create a view over a federated datasource?&lt;/p&gt;\n\n&lt;p&gt;thats a great shame :(&lt;/p&gt;\n\n&lt;p&gt;We use Superset to access our datalake, and it&amp;#39;s nice to push query complexity into Athena rather than doing it in Superset!  (In fact, i&amp;#39;m not even sure superset sees the federated sources)&lt;/p&gt;\n\n&lt;p&gt;Has anyone seen if this restriction is documented?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not terrible tho to use a virtual dataset, just inconsistent in the architecture we have right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "112v475", "is_robot_indexable": true, "report_reasons": null, "author": "codek1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112v475/athena_no_views_over_federated_data_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112v475/athena_no_views_over_federated_data_sources/", "subreddit_subscribers": 89663, "created_utc": 1676457513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nI wanted to ask about any experience with SAS Viya in terms of ease of learning and functionality. \n\nI have around a bit above 3 years experience working with AWS and I wanted to know, is my knowledge is actually transferable to work with SAS cloud. \n\nI am applying to a job and discovered that they run a multi cloud platform, partially with AWS and partially with SAS and that\u2019s why I am asking. \n\nCheers", "author_fullname": "t2_64nmnknh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with SAS Viya", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112uv40", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676456540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nI wanted to ask about any experience with SAS Viya in terms of ease of learning and functionality. &lt;/p&gt;\n\n&lt;p&gt;I have around a bit above 3 years experience working with AWS and I wanted to know, is my knowledge is actually transferable to work with SAS cloud. &lt;/p&gt;\n\n&lt;p&gt;I am applying to a job and discovered that they run a multi cloud platform, partially with AWS and partially with SAS and that\u2019s why I am asking. &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "112uv40", "is_robot_indexable": true, "report_reasons": null, "author": "SnooDoggos5883", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112uv40/experience_with_sas_viya/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112uv40/experience_with_sas_viya/", "subreddit_subscribers": 89663, "created_utc": 1676456540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am looking for a tool (open source preferably) which can help me in data cataloging, lineage and governance along with a UI for data preparation ( a front end from which business team can search for fields based on keywords picked from the catalog and choose those fields to prepare reports) the backend has to be data warehouse ( snowflake in my case). Are there such tools available in market?", "author_fullname": "t2_eb80kwgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Governance, Catalogue, Lineage tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112uts1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676456396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am looking for a tool (open source preferably) which can help me in data cataloging, lineage and governance along with a UI for data preparation ( a front end from which business team can search for fields based on keywords picked from the catalog and choose those fields to prepare reports) the backend has to be data warehouse ( snowflake in my case). Are there such tools available in market?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112uts1", "is_robot_indexable": true, "report_reasons": null, "author": "No-Caregiver-1204", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112uts1/data_governance_catalogue_lineage_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112uts1/data_governance_catalogue_lineage_tool/", "subreddit_subscribers": 89663, "created_utc": 1676456396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nHello,\n\nI just found out that our delete queries in BigQuery are unoptimised. It was first designed by our consultant and we follow his format.\n\nWe are dumping the raw data in our staging area in bq and then performs delete and insert queries to the production table. \n\nHere s the sample of our delete query\n\n```\nDELETE FROM `mydataset.prod_table`\nWHERE EXISTS (\n  SELECT * FROM `staging.mytable` stg\n  WHERE prod_table.partition_column =stg.partition_column \n  AND other_column = some_value\n)\nAnd cluster_column = some_value\n```\n\nJust realized putting the partition column inside the where exists would make the query unoptimized.\n\nIm thinking about putting the partition column outside the where exists and performs a subquery. \n\nPlease advise .", "author_fullname": "t2_are11xb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Delete query in BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_112opzv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676432982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I just found out that our delete queries in BigQuery are unoptimised. It was first designed by our consultant and we follow his format.&lt;/p&gt;\n\n&lt;p&gt;We are dumping the raw data in our staging area in bq and then performs delete and insert queries to the production table. &lt;/p&gt;\n\n&lt;p&gt;Here s the sample of our delete query&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nDELETE FROM `mydataset.prod_table`\nWHERE EXISTS (\n  SELECT * FROM `staging.mytable` stg\n  WHERE prod_table.partition_column =stg.partition_column \n  AND other_column = some_value\n)\nAnd cluster_column = some_value\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Just realized putting the partition column inside the where exists would make the query unoptimized.&lt;/p&gt;\n\n&lt;p&gt;Im thinking about putting the partition column outside the where exists and performs a subquery. &lt;/p&gt;\n\n&lt;p&gt;Please advise .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "112opzv", "is_robot_indexable": true, "report_reasons": null, "author": "Sublime-01", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/112opzv/optimizing_delete_query_in_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/112opzv/optimizing_delete_query_in_bigquery/", "subreddit_subscribers": 89663, "created_utc": 1676432982.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}