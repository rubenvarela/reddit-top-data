{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_brkxjomi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Valentine's for your data sweetheart \ud83e\udef6", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"biduu0mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/biduu0mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79381bd9167be6bc4a6801a37db2b2c399155875"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/biduu0mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03da64f559ff0b77ce2202f22cd238438cfa7274"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/biduu0mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09ecbb1fd34068c8d8760d287e619947bc1fd49c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/biduu0mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83231bcaf3e67deb8fb3c67189c7891f09e47ab2"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/biduu0mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b432ecd6093ff860444963d6dc65d13409546d"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/biduu0mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=afe55d32aed951d2a501dee5d2343482da26cbb6"}, "id": "biduu0mtffha1"}, "ryo2izltffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ryo2izltffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf6105f3c39c2eb2f3dc6e9211cdb0bd3ed04d41"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ryo2izltffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7787c7546a5ca67acc869986ae7ca30d26c530c8"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/ryo2izltffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b45e98685f26d1688e1900aec957a5e318d9a6"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/ryo2izltffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf130fe2d485801b60b1a6f806f7cb4c731072e6"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/ryo2izltffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dd73d7d82d9269117b82ccc09f9837c6be64459"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/ryo2izltffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=74ae962e6fffe5593810c429220809ada4d34d36"}, "id": "ryo2izltffha1"}, "xj1kc2mtffha1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f519c75e14dc49b1f0ff605527ca50a60757883"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aaf79cdeec3be1f685f0458862ef50ebe3e6cbe"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e682512352eeb61f1c5994ff667c2ecc13bcc99a"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b4563feeeb86c6b21cd0efca1b4fadd08df04e"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=999e144157716bc6c62dd03f600dc0ad06e5ffb9"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=adaeca8dcb921f37da10cdcb3431878ca00f6623"}, "id": "xj1kc2mtffha1"}, "17fehyltffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/17fehyltffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac24ecb8bc5aad89c5a20c3960b2ae4e791c88f7"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/17fehyltffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67bc4621be7bda93147c7203e74c142eecd35465"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/17fehyltffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f066215eb3f86dd0ad5ed938ff00caf9880d22c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/17fehyltffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21f7b6fb93b603495641b15ce5aedac34b847bb3"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/17fehyltffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0475e63182927b6c7bbfea6461cb540189a1e6c9"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/17fehyltffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=80a98dfcb93576438446c70cd56b955d95ae62f4"}, "id": "17fehyltffha1"}, "atd821mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/atd821mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193c7cfe15eb4c07ad44ab8ca9bc7bfe59dfa5ab"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/atd821mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=513c05e5e4182710264771882d427dbf4f09fcfc"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/atd821mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6024b102abb3bc7da079e72117920c698b5e0e07"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/atd821mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fef394e19adf67defd1e61f85f9674bf4aad2da"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/atd821mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7642046c5955c3ff4fcff245be24db0dd61148b"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/atd821mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=11f273bc54db2d84120dc5537dd1ffc261434fbb"}, "id": "atd821mtffha1"}, "n9jjt1mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc475144c683e58f59a61985e6c1841d80a732bd"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aafead2bd7b2106ec3d3488c08c32885bae5ab6"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=010168a10ee1b3d4292f0d5cf025906097f5b998"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0084fd2c5f0ff5de7079dc81efcd72e942cce43f"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=171505d0fca2097db573ffef84aa9f6f608c7b8d"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b4c9a1c9e4858df2a986bc98bd93d43052da8be"}, "id": "n9jjt1mtffha1"}}, "name": "t3_10z37l5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 104, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "ryo2izltffha1", "id": 239274768}, {"media_id": "17fehyltffha1", "id": 239274769}, {"media_id": "biduu0mtffha1", "id": 239274770}, {"media_id": "atd821mtffha1", "id": 239274771}, {"media_id": "n9jjt1mtffha1", "id": 239274772}, {"media_id": "xj1kc2mtffha1", "id": 239274773}]}, "link_flair_text": "Meme", "can_mod_post": false, "score": 104, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/akDVWEtAYUZrcqWztA1PtaB5z4ncRPFVMAhMrdocQL4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676063522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/10z37l5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10z37l5", "is_robot_indexable": true, "report_reasons": null, "author": "Straight_House8628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z37l5/valentines_for_your_data_sweetheart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/10z37l5", "subreddit_subscribers": 89200, "created_utc": 1676063522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.", "author_fullname": "t2_t3ffq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data is Dead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10z1ft9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1pO0RBiV6vwhGezwNmiqCZro0ztqzvrl3hQ2Ib5SB8A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676059302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "motherduck.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://motherduck.com/blog/big-data-is-dead/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?auto=webp&amp;v=enabled&amp;s=422507ff3e1d876f1565f9afa286ac2a3b12b70d", "width": 1024, "height": 535}, "resolutions": [{"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eea8d6f5a4b37bc287fcc1bd5d4b8364089b4589", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84293dc6fd5ff654378e1403be79188e69070a05", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8c1be6a0050d06000aac1c8326148e53643be6b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1af5f5964cf9dc80b6d7c498f369c8505d1a4dc3", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ab1c4a64b03a27503a012ea8235765e4de0220", "width": 960, "height": 501}], "variants": {}, "id": "C3v2JVQcD5hONfZJ0FsXvcBoNEQPQWKeM2eQkWh2PMw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10z1ft9", "is_robot_indexable": true, "report_reasons": null, "author": "FortunOfficial", "discussion_type": null, "num_comments": 46, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10z1ft9/big_data_is_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://motherduck.com/blog/big-data-is-dead/", "subreddit_subscribers": 89200, "created_utc": 1676059302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,  \nWe used to migrate data with a CDC based architecture. Then one day, someone said:\n\n\"What if we discover some contaminated data? We'd need to fix the data in the source, and then replay the migration from that point. Does your architecture handle that?\"  \n\n\n\\*gulp\\*  \n\n\nQuestion: How does CDC fit into your company? Is it an *extra* migration pipeline that feeds real-time solutions? Do you still have an \"SQL-based\" pipeline just in case?", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC and Backfill", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ytfl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676039499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;br/&gt;\nWe used to migrate data with a CDC based architecture. Then one day, someone said:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;What if we discover some contaminated data? We&amp;#39;d need to fix the data in the source, and then replay the migration from that point. Does your architecture handle that?&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;*gulp*  &lt;/p&gt;\n\n&lt;p&gt;Question: How does CDC fit into your company? Is it an &lt;em&gt;extra&lt;/em&gt; migration pipeline that feeds real-time solutions? Do you still have an &amp;quot;SQL-based&amp;quot; pipeline just in case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ytfl3", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ytfl3/cdc_and_backfill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ytfl3/cdc_and_backfill/", "subreddit_subscribers": 89200, "created_utc": 1676039499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We need to store somewhere realtime data and I am considering OLAP databases like Druid, Pinot, Clickhouse and timeseries databases like TimescaleDB, Influx.. \nWhy should one prefer one over other? What are the use cases one can handle the other can not? What is one better at than the other?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realtime data - OLAP or Timeseries databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zf6rn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676100508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We need to store somewhere realtime data and I am considering OLAP databases like Druid, Pinot, Clickhouse and timeseries databases like TimescaleDB, Influx.. \nWhy should one prefer one over other? What are the use cases one can handle the other can not? What is one better at than the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zf6rn", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zf6rn/realtime_data_olap_or_timeseries_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zf6rn/realtime_data_olap_or_timeseries_databases/", "subreddit_subscribers": 89200, "created_utc": 1676100508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Passing Google Cloud Certified Professional Data Engineer Exam in 2023\n\n\\[English version\\] [Passing Google Cloud Professional Data Engineer Exam in 2023](https://medium.com/aidatatools/passing-google-cloud-professional-data-engineer-exam-in-2023-7eb2e2e8ea0e)\n\n\u901a\u904eGCP\u8a8d\u8b49\u8cc7\u6599\u5de5\u7a0b\u5e2b\u7684\u8003\u8a66\u5fc3\u5f97\u5206\u4eab\n\n\\[\u96d9\u8a9e\u7248\\] [Passing Google Cloud Professional Data Engineer Exam in 2023](https://jasonchuang.substack.com/p/passing-google-cloud-professional)", "author_fullname": "t2_7uik3gn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passing Google Cloud Certified Professional Data Engineer Exam in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z47fd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Passing Google Cloud Certified Professional Data Engineer Exam in 2023&lt;/p&gt;\n\n&lt;p&gt;[English version] &lt;a href=\"https://medium.com/aidatatools/passing-google-cloud-professional-data-engineer-exam-in-2023-7eb2e2e8ea0e\"&gt;Passing Google Cloud Professional Data Engineer Exam in 2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\u901a\u904eGCP\u8a8d\u8b49\u8cc7\u6599\u5de5\u7a0b\u5e2b\u7684\u8003\u8a66\u5fc3\u5f97\u5206\u4eab&lt;/p&gt;\n\n&lt;p&gt;[\u96d9\u8a9e\u7248] &lt;a href=\"https://jasonchuang.substack.com/p/passing-google-cloud-professional\"&gt;Passing Google Cloud Professional Data Engineer Exam in 2023&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?auto=webp&amp;v=enabled&amp;s=19257639f54d68848b831958e26796285bcac00e", "width": 617, "height": 472}, "resolutions": [{"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65d915362523383343976db338ed4d5b1836875b", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c4297d241bd67bdd69ac6b6ad35acd0ccae6b67", "width": 216, "height": 165}, {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=276c1e31603cffeea2d0cada8f6fdcd4f9f7615d", "width": 320, "height": 244}], "variants": {}, "id": "HbdGQG9XOZg3TP_DSmXkz4yQjrRAIxAn4tROL699ahY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10z47fd", "is_robot_indexable": true, "report_reasons": null, "author": "Few_Knee1141", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z47fd/passing_google_cloud_certified_professional_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z47fd/passing_google_cloud_certified_professional_data/", "subreddit_subscribers": 89200, "created_utc": 1676066157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it good enough to receive terabytes of data from api using simple python request module in glue and spark to store it in s3 with delta format? The project is to process daily data from company db to s3, and the company will provide an api to access the data.\n\nMy current concern is size limit of data that api can handle, is there any limit for an api? I don't know what kind of api will be given because this is for a project in poc phase. \n\nI am fairly new in data engineering so feel free to give any advice, thank you.", "author_fullname": "t2_ityodnp4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing terabytes from api in aws glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yuub4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676043015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it good enough to receive terabytes of data from api using simple python request module in glue and spark to store it in s3 with delta format? The project is to process daily data from company db to s3, and the company will provide an api to access the data.&lt;/p&gt;\n\n&lt;p&gt;My current concern is size limit of data that api can handle, is there any limit for an api? I don&amp;#39;t know what kind of api will be given because this is for a project in poc phase. &lt;/p&gt;\n\n&lt;p&gt;I am fairly new in data engineering so feel free to give any advice, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yuub4", "is_robot_indexable": true, "report_reasons": null, "author": "LimeDine", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yuub4/processing_terabytes_from_api_in_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yuub4/processing_terabytes_from_api_in_aws_glue/", "subreddit_subscribers": 89200, "created_utc": 1676043015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I often see lakehouse stages named like: bronze, silver, gold. I guess bronze represents the very raw data (e.g. downloaded jsons, csv, etc). Gold is the ultimate result for end user (stored using e.g. Iceberg). But what silver is supposed to be? What should be the differences between silver and gold?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse stages naming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zei0k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676097942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I often see lakehouse stages named like: bronze, silver, gold. I guess bronze represents the very raw data (e.g. downloaded jsons, csv, etc). Gold is the ultimate result for end user (stored using e.g. Iceberg). But what silver is supposed to be? What should be the differences between silver and gold?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zei0k", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zei0k/lakehouse_stages_naming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zei0k/lakehouse_stages_naming/", "subreddit_subscribers": 89200, "created_utc": 1676097942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I left an SSIS ETL gig about 8 years ago to join a much bigger company as a Business Intelligence (glorified reporting guy). I have kept working on side gigs using SSIS and DBT\u2026 now, because I have a giant gap of not related Data engineering positions.  I get an automatic rejection from HR and I\u2019m at lost.  I hate doing tableau and making things pretty for idiotic reasons.  I need to get back home and I need some feedback on how to fix my resume.", "author_fullname": "t2_4i6f73l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get back into data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yssm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676037807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I left an SSIS ETL gig about 8 years ago to join a much bigger company as a Business Intelligence (glorified reporting guy). I have kept working on side gigs using SSIS and DBT\u2026 now, because I have a giant gap of not related Data engineering positions.  I get an automatic rejection from HR and I\u2019m at lost.  I hate doing tableau and making things pretty for idiotic reasons.  I need to get back home and I need some feedback on how to fix my resume.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10yssm1", "is_robot_indexable": true, "report_reasons": null, "author": "SadDogOwner27", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yssm1/how_to_get_back_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yssm1/how_to_get_back_into_data_engineering/", "subreddit_subscribers": 89200, "created_utc": 1676037807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm getting quite confused about this. So it's very typical in a transactional system to have header line data, like purchase order and purchase line.\n\nWhen I bring this into a data warehouse and I'm designing the fact tables, how do I model this relationship? If I add columns for the header detail I'll have tonnes of redundant, repeated data. But having 2 linked facts is also not correct.\n\nAny help is appreciated thanks!", "author_fullname": "t2_32y0tqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model header line data in a DWH without duplicating data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yxiy6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676049784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting quite confused about this. So it&amp;#39;s very typical in a transactional system to have header line data, like purchase order and purchase line.&lt;/p&gt;\n\n&lt;p&gt;When I bring this into a data warehouse and I&amp;#39;m designing the fact tables, how do I model this relationship? If I add columns for the header detail I&amp;#39;ll have tonnes of redundant, repeated data. But having 2 linked facts is also not correct.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yxiy6", "is_robot_indexable": true, "report_reasons": null, "author": "MrWriter1234", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yxiy6/how_to_model_header_line_data_in_a_dwh_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yxiy6/how_to_model_header_line_data_in_a_dwh_without/", "subreddit_subscribers": 89200, "created_utc": 1676049784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So our security division is extracting data from a big cloud security system into our analytics server and there is a lot of beef between our data team and their team, they are messing our analytics server performance, delaying our subscriptions and so on, and our rules of bouncing every query that is taking more than 2 hours is messing up with their scripts.(Tbf not even sure why their queries are taking more than 2 hours)\n\nI need to mention that we use the data extracted for analytics as well, but they also use it for monitoring and dashboarding.\n\nNow I want to take this issue in my hands and do something about it, but for that I need to convince my manager.\n\nNow I am thinking this should be a pretty easy fix, we either create another server specifically for the security team where they can use it for whatever they want, and we connect to their server to extract the data to build our dwh, or we move them on our etl server, but I am looking more into the first option.\n\nThat\u2019s why Im posting here, I would like some opinions from more seasoned Data Engineers.\n\nHope you guys can help me with some insights.", "author_fullname": "t2_4lcvdsdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beef between security and data team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yvm1y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676044891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So our security division is extracting data from a big cloud security system into our analytics server and there is a lot of beef between our data team and their team, they are messing our analytics server performance, delaying our subscriptions and so on, and our rules of bouncing every query that is taking more than 2 hours is messing up with their scripts.(Tbf not even sure why their queries are taking more than 2 hours)&lt;/p&gt;\n\n&lt;p&gt;I need to mention that we use the data extracted for analytics as well, but they also use it for monitoring and dashboarding.&lt;/p&gt;\n\n&lt;p&gt;Now I want to take this issue in my hands and do something about it, but for that I need to convince my manager.&lt;/p&gt;\n\n&lt;p&gt;Now I am thinking this should be a pretty easy fix, we either create another server specifically for the security team where they can use it for whatever they want, and we connect to their server to extract the data to build our dwh, or we move them on our etl server, but I am looking more into the first option.&lt;/p&gt;\n\n&lt;p&gt;That\u2019s why Im posting here, I would like some opinions from more seasoned Data Engineers.&lt;/p&gt;\n\n&lt;p&gt;Hope you guys can help me with some insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yvm1y", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional_Key", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yvm1y/beef_between_security_and_data_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yvm1y/beef_between_security_and_data_team/", "subreddit_subscribers": 89200, "created_utc": 1676044891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says I have my first technical interview next week for an entry level role. I\u2019ve been asked to give a 10 min presentation but what I am worried about is the interview is 90 mins. What other things  are they gonna ask me for that long? Ive been doing personal projects for a while and now seeking my first role so the imposter syndrome is sky high.\n\nAny help would be appreciated", "author_fullname": "t2_4ba5z1zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First ever data engineering technical interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zjhl6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676112772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says I have my first technical interview next week for an entry level role. I\u2019ve been asked to give a 10 min presentation but what I am worried about is the interview is 90 mins. What other things  are they gonna ask me for that long? Ive been doing personal projects for a while and now seeking my first role so the imposter syndrome is sky high.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10zjhl6", "is_robot_indexable": true, "report_reasons": null, "author": "dreamr49", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zjhl6/first_ever_data_engineering_technical_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zjhl6/first_ever_data_engineering_technical_interview/", "subreddit_subscribers": 89200, "created_utc": 1676112772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "tldr: I'm working as an intern in a Data Pipeline built on top of Apache Nifi, Kafka, Spark (in Scala) and Cassandra and Postgress. CSV files are ingested and, after some transformations, analytics and graphs are provided in a web application. If I were to be hired, could my job title be *Junior Data Engineer*? What should I be doing if I wanted that to be my role?\n\nHi, I'm a Computer Science B.Sc, yet to graduate.\n\nA year ago I decided I wanted work towards entering into the Data Science field. Specifically, aiming for a Data Engineering role. I landed this internship whose title is \"*Internship in Big Data Analytics*\", but I'm not sure if the experience I am getting could enable me to portrait myself as an *entry-level Data Engineer* of sorts.\n\nI basically work in a Data Pipeline that is going to be sold as a product/service. CSV Files are ingested by apache NiFi and data is stored in Cassandra and PostgreSQL. Then, an Spark application (coded in Scala) is launched to apply some transformations, store them and, finally, a web application present results in many dashboards to the end-user.\n\nSome tasks I've done so far:\n\n* Deploying the solution in docker, some troubleshooting and investigating how to configure some parameters in the application within the container. Working also with Linux servers.\n* Documenting the Data Model, by reverse engineering the application. Describing the purpose of tables and columns.\n* Doing Performance Tests on the application, identifying bottlenecks so execution takes acceptable times. Receiving a lot of support while doing it, not really diagnosing the problem and providing the solution on my own. Documenting reports about it.\n* Re-implementing an already existing functionality to make it more efficient. Basically writing queries with Scala in SparkSQL. Working with an Scala codebase.\n* Making sure NiFi was able to ingest files from an SFTP server.\n\nI'm asking this because in roughly 3 months there's a high chance I'll be hired in this company to continue working in this same project. \n\nHaving *Junior Data Engineer* in my job title would mean a lot to me, as I would have a better stance when applying for other positions elsewhere. When the time comes, I would like to have this conversation. But I'm not really sure I calling myself a *Junior Data Engineer* is stretching it too much...\n\nThanks for reading.", "author_fullname": "t2_fz3dp22k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intern working in a Data Pipeline, could I describe my experience as a Data Engineering internship?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zhghm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676109396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tldr: I&amp;#39;m working as an intern in a Data Pipeline built on top of Apache Nifi, Kafka, Spark (in Scala) and Cassandra and Postgress. CSV files are ingested and, after some transformations, analytics and graphs are provided in a web application. If I were to be hired, could my job title be &lt;em&gt;Junior Data Engineer&lt;/em&gt;? What should I be doing if I wanted that to be my role?&lt;/p&gt;\n\n&lt;p&gt;Hi, I&amp;#39;m a Computer Science B.Sc, yet to graduate.&lt;/p&gt;\n\n&lt;p&gt;A year ago I decided I wanted work towards entering into the Data Science field. Specifically, aiming for a Data Engineering role. I landed this internship whose title is &amp;quot;&lt;em&gt;Internship in Big Data Analytics&lt;/em&gt;&amp;quot;, but I&amp;#39;m not sure if the experience I am getting could enable me to portrait myself as an &lt;em&gt;entry-level Data Engineer&lt;/em&gt; of sorts.&lt;/p&gt;\n\n&lt;p&gt;I basically work in a Data Pipeline that is going to be sold as a product/service. CSV Files are ingested by apache NiFi and data is stored in Cassandra and PostgreSQL. Then, an Spark application (coded in Scala) is launched to apply some transformations, store them and, finally, a web application present results in many dashboards to the end-user.&lt;/p&gt;\n\n&lt;p&gt;Some tasks I&amp;#39;ve done so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Deploying the solution in docker, some troubleshooting and investigating how to configure some parameters in the application within the container. Working also with Linux servers.&lt;/li&gt;\n&lt;li&gt;Documenting the Data Model, by reverse engineering the application. Describing the purpose of tables and columns.&lt;/li&gt;\n&lt;li&gt;Doing Performance Tests on the application, identifying bottlenecks so execution takes acceptable times. Receiving a lot of support while doing it, not really diagnosing the problem and providing the solution on my own. Documenting reports about it.&lt;/li&gt;\n&lt;li&gt;Re-implementing an already existing functionality to make it more efficient. Basically writing queries with Scala in SparkSQL. Working with an Scala codebase.&lt;/li&gt;\n&lt;li&gt;Making sure NiFi was able to ingest files from an SFTP server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m asking this because in roughly 3 months there&amp;#39;s a high chance I&amp;#39;ll be hired in this company to continue working in this same project. &lt;/p&gt;\n\n&lt;p&gt;Having &lt;em&gt;Junior Data Engineer&lt;/em&gt; in my job title would mean a lot to me, as I would have a better stance when applying for other positions elsewhere. When the time comes, I would like to have this conversation. But I&amp;#39;m not really sure I calling myself a &lt;em&gt;Junior Data Engineer&lt;/em&gt; is stretching it too much...&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10zhghm", "is_robot_indexable": true, "report_reasons": null, "author": "racsoluk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zhghm/intern_working_in_a_data_pipeline_could_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zhghm/intern_working_in_a_data_pipeline_could_i/", "subreddit_subscribers": 89200, "created_utc": 1676109396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why should I consider creating a semantic layer using tools such as Cube when I can create a simple database views? Is it just because of caching and authentication? On the other hand it adds another layer of complexity. Would love to hear your thoughts.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why should one care about a semantic layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zhb45", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676108770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why should I consider creating a semantic layer using tools such as Cube when I can create a simple database views? Is it just because of caching and authentication? On the other hand it adds another layer of complexity. Would love to hear your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zhb45", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zhb45/why_should_one_care_about_a_semantic_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zhb45/why_should_one_care_about_a_semantic_layer/", "subreddit_subscribers": 89200, "created_utc": 1676108770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm making a data cleaning system for my company and I'd appreciate some feedback. It's more from an architectural point of view as to whether this is the best way to set things up. I'm new to this so all suggestions welcome.\n\nI have a monthly set of dirty files that need cleaning. I'd have an app that sits over an API so the files can be dropped and uploaded to a lake. This would trigger a cleaning workflow, using Apache Airflow, involving external cleaning tools. The resulting metrics of the cleaning job would be pushed to a database which is seen by the app. If the human is happy then the database record is changed from 'submitted' to 'accepted'. The final interaction is that in the app I'd have a dashboard sitting over the accepted lake data so I can extract information.\n\nI was told all apps should sit on an API to separate the logic into its own component. Is that widely shared? Should there be a single API for ingestion, database control and dashboarding? Is this workflow standard or is there another architecture?\n\nThank you!", "author_fullname": "t2_djdhkrg6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I set up an interactive data cleaning workflow properly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zfy3w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676103387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m making a data cleaning system for my company and I&amp;#39;d appreciate some feedback. It&amp;#39;s more from an architectural point of view as to whether this is the best way to set things up. I&amp;#39;m new to this so all suggestions welcome.&lt;/p&gt;\n\n&lt;p&gt;I have a monthly set of dirty files that need cleaning. I&amp;#39;d have an app that sits over an API so the files can be dropped and uploaded to a lake. This would trigger a cleaning workflow, using Apache Airflow, involving external cleaning tools. The resulting metrics of the cleaning job would be pushed to a database which is seen by the app. If the human is happy then the database record is changed from &amp;#39;submitted&amp;#39; to &amp;#39;accepted&amp;#39;. The final interaction is that in the app I&amp;#39;d have a dashboard sitting over the accepted lake data so I can extract information.&lt;/p&gt;\n\n&lt;p&gt;I was told all apps should sit on an API to separate the logic into its own component. Is that widely shared? Should there be a single API for ingestion, database control and dashboarding? Is this workflow standard or is there another architecture?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10zfy3w", "is_robot_indexable": true, "report_reasons": null, "author": "user192034", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zfy3w/how_do_i_set_up_an_interactive_data_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zfy3w/how_do_i_set_up_an_interactive_data_cleaning/", "subreddit_subscribers": 89200, "created_utc": 1676103387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is maybe a dumb question.\n\nOf the three Data Lakehouse solutions - Iceberg, Hudi, Delta Lake - can any be setup without Spark?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lakehouse non-Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z4ae3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is maybe a dumb question.&lt;/p&gt;\n\n&lt;p&gt;Of the three Data Lakehouse solutions - Iceberg, Hudi, Delta Lake - can any be setup without Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z4ae3", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z4ae3/data_lakehouse_nonspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z4ae3/data_lakehouse_nonspark/", "subreddit_subscribers": 89200, "created_utc": 1676066401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Use dbt With Snowpark Python to Implement Sentiment Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10z35fe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1Zk8X9mW1UjRExYc8VOmaw-to607AvYbVA0S5ufqZ0o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676063367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/how-to-use-dbt-with-snowpark-python-to-implement-sentiment-analysis/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?auto=webp&amp;v=enabled&amp;s=3ebb0325aacaa6e222c6f43986ef8f9d5c87b7b4", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8ea15e9306fd11888c65aaf5687e2342c006f53", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b37907e4ba460dba067872818766a4ecfbe9f52e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e76a85a3bcc92b92f26db83c65c6491c9c315492", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7609acf14b5263a4d5ed6eff6a08a20478e39186", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4583e1bee0e72219f0ebf0f798b08326bdbd71e9", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20e692234089d8a181a1408f904b286257e628bc", "width": 1080, "height": 565}], "variants": {}, "id": "9LtNOZb3NItXy3SHc0Bi4CxfR3vcwSkmIZJbXRCJPXY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10z35fe", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z35fe/how_to_use_dbt_with_snowpark_python_to_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/how-to-use-dbt-with-snowpark-python-to-implement-sentiment-analysis/", "subreddit_subscribers": 89200, "created_utc": 1676063367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,We are building out a Data Lake in S3. That much is straight forward. We will pipe data from a Postgres DB to S3 in Parquet format. But something confuses me. I'll explain.\n\nPostgres is \"dynamic\". If I update a record there, the correct record will have the new value. But files in S3 are \"static\". After a file is written there, it's there forever. If in the source (Postgres), a record is updated, a NEW file will be written to S3.\n\nQuestion: if all that is true, how can I meaningfully query this data without having duplicate IDs (records appearing more than once, but with different 'updated' timestamps)?\n\nSome ideas:\n\n1. a separate deduplication ETL pipeline?\n2. perhaps Athena can \"just do it\"?\n3. my premise is not accurate.\n\nPlease and thank you for your advice.  \nEDIT: clarified something in my question.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S3 Data Lake and duplicate entries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z22gn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676061486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676060773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,We are building out a Data Lake in S3. That much is straight forward. We will pipe data from a Postgres DB to S3 in Parquet format. But something confuses me. I&amp;#39;ll explain.&lt;/p&gt;\n\n&lt;p&gt;Postgres is &amp;quot;dynamic&amp;quot;. If I update a record there, the correct record will have the new value. But files in S3 are &amp;quot;static&amp;quot;. After a file is written there, it&amp;#39;s there forever. If in the source (Postgres), a record is updated, a NEW file will be written to S3.&lt;/p&gt;\n\n&lt;p&gt;Question: if all that is true, how can I meaningfully query this data without having duplicate IDs (records appearing more than once, but with different &amp;#39;updated&amp;#39; timestamps)?&lt;/p&gt;\n\n&lt;p&gt;Some ideas:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a separate deduplication ETL pipeline?&lt;/li&gt;\n&lt;li&gt;perhaps Athena can &amp;quot;just do it&amp;quot;?&lt;/li&gt;\n&lt;li&gt;my premise is not accurate.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please and thank you for your advice.&lt;br/&gt;\nEDIT: clarified something in my question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10z22gn", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z22gn/s3_data_lake_and_duplicate_entries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z22gn/s3_data_lake_and_duplicate_entries/", "subreddit_subscribers": 89200, "created_utc": 1676060773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I want to design a data warehouse to support BI workloads, where a BI Developer can create semantic models (sourced from the warehouse) to support various business reports. In the design of the warehouse, I know I can create multiple schemas to organize groups of tables with CREATE SCHEMA followed by several CREATE TABLE's to add tables within a particular schema. \n\nLet's say that the data engineers at an organization favor to load only denormalized tables (OBT) into their data warehouse for the BI workloads. My question is: would the data engineers create individual schemas in the warehouse for each one of these denormalized tables (Each schema has only 1 wide table)? So it is like a 1:1 mapping of schema to table.\n\nIf a different data engineering team favored the star schema instead, would there just be a single star schema inside of each schema?\n\nWe can also assume that there is no need to create schemas for security/permission reasons. All BI developers will have access to the entire warehouse.\n\nIs there ever a scenario where schema separation is irrelevant in a data warehouse?\n\nThanks in advanced for any help!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused about Data Warehouse Schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z19dl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676058884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I want to design a data warehouse to support BI workloads, where a BI Developer can create semantic models (sourced from the warehouse) to support various business reports. In the design of the warehouse, I know I can create multiple schemas to organize groups of tables with CREATE SCHEMA followed by several CREATE TABLE&amp;#39;s to add tables within a particular schema. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say that the data engineers at an organization favor to load only denormalized tables (OBT) into their data warehouse for the BI workloads. My question is: would the data engineers create individual schemas in the warehouse for each one of these denormalized tables (Each schema has only 1 wide table)? So it is like a 1:1 mapping of schema to table.&lt;/p&gt;\n\n&lt;p&gt;If a different data engineering team favored the star schema instead, would there just be a single star schema inside of each schema?&lt;/p&gt;\n\n&lt;p&gt;We can also assume that there is no need to create schemas for security/permission reasons. All BI developers will have access to the entire warehouse.&lt;/p&gt;\n\n&lt;p&gt;Is there ever a scenario where schema separation is irrelevant in a data warehouse?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advanced for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z19dl", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z19dl/confused_about_data_warehouse_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z19dl/confused_about_data_warehouse_schema/", "subreddit_subscribers": 89200, "created_utc": 1676058884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We'd like to use Production data to test performance in our dev environment. But the company doesn't allow us to use production data in dev. So we're looking to replace characters in specific fields in the parquet to the point where it's not considered production data so we can test it in dev. The requirement is that the data structure/characteristics must remain the same. Meaning, if we change \"John\" to \"Alex\" then all \"John\" must change to \"Alex\", same for addresses, phone numbers, IP addresses etc.\n\nCan we easily achieve this somehow with parquet files?  \n\n\nAfter doing a bit of research, seems like the best way to do this is:  \n1. Convert parquet to a CSV  \n2. Make changes to CSV  \n3. Create parquet files based on CSV", "author_fullname": "t2_1tbzh2ae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing characters &amp; numbers in parquet files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yy44g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676051311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;d like to use Production data to test performance in our dev environment. But the company doesn&amp;#39;t allow us to use production data in dev. So we&amp;#39;re looking to replace characters in specific fields in the parquet to the point where it&amp;#39;s not considered production data so we can test it in dev. The requirement is that the data structure/characteristics must remain the same. Meaning, if we change &amp;quot;John&amp;quot; to &amp;quot;Alex&amp;quot; then all &amp;quot;John&amp;quot; must change to &amp;quot;Alex&amp;quot;, same for addresses, phone numbers, IP addresses etc.&lt;/p&gt;\n\n&lt;p&gt;Can we easily achieve this somehow with parquet files?  &lt;/p&gt;\n\n&lt;p&gt;After doing a bit of research, seems like the best way to do this is:&lt;br/&gt;\n1. Convert parquet to a CSV&lt;br/&gt;\n2. Make changes to CSV&lt;br/&gt;\n3. Create parquet files based on CSV&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10yy44g", "is_robot_indexable": true, "report_reasons": null, "author": "TeslaMecca", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yy44g/replacing_characters_numbers_in_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yy44g/replacing_characters_numbers_in_parquet_files/", "subreddit_subscribers": 89200, "created_utc": 1676051311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on the meta database engineering certificate on coursera and I am almost done with it.\n\nI\u2019m wondering what next to do when I am done ?\n\nThe course does not go into detail about data warehousing tools, ETL tools or cloud technologies. So I am wondering if I should look more into that first. \nWhat do you guys think?", "author_fullname": "t2_83snl7wc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Steps for a career in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10zm3fv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676117061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on the meta database engineering certificate on coursera and I am almost done with it.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering what next to do when I am done ?&lt;/p&gt;\n\n&lt;p&gt;The course does not go into detail about data warehousing tools, ETL tools or cloud technologies. So I am wondering if I should look more into that first. \nWhat do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10zm3fv", "is_robot_indexable": true, "report_reasons": null, "author": "nbatobs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zm3fv/steps_for_a_career_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zm3fv/steps_for_a_career_in_data_engineering/", "subreddit_subscribers": 89200, "created_utc": 1676117061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started taking this three quarter long certificate course for data engineering. I am currently in the second quarter, and it has been okay but it's more about learning histories of big data and a little bit of learning to use spark dataframe and spark sql and scala. There was also some assignment using hbase and cassandra. \n\nBut overall I'm wondering if I should continue. I don't have any computer science background. I don't even have any background data analysis. Although my current job involves a little bit of data handling so I have used python and pandas. I guess I had high hopes from this course that it will make me hireable as a junior data engineer. I thought this would be something like a bootcamp but this is just once a week, 3hour lecture and some weekly assignment that takes 2-3hours.\n\nThe reason why I started was from recommendation from a friend. He told me that since I already have some experience using python for handling some data,  pursuing data engineering might be worth it. However, since then I've started taking second bachelor's degree for computer science. And I feel like I'll rather just focus on getting that bachelor's degree in computer science rather than trying to become a data engineer through a certificate.\n\nSo far, I feel like I'm just getting a taste of what data engineering is. Additionally, it feels like this course is more for already working software engineers who just want to specialize in data engineering. So I'm wondering if I should continue taking this course?\n\nDo you think that just knowing how to use some of these tools will make me hireable as a junior data engineer?\n\nI feel like I have this sunken cost, I just need to finish one more quarter. However, I think that this data engineering certificate will be worthless on my resume. I haven't decided what I want to do once I get the computer science degree. Likely to interview will ask me why I have the certificate but I feel like I don't have a good reason.\n\nPerhaps if I took this course in the future, I might have liked it more.", "author_fullname": "t2_89odwyx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I give up on Data engineering certificate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z3w7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676065256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started taking this three quarter long certificate course for data engineering. I am currently in the second quarter, and it has been okay but it&amp;#39;s more about learning histories of big data and a little bit of learning to use spark dataframe and spark sql and scala. There was also some assignment using hbase and cassandra. &lt;/p&gt;\n\n&lt;p&gt;But overall I&amp;#39;m wondering if I should continue. I don&amp;#39;t have any computer science background. I don&amp;#39;t even have any background data analysis. Although my current job involves a little bit of data handling so I have used python and pandas. I guess I had high hopes from this course that it will make me hireable as a junior data engineer. I thought this would be something like a bootcamp but this is just once a week, 3hour lecture and some weekly assignment that takes 2-3hours.&lt;/p&gt;\n\n&lt;p&gt;The reason why I started was from recommendation from a friend. He told me that since I already have some experience using python for handling some data,  pursuing data engineering might be worth it. However, since then I&amp;#39;ve started taking second bachelor&amp;#39;s degree for computer science. And I feel like I&amp;#39;ll rather just focus on getting that bachelor&amp;#39;s degree in computer science rather than trying to become a data engineer through a certificate.&lt;/p&gt;\n\n&lt;p&gt;So far, I feel like I&amp;#39;m just getting a taste of what data engineering is. Additionally, it feels like this course is more for already working software engineers who just want to specialize in data engineering. So I&amp;#39;m wondering if I should continue taking this course?&lt;/p&gt;\n\n&lt;p&gt;Do you think that just knowing how to use some of these tools will make me hireable as a junior data engineer?&lt;/p&gt;\n\n&lt;p&gt;I feel like I have this sunken cost, I just need to finish one more quarter. However, I think that this data engineering certificate will be worthless on my resume. I haven&amp;#39;t decided what I want to do once I get the computer science degree. Likely to interview will ask me why I have the certificate but I feel like I don&amp;#39;t have a good reason.&lt;/p&gt;\n\n&lt;p&gt;Perhaps if I took this course in the future, I might have liked it more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10z3w7i", "is_robot_indexable": true, "report_reasons": null, "author": "wsb_degen_number9999", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z3w7i/should_i_give_up_on_data_engineering_certificate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z3w7i/should_i_give_up_on_data_engineering_certificate/", "subreddit_subscribers": 89200, "created_utc": 1676065256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to engage some Collibra developers but the Collibra subreddit is gone now - what is the right place to find smart and interesting Collibra people? Thank you!", "author_fullname": "t2_4gb7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do Collibra developers find jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yxr90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676050377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to engage some Collibra developers but the Collibra subreddit is gone now - what is the right place to find smart and interesting Collibra people? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10yxr90", "is_robot_indexable": true, "report_reasons": null, "author": "xiangw", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yxr90/where_do_collibra_developers_find_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yxr90/where_do_collibra_developers_find_jobs/", "subreddit_subscribers": 89200, "created_utc": 1676050377.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}