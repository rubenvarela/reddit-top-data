{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_brkxjomi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Valentine's for your data sweetheart \ud83e\udef6", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"biduu0mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/biduu0mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79381bd9167be6bc4a6801a37db2b2c399155875"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/biduu0mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03da64f559ff0b77ce2202f22cd238438cfa7274"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/biduu0mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09ecbb1fd34068c8d8760d287e619947bc1fd49c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/biduu0mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83231bcaf3e67deb8fb3c67189c7891f09e47ab2"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/biduu0mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b432ecd6093ff860444963d6dc65d13409546d"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/biduu0mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=afe55d32aed951d2a501dee5d2343482da26cbb6"}, "id": "biduu0mtffha1"}, "ryo2izltffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ryo2izltffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf6105f3c39c2eb2f3dc6e9211cdb0bd3ed04d41"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ryo2izltffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7787c7546a5ca67acc869986ae7ca30d26c530c8"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/ryo2izltffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12b45e98685f26d1688e1900aec957a5e318d9a6"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/ryo2izltffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf130fe2d485801b60b1a6f806f7cb4c731072e6"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/ryo2izltffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dd73d7d82d9269117b82ccc09f9837c6be64459"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/ryo2izltffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=74ae962e6fffe5593810c429220809ada4d34d36"}, "id": "ryo2izltffha1"}, "xj1kc2mtffha1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f519c75e14dc49b1f0ff605527ca50a60757883"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aaf79cdeec3be1f685f0458862ef50ebe3e6cbe"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e682512352eeb61f1c5994ff667c2ecc13bcc99a"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b4563feeeb86c6b21cd0efca1b4fadd08df04e"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=999e144157716bc6c62dd03f600dc0ad06e5ffb9"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/xj1kc2mtffha1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=adaeca8dcb921f37da10cdcb3431878ca00f6623"}, "id": "xj1kc2mtffha1"}, "17fehyltffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/17fehyltffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac24ecb8bc5aad89c5a20c3960b2ae4e791c88f7"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/17fehyltffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67bc4621be7bda93147c7203e74c142eecd35465"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/17fehyltffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f066215eb3f86dd0ad5ed938ff00caf9880d22c"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/17fehyltffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21f7b6fb93b603495641b15ce5aedac34b847bb3"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/17fehyltffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0475e63182927b6c7bbfea6461cb540189a1e6c9"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/17fehyltffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=80a98dfcb93576438446c70cd56b955d95ae62f4"}, "id": "17fehyltffha1"}, "atd821mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/atd821mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193c7cfe15eb4c07ad44ab8ca9bc7bfe59dfa5ab"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/atd821mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=513c05e5e4182710264771882d427dbf4f09fcfc"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/atd821mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6024b102abb3bc7da079e72117920c698b5e0e07"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/atd821mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fef394e19adf67defd1e61f85f9674bf4aad2da"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/atd821mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7642046c5955c3ff4fcff245be24db0dd61148b"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/atd821mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=11f273bc54db2d84120dc5537dd1ffc261434fbb"}, "id": "atd821mtffha1"}, "n9jjt1mtffha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc475144c683e58f59a61985e6c1841d80a732bd"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9aafead2bd7b2106ec3d3488c08c32885bae5ab6"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=010168a10ee1b3d4292f0d5cf025906097f5b998"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0084fd2c5f0ff5de7079dc81efcd72e942cce43f"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=171505d0fca2097db573ffef84aa9f6f608c7b8d"}], "s": {"y": 540, "x": 960, "u": "https://preview.redd.it/n9jjt1mtffha1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7b4c9a1c9e4858df2a986bc98bd93d43052da8be"}, "id": "n9jjt1mtffha1"}}, "name": "t3_10z37l5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 118, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "ryo2izltffha1", "id": 239274768}, {"media_id": "17fehyltffha1", "id": 239274769}, {"media_id": "biduu0mtffha1", "id": 239274770}, {"media_id": "atd821mtffha1", "id": 239274771}, {"media_id": "n9jjt1mtffha1", "id": 239274772}, {"media_id": "xj1kc2mtffha1", "id": 239274773}]}, "link_flair_text": "Meme", "can_mod_post": false, "score": 118, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/akDVWEtAYUZrcqWztA1PtaB5z4ncRPFVMAhMrdocQL4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676063522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/10z37l5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10z37l5", "is_robot_indexable": true, "report_reasons": null, "author": "Straight_House8628", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z37l5/valentines_for_your_data_sweetheart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/10z37l5", "subreddit_subscribers": 89216, "created_utc": 1676063522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.", "author_fullname": "t2_t3ffq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data is Dead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10z1ft9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": "transparent", "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1pO0RBiV6vwhGezwNmiqCZro0ztqzvrl3hQ2Ib5SB8A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676059302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "motherduck.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://motherduck.com/blog/big-data-is-dead/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?auto=webp&amp;v=enabled&amp;s=422507ff3e1d876f1565f9afa286ac2a3b12b70d", "width": 1024, "height": 535}, "resolutions": [{"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eea8d6f5a4b37bc287fcc1bd5d4b8364089b4589", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84293dc6fd5ff654378e1403be79188e69070a05", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8c1be6a0050d06000aac1c8326148e53643be6b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1af5f5964cf9dc80b6d7c498f369c8505d1a4dc3", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/TqQR4yr7OcSlC9Ff2n9_e25bqRinYrh6yPODZ9k0L-c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ab1c4a64b03a27503a012ea8235765e4de0220", "width": 960, "height": 501}], "variants": {}, "id": "C3v2JVQcD5hONfZJ0FsXvcBoNEQPQWKeM2eQkWh2PMw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10z1ft9", "is_robot_indexable": true, "report_reasons": null, "author": "FortunOfficial", "discussion_type": null, "num_comments": 52, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10z1ft9/big_data_is_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://motherduck.com/blog/big-data-is-dead/", "subreddit_subscribers": 89216, "created_utc": 1676059302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We need to store somewhere realtime data and I am considering OLAP databases like Druid, Pinot, Clickhouse and timeseries databases like TimescaleDB, Influx.. \nWhy should one prefer one over other? What are the use cases one can handle the other can not? What is one better at than the other?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realtime data - OLAP or Timeseries databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zf6rn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676100508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We need to store somewhere realtime data and I am considering OLAP databases like Druid, Pinot, Clickhouse and timeseries databases like TimescaleDB, Influx.. \nWhy should one prefer one over other? What are the use cases one can handle the other can not? What is one better at than the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zf6rn", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zf6rn/realtime_data_olap_or_timeseries_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zf6rn/realtime_data_olap_or_timeseries_databases/", "subreddit_subscribers": 89216, "created_utc": 1676100508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I often see lakehouse stages named like: bronze, silver, gold. I guess bronze represents the very raw data (e.g. downloaded jsons, csv, etc). Gold is the ultimate result for end user (stored using e.g. Iceberg). But what silver is supposed to be? What should be the differences between silver and gold?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse stages naming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zei0k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676097942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I often see lakehouse stages named like: bronze, silver, gold. I guess bronze represents the very raw data (e.g. downloaded jsons, csv, etc). Gold is the ultimate result for end user (stored using e.g. Iceberg). But what silver is supposed to be? What should be the differences between silver and gold?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zei0k", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zei0k/lakehouse_stages_naming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zei0k/lakehouse_stages_naming/", "subreddit_subscribers": 89216, "created_utc": 1676097942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Passing Google Cloud Certified Professional Data Engineer Exam in 2023\n\n\\[English version\\] [Passing Google Cloud Professional Data Engineer Exam in 2023](https://medium.com/aidatatools/passing-google-cloud-professional-data-engineer-exam-in-2023-7eb2e2e8ea0e)\n\n\u901a\u904eGCP\u8a8d\u8b49\u8cc7\u6599\u5de5\u7a0b\u5e2b\u7684\u8003\u8a66\u5fc3\u5f97\u5206\u4eab\n\n\\[\u96d9\u8a9e\u7248\\] [Passing Google Cloud Professional Data Engineer Exam in 2023](https://jasonchuang.substack.com/p/passing-google-cloud-professional)", "author_fullname": "t2_7uik3gn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passing Google Cloud Certified Professional Data Engineer Exam in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z47fd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Passing Google Cloud Certified Professional Data Engineer Exam in 2023&lt;/p&gt;\n\n&lt;p&gt;[English version] &lt;a href=\"https://medium.com/aidatatools/passing-google-cloud-professional-data-engineer-exam-in-2023-7eb2e2e8ea0e\"&gt;Passing Google Cloud Professional Data Engineer Exam in 2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\u901a\u904eGCP\u8a8d\u8b49\u8cc7\u6599\u5de5\u7a0b\u5e2b\u7684\u8003\u8a66\u5fc3\u5f97\u5206\u4eab&lt;/p&gt;\n\n&lt;p&gt;[\u96d9\u8a9e\u7248] &lt;a href=\"https://jasonchuang.substack.com/p/passing-google-cloud-professional\"&gt;Passing Google Cloud Professional Data Engineer Exam in 2023&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?auto=webp&amp;v=enabled&amp;s=19257639f54d68848b831958e26796285bcac00e", "width": 617, "height": 472}, "resolutions": [{"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65d915362523383343976db338ed4d5b1836875b", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c4297d241bd67bdd69ac6b6ad35acd0ccae6b67", "width": 216, "height": 165}, {"url": "https://external-preview.redd.it/NKw0dgV7sanfRe4-HESiPNfdZRvfFBdzWS6arL4eZQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=276c1e31603cffeea2d0cada8f6fdcd4f9f7615d", "width": 320, "height": 244}], "variants": {}, "id": "HbdGQG9XOZg3TP_DSmXkz4yQjrRAIxAn4tROL699ahY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10z47fd", "is_robot_indexable": true, "report_reasons": null, "author": "Few_Knee1141", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z47fd/passing_google_cloud_certified_professional_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z47fd/passing_google_cloud_certified_professional_data/", "subreddit_subscribers": 89216, "created_utc": 1676066157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says I have my first technical interview next week for an entry level role. I\u2019ve been asked to give a 10 min presentation but what I am worried about is the interview is 90 mins. What other things  are they gonna ask me for that long? Ive been doing personal projects for a while and now seeking my first role so the imposter syndrome is sky high.\n\nAny help would be appreciated", "author_fullname": "t2_4ba5z1zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First ever data engineering technical interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zjhl6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676112772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says I have my first technical interview next week for an entry level role. I\u2019ve been asked to give a 10 min presentation but what I am worried about is the interview is 90 mins. What other things  are they gonna ask me for that long? Ive been doing personal projects for a while and now seeking my first role so the imposter syndrome is sky high.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10zjhl6", "is_robot_indexable": true, "report_reasons": null, "author": "dreamr49", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zjhl6/first_ever_data_engineering_technical_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zjhl6/first_ever_data_engineering_technical_interview/", "subreddit_subscribers": 89216, "created_utc": 1676112772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "tldr: I'm working as an intern in a Data Pipeline built on top of Apache Nifi, Kafka, Spark (in Scala) and Cassandra and Postgress. CSV files are ingested and, after some transformations, analytics and graphs are provided in a web application. If I were to be hired, could my job title be *Junior Data Engineer*? What should I be doing if I wanted that to be my role?\n\nHi, I'm a Computer Science B.Sc, yet to graduate.\n\nA year ago I decided I wanted work towards entering into the Data Science field. Specifically, aiming for a Data Engineering role. I landed this internship whose title is \"*Internship in Big Data Analytics*\", but I'm not sure if the experience I am getting could enable me to portrait myself as an *entry-level Data Engineer* of sorts.\n\nI basically work in a Data Pipeline that is going to be sold as a product/service. CSV Files are ingested by apache NiFi and data is stored in Cassandra and PostgreSQL. Then, an Spark application (coded in Scala) is launched to apply some transformations, store them and, finally, a web application present results in many dashboards to the end-user.\n\nSome tasks I've done so far:\n\n* Deploying the solution in docker, some troubleshooting and investigating how to configure some parameters in the application within the container. Working also with Linux servers.\n* Documenting the Data Model, by reverse engineering the application. Describing the purpose of tables and columns.\n* Doing Performance Tests on the application, identifying bottlenecks so execution takes acceptable times. Receiving a lot of support while doing it, not really diagnosing the problem and providing the solution on my own. Documenting reports about it.\n* Re-implementing an already existing functionality to make it more efficient. Basically writing queries with Scala in SparkSQL. Working with an Scala codebase.\n* Making sure NiFi was able to ingest files from an SFTP server.\n\nI'm asking this because in roughly 3 months there's a high chance I'll be hired in this company to continue working in this same project. \n\nHaving *Junior Data Engineer* in my job title would mean a lot to me, as I would have a better stance when applying for other positions elsewhere. When the time comes, I would like to have this conversation. But I'm not really sure I calling myself a *Junior Data Engineer* is stretching it too much...\n\nThanks for reading.", "author_fullname": "t2_fz3dp22k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intern working in a Data Pipeline, could I describe my experience as a Data Engineering internship?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zhghm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676109396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tldr: I&amp;#39;m working as an intern in a Data Pipeline built on top of Apache Nifi, Kafka, Spark (in Scala) and Cassandra and Postgress. CSV files are ingested and, after some transformations, analytics and graphs are provided in a web application. If I were to be hired, could my job title be &lt;em&gt;Junior Data Engineer&lt;/em&gt;? What should I be doing if I wanted that to be my role?&lt;/p&gt;\n\n&lt;p&gt;Hi, I&amp;#39;m a Computer Science B.Sc, yet to graduate.&lt;/p&gt;\n\n&lt;p&gt;A year ago I decided I wanted work towards entering into the Data Science field. Specifically, aiming for a Data Engineering role. I landed this internship whose title is &amp;quot;&lt;em&gt;Internship in Big Data Analytics&lt;/em&gt;&amp;quot;, but I&amp;#39;m not sure if the experience I am getting could enable me to portrait myself as an &lt;em&gt;entry-level Data Engineer&lt;/em&gt; of sorts.&lt;/p&gt;\n\n&lt;p&gt;I basically work in a Data Pipeline that is going to be sold as a product/service. CSV Files are ingested by apache NiFi and data is stored in Cassandra and PostgreSQL. Then, an Spark application (coded in Scala) is launched to apply some transformations, store them and, finally, a web application present results in many dashboards to the end-user.&lt;/p&gt;\n\n&lt;p&gt;Some tasks I&amp;#39;ve done so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Deploying the solution in docker, some troubleshooting and investigating how to configure some parameters in the application within the container. Working also with Linux servers.&lt;/li&gt;\n&lt;li&gt;Documenting the Data Model, by reverse engineering the application. Describing the purpose of tables and columns.&lt;/li&gt;\n&lt;li&gt;Doing Performance Tests on the application, identifying bottlenecks so execution takes acceptable times. Receiving a lot of support while doing it, not really diagnosing the problem and providing the solution on my own. Documenting reports about it.&lt;/li&gt;\n&lt;li&gt;Re-implementing an already existing functionality to make it more efficient. Basically writing queries with Scala in SparkSQL. Working with an Scala codebase.&lt;/li&gt;\n&lt;li&gt;Making sure NiFi was able to ingest files from an SFTP server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m asking this because in roughly 3 months there&amp;#39;s a high chance I&amp;#39;ll be hired in this company to continue working in this same project. &lt;/p&gt;\n\n&lt;p&gt;Having &lt;em&gt;Junior Data Engineer&lt;/em&gt; in my job title would mean a lot to me, as I would have a better stance when applying for other positions elsewhere. When the time comes, I would like to have this conversation. But I&amp;#39;m not really sure I calling myself a &lt;em&gt;Junior Data Engineer&lt;/em&gt; is stretching it too much...&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10zhghm", "is_robot_indexable": true, "report_reasons": null, "author": "racsoluk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zhghm/intern_working_in_a_data_pipeline_could_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zhghm/intern_working_in_a_data_pipeline_could_i/", "subreddit_subscribers": 89216, "created_utc": 1676109396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why should I consider creating a semantic layer using tools such as Cube when I can create a simple database views? Is it just because of caching and authentication? On the other hand it adds another layer of complexity. Would love to hear your thoughts.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why should one care about a semantic layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zhb45", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676108770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why should I consider creating a semantic layer using tools such as Cube when I can create a simple database views? Is it just because of caching and authentication? On the other hand it adds another layer of complexity. Would love to hear your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zhb45", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zhb45/why_should_one_care_about_a_semantic_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zhb45/why_should_one_care_about_a_semantic_layer/", "subreddit_subscribers": 89216, "created_utc": 1676108770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm getting quite confused about this. So it's very typical in a transactional system to have header line data, like purchase order and purchase line.\n\nWhen I bring this into a data warehouse and I'm designing the fact tables, how do I model this relationship? If I add columns for the header detail I'll have tonnes of redundant, repeated data. But having 2 linked facts is also not correct.\n\nAny help is appreciated thanks!", "author_fullname": "t2_32y0tqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model header line data in a DWH without duplicating data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yxiy6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676049784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting quite confused about this. So it&amp;#39;s very typical in a transactional system to have header line data, like purchase order and purchase line.&lt;/p&gt;\n\n&lt;p&gt;When I bring this into a data warehouse and I&amp;#39;m designing the fact tables, how do I model this relationship? If I add columns for the header detail I&amp;#39;ll have tonnes of redundant, repeated data. But having 2 linked facts is also not correct.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10yxiy6", "is_robot_indexable": true, "report_reasons": null, "author": "MrWriter1234", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yxiy6/how_to_model_header_line_data_in_a_dwh_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yxiy6/how_to_model_header_line_data_in_a_dwh_without/", "subreddit_subscribers": 89216, "created_utc": 1676049784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm clearly searching wrong because I cant find the answer even in their documentation. I assume the limit depends on the size of the cluster, but I'm struggling to find the number...", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many concurrent queries does Databricks SQL Compute warehouses support? I need this to decide the minimum number of clusters that need to be active to meet a set concurrency level and SLA. in order to Cant find anyway to set this in the cluster config.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10zq7t2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676130045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m clearly searching wrong because I cant find the answer even in their documentation. I assume the limit depends on the size of the cluster, but I&amp;#39;m struggling to find the number...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zq7t2", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zq7t2/how_many_concurrent_queries_does_databricks_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zq7t2/how_many_concurrent_queries_does_databricks_sql/", "subreddit_subscribers": 89216, "created_utc": 1676130045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm making a data cleaning system for my company and I'd appreciate some feedback. It's more from an architectural point of view as to whether this is the best way to set things up. I'm new to this so all suggestions welcome.\n\nI have a monthly set of dirty files that need cleaning. I'd have an app that sits over an API so the files can be dropped and uploaded to a lake. This would trigger a cleaning workflow, using Apache Airflow, involving external cleaning tools. The resulting metrics of the cleaning job would be pushed to a database which is seen by the app. If the human is happy then the database record is changed from 'submitted' to 'accepted'. The final interaction is that in the app I'd have a dashboard sitting over the accepted lake data so I can extract information.\n\nI was told all apps should sit on an API to separate the logic into its own component. Is that widely shared? Should there be a single API for ingestion, database control and dashboarding? Is this workflow standard or is there another architecture?\n\nThank you!", "author_fullname": "t2_djdhkrg6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I set up an interactive data cleaning workflow properly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zfy3w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676103387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m making a data cleaning system for my company and I&amp;#39;d appreciate some feedback. It&amp;#39;s more from an architectural point of view as to whether this is the best way to set things up. I&amp;#39;m new to this so all suggestions welcome.&lt;/p&gt;\n\n&lt;p&gt;I have a monthly set of dirty files that need cleaning. I&amp;#39;d have an app that sits over an API so the files can be dropped and uploaded to a lake. This would trigger a cleaning workflow, using Apache Airflow, involving external cleaning tools. The resulting metrics of the cleaning job would be pushed to a database which is seen by the app. If the human is happy then the database record is changed from &amp;#39;submitted&amp;#39; to &amp;#39;accepted&amp;#39;. The final interaction is that in the app I&amp;#39;d have a dashboard sitting over the accepted lake data so I can extract information.&lt;/p&gt;\n\n&lt;p&gt;I was told all apps should sit on an API to separate the logic into its own component. Is that widely shared? Should there be a single API for ingestion, database control and dashboarding? Is this workflow standard or is there another architecture?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10zfy3w", "is_robot_indexable": true, "report_reasons": null, "author": "user192034", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zfy3w/how_do_i_set_up_an_interactive_data_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zfy3w/how_do_i_set_up_an_interactive_data_cleaning/", "subreddit_subscribers": 89216, "created_utc": 1676103387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is maybe a dumb question.\n\nOf the three Data Lakehouse solutions - Iceberg, Hudi, Delta Lake - can any be setup without Spark?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lakehouse non-Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z4ae3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676066401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is maybe a dumb question.&lt;/p&gt;\n\n&lt;p&gt;Of the three Data Lakehouse solutions - Iceberg, Hudi, Delta Lake - can any be setup without Spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z4ae3", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z4ae3/data_lakehouse_nonspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z4ae3/data_lakehouse_nonspark/", "subreddit_subscribers": 89216, "created_utc": 1676066401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6hz5qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Use dbt With Snowpark Python to Implement Sentiment Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_10z35fe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1Zk8X9mW1UjRExYc8VOmaw-to607AvYbVA0S5ufqZ0o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676063367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phdata.io/blog/how-to-use-dbt-with-snowpark-python-to-implement-sentiment-analysis/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?auto=webp&amp;v=enabled&amp;s=3ebb0325aacaa6e222c6f43986ef8f9d5c87b7b4", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8ea15e9306fd11888c65aaf5687e2342c006f53", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b37907e4ba460dba067872818766a4ecfbe9f52e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e76a85a3bcc92b92f26db83c65c6491c9c315492", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7609acf14b5263a4d5ed6eff6a08a20478e39186", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4583e1bee0e72219f0ebf0f798b08326bdbd71e9", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/WBYXNsJB5xfFuQEkPd57dphjE7kJ2E25InomV-n2cKI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20e692234089d8a181a1408f904b286257e628bc", "width": 1080, "height": 565}], "variants": {}, "id": "9LtNOZb3NItXy3SHc0Bi4CxfR3vcwSkmIZJbXRCJPXY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10z35fe", "is_robot_indexable": true, "report_reasons": null, "author": "OptimizedGradient", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z35fe/how_to_use_dbt_with_snowpark_python_to_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phdata.io/blog/how-to-use-dbt-with-snowpark-python-to-implement-sentiment-analysis/", "subreddit_subscribers": 89216, "created_utc": 1676063367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,We are building out a Data Lake in S3. That much is straight forward. We will pipe data from a Postgres DB to S3 in Parquet format. But something confuses me. I'll explain.\n\nPostgres is \"dynamic\". If I update a record there, the correct record will have the new value. But files in S3 are \"static\". After a file is written there, it's there forever. If in the source (Postgres), a record is updated, a NEW file will be written to S3.\n\nQuestion: if all that is true, how can I meaningfully query this data without having duplicate IDs (records appearing more than once, but with different 'updated' timestamps)?\n\nSome ideas:\n\n1. a separate deduplication ETL pipeline?\n2. perhaps Athena can \"just do it\"?\n3. my premise is not accurate.\n\nPlease and thank you for your advice.  \nEDIT: clarified something in my question.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S3 Data Lake and duplicate entries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z22gn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676061486.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676060773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,We are building out a Data Lake in S3. That much is straight forward. We will pipe data from a Postgres DB to S3 in Parquet format. But something confuses me. I&amp;#39;ll explain.&lt;/p&gt;\n\n&lt;p&gt;Postgres is &amp;quot;dynamic&amp;quot;. If I update a record there, the correct record will have the new value. But files in S3 are &amp;quot;static&amp;quot;. After a file is written there, it&amp;#39;s there forever. If in the source (Postgres), a record is updated, a NEW file will be written to S3.&lt;/p&gt;\n\n&lt;p&gt;Question: if all that is true, how can I meaningfully query this data without having duplicate IDs (records appearing more than once, but with different &amp;#39;updated&amp;#39; timestamps)?&lt;/p&gt;\n\n&lt;p&gt;Some ideas:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a separate deduplication ETL pipeline?&lt;/li&gt;\n&lt;li&gt;perhaps Athena can &amp;quot;just do it&amp;quot;?&lt;/li&gt;\n&lt;li&gt;my premise is not accurate.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please and thank you for your advice.&lt;br/&gt;\nEDIT: clarified something in my question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10z22gn", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z22gn/s3_data_lake_and_duplicate_entries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z22gn/s3_data_lake_and_duplicate_entries/", "subreddit_subscribers": 89216, "created_utc": 1676060773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say I want to design a data warehouse to support BI workloads, where a BI Developer can create semantic models (sourced from the warehouse) to support various business reports. In the design of the warehouse, I know I can create multiple schemas to organize groups of tables with CREATE SCHEMA followed by several CREATE TABLE's to add tables within a particular schema. \n\nLet's say that the data engineers at an organization favor to load only denormalized tables (OBT) into their data warehouse for the BI workloads. My question is: would the data engineers create individual schemas in the warehouse for each one of these denormalized tables (Each schema has only 1 wide table)? So it is like a 1:1 mapping of schema to table.\n\nIf a different data engineering team favored the star schema instead, would there just be a single star schema inside of each schema?\n\nWe can also assume that there is no need to create schemas for security/permission reasons. All BI developers will have access to the entire warehouse.\n\nIs there ever a scenario where schema separation is irrelevant in a data warehouse?\n\nThanks in advanced for any help!", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused about Data Warehouse Schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z19dl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676058884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I want to design a data warehouse to support BI workloads, where a BI Developer can create semantic models (sourced from the warehouse) to support various business reports. In the design of the warehouse, I know I can create multiple schemas to organize groups of tables with CREATE SCHEMA followed by several CREATE TABLE&amp;#39;s to add tables within a particular schema. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say that the data engineers at an organization favor to load only denormalized tables (OBT) into their data warehouse for the BI workloads. My question is: would the data engineers create individual schemas in the warehouse for each one of these denormalized tables (Each schema has only 1 wide table)? So it is like a 1:1 mapping of schema to table.&lt;/p&gt;\n\n&lt;p&gt;If a different data engineering team favored the star schema instead, would there just be a single star schema inside of each schema?&lt;/p&gt;\n\n&lt;p&gt;We can also assume that there is no need to create schemas for security/permission reasons. All BI developers will have access to the entire warehouse.&lt;/p&gt;\n\n&lt;p&gt;Is there ever a scenario where schema separation is irrelevant in a data warehouse?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advanced for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10z19dl", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z19dl/confused_about_data_warehouse_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z19dl/confused_about_data_warehouse_schema/", "subreddit_subscribers": 89216, "created_utc": 1676058884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We'd like to use Production data to test performance in our dev environment. But the company doesn't allow us to use production data in dev. So we're looking to replace characters in specific fields in the parquet to the point where it's not considered production data so we can test it in dev. The requirement is that the data structure/characteristics must remain the same. Meaning, if we change \"John\" to \"Alex\" then all \"John\" must change to \"Alex\", same for addresses, phone numbers, IP addresses etc.\n\nCan we easily achieve this somehow with parquet files?  \n\n\nAfter doing a bit of research, seems like the best way to do this is:  \n1. Convert parquet to a CSV  \n2. Make changes to CSV  \n3. Create parquet files based on CSV", "author_fullname": "t2_1tbzh2ae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing characters &amp; numbers in parquet files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yy44g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676051311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;d like to use Production data to test performance in our dev environment. But the company doesn&amp;#39;t allow us to use production data in dev. So we&amp;#39;re looking to replace characters in specific fields in the parquet to the point where it&amp;#39;s not considered production data so we can test it in dev. The requirement is that the data structure/characteristics must remain the same. Meaning, if we change &amp;quot;John&amp;quot; to &amp;quot;Alex&amp;quot; then all &amp;quot;John&amp;quot; must change to &amp;quot;Alex&amp;quot;, same for addresses, phone numbers, IP addresses etc.&lt;/p&gt;\n\n&lt;p&gt;Can we easily achieve this somehow with parquet files?  &lt;/p&gt;\n\n&lt;p&gt;After doing a bit of research, seems like the best way to do this is:&lt;br/&gt;\n1. Convert parquet to a CSV&lt;br/&gt;\n2. Make changes to CSV&lt;br/&gt;\n3. Create parquet files based on CSV&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10yy44g", "is_robot_indexable": true, "report_reasons": null, "author": "TeslaMecca", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yy44g/replacing_characters_numbers_in_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yy44g/replacing_characters_numbers_in_parquet_files/", "subreddit_subscribers": 89216, "created_utc": 1676051311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\nI am trying to find an efficient and collaborative way to insert relational data records into an RDBMS. \n\nIt will have to do foreign key lookups to help data population by suggesting the foreign key lookup. \n\nI can build a custom html web app to do this but couldn\u2019t find and framework/tool that does this. \n\nThank you.", "author_fullname": "t2_vnxs1lvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDBMS data entry", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10zqtg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676131251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nI am trying to find an efficient and collaborative way to insert relational data records into an RDBMS. &lt;/p&gt;\n\n&lt;p&gt;It will have to do foreign key lookups to help data population by suggesting the foreign key lookup. &lt;/p&gt;\n\n&lt;p&gt;I can build a custom html web app to do this but couldn\u2019t find and framework/tool that does this. &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zqtg3", "is_robot_indexable": true, "report_reasons": null, "author": "sumosumo234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zqtg3/rdbms_data_entry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zqtg3/rdbms_data_entry/", "subreddit_subscribers": 89216, "created_utc": 1676131251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I joined a company as the only data engineer, my first task is to make an overall presentation of the best practices to use the Azure data engineering stack to make a data warehouse for our business analysts. I was curious what people in other organizations are using?\n\nOur data sources include some legacy applications, APIs, SaaS applications etc. I was thinking of using synapse pipelines or data factory to extract all the data in a data lake, then use either Azure functions or SQL scripts in synapse to do any transformations and push it to a dedicated sql pool, and use it as the delivery platform for our BI analysts.\n\nAlthough I really want to use databricks since coding my solutions makes more sense to me, but from a pricing and 'low-code' standpoint from my management, I am stuck to keeping my solutions low code as possible and only use when it's necessary.\n\nAny advices on best practices to how others are using Azure are welcome.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices For Data Engineering on Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10zqt7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676131236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I joined a company as the only data engineer, my first task is to make an overall presentation of the best practices to use the Azure data engineering stack to make a data warehouse for our business analysts. I was curious what people in other organizations are using?&lt;/p&gt;\n\n&lt;p&gt;Our data sources include some legacy applications, APIs, SaaS applications etc. I was thinking of using synapse pipelines or data factory to extract all the data in a data lake, then use either Azure functions or SQL scripts in synapse to do any transformations and push it to a dedicated sql pool, and use it as the delivery platform for our BI analysts.&lt;/p&gt;\n\n&lt;p&gt;Although I really want to use databricks since coding my solutions makes more sense to me, but from a pricing and &amp;#39;low-code&amp;#39; standpoint from my management, I am stuck to keeping my solutions low code as possible and only use when it&amp;#39;s necessary.&lt;/p&gt;\n\n&lt;p&gt;Any advices on best practices to how others are using Azure are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10zqt7n", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zqt7n/best_practices_for_data_engineering_on_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zqt7n/best_practices_for_data_engineering_on_azure/", "subreddit_subscribers": 89216, "created_utc": 1676131236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am currently starting to use AWS for a project. However I am unsure how to treat access to services. F.e I have Airflow deployed on an ec2 instance and it runs a Python script which pushes data in S3 etc. To load the files in S3 I need Access Keys, for which I have created a new user. I have stored them in Secrets Manager. To access the keys in secrets Manager from my script, I again need Access Keys (aka a new user). F.e for using various other aws operators in airflow, one also has to attach access keys.\n\nI now have created new users for the services which have the minimal needed policy attached. But it seems a bit wrong to use users for that.  Having one user with enough access rights to run everything seems to be wrong too. And I am roles don't provide me keys which I can use. \n\nWhat is best practice to solve this?", "author_fullname": "t2_v219tksh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle access to different aws services with access keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10zpsp2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676129440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am currently starting to use AWS for a project. However I am unsure how to treat access to services. F.e I have Airflow deployed on an ec2 instance and it runs a Python script which pushes data in S3 etc. To load the files in S3 I need Access Keys, for which I have created a new user. I have stored them in Secrets Manager. To access the keys in secrets Manager from my script, I again need Access Keys (aka a new user). F.e for using various other aws operators in airflow, one also has to attach access keys.&lt;/p&gt;\n\n&lt;p&gt;I now have created new users for the services which have the minimal needed policy attached. But it seems a bit wrong to use users for that.  Having one user with enough access rights to run everything seems to be wrong too. And I am roles don&amp;#39;t provide me keys which I can use. &lt;/p&gt;\n\n&lt;p&gt;What is best practice to solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10zpsp2", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Hand-577", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zpsp2/how_to_handle_access_to_different_aws_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zpsp2/how_to_handle_access_to_different_aws_services/", "subreddit_subscribers": 89216, "created_utc": 1676129440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am currently starting to use AWS for a project. However I am unsure how to treat access to services. F.e I have Airflow deployed on an ec2 instance and it runs a Python script which pushes data in S3 etc. To load the files in S3 I need Access Keys, for which I have created a new user. I have stored them in Secrets Manager. To access the keys in secrets Manager from my script, I again need Access Keys (aka a new user). F.e for using various other aws operators in airflow, one also has to attach access keys.\n\nI now have created new users for the services which have the minimal needed policy attached. But it seems a bit wrong to use users for that.  Having one user with enough access rights to run everything seems to be wrong too. And I am roles don't provide me keys which I can use. \n\nWhat is best practice to solve this?", "author_fullname": "t2_v219tksh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle access to different aws services with access keys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10zpf3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676128021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am currently starting to use AWS for a project. However I am unsure how to treat access to services. F.e I have Airflow deployed on an ec2 instance and it runs a Python script which pushes data in S3 etc. To load the files in S3 I need Access Keys, for which I have created a new user. I have stored them in Secrets Manager. To access the keys in secrets Manager from my script, I again need Access Keys (aka a new user). F.e for using various other aws operators in airflow, one also has to attach access keys.&lt;/p&gt;\n\n&lt;p&gt;I now have created new users for the services which have the minimal needed policy attached. But it seems a bit wrong to use users for that.  Having one user with enough access rights to run everything seems to be wrong too. And I am roles don&amp;#39;t provide me keys which I can use. &lt;/p&gt;\n\n&lt;p&gt;What is best practice to solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10zpf3i", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Hand-577", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zpf3i/how_to_handle_access_to_different_aws_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zpf3i/how_to_handle_access_to_different_aws_services/", "subreddit_subscribers": 89216, "created_utc": 1676128021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to create Databricks cluster, jobs in my workspace \n\nApart from Terraform(due to client policies), please suggest steps to create databricks cluster, job etc in azure devops cicd process", "author_fullname": "t2_6iqir5tk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks \u00d7 Azure Devops", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10zokyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676124842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to create Databricks cluster, jobs in my workspace &lt;/p&gt;\n\n&lt;p&gt;Apart from Terraform(due to client policies), please suggest steps to create databricks cluster, job etc in azure devops cicd process&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10zokyk", "is_robot_indexable": true, "report_reasons": null, "author": "pinky_07", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10zokyk/databricks_azure_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10zokyk/databricks_azure_devops/", "subreddit_subscribers": 89216, "created_utc": 1676124842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started taking this three quarter long certificate course for data engineering. I am currently in the second quarter, and it has been okay but it's more about learning histories of big data and a little bit of learning to use spark dataframe and spark sql and scala. There was also some assignment using hbase and cassandra. \n\nBut overall I'm wondering if I should continue. I don't have any computer science background. I don't even have any background data analysis. Although my current job involves a little bit of data handling so I have used python and pandas. I guess I had high hopes from this course that it will make me hireable as a junior data engineer. I thought this would be something like a bootcamp but this is just once a week, 3hour lecture and some weekly assignment that takes 2-3hours.\n\nThe reason why I started was from recommendation from a friend. He told me that since I already have some experience using python for handling some data,  pursuing data engineering might be worth it. However, since then I've started taking second bachelor's degree for computer science. And I feel like I'll rather just focus on getting that bachelor's degree in computer science rather than trying to become a data engineer through a certificate.\n\nSo far, I feel like I'm just getting a taste of what data engineering is. Additionally, it feels like this course is more for already working software engineers who just want to specialize in data engineering. So I'm wondering if I should continue taking this course?\n\nDo you think that just knowing how to use some of these tools will make me hireable as a junior data engineer?\n\nI feel like I have this sunken cost, I just need to finish one more quarter. However, I think that this data engineering certificate will be worthless on my resume. I haven't decided what I want to do once I get the computer science degree. Likely to interview will ask me why I have the certificate but I feel like I don't have a good reason.\n\nPerhaps if I took this course in the future, I might have liked it more.", "author_fullname": "t2_89odwyx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I give up on Data engineering certificate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10z3w7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676065256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started taking this three quarter long certificate course for data engineering. I am currently in the second quarter, and it has been okay but it&amp;#39;s more about learning histories of big data and a little bit of learning to use spark dataframe and spark sql and scala. There was also some assignment using hbase and cassandra. &lt;/p&gt;\n\n&lt;p&gt;But overall I&amp;#39;m wondering if I should continue. I don&amp;#39;t have any computer science background. I don&amp;#39;t even have any background data analysis. Although my current job involves a little bit of data handling so I have used python and pandas. I guess I had high hopes from this course that it will make me hireable as a junior data engineer. I thought this would be something like a bootcamp but this is just once a week, 3hour lecture and some weekly assignment that takes 2-3hours.&lt;/p&gt;\n\n&lt;p&gt;The reason why I started was from recommendation from a friend. He told me that since I already have some experience using python for handling some data,  pursuing data engineering might be worth it. However, since then I&amp;#39;ve started taking second bachelor&amp;#39;s degree for computer science. And I feel like I&amp;#39;ll rather just focus on getting that bachelor&amp;#39;s degree in computer science rather than trying to become a data engineer through a certificate.&lt;/p&gt;\n\n&lt;p&gt;So far, I feel like I&amp;#39;m just getting a taste of what data engineering is. Additionally, it feels like this course is more for already working software engineers who just want to specialize in data engineering. So I&amp;#39;m wondering if I should continue taking this course?&lt;/p&gt;\n\n&lt;p&gt;Do you think that just knowing how to use some of these tools will make me hireable as a junior data engineer?&lt;/p&gt;\n\n&lt;p&gt;I feel like I have this sunken cost, I just need to finish one more quarter. However, I think that this data engineering certificate will be worthless on my resume. I haven&amp;#39;t decided what I want to do once I get the computer science degree. Likely to interview will ask me why I have the certificate but I feel like I don&amp;#39;t have a good reason.&lt;/p&gt;\n\n&lt;p&gt;Perhaps if I took this course in the future, I might have liked it more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10z3w7i", "is_robot_indexable": true, "report_reasons": null, "author": "wsb_degen_number9999", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10z3w7i/should_i_give_up_on_data_engineering_certificate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10z3w7i/should_i_give_up_on_data_engineering_certificate/", "subreddit_subscribers": 89216, "created_utc": 1676065256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to engage some Collibra developers but the Collibra subreddit is gone now - what is the right place to find smart and interesting Collibra people? Thank you!", "author_fullname": "t2_4gb7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do Collibra developers find jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10yxr90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676050377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to engage some Collibra developers but the Collibra subreddit is gone now - what is the right place to find smart and interesting Collibra people? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10yxr90", "is_robot_indexable": true, "report_reasons": null, "author": "xiangw", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10yxr90/where_do_collibra_developers_find_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10yxr90/where_do_collibra_developers_find_jobs/", "subreddit_subscribers": 89216, "created_utc": 1676050377.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}