{"kind": "Listing", "data": {"after": "t3_11217n7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have experience with or have heard about specific data engineering certificates/ courses, could you share? Which one is the best in terms of preparing you and landing a job?\n\nI\u2019m aware of Udacity nano degree, MIT xPro,  and Coursera IBM professional certificates.\n\nWould love input, especially for programs tailored more at the beginner level to start.", "author_fullname": "t2_bqvtlw0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing online certs and courses- which one is the best?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111j3kb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676320195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have experience with or have heard about specific data engineering certificates/ courses, could you share? Which one is the best in terms of preparing you and landing a job?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m aware of Udacity nano degree, MIT xPro,  and Coursera IBM professional certificates.&lt;/p&gt;\n\n&lt;p&gt;Would love input, especially for programs tailored more at the beginner level to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111j3kb", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy_Pop_6966", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111j3kb/comparing_online_certs_and_courses_which_one_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111j3kb/comparing_online_certs_and_courses_which_one_is/", "subreddit_subscribers": 89563, "created_utc": 1676320195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am a junior data engineer has been working for about half a year now and I would like to work on some interesting open-source project in my spare time. Partly so I can develop faster as well as just do something for fun. I mainly work with Python, docker and Linux. \n\nMaybe one of you is active in some such project ? I would be happy to help \n\nGreetings", "author_fullname": "t2_uqgrebp4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an open-source project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ha7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676315633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am a junior data engineer has been working for about half a year now and I would like to work on some interesting open-source project in my spare time. Partly so I can develop faster as well as just do something for fun. I mainly work with Python, docker and Linux. &lt;/p&gt;\n\n&lt;p&gt;Maybe one of you is active in some such project ? I would be happy to help &lt;/p&gt;\n\n&lt;p&gt;Greetings&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "111ha7n", "is_robot_indexable": true, "report_reasons": null, "author": "Scyzoryk881", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111ha7n/looking_for_an_opensource_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111ha7n/looking_for_an_opensource_project/", "subreddit_subscribers": 89563, "created_utc": 1676315633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a string ID per each customer table which we use to join the tables, with 30+ characters per string. I created a new column using farm_fingerprint to hash the string to int. However, in my testing, the joins are now slower on some tables. I can't figure out why this behaviour would be happening. Would anybody have any insight?", "author_fullname": "t2_2gzsok4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hashed strings slower on joins than strings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111x9jg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676361542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a string ID per each customer table which we use to join the tables, with 30+ characters per string. I created a new column using farm_fingerprint to hash the string to int. However, in my testing, the joins are now slower on some tables. I can&amp;#39;t figure out why this behaviour would be happening. Would anybody have any insight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111x9jg", "is_robot_indexable": true, "report_reasons": null, "author": "ciarandeceol1", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111x9jg/hashed_strings_slower_on_joins_than_strings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111x9jg/hashed_strings_slower_on_joins_than_strings/", "subreddit_subscribers": 89563, "created_utc": 1676361542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to create a python ETL pipelines transforming raw data (JSONs, CSV etc.) to parquet. \n\nI guess the most obvious approach is to use pandas or pyspark. \n\nWhat are your approaches? Are there any better alternatives? Or does it even make sense to look for alternatives? Or is it just enough to use pandas/polars and forget about other stuff?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python library for storing parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111i04f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676317433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to create a python ETL pipelines transforming raw data (JSONs, CSV etc.) to parquet. &lt;/p&gt;\n\n&lt;p&gt;I guess the most obvious approach is to use pandas or pyspark. &lt;/p&gt;\n\n&lt;p&gt;What are your approaches? Are there any better alternatives? Or does it even make sense to look for alternatives? Or is it just enough to use pandas/polars and forget about other stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111i04f", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111i04f/python_library_for_storing_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111i04f/python_library_for_storing_parquet/", "subreddit_subscribers": 89563, "created_utc": 1676317433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the past two years I switched jobs two times, I find it hard in the long run to still be stimulated by what I do on a daily basis, or even to keep learning. But switching also has cons because it means getting to discover a whole other environment and starting over again from scratch in terms of knowledge of the industry you work on. \n\nI am aware that I can try and learn new things on the side of work to keep growing technically, but I find it very hard to learn alone about a subject if it doesn\u2019t apply to challenges I face while trying to do my tasks, I can\u2019t establish a clear roadmap of what I should learn or read, it feels too abstract, while if I have a challenge related to my job I know specifically what to look for and what to document myself on.\n\nSo my question is are you still able to find stimulating tasks in your daily activities as months go by, if so how? Do you learn new things on the side? Experience more routine in your job?", "author_fullname": "t2_6jles2jd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you find your daily tasks interesting / stimulating?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1124c0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676378086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past two years I switched jobs two times, I find it hard in the long run to still be stimulated by what I do on a daily basis, or even to keep learning. But switching also has cons because it means getting to discover a whole other environment and starting over again from scratch in terms of knowledge of the industry you work on. &lt;/p&gt;\n\n&lt;p&gt;I am aware that I can try and learn new things on the side of work to keep growing technically, but I find it very hard to learn alone about a subject if it doesn\u2019t apply to challenges I face while trying to do my tasks, I can\u2019t establish a clear roadmap of what I should learn or read, it feels too abstract, while if I have a challenge related to my job I know specifically what to look for and what to document myself on.&lt;/p&gt;\n\n&lt;p&gt;So my question is are you still able to find stimulating tasks in your daily activities as months go by, if so how? Do you learn new things on the side? Experience more routine in your job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1124c0r", "is_robot_indexable": true, "report_reasons": null, "author": "barbapapalone", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1124c0r/do_you_find_your_daily_tasks_interesting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1124c0r/do_you_find_your_daily_tasks_interesting/", "subreddit_subscribers": 89563, "created_utc": 1676378086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is your opinion about following data architecture?\n\n1. Raw JSON/CSV data extracted everyday using Python apps\n1b. Source data from databases extracted everyday using python apps\n2. Raw data stored in S3\n\n2b. Data from database sources goes straight into the next (3) step\n\n3. Python apps (polars/duckdb/?) performing transformations/cleansing on the raw data and saving into the next \u201cstage\u201d of S3 using parquet file format\n\n4. Parquet files registered as Hive tables using Trino\n\n5. Aggregations/joins etc. jobs run as SQL scripts using Trino saved as \u201cdatabase\u201d tables (again in the Hive catalog) in Iceberg table format\n\n6. End users (and apps like Metabase and others) access the \u201cparquet\u201d and \u201cIceberg\u201d tables using Trino\n\nAll jobs orchestrated by Airflow. \n\nQuestions:\n\na. I guess it does not make sense to extract data from databases and dump them into files such as csv and then take these files and convert into parquet - thats why I want to grab db data and turn in directly into parquet. Is that good approach?\n\nb. Should I transform ALL tables into the Iceberg, even if some of them would be totally the same? Or leave \u201cfinal\u201d tables as they are and create only \u201caggregate\u201d tables as new (Iceberg) layer? Somehow it is more appealing to me to convert ALL the tables into Iceberg so it is all stored in the same way. Also reading of such data should be faster than reading pure parquet I think. \n\nc. Which stage should include data validations (check for missings, deduplicate etc.)? I guess step 3(?)\n\nd. Do I miss some important component? Maybe something like semantical layer on top of that?\n\ne. Do you see any pitfalls? Or do you have any recommendations what (not) to do?\n\nedit:\n\nf. what would be the best way to actually model the tables? I mean the CREATE TABLE statements. Should I use e.g. dbt for that? Can dbt work with Iceberg? Or I have to \u201cmodel it manually\u201d using tools such as SQLAlchemy/sqlmodel?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture opinion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ioyr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676347383.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676319144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is your opinion about following data architecture?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Raw JSON/CSV data extracted everyday using Python apps\n1b. Source data from databases extracted everyday using python apps&lt;/li&gt;\n&lt;li&gt;Raw data stored in S3&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;2b. Data from database sources goes straight into the next (3) step&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Python apps (polars/duckdb/?) performing transformations/cleansing on the raw data and saving into the next \u201cstage\u201d of S3 using parquet file format&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Parquet files registered as Hive tables using Trino&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Aggregations/joins etc. jobs run as SQL scripts using Trino saved as \u201cdatabase\u201d tables (again in the Hive catalog) in Iceberg table format&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;End users (and apps like Metabase and others) access the \u201cparquet\u201d and \u201cIceberg\u201d tables using Trino&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;All jobs orchestrated by Airflow. &lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;p&gt;a. I guess it does not make sense to extract data from databases and dump them into files such as csv and then take these files and convert into parquet - thats why I want to grab db data and turn in directly into parquet. Is that good approach?&lt;/p&gt;\n\n&lt;p&gt;b. Should I transform ALL tables into the Iceberg, even if some of them would be totally the same? Or leave \u201cfinal\u201d tables as they are and create only \u201caggregate\u201d tables as new (Iceberg) layer? Somehow it is more appealing to me to convert ALL the tables into Iceberg so it is all stored in the same way. Also reading of such data should be faster than reading pure parquet I think. &lt;/p&gt;\n\n&lt;p&gt;c. Which stage should include data validations (check for missings, deduplicate etc.)? I guess step 3(?)&lt;/p&gt;\n\n&lt;p&gt;d. Do I miss some important component? Maybe something like semantical layer on top of that?&lt;/p&gt;\n\n&lt;p&gt;e. Do you see any pitfalls? Or do you have any recommendations what (not) to do?&lt;/p&gt;\n\n&lt;p&gt;edit:&lt;/p&gt;\n\n&lt;p&gt;f. what would be the best way to actually model the tables? I mean the CREATE TABLE statements. Should I use e.g. dbt for that? Can dbt work with Iceberg? Or I have to \u201cmodel it manually\u201d using tools such as SQLAlchemy/sqlmodel?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111ioyr", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111ioyr/data_architecture_opinion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111ioyr/data_architecture_opinion/", "subreddit_subscribers": 89563, "created_utc": 1676319144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel more connected to DE, it is more suitable for me, but I keep hearing from all around me and especially in this subreddit that DE is like taking the easier route,and is not guaranteed as CS.\n\nDISCLAIMER: the major exists,*it is real*.\n\nThought's?", "author_fullname": "t2_tzb9l6f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is having a CS degree better than a DE degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111tz3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676356238.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676349616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel more connected to DE, it is more suitable for me, but I keep hearing from all around me and especially in this subreddit that DE is like taking the easier route,and is not guaranteed as CS.&lt;/p&gt;\n\n&lt;p&gt;DISCLAIMER: the major exists,&lt;em&gt;it is real&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;Thought&amp;#39;s?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111tz3j", "is_robot_indexable": true, "report_reasons": null, "author": "khtoto", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111tz3j/is_having_a_cs_degree_better_than_a_de_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111tz3j/is_having_a_cs_degree_better_than_a_de_degree/", "subreddit_subscribers": 89563, "created_utc": 1676349616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some help with Lyft/Uber sftp integration, from folks who have a successfully running integration with these sources. \n\nThe challenge we are facing is that we get multiple files with a different set of columns, which also keep changing. Also the files do not land at a consistent time every day.\n\nWould love to know how you are doing this. \n\nThanks.", "author_fullname": "t2_26djbmc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lyft/Uber sftp integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1126942", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676390244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676383770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some help with Lyft/Uber sftp integration, from folks who have a successfully running integration with these sources. &lt;/p&gt;\n\n&lt;p&gt;The challenge we are facing is that we get multiple files with a different set of columns, which also keep changing. Also the files do not land at a consistent time every day.&lt;/p&gt;\n\n&lt;p&gt;Would love to know how you are doing this. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1126942", "is_robot_indexable": true, "report_reasons": null, "author": "sanimesa", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1126942/lyftuber_sftp_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1126942/lyftuber_sftp_integration/", "subreddit_subscribers": 89563, "created_utc": 1676383770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, corporate finance worker who\u2019s been considering moving over to Data Engineering for a couple of months. \n\nI\u2019ve been following AI for a while now, and it seems like we\u2019re at the bottom of some sort of exponential curve in the applications in practical settings, and I\u2019m getting the feeling that a lot of things are going to change in the white collar space.\n\nI have been trying to think ahead of the curve as far as my career goes, and I had a thought today, and wanted some feedback from people who are experienced in the field.\n\nI\u2019m thinking that AI will lead to a boon in DE jobs, as in the future, tech companies will begin packaging SAAS AI programs that are trained on a specific company\u2019s data (financials, operational data, correspondence, emails) to eventually replace or augment many white collar roles. However most of us know very well that company data is disjointed and often garbage, hence the demand for engineers to bridge the gap.\n\nDoes this theory sound plausible? Forgive me if this post is too outlandish.", "author_fullname": "t2_11jujz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering+AI automation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111w62d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676358061.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676357229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, corporate finance worker who\u2019s been considering moving over to Data Engineering for a couple of months. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been following AI for a while now, and it seems like we\u2019re at the bottom of some sort of exponential curve in the applications in practical settings, and I\u2019m getting the feeling that a lot of things are going to change in the white collar space.&lt;/p&gt;\n\n&lt;p&gt;I have been trying to think ahead of the curve as far as my career goes, and I had a thought today, and wanted some feedback from people who are experienced in the field.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking that AI will lead to a boon in DE jobs, as in the future, tech companies will begin packaging SAAS AI programs that are trained on a specific company\u2019s data (financials, operational data, correspondence, emails) to eventually replace or augment many white collar roles. However most of us know very well that company data is disjointed and often garbage, hence the demand for engineers to bridge the gap.&lt;/p&gt;\n\n&lt;p&gt;Does this theory sound plausible? Forgive me if this post is too outlandish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111w62d", "is_robot_indexable": true, "report_reasons": null, "author": "RadiantVessel", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111w62d/data_engineeringai_automation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111w62d/data_engineeringai_automation/", "subreddit_subscribers": 89563, "created_utc": 1676357229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to learn more about lakehouse design. We don\u2019t use it at work, and I have an 11 month old baby at home so not too much time for self study. \n\nThis question might be an easy answer\u2026\n\nI understand the three lakehouse approaches can hold semi-structured data, and also store data in parquet files. My understanding is that parquet, as a column format file needs structured data.\n\nHow exactly are json or images stored within a lakehouse? Is it in a variant type column such as is the case with Snowflake, or is is just saved down raw with a pointer to its location and metadata about it?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unstructured/Semi-structured data in Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111j1me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676320055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to learn more about lakehouse design. We don\u2019t use it at work, and I have an 11 month old baby at home so not too much time for self study. &lt;/p&gt;\n\n&lt;p&gt;This question might be an easy answer\u2026&lt;/p&gt;\n\n&lt;p&gt;I understand the three lakehouse approaches can hold semi-structured data, and also store data in parquet files. My understanding is that parquet, as a column format file needs structured data.&lt;/p&gt;\n\n&lt;p&gt;How exactly are json or images stored within a lakehouse? Is it in a variant type column such as is the case with Snowflake, or is is just saved down raw with a pointer to its location and metadata about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111j1me", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111j1me/unstructuredsemistructured_data_in_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111j1me/unstructuredsemistructured_data_in_lakehouse/", "subreddit_subscribers": 89563, "created_utc": 1676320055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, for people that use spark/pyspark to build ETLs, do you always go for spark even when it's not for a huge load (example: csv with 50k rows, small db tables ..) ?\nI used to work with sqlalchemy/pandas when the data is manageable without spark, but nowadays spark got so ahead and clean, it just feels simpler to use it instead of the regular sqlalchemy/pandas.\n\nWhat you guys think ? Do you use spark for your low load pipelines too ?", "author_fullname": "t2_9k40zzr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is spark always your go to solution ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ijh9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676318757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, for people that use spark/pyspark to build ETLs, do you always go for spark even when it&amp;#39;s not for a huge load (example: csv with 50k rows, small db tables ..) ?\nI used to work with sqlalchemy/pandas when the data is manageable without spark, but nowadays spark got so ahead and clean, it just feels simpler to use it instead of the regular sqlalchemy/pandas.&lt;/p&gt;\n\n&lt;p&gt;What you guys think ? Do you use spark for your low load pipelines too ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111ijh9", "is_robot_indexable": true, "report_reasons": null, "author": "mohaidoss", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111ijh9/is_spark_always_your_go_to_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111ijh9/is_spark_always_your_go_to_solution/", "subreddit_subscribers": 89563, "created_utc": 1676318757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, Is it a good practice to have side effects in the delta live table pipeline? I would like to write data into Postgres DB as a last step. This will be continuously running DLT\n\nSomething like this\n\n    @dlt.table\n    def table():\n        df = read stream data\n        df.save -- to db\n        return df", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to write a database (RDS) from within delta live table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111opk0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676334132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, Is it a good practice to have side effects in the delta live table pipeline? I would like to write data into Postgres DB as a last step. This will be continuously running DLT&lt;/p&gt;\n\n&lt;p&gt;Something like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@dlt.table\ndef table():\n    df = read stream data\n    df.save -- to db\n    return df\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111opk0", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111opk0/is_it_possible_to_write_a_database_rds_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111opk0/is_it_possible_to_write_a_database_rds_from/", "subreddit_subscribers": 89563, "created_utc": 1676334132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I am doing an exercise and it's about a conference where author (s) submit a paper where it gets reviewed by a reviewer , now there is one part that I don't know how to implement into er diagram : \n\nEach review contains 2 types of comments, 1 to be seen by reviewers and 1 to be seen by author (kinda like feedback)", "author_fullname": "t2_2ah0kkor", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how do I do this ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1126t8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676385251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am doing an exercise and it&amp;#39;s about a conference where author (s) submit a paper where it gets reviewed by a reviewer , now there is one part that I don&amp;#39;t know how to implement into er diagram : &lt;/p&gt;\n\n&lt;p&gt;Each review contains 2 types of comments, 1 to be seen by reviewers and 1 to be seen by author (kinda like feedback)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1126t8s", "is_robot_indexable": true, "report_reasons": null, "author": "omidhhh", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1126t8s/how_do_i_do_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1126t8s/how_do_i_do_this/", "subreddit_subscribers": 89563, "created_utc": 1676385251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Anyone relate ? \ud83d\ude02](https://preview.redd.it/0a7oanyld5ia1.png?width=794&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aeef29539ee5f243953cb1d27e4bffd7042405f2)", "author_fullname": "t2_txucj9ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science model goes to production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0a7oanyld5ia1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 120, "x": 108, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=313c8d88fe5ec1f8e9eb56350cfe9b541c6f50ba"}, {"y": 241, "x": 216, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bcbb0fc719c0e46cda772897d4c69df356cd97c"}, {"y": 357, "x": 320, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fda46028a6593c9fdfc98526785ebb579280ca85"}, {"y": 714, "x": 640, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=462a3838d03a60160336f4b3f76df0ba240434ae"}], "s": {"y": 886, "x": 794, "u": "https://preview.redd.it/0a7oanyld5ia1.png?width=794&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aeef29539ee5f243953cb1d27e4bffd7042405f2"}, "id": "0a7oanyld5ia1"}}, "name": "t3_11247gg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/IsrGi0XcqxJ93lSqk_wtestGqaNQMk1QmaK53B06GE0.jpg", "edited": 1676390783.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676377700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0a7oanyld5ia1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=aeef29539ee5f243953cb1d27e4bffd7042405f2\"&gt;Anyone relate ? \ud83d\ude02&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11247gg", "is_robot_indexable": true, "report_reasons": null, "author": "nxt-engineering", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11247gg/data_science_model_goes_to_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11247gg/data_science_model_goes_to_production/", "subreddit_subscribers": 89563, "created_utc": 1676377700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've just been offered this DE position, and as it will be my first ever, I've wanted to get your opinion on the tech stack they use:\n\nOracle, Hadoop (Cloudera), Nifi, Kafka, Airflow, Flink, MLFlow, GCS, BigQuery\n\nWill working with these technologies give me an edge when looking for roles in other companies? Thanks", "author_fullname": "t2_777e0jj2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this tech stack good for my career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122wme", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676373228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve just been offered this DE position, and as it will be my first ever, I&amp;#39;ve wanted to get your opinion on the tech stack they use:&lt;/p&gt;\n\n&lt;p&gt;Oracle, Hadoop (Cloudera), Nifi, Kafka, Airflow, Flink, MLFlow, GCS, BigQuery&lt;/p&gt;\n\n&lt;p&gt;Will working with these technologies give me an edge when looking for roles in other companies? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1122wme", "is_robot_indexable": true, "report_reasons": null, "author": "lil_colon_69", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1122wme/is_this_tech_stack_good_for_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1122wme/is_this_tech_stack_good_for_my_career/", "subreddit_subscribers": 89563, "created_utc": 1676373228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does Trino handle incremental updates of the table when used with S3 as a storage layer?\n\nMy scenario is that I have S3 bucket with some tables stored as parquet files. Every day I would get raw JSON data from source, run ETL pipeline and save new data into parquet adding it to the S3 bucket. \n\nDoes Trino automatically load this new data or I have to somehow manually update it?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino/S3 - incremental loads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111i3ld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676317680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does Trino handle incremental updates of the table when used with S3 as a storage layer?&lt;/p&gt;\n\n&lt;p&gt;My scenario is that I have S3 bucket with some tables stored as parquet files. Every day I would get raw JSON data from source, run ETL pipeline and save new data into parquet adding it to the S3 bucket. &lt;/p&gt;\n\n&lt;p&gt;Does Trino automatically load this new data or I have to somehow manually update it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111i3ld", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111i3ld/trinos3_incremental_loads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111i3ld/trinos3_incremental_loads/", "subreddit_subscribers": 89563, "created_utc": 1676317680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a media company  client that has DW for internal and client reporting (integrating data for multiple properties) and part of the data is Google Analytics, they wish to move reporting from being based on UA to GA4, I'm not too familiar with the changes in GA4, but if any of you worked on anything, similar - **how much work was it**?\n\nMy assumption going in, is that the final data model does not change, I just plug out the old and plug in the new. I've noticed that certain metrics do not have one to one correspondance, so there is going to be some research involved and with some slack for unexpected things and testing I'm thinking **3-4 weeks?** \n\nWould this be reasonable or am I way over optimistic/pessimistic?\n\n\nAny pitfalls or issues you ran into I should be aware of before starting?\n\nEdit: \n\nThe setup:\n\n * EL is done via Stitch into Snowflake\n\n * T is done via dbt", "author_fullname": "t2_9d1jjuxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating from UA to GA4 for the DW? How much work is it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1123i3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676375350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a media company  client that has DW for internal and client reporting (integrating data for multiple properties) and part of the data is Google Analytics, they wish to move reporting from being based on UA to GA4, I&amp;#39;m not too familiar with the changes in GA4, but if any of you worked on anything, similar - &lt;strong&gt;how much work was it&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;My assumption going in, is that the final data model does not change, I just plug out the old and plug in the new. I&amp;#39;ve noticed that certain metrics do not have one to one correspondance, so there is going to be some research involved and with some slack for unexpected things and testing I&amp;#39;m thinking &lt;strong&gt;3-4 weeks?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Would this be reasonable or am I way over optimistic/pessimistic?&lt;/p&gt;\n\n&lt;p&gt;Any pitfalls or issues you ran into I should be aware of before starting?&lt;/p&gt;\n\n&lt;p&gt;Edit: &lt;/p&gt;\n\n&lt;p&gt;The setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;EL is done via Stitch into Snowflake&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;T is done via dbt&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1123i3u", "is_robot_indexable": true, "report_reasons": null, "author": "boggle_thy_mind", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1123i3u/migrating_from_ua_to_ga4_for_the_dw_how_much_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1123i3u/migrating_from_ua_to_ga4_for_the_dw_how_much_work/", "subreddit_subscribers": 89563, "created_utc": 1676375350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a very security-minded org that has very poor environment parity across all of our source systems.  We have very sparse/non-existent dev data and I'm looking for some solutions and wanted to see if this community has any tooling recommendations to help.\n\nThe requirement is to build out synthetic data with very similar size and shape to our production data and ideally have a framework to do this, maybe at some level use our data and \"de-productionize\" it.  [gretel.ai](https://gretel.ai) looks like it may be a fit, but from what I see I need to upload production data to their environment and that's a no-go.\n\n&amp;#x200B;\n\nHas anyone come across a similar problem and what are some solutions?\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_3q5hfpq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generating Synthetic Data from Prod systems into Dev/UAT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111djgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676306331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a very security-minded org that has very poor environment parity across all of our source systems.  We have very sparse/non-existent dev data and I&amp;#39;m looking for some solutions and wanted to see if this community has any tooling recommendations to help.&lt;/p&gt;\n\n&lt;p&gt;The requirement is to build out synthetic data with very similar size and shape to our production data and ideally have a framework to do this, maybe at some level use our data and &amp;quot;de-productionize&amp;quot; it.  &lt;a href=\"https://gretel.ai\"&gt;gretel.ai&lt;/a&gt; looks like it may be a fit, but from what I see I need to upload production data to their environment and that&amp;#39;s a no-go.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across a similar problem and what are some solutions?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?auto=webp&amp;v=enabled&amp;s=2ba92a939a095501fb155c357863e8a0c95ca82b", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4f34c328c74a03183c562084539022e0e4db596", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cce563ca19a1be89ed5a347ea1b639594e3bded4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8588b7410b33d54a1d311f88be9432dbe4c23f34", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5efcd2d79f439ba042ed5abeaf109eb20084d6d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed601ca17686ef686d064a42f9a105f384094833", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7d17ee8c3bb78f99e891c45ad65d95cd31d8450", "width": 1080, "height": 567}], "variants": {}, "id": "o0qqAq5goPMeGx3o_KWtBt4Uz4NUKAsNYfk7pyjJwGM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111djgk", "is_robot_indexable": true, "report_reasons": null, "author": "timewarp80", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111djgk/generating_synthetic_data_from_prod_systems_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111djgk/generating_synthetic_data_from_prod_systems_into/", "subreddit_subscribers": 89563, "created_utc": 1676306331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, this is a question to Polars users. How confident do you feel about Polars being a Pandas replacement? Have you already or are thinking of switching your production code to Polars? How is/was the experience so far?\n\n\\---\n\nA little background.\n\nI am experimenting with Polars and was trying to replace reading parquet files from Pandas to it because I really enjoyed the speed bump. However there are some nuances that seems very strange. For example, those files are written with PySpark, usually they have one partition. So the dir structure is like this:\n\n    .\n    \u2514\u2500\u2500 master_data.snappy.parquet\n        \u251c\u2500\u2500 _SUCCESS\n        \u251c\u2500\u2500 country=Albania\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n        \u251c\u2500\u2500 country=Algeria\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n        ...\n\nFrom the [documentation](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html#polars-read-parquet) I see that it should be very straightforward to switch Pandas to Polars in this regard:\n\n    import pandas as pd\n    import polars as pl\n    \n    # This works\n    dd = pd.read_parquet(\"cm_mbs_spend_v3.snappy.parquet\")\n    # This doesn't\n    dd = pd.read_parquet(\"cm_mbs_spend_v3.snappy.parquet\")\n\nThe Polars version throws this error:\n\n&gt;IsADirectoryError: Expected a file path; 'cm\\_mbs\\_spend\\_v3.snappy.parquet' is a directory\n\nPolars should be able to understand partitioned dataset. However from my experiment I realized the dataset cannot have any extra meta files like `_SUCCESS`. So if you do this, it works:\n\n    dd = pl.read_parquet(\"master_data.snappy.parquet/**/*.parquet\")", "author_fullname": "t2_12lkky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replace Pandas with Parquet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11234oo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676382074.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676374040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, this is a question to Polars users. How confident do you feel about Polars being a Pandas replacement? Have you already or are thinking of switching your production code to Polars? How is/was the experience so far?&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;A little background.&lt;/p&gt;\n\n&lt;p&gt;I am experimenting with Polars and was trying to replace reading parquet files from Pandas to it because I really enjoyed the speed bump. However there are some nuances that seems very strange. For example, those files are written with PySpark, usually they have one partition. So the dir structure is like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;.\n\u2514\u2500\u2500 master_data.snappy.parquet\n    \u251c\u2500\u2500 _SUCCESS\n    \u251c\u2500\u2500 country=Albania\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n    \u251c\u2500\u2500 country=Algeria\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _committed_2189762722324180329\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 _started_2189762722324180329\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-tid-...c000.snappy.parquet\n    ...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;From the &lt;a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html#polars-read-parquet\"&gt;documentation&lt;/a&gt; I see that it should be very straightforward to switch Pandas to Polars in this regard:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pandas as pd\nimport polars as pl\n\n# This works\ndd = pd.read_parquet(&amp;quot;cm_mbs_spend_v3.snappy.parquet&amp;quot;)\n# This doesn&amp;#39;t\ndd = pd.read_parquet(&amp;quot;cm_mbs_spend_v3.snappy.parquet&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The Polars version throws this error:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;IsADirectoryError: Expected a file path; &amp;#39;cm_mbs_spend_v3.snappy.parquet&amp;#39; is a directory&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Polars should be able to understand partitioned dataset. However from my experiment I realized the dataset cannot have any extra meta files like &lt;code&gt;_SUCCESS&lt;/code&gt;. So if you do this, it works:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dd = pl.read_parquet(&amp;quot;master_data.snappy.parquet/**/*.parquet&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11234oo", "is_robot_indexable": true, "report_reasons": null, "author": "ratulotron", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11234oo/replace_pandas_with_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11234oo/replace_pandas_with_parquet/", "subreddit_subscribers": 89563, "created_utc": 1676374040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A friend of mine wants to embark on a DE journey. I am a software engineer myself. He is a Microsoft fan so he wants to do Azure. \n\nI recommended that he gets on the AWS track, since there are more jobs for AWS than there are for Azure. \n\nHe said there are fewer people that are Azure-qualified than there are AWS so it balances itself out, but when I did my research I think I concluded that there are infact more qualified Azure people than there are AWS, which supports going the AWS way even more. \n\nHe is interested in pursuing jobs in the U.S and Europe - for context. And he is not going to do both. \n\nLet me know what do you guys think ?", "author_fullname": "t2_uo9gwrfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure vs AWS DE tracks.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122ou8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676372452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend of mine wants to embark on a DE journey. I am a software engineer myself. He is a Microsoft fan so he wants to do Azure. &lt;/p&gt;\n\n&lt;p&gt;I recommended that he gets on the AWS track, since there are more jobs for AWS than there are for Azure. &lt;/p&gt;\n\n&lt;p&gt;He said there are fewer people that are Azure-qualified than there are AWS so it balances itself out, but when I did my research I think I concluded that there are infact more qualified Azure people than there are AWS, which supports going the AWS way even more. &lt;/p&gt;\n\n&lt;p&gt;He is interested in pursuing jobs in the U.S and Europe - for context. And he is not going to do both. &lt;/p&gt;\n\n&lt;p&gt;Let me know what do you guys think ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1122ou8", "is_robot_indexable": true, "report_reasons": null, "author": "Rami_zaki", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1122ou8/azure_vs_aws_de_tracks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1122ou8/azure_vs_aws_de_tracks/", "subreddit_subscribers": 89563, "created_utc": 1676372452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\nIm building a ML stack using Clickhouse and Sagemaker. Both are kinda invariant at this point, although I may arrange Azure ML these days.\n\nClickhouse will contained labeled data - it is actually where our labeling team will save bounding boxes, etc.\n\nIm trying to figure out the other pieces of the pipeline - to move data to Sagemaker and kick off a training+inference run.\n\nIm considering Airflow + dbt + Airbyte based on my research. I haven't done this before, so im grateful for any suggestions.", "author_fullname": "t2_3auav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "building a ML training stack with Clickhouse &amp; Sagemaker. What should be the other pieces?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122l1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676372090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nIm building a ML stack using Clickhouse and Sagemaker. Both are kinda invariant at this point, although I may arrange Azure ML these days.&lt;/p&gt;\n\n&lt;p&gt;Clickhouse will contained labeled data - it is actually where our labeling team will save bounding boxes, etc.&lt;/p&gt;\n\n&lt;p&gt;Im trying to figure out the other pieces of the pipeline - to move data to Sagemaker and kick off a training+inference run.&lt;/p&gt;\n\n&lt;p&gt;Im considering Airflow + dbt + Airbyte based on my research. I haven&amp;#39;t done this before, so im grateful for any suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1122l1k", "is_robot_indexable": true, "report_reasons": null, "author": "sandys1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1122l1k/building_a_ml_training_stack_with_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1122l1k/building_a_ml_training_stack_with_clickhouse/", "subreddit_subscribers": 89563, "created_utc": 1676372090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're attempting to create scheduled tasks that involve:\n\n1. Initiating a Lambda function to extract tar.gz files to a disk(temporary).\n\n1. Running AWS Glue to read the extracted files from the disk(temporary).\n\n1. Saving the output as a Parquet file to S3.\n\nHowever, I'm uncertain if this is the best approach or if there are alternative methods.", "author_fullname": "t2_eckrjl60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If we have some tar.gz file in s3, how to make use of aws Glue to parse it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111sim5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676345024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re attempting to create scheduled tasks that involve:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Initiating a Lambda function to extract tar.gz files to a disk(temporary).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Running AWS Glue to read the extracted files from the disk(temporary).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Saving the output as a Parquet file to S3.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;However, I&amp;#39;m uncertain if this is the best approach or if there are alternative methods.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111sim5", "is_robot_indexable": true, "report_reasons": null, "author": "b-y-f", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111sim5/if_we_have_some_targz_file_in_s3_how_to_make_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111sim5/if_we_have_some_targz_file_in_s3_how_to_make_use/", "subreddit_subscribers": 89563, "created_utc": 1676345024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, can someone help me with the meaning of \u201cucd\u201d in the DataFrame class in the vaex 4.16.0.\n\nNot very well versed in data engineering but couldn\u2019t find anything with google searches.", "author_fullname": "t2_306r5wtz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vaex documentation help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111hz5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676317363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, can someone help me with the meaning of \u201cucd\u201d in the DataFrame class in the vaex 4.16.0.&lt;/p&gt;\n\n&lt;p&gt;Not very well versed in data engineering but couldn\u2019t find anything with google searches.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111hz5q", "is_robot_indexable": true, "report_reasons": null, "author": "AbbuBumPhodo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111hz5q/vaex_documentation_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111hz5q/vaex_documentation_help/", "subreddit_subscribers": 89563, "created_utc": 1676317363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have made an API call script to get some data from one online store.\n\nEach store requires unique store\\_id and api key, so I have to change these field in the scriptHowever, there are 13 stores in total, so I am reluctant to duplicate 13 almost identical scripts to get all the data from different stores.\n\n1. How can I make this query more efficient and maintainable\n2. Actually, I am using AWS step function to orchestrate lambda functions, so ideally, the marketID and api key has to pass to the variables in the scripts of functions.\n3. I am aware of setting up environment variables in aws lambda, but I have to pass the one env variable each by each. Would it be a loop to solve this task ?\n4. The script will generate a some kind id and it's paired with marketplace id and api key. So I have to wait one script to finish before passing a new environment variables  \n", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to loop all the environment variables and pass them to a script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1128ci8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676389740.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676389104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have made an API call script to get some data from one online store.&lt;/p&gt;\n\n&lt;p&gt;Each store requires unique store_id and api key, so I have to change these field in the scriptHowever, there are 13 stores in total, so I am reluctant to duplicate 13 almost identical scripts to get all the data from different stores.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How can I make this query more efficient and maintainable&lt;/li&gt;\n&lt;li&gt;Actually, I am using AWS step function to orchestrate lambda functions, so ideally, the marketID and api key has to pass to the variables in the scripts of functions.&lt;/li&gt;\n&lt;li&gt;I am aware of setting up environment variables in aws lambda, but I have to pass the one env variable each by each. Would it be a loop to solve this task ?&lt;/li&gt;\n&lt;li&gt;The script will generate a some kind id and it&amp;#39;s paired with marketplace id and api key. So I have to wait one script to finish before passing a new environment variables&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1128ci8", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1128ci8/how_to_loop_all_the_environment_variables_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1128ci8/how_to_loop_all_the_environment_variables_and/", "subreddit_subscribers": 89563, "created_utc": 1676389104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I thought this was a useful article [**Unit Testing for Data Engineers**](https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers). It covers the variety of reasons why people don't unit test, and suggests some tips for writing testable code. \n\nI wonder though, do DEs really not write unit tests? What's your experience? would love to know.", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit Testing for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11217n7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676369531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought this was a useful article &lt;a href=\"https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers\"&gt;&lt;strong&gt;Unit Testing for Data Engineers&lt;/strong&gt;&lt;/a&gt;. It covers the variety of reasons why people don&amp;#39;t unit test, and suggests some tips for writing testable code. &lt;/p&gt;\n\n&lt;p&gt;I wonder though, do DEs really not write unit tests? What&amp;#39;s your experience? would love to know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?auto=webp&amp;v=enabled&amp;s=1b3b7c5418d793ecc90f7af41dd70d237c1587c0", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dc65a9ff168e3d268e62fc66ca617575d19f7a8", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a80233610d688ad8682934ed511c36e0858ebe5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c26c7c6829f1d139b4476d6ba2153609e3833a4", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/oZNg7WZdwX7OQBugU4jATMg32U29P_zdftK4-DtltDI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62eaba84e361dd7899286abe3071118182c7beda", "width": 640, "height": 480}], "variants": {}, "id": "1GzRNniO0GB5cJxUYNRFNIyvjlQ3ogl487sx3wSKEhA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11217n7", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11217n7/unit_testing_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11217n7/unit_testing_for_data_engineers/", "subreddit_subscribers": 89563, "created_utc": 1676369531.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}