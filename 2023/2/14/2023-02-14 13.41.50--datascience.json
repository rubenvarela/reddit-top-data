{"kind": "Listing", "data": {"after": "t3_111qxiw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7b3mpu85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "you're an angel!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_111dnva", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 944, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 944, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fRQStMinHGCEMfJNW1fDwAN-WrKdyQIPa4hoDaumscM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676306633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/539jft6hizha1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/539jft6hizha1.jpg?auto=webp&amp;v=enabled&amp;s=698d7256b3867119033c96be6c48e90c971650b9", "width": 1062, "height": 1092}, "resolutions": [{"url": "https://preview.redd.it/539jft6hizha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9031897f37c64ce7b642c25d702717308dafef7b", "width": 108, "height": 111}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4278d9727a077b0eec3dd838b5759170f3e1bcbb", "width": 216, "height": 222}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbe1515e438b8985385afc2b17623cd1496bebbd", "width": 320, "height": 329}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abc1e4cf33111e2851e27c8e3e5b19cdc2514374", "width": 640, "height": 658}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cfe01728f0e30a84798781b0537d8933e19b77b", "width": 960, "height": 987}], "variants": {}, "id": "-z2k3VtrVjUY1u9BHjb7MXRvRdYqO9kNH74Rp_GmBbw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "111dnva", "is_robot_indexable": true, "report_reasons": null, "author": "BrilliantCashew", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111dnva/youre_an_angel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/539jft6hizha1.jpg", "subreddit_subscribers": 847683, "created_utc": 1676306633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "One very useful benefit I found with ChatGPT for job seekers is asking it to serve as a Hiring Manager that asks you questions for the specific role you're interested in.\n\nTo help it guide you, you need to provide it with some context based on the job description. For example, you list out the job description to ChatGPT and say that you want to be asked questions on causality and experimentation.\n\nHere's the response from ChatGPT based on the following prompt I provided it after sharing the job description:\n\n===**PROMPT BY ME===**\n\n&gt;I would like you to be a hiring manager. I'll be soon interviewing with you for a Data Science role, and you are going to ask me questions related to causality and experimentation. What are the questions you would ask to test my knowledge?\n\n**==RESPONSE BY CHATGPT===**\n\nSure! As a hiring manager for a data science role, I would like to assess your understanding of causality and experimentation. Here are a few questions I might ask:\n\n1. Can you explain the difference between correlation and causation, and give an example of each?\n2. How do you design an experiment to test a causal hypothesis?\n3. How do you ensure that the results of an experiment are valid and reliable?\n4. Can you explain the concept of confounding variables, and how do you control for them in an experiment?\n5. Can you explain the difference between a randomized controlled trial (RCT) and an observational study?\n6. How do you measure the impact of a treatment or intervention in an experiment?\n7. Have you worked with any causal inference methods such as propensity score matching or instrumental variables? Can you give an example of when you used them?\n8. How do you determine which variables to include in an experiment, and how do you avoid overfitting in the modeling process?\n9. Can you discuss any challenges or limitations you have encountered in designing and conducting experiments, and how you overcame them?\n10. Can you give an example of a real-world scenario in which you applied causal inference or experimentation methods to solve a business problem?\n\n====\n\nYou can ask ChatGPT to provide a summary answer for each. However, I would highly recommend you validate the answers by researching as well as ChatGPT can give confident, wrong answers.\n\nHope this helps others!", "author_fullname": "t2_48648", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Hunting Tip: Practice Interviewing with ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111h9pp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 191, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 191, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676315601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One very useful benefit I found with ChatGPT for job seekers is asking it to serve as a Hiring Manager that asks you questions for the specific role you&amp;#39;re interested in.&lt;/p&gt;\n\n&lt;p&gt;To help it guide you, you need to provide it with some context based on the job description. For example, you list out the job description to ChatGPT and say that you want to be asked questions on causality and experimentation.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the response from ChatGPT based on the following prompt I provided it after sharing the job description:&lt;/p&gt;\n\n&lt;p&gt;===&lt;strong&gt;PROMPT BY ME===&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I would like you to be a hiring manager. I&amp;#39;ll be soon interviewing with you for a Data Science role, and you are going to ask me questions related to causality and experimentation. What are the questions you would ask to test my knowledge?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;==RESPONSE BY CHATGPT===&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Sure! As a hiring manager for a data science role, I would like to assess your understanding of causality and experimentation. Here are a few questions I might ask:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Can you explain the difference between correlation and causation, and give an example of each?&lt;/li&gt;\n&lt;li&gt;How do you design an experiment to test a causal hypothesis?&lt;/li&gt;\n&lt;li&gt;How do you ensure that the results of an experiment are valid and reliable?&lt;/li&gt;\n&lt;li&gt;Can you explain the concept of confounding variables, and how do you control for them in an experiment?&lt;/li&gt;\n&lt;li&gt;Can you explain the difference between a randomized controlled trial (RCT) and an observational study?&lt;/li&gt;\n&lt;li&gt;How do you measure the impact of a treatment or intervention in an experiment?&lt;/li&gt;\n&lt;li&gt;Have you worked with any causal inference methods such as propensity score matching or instrumental variables? Can you give an example of when you used them?&lt;/li&gt;\n&lt;li&gt;How do you determine which variables to include in an experiment, and how do you avoid overfitting in the modeling process?&lt;/li&gt;\n&lt;li&gt;Can you discuss any challenges or limitations you have encountered in designing and conducting experiments, and how you overcame them?&lt;/li&gt;\n&lt;li&gt;Can you give an example of a real-world scenario in which you applied causal inference or experimentation methods to solve a business problem?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;You can ask ChatGPT to provide a summary answer for each. However, I would highly recommend you validate the answers by researching as well as ChatGPT can give confident, wrong answers.&lt;/p&gt;\n\n&lt;p&gt;Hope this helps others!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111h9pp", "is_robot_indexable": true, "report_reasons": null, "author": "forbiscuit", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111h9pp/job_hunting_tip_practice_interviewing_with_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111h9pp/job_hunting_tip_practice_interviewing_with_chatgpt/", "subreddit_subscribers": 847683, "created_utc": 1676315601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a3gidxwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use tools for automatic EDA? Which ones? Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 124, "top_awarded_type": null, "hide_score": false, "name": "t3_111bdx7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0n-XkGoK9DGBd2apAPMmt0LMqEZD_mgZ2OiYHWzsGVQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676300959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ebndmkmo1zha1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ebndmkmo1zha1.png?auto=webp&amp;v=enabled&amp;s=361760fbb896149ca6627a89a02e5c114c1e3d37", "width": 573, "height": 511}, "resolutions": [{"url": "https://preview.redd.it/ebndmkmo1zha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=daeee7c14463aaf455054d332f8070a5890259c3", "width": 108, "height": 96}, {"url": "https://preview.redd.it/ebndmkmo1zha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9652ddc761884172b566f646aab38498570190d", "width": 216, "height": 192}, {"url": "https://preview.redd.it/ebndmkmo1zha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74a098147bdc4010d91343568db317ee8b207aae", "width": 320, "height": 285}], "variants": {}, "id": "zdkAAF1c8bR8rK7ISrL8SFsPBgatJO54h2qCQfS2lRU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "111bdx7", "is_robot_indexable": true, "report_reasons": null, "author": "malirkan", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111bdx7/do_you_use_tools_for_automatic_eda_which_ones_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ebndmkmo1zha1.png", "subreddit_subscribers": 847683, "created_utc": 1676300959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Should job posting with mentions of these tools be ignored or are they useful for a data scientists?", "author_fullname": "t2_17yq29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What role does Tableau and Power BI play in your analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111lroh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676330349.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676326649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should job posting with mentions of these tools be ignored or are they useful for a data scientists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111lroh", "is_robot_indexable": true, "report_reasons": null, "author": "shastaslacker", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111lroh/what_role_does_tableau_and_power_bi_play_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111lroh/what_role_does_tableau_and_power_bi_play_in_your/", "subreddit_subscribers": 847683, "created_utc": 1676326649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Been getting a tad annoyed with Conda lately, at least as a package manager. So I wanted to hear what everyone else likes to use.", "author_fullname": "t2_dqghhocz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use to manage your Python packages and environments? Do you prefer Conda or something like virtualenv + pip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111djvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676306360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been getting a tad annoyed with Conda lately, at least as a package manager. So I wanted to hear what everyone else likes to use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111djvu", "is_robot_indexable": true, "report_reasons": null, "author": "CardboardBoxPlot", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111djvu/what_do_you_use_to_manage_your_python_packages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111djvu/what_do_you_use_to_manage_your_python_packages/", "subreddit_subscribers": 847683, "created_utc": 1676306360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I'm currently in my 6 month journey learning DS( I come from a mechanical engineering background), and I was wondering what tools/topics should I focus on if I want to get an entry level job in data science in the field of cancer research? \nIf anyone had an idea about this topic I would really appreciate your insight. Thank you!", "author_fullname": "t2_8773zq8m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going into cancer research in data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111m24k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676327352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m currently in my 6 month journey learning DS( I come from a mechanical engineering background), and I was wondering what tools/topics should I focus on if I want to get an entry level job in data science in the field of cancer research? \nIf anyone had an idea about this topic I would really appreciate your insight. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111m24k", "is_robot_indexable": true, "report_reasons": null, "author": "baydati", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111m24k/going_into_cancer_research_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111m24k/going_into_cancer_research_in_data_science/", "subreddit_subscribers": 847683, "created_utc": 1676327352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I knew that I wanted to do something data-related (or computer science-based) for a long time, but unfortunately my undergrad major was completely unrelated to DS. I self-taught myself Python/R while working a marketing job and was suuper fortunate to get into a DS/DA Master\u2019s Program at one of the Ivy League schools. I worked 24/7 to learn a ton about best data practices/tools, had 2 DS co-ops, generated a ton of material for my GitHub, etc.\n\nObviously, just finishing a Master\u2019s program isn\u2019t enough to enter DS, so my career counselor recommended me to \u201ctake a data analyst job at a low-maturity company, use your skills to upgrade the way they use data, become a data rockstar and you\u2019ll get all kinds of doors opened in no time.\u201d **Spoiler** \\- *I did not become a data rockstar*.\n\nI landed a marketing analyst position at a large insurance company, and, long story short, this job is just a mess. My boss is a lifer that recently celebrated 20 years in the same position at the same company and got promoted to manager right as I joined. 30% of my work is shuffling around Excel sheets/making ugly (because the style is regulated) PowerPoints, another 30% is attending useless meetings that she sets up for me to \u201chear what conversations are happening around here\u201d; 20% is making sense of the broken SPSS code that someone wrote for her 15 years ago; the rest of the time is allocated to using Power BI, SQL and R to pull data from various sources and create reports for the internal stakeholders.\n\nNothing at my job is Googlable because all of the processes are either created by someone from 20 years ago or my manager herself; each process can require up to 100 little steps of moving data between million Excel spreadsheets populated with billion formulas making it impossible to comprehend or even load. The documentation she provides me with is always incorrect/incomplete, forcing me to ask for her help, as it would take me many hours to figure this out myself. When I try to do things my way, she gets clearly upset and sometimes starts micromanaging me, making us miss a ton of deadlines just for the sake of making everything look perfect according to the internal standards.\u00a0\n\nI spend a ton of time outside of work hours (the last one to leave the office every single day) trying to make time for extra DS projects but the amount of \u201chigh-priority\u201d tasks I get assigned is too overwhelming. The most technical thing I've done is used VDI to automate an R script that imports a bunch of files, anti-joins them and outputs certain distributions into an Excel doc, but that was a one-time project. I told my manager that I enjoy coding the most and she promised to land me a \u201ccoding-heavy\u201d project with the DS team. However, I followed up 3 times throughout a month and she ended up telling me that the person who asked for help decided to leave the company (jeez I wonder why.)\u00a0\n\nI get so much stress from a seemingly \u201ceasy\u201d job (no one seems to care that everything is 2+ weeks past due or indefinitely undelivered) because I\u2019m trying so hard to get a chance to make an impact and have at least something I can use when applying for DS roles after hitting that 1-year mark. I\u2019m afraid that the advanced skills I acquired are slowly fading away and I\u2019m learning a lot of bad habits from my manager.\n\nWhat would you recommend in my situation to stay on the right track? I usually work through my lunch breaks too (I know, I know), but I had an idea of maybe using that time to read a stats book or something DS-related - any suggestions? Any advice is appreciated :)\n\nEdit: the reason I put \"Ivy League School\" is because, a couple of months ago, my manager gave me an impossible task; I asked for a suggestion on how to better approach this task, and she responded with \"well, I'm not the one with the Master's Degree from the Ivy Leagues.\" I got so butthurt from this because I haven't brought it up once since joining the company lol.\n\nTLDR: want to break into DS through low-maturity analytics job but feeling like it has a negative effect on my career potential - tips?\u00a0", "author_fullname": "t2_6kb6g3y1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a mistake to enter DS through low-maturity Data Analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111wcmt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676358583.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676357925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I knew that I wanted to do something data-related (or computer science-based) for a long time, but unfortunately my undergrad major was completely unrelated to DS. I self-taught myself Python/R while working a marketing job and was suuper fortunate to get into a DS/DA Master\u2019s Program at one of the Ivy League schools. I worked 24/7 to learn a ton about best data practices/tools, had 2 DS co-ops, generated a ton of material for my GitHub, etc.&lt;/p&gt;\n\n&lt;p&gt;Obviously, just finishing a Master\u2019s program isn\u2019t enough to enter DS, so my career counselor recommended me to \u201ctake a data analyst job at a low-maturity company, use your skills to upgrade the way they use data, become a data rockstar and you\u2019ll get all kinds of doors opened in no time.\u201d &lt;strong&gt;Spoiler&lt;/strong&gt; - &lt;em&gt;I did not become a data rockstar&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;I landed a marketing analyst position at a large insurance company, and, long story short, this job is just a mess. My boss is a lifer that recently celebrated 20 years in the same position at the same company and got promoted to manager right as I joined. 30% of my work is shuffling around Excel sheets/making ugly (because the style is regulated) PowerPoints, another 30% is attending useless meetings that she sets up for me to \u201chear what conversations are happening around here\u201d; 20% is making sense of the broken SPSS code that someone wrote for her 15 years ago; the rest of the time is allocated to using Power BI, SQL and R to pull data from various sources and create reports for the internal stakeholders.&lt;/p&gt;\n\n&lt;p&gt;Nothing at my job is Googlable because all of the processes are either created by someone from 20 years ago or my manager herself; each process can require up to 100 little steps of moving data between million Excel spreadsheets populated with billion formulas making it impossible to comprehend or even load. The documentation she provides me with is always incorrect/incomplete, forcing me to ask for her help, as it would take me many hours to figure this out myself. When I try to do things my way, she gets clearly upset and sometimes starts micromanaging me, making us miss a ton of deadlines just for the sake of making everything look perfect according to the internal standards.\u00a0&lt;/p&gt;\n\n&lt;p&gt;I spend a ton of time outside of work hours (the last one to leave the office every single day) trying to make time for extra DS projects but the amount of \u201chigh-priority\u201d tasks I get assigned is too overwhelming. The most technical thing I&amp;#39;ve done is used VDI to automate an R script that imports a bunch of files, anti-joins them and outputs certain distributions into an Excel doc, but that was a one-time project. I told my manager that I enjoy coding the most and she promised to land me a \u201ccoding-heavy\u201d project with the DS team. However, I followed up 3 times throughout a month and she ended up telling me that the person who asked for help decided to leave the company (jeez I wonder why.)\u00a0&lt;/p&gt;\n\n&lt;p&gt;I get so much stress from a seemingly \u201ceasy\u201d job (no one seems to care that everything is 2+ weeks past due or indefinitely undelivered) because I\u2019m trying so hard to get a chance to make an impact and have at least something I can use when applying for DS roles after hitting that 1-year mark. I\u2019m afraid that the advanced skills I acquired are slowly fading away and I\u2019m learning a lot of bad habits from my manager.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend in my situation to stay on the right track? I usually work through my lunch breaks too (I know, I know), but I had an idea of maybe using that time to read a stats book or something DS-related - any suggestions? Any advice is appreciated :)&lt;/p&gt;\n\n&lt;p&gt;Edit: the reason I put &amp;quot;Ivy League School&amp;quot; is because, a couple of months ago, my manager gave me an impossible task; I asked for a suggestion on how to better approach this task, and she responded with &amp;quot;well, I&amp;#39;m not the one with the Master&amp;#39;s Degree from the Ivy Leagues.&amp;quot; I got so butthurt from this because I haven&amp;#39;t brought it up once since joining the company lol.&lt;/p&gt;\n\n&lt;p&gt;TLDR: want to break into DS through low-maturity analytics job but feeling like it has a negative effect on my career potential - tips?\u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111wcmt", "is_robot_indexable": true, "report_reasons": null, "author": "busy_bee_movie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111wcmt/is_it_a_mistake_to_enter_ds_through_lowmaturity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111wcmt/is_it_a_mistake_to_enter_ds_through_lowmaturity/", "subreddit_subscribers": 847683, "created_utc": 1676357925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'd love some clarification on these dimensionality reduction methods. I see PCA/PCR recommended as the go-to method for this type of task and almost never see people talk about using PLS to get principal components. It's my understanding that the key difference is that PLS considers the target when making components. \n\nIntuitively, I feel like this would make PLS stronger in many datasets, as it uses more of the available information to better inform the creation of components. I'm not an expert here and don't actually have a ton of expertise with how either of these methods work on a deep level, so any feedback from more experienced data scientists would be much appreciated. Thanks in advance for any thoughts or opinions here!", "author_fullname": "t2_vj9xwd07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is PCR recommended much more often than PLS for dimension reduction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111kw8i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676324549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;d love some clarification on these dimensionality reduction methods. I see PCA/PCR recommended as the go-to method for this type of task and almost never see people talk about using PLS to get principal components. It&amp;#39;s my understanding that the key difference is that PLS considers the target when making components. &lt;/p&gt;\n\n&lt;p&gt;Intuitively, I feel like this would make PLS stronger in many datasets, as it uses more of the available information to better inform the creation of components. I&amp;#39;m not an expert here and don&amp;#39;t actually have a ton of expertise with how either of these methods work on a deep level, so any feedback from more experienced data scientists would be much appreciated. Thanks in advance for any thoughts or opinions here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111kw8i", "is_robot_indexable": true, "report_reasons": null, "author": "NDVGuy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111kw8i/why_is_pcr_recommended_much_more_often_than_pls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111kw8i/why_is_pcr_recommended_much_more_often_than_pls/", "subreddit_subscribers": 847683, "created_utc": 1676324549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m trying create a predictive model for a binary outcome (yes or no).\nI have 16 predictor variables, which are a combination of categorical and continuous variables. How do I go about deciding which to include in my model?\n\nI can\u2019t find any literature on the requirements/assumptions of x in a logistic regression. I\u2019m modelling in R therefore I can\u2019t really add all variables and use a Backwards/Forwards variable selection method used in programs such as SAS.\n\n\nAppreciate any assistance!", "author_fullname": "t2_tir3dln2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deciding on Predictors for Logistic Regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111xm2s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676362978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying create a predictive model for a binary outcome (yes or no).\nI have 16 predictor variables, which are a combination of categorical and continuous variables. How do I go about deciding which to include in my model?&lt;/p&gt;\n\n&lt;p&gt;I can\u2019t find any literature on the requirements/assumptions of x in a logistic regression. I\u2019m modelling in R therefore I can\u2019t really add all variables and use a Backwards/Forwards variable selection method used in programs such as SAS.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111xm2s", "is_robot_indexable": true, "report_reasons": null, "author": "Fun_Elevator_814", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111xm2s/deciding_on_predictors_for_logistic_regression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111xm2s/deciding_on_predictors_for_logistic_regression/", "subreddit_subscribers": 847683, "created_utc": 1676362978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[This paper](https://arxiv.org/abs/2112.13299) is super interesting, and by the same author who introduced CUPED.\n\nIn short, experiments with one sided non-compliance can result in a watered down ATE due to few users actually engaging with the treatment.  This paper builds on the instrumental variables literature (which we leverage a lot for these types of experiments) to use cuped to further reduce variance.\n\nIts super interesting and really useful for those of us who run experiments and need to increase precision of our estimates.  Best of all, there is some fairly food code attached as a footnote.", "author_fullname": "t2_131vu3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zero to Hero: Exploiting Null Effects to Achieve Variance Reduction in Experiments with One-sided Triggering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111rr5g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "modflair", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676342770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2112.13299\"&gt;This paper&lt;/a&gt; is super interesting, and by the same author who introduced CUPED.&lt;/p&gt;\n\n&lt;p&gt;In short, experiments with one sided non-compliance can result in a watered down ATE due to few users actually engaging with the treatment.  This paper builds on the instrumental variables literature (which we leverage a lot for these types of experiments) to use cuped to further reduce variance.&lt;/p&gt;\n\n&lt;p&gt;Its super interesting and really useful for those of us who run experiments and need to increase precision of our estimates.  Best of all, there is some fairly food code attached as a footnote.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111rr5g", "is_robot_indexable": true, "report_reasons": null, "author": "__compactsupport__", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/111rr5g/zero_to_hero_exploiting_null_effects_to_achieve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111rr5g/zero_to_hero_exploiting_null_effects_to_achieve/", "subreddit_subscribers": 847683, "created_utc": 1676342770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\n\nI wrote 2 posts together with 2 colleagues on how to solve a compression problem from the insurance industry. We applied the Simplex algorithm to compress the number of model points (every model point is an insurance contract component like life-insurance) and fit them to our benchmark. \n\nHope it's ok to post this here, we'd like to get some input on our approach :)\n\n[https://medium.com/@patrick\\_hoefler/optimizing-model-points-of-a-life-insurance-portfolio-e9a61f01c5bc](https://medium.com/@patrick_hoefler/optimizing-model-points-of-a-life-insurance-portfolio-e9a61f01c5bc)", "author_fullname": "t2_o3c8q92a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing model points of a life insurance portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1123lts", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676375709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I wrote 2 posts together with 2 colleagues on how to solve a compression problem from the insurance industry. We applied the Simplex algorithm to compress the number of model points (every model point is an insurance contract component like life-insurance) and fit them to our benchmark. &lt;/p&gt;\n\n&lt;p&gt;Hope it&amp;#39;s ok to post this here, we&amp;#39;d like to get some input on our approach :)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@patrick_hoefler/optimizing-model-points-of-a-life-insurance-portfolio-e9a61f01c5bc\"&gt;https://medium.com/@patrick_hoefler/optimizing-model-points-of-a-life-insurance-portfolio-e9a61f01c5bc&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?auto=webp&amp;v=enabled&amp;s=39b7487c11640083423cc5f72878e41d50c52985", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a87dc5fd57601babd8cccd07ab08f152705db880", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16d9dcb285e4a0fcedf09daf75fde631e365c811", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1da345e1a0183836f33f06324a40e14c3d07797b", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9deeede5e62a8a28bf2b79b6cc86e9f050424f0b", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25a9d51b8f7fd5b613cda0840d860d090b44dabb", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/48RYEuW1lV4l7wX862MqoKVcHKxktBraOCFOjWAQDUQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dd54cb0ce0513b9e57426247a325066126e823b", "width": 1080, "height": 606}], "variants": {}, "id": "OpkwD9oERvd4BGZ6Ps_8K-Wx2LjHM1jpWRNbPL6rYvc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1123lts", "is_robot_indexable": true, "report_reasons": null, "author": "phofl93", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1123lts/optimizing_model_points_of_a_life_insurance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1123lts/optimizing_model_points_of_a_life_insurance/", "subreddit_subscribers": 847683, "created_utc": 1676375709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "hey, im currently a 3rd year data science student looking to improve my DSA skills. I have some knowledge about DSA in C++ but i find C++ quite hard. i came across [this](https://www.udemy.com/course/data-structures-and-algorithms-bootcamp-in-python/) course which is DSA using python. Seem to cover most of the topics and has coding exercises and a few projects as well.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/dzyq3sdbn4ia1.png?width=1919&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2b77018ed3f7b99efadc2097c62725ed58034df1\n\nCan anyone share their experience if they have taken this course? is it worth it? Thanks!", "author_fullname": "t2_ch7655b2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this a good DSA course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dzyq3sdbn4ia1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d3c0d1cce45ff648fb146d272beaefd9c7e10e1"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b676f578fdf5a625c710ef785b1ab8c267569fb"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e0d7a9ef89c17f958cd48d6c4e4a8b03a4f8d52"}, {"y": 309, "x": 640, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afe34c3669233fcbd077da6638565b3029784ea4"}, {"y": 464, "x": 960, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18cabdd4be6c73373bf306a9d99e3fd40f4c3acf"}, {"y": 522, "x": 1080, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e0e44fa17cf89e757cb762f774cec8a89626c50"}], "s": {"y": 929, "x": 1919, "u": "https://preview.redd.it/dzyq3sdbn4ia1.png?width=1919&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2b77018ed3f7b99efadc2097c62725ed58034df1"}, "id": "dzyq3sdbn4ia1"}}, "name": "t3_1120x9u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LdZ5dXEqoN07EJIBk_dOsRGeSgGydtxuY3fmEbQKC-o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1676369188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey, im currently a 3rd year data science student looking to improve my DSA skills. I have some knowledge about DSA in C++ but i find C++ quite hard. i came across &lt;a href=\"https://www.udemy.com/course/data-structures-and-algorithms-bootcamp-in-python/\"&gt;this&lt;/a&gt; course which is DSA using python. Seem to cover most of the topics and has coding exercises and a few projects as well.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dzyq3sdbn4ia1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2b77018ed3f7b99efadc2097c62725ed58034df1\"&gt;https://preview.redd.it/dzyq3sdbn4ia1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2b77018ed3f7b99efadc2097c62725ed58034df1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can anyone share their experience if they have taken this course? is it worth it? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1UyxmMpDV4aOgzMDSsaH07C7J3ugsS2_rvR_fbJwOLs.jpg?auto=webp&amp;v=enabled&amp;s=2b6c15109bf537458fe84f654f0878e99a414666", "width": 480, "height": 270}, "resolutions": [{"url": "https://external-preview.redd.it/1UyxmMpDV4aOgzMDSsaH07C7J3ugsS2_rvR_fbJwOLs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e376fe87d706de37aa9fb69329a5b250391e04bf", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/1UyxmMpDV4aOgzMDSsaH07C7J3ugsS2_rvR_fbJwOLs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b1199fcd770c83cc79cbf0acd3e44aaaa9b2575", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/1UyxmMpDV4aOgzMDSsaH07C7J3ugsS2_rvR_fbJwOLs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e3cf345702747971dcdeeb972e10179defed76a", "width": 320, "height": 180}], "variants": {}, "id": "wTDgQS8KR6GH3dRBl51MiRq4yzRC5eYjErPu00M2_Vc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1120x9u", "is_robot_indexable": true, "report_reasons": null, "author": "swift__7", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1120x9u/is_this_a_good_dsa_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1120x9u/is_this_a_good_dsa_course/", "subreddit_subscribers": 847683, "created_utc": 1676369188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys,\n\nI'm working on a pet-project where I'm trying to predict rental prices in a city. I web scrapped data from Facebook Marketplace, but now I'm facing a problem. When I'm trying to extract the areas of the properties, I'm getting a lot of NaN values (\\~80%). I'm not sure how to deal with them correctly.\n\nI was thinking about using the mean or median values based on specific property type, number of bedrooms, and bathrooms. But I'm not sure if that's the best approach. I would appreciate any suggestions or advice on how to handle these NaN values.\n\nThanks in advance for your help!", "author_fullname": "t2_9spnm15l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with handling NaN values in rental price prediction project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111h0jz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676314978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a pet-project where I&amp;#39;m trying to predict rental prices in a city. I web scrapped data from Facebook Marketplace, but now I&amp;#39;m facing a problem. When I&amp;#39;m trying to extract the areas of the properties, I&amp;#39;m getting a lot of NaN values (~80%). I&amp;#39;m not sure how to deal with them correctly.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about using the mean or median values based on specific property type, number of bedrooms, and bathrooms. But I&amp;#39;m not sure if that&amp;#39;s the best approach. I would appreciate any suggestions or advice on how to handle these NaN values.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111h0jz", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant_Bad1311", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111h0jz/need_help_with_handling_nan_values_in_rental/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111h0jz/need_help_with_handling_nan_values_in_rental/", "subreddit_subscribers": 847683, "created_utc": 1676314978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'm attempting to experiment with BERTopic on a small set of review site data (less than 1000 small text passages). I'm on an Intel Macbook air that is a couple years old, running python v 3.9.13. I've worked through some initial errors related to hdbscan, and now am getting the following error.\n\n    segmentation fault  python ml_bertopic.py\n\nAfter some googling, I [found this](https://github.com/MaartenGr/BERTopic/issues/112) (but for M1 chip Mac) --I wonder if I'm stuck. Is this laptop just not up for the job of working with sentence transformers? Appreciate your advice\n\nHere's the full code. When I run this in Jupyter Notebook, the kernal dies\n\n    import csv\n    import json\n    from bertopic import BERTopic\n    import pandas as pd\n\n    comm_list = []\n\n    with open(\"comments_short.csv\", \"r\") as f:\n        for i in f:\n            comm_list.append(i)\n\n\n    print(comm_list[:5])\n\n    topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", verbose=True)\n\n    topics, probs = topic_model.fit_transform(comm_list)\n\n    topic_model.get_topic_info()", "author_fullname": "t2_z075s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sentence transformers (BERTopic) on a Macbook Air", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ghwu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676318158.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676313715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m attempting to experiment with BERTopic on a small set of review site data (less than 1000 small text passages). I&amp;#39;m on an Intel Macbook air that is a couple years old, running python v 3.9.13. I&amp;#39;ve worked through some initial errors related to hdbscan, and now am getting the following error.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;segmentation fault  python ml_bertopic.py\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;After some googling, I &lt;a href=\"https://github.com/MaartenGr/BERTopic/issues/112\"&gt;found this&lt;/a&gt; (but for M1 chip Mac) --I wonder if I&amp;#39;m stuck. Is this laptop just not up for the job of working with sentence transformers? Appreciate your advice&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the full code. When I run this in Jupyter Notebook, the kernal dies&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import csv\nimport json\nfrom bertopic import BERTopic\nimport pandas as pd\n\ncomm_list = []\n\nwith open(&amp;quot;comments_short.csv&amp;quot;, &amp;quot;r&amp;quot;) as f:\n    for i in f:\n        comm_list.append(i)\n\n\nprint(comm_list[:5])\n\ntopic_model = BERTopic(embedding_model=&amp;quot;all-MiniLM-L6-v2&amp;quot;, verbose=True)\n\ntopics, probs = topic_model.fit_transform(comm_list)\n\ntopic_model.get_topic_info()\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?auto=webp&amp;v=enabled&amp;s=bbf226a9c6b0720dd81923963fd2f24a15641dc4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=227d45a3f306bcfac12c4ceda1049ff3a0ff71c9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c6e4812556d1a69709eadc8eeccde110b39f43f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9df78f0a92990abc3e8a390953bef7ba97b11609", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae675f42c933fce8c3cc320c4c9e6b5dbff2492f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=118a9b11bdd6f7776c173b903a98b220803d900d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26b9b660367bfc5b57f6ccc2a5c92fe433a8f70a", "width": 1080, "height": 540}], "variants": {}, "id": "S1XXqx7VXEl0y3YXL_GIXCAq06KFO9bt-xoKMSyC1Gc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111ghwu", "is_robot_indexable": true, "report_reasons": null, "author": "mogla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111ghwu/sentence_transformers_bertopic_on_a_macbook_air/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111ghwu/sentence_transformers_bertopic_on_a_macbook_air/", "subreddit_subscribers": 847683, "created_utc": 1676313715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_vuozxz2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reverse causality (reverse causation): an important concept when analyzing time series data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111duqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1676307109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dsdaily.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://dsdaily.substack.com/p/ds-daily-reverse-causality?sd=pf", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111duqu", "is_robot_indexable": true, "report_reasons": null, "author": "RAFisherman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111duqu/reverse_causality_reverse_causation_an_important/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dsdaily.substack.com/p/ds-daily-reverse-causality?sd=pf", "subreddit_subscribers": 847683, "created_utc": 1676307109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am plotting my result, and the surface plot look really off. I know I am plotting right just its shape is not the best for clearness in 3D. Should I rotate or something or just use a heatmap instead of a surface plot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "name": "t3_11188bo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_70mnmect", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kex9KJDZoklIwlieJpu0bii6k_Jz0vJrrSlqScJ07QQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "labrats", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8\n\nI used another dataset for similar shape so that my PI wont kill me", "author_fullname": "t2_70mnmect", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am plotting my result, and the surface plot look really off. I know I am plotting right just its shape is not the best for clearness in 3D. Should I rotate or something or just use a heatmap instead of a surface plot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/labrats", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0in8x9m7jyha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d95166fa29b40fe643aa853bc1fa0abfa561165"}, {"y": 161, "x": 216, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fe0b4b6cd770c5d458aa872c92d5375a6b10447"}, {"y": 239, "x": 320, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c159efdd5754aaa58d36b9f9bde00a7be5eaab7"}, {"y": 479, "x": 640, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e9715f18e6d2e63f27b9a61eb3a1fd4f18865e3"}, {"y": 719, "x": 960, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eeb73c76ef5e6f72f1e9b5ac092bea4880c96e0a"}, {"y": 809, "x": 1080, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d54e1c5906168529d31e4c7e8a4bba13f4c4e6f3"}], "s": {"y": 834, "x": 1113, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8"}, "id": "0in8x9m7jyha1"}}, "name": "t3_111864p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "36fb6490-02ad-11e1-bcca-12313d192a33", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kex9KJDZoklIwlieJpu0bii6k_Jz0vJrrSlqScJ07QQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676294700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.labrats", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8\"&gt;https://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I used another dataset for similar shape so that my PI wont kill me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "labrat that is in control group ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2srnv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111864p", "is_robot_indexable": true, "report_reasons": null, "author": "Striking-Warning9533", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "subreddit_subscribers": 604949, "created_utc": 1676294700.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1676294885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.labrats", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11188bo", "is_robot_indexable": true, "report_reasons": null, "author": "Striking-Warning9533", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_111864p", "author_flair_text_color": null, "permalink": "/r/datascience/comments/11188bo/i_am_plotting_my_result_and_the_surface_plot_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "subreddit_subscribers": 847683, "created_utc": 1676294885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nI am working with a binary classification dataset with many features, and not much info about them. I made an extra trees classifier with an F1 of 0.99. I believe the data originated from a synthetic source, so I\u2019ll take that 0.99 as viable. \n\nAnyway, the many features give the most important ones about a 0.02 influence by itself. My question is, how can I see how these features may work together to influence the outcome? For example , perhaps features 3, 21, 78 create a 0.15 impact together. \n\nAny insight is appreciated!", "author_fullname": "t2_2f2zz72s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Model interpretation with many features", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111884j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676294869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a binary classification dataset with many features, and not much info about them. I made an extra trees classifier with an F1 of 0.99. I believe the data originated from a synthetic source, so I\u2019ll take that 0.99 as viable. &lt;/p&gt;\n\n&lt;p&gt;Anyway, the many features give the most important ones about a 0.02 influence by itself. My question is, how can I see how these features may work together to influence the outcome? For example , perhaps features 3, 21, 78 create a 0.15 impact together. &lt;/p&gt;\n\n&lt;p&gt;Any insight is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111884j", "is_robot_indexable": true, "report_reasons": null, "author": "anon67543", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111884j/model_interpretation_with_many_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111884j/model_interpretation_with_many_features/", "subreddit_subscribers": 847683, "created_utc": 1676294869.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking for a free/open source alternative to AWS data wrangler.\nI'd like a unified platform where to:\n- visualize the data, possibly from multiple data sources and pre-configured visualizations\n- preprocess the data with no code solutions, but with the possibility to add code and export the data transformation in python code\n\nDoes anything like this exist?", "author_fullname": "t2_11tvii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data wrangler alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1123lnu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676375693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a free/open source alternative to AWS data wrangler.\nI&amp;#39;d like a unified platform where to:\n- visualize the data, possibly from multiple data sources and pre-configured visualizations\n- preprocess the data with no code solutions, but with the possibility to add code and export the data transformation in python code&lt;/p&gt;\n\n&lt;p&gt;Does anything like this exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1123lnu", "is_robot_indexable": true, "report_reasons": null, "author": "LumosNox99", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1123lnu/aws_data_wrangler_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1123lnu/aws_data_wrangler_alternatives/", "subreddit_subscribers": 847683, "created_utc": 1676375693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am looking for help and opinions on where to start with getting some \u201everified\u201c background on data science (besides doing Bachelors/Masters). Are there any courses / trainings / bootcamps that provide a good start as well as a (kind of) official certificate?\n\nBackground: I did my bachelors in applied linguistics (statistics was a big part of it, eg linear regression on text corpora to determine readability) and have an MBA. Currently I am working in a Compliance/Risk Management Team and kind of drifted to a data analysis side role. Regularly I write sql code to extract data from our databases and see what we can learn from it. As I had no prior experience, I learned it from scratch by trial and error, asking around and some online tutorials. \n\nI have of lot of fun doing this (exploring data, writing code) and am looking to steer my career towards that direction more. For starters, I was thinking of doing something official and have some training budget to spend. Hence my question: do you have any tips on trainings that also issue certificates (ideally stuff that means sth). I am typically more of a hands on guy and don\u2019t think doing a 800\u20ac training is better than learning with coursera - but maybe I can have it both?what do you guys think? Is that a good idea?  or should I just do a python bootcamp and start from there? I don\u2019t know much about the field in general and have just started gathering information so any related tips are also highly appreciated.\n(Ofc I did some googling but I am overwhelmed and would appreciate your expertise)", "author_fullname": "t2_c7ye47jd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Training / Accreditation in Data Science - best start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1122vt6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676373140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for help and opinions on where to start with getting some \u201everified\u201c background on data science (besides doing Bachelors/Masters). Are there any courses / trainings / bootcamps that provide a good start as well as a (kind of) official certificate?&lt;/p&gt;\n\n&lt;p&gt;Background: I did my bachelors in applied linguistics (statistics was a big part of it, eg linear regression on text corpora to determine readability) and have an MBA. Currently I am working in a Compliance/Risk Management Team and kind of drifted to a data analysis side role. Regularly I write sql code to extract data from our databases and see what we can learn from it. As I had no prior experience, I learned it from scratch by trial and error, asking around and some online tutorials. &lt;/p&gt;\n\n&lt;p&gt;I have of lot of fun doing this (exploring data, writing code) and am looking to steer my career towards that direction more. For starters, I was thinking of doing something official and have some training budget to spend. Hence my question: do you have any tips on trainings that also issue certificates (ideally stuff that means sth). I am typically more of a hands on guy and don\u2019t think doing a 800\u20ac training is better than learning with coursera - but maybe I can have it both?what do you guys think? Is that a good idea?  or should I just do a python bootcamp and start from there? I don\u2019t know much about the field in general and have just started gathering information so any related tips are also highly appreciated.\n(Ofc I did some googling but I am overwhelmed and would appreciate your expertise)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1122vt6", "is_robot_indexable": true, "report_reasons": null, "author": "GoblinSwineRider", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1122vt6/training_accreditation_in_data_science_best_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1122vt6/training_accreditation_in_data_science_best_start/", "subreddit_subscribers": 847683, "created_utc": 1676373140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been asked to take a metis ACE and MPA test before an interview and I'm wondering how to approach it.\n\nBeen practising a bit today and mainly I wonder if I should just answer truthfully to the questions or if I should e.g. google which answers that are considered \"good analytical personality\"?\n\nFirst time I've come across one of these during my job hunting, kinda happy that they seem somewhat rare here.", "author_fullname": "t2_16colm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Logical/Analytical tests, ACE/MPA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11228o3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676371011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been asked to take a metis ACE and MPA test before an interview and I&amp;#39;m wondering how to approach it.&lt;/p&gt;\n\n&lt;p&gt;Been practising a bit today and mainly I wonder if I should just answer truthfully to the questions or if I should e.g. google which answers that are considered &amp;quot;good analytical personality&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;First time I&amp;#39;ve come across one of these during my job hunting, kinda happy that they seem somewhat rare here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11228o3", "is_robot_indexable": true, "report_reasons": null, "author": "Izzu96", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/11228o3/logicalanalytical_tests_acempa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/11228o3/logicalanalytical_tests_acempa/", "subreddit_subscribers": 847683, "created_utc": 1676371011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to set up a ci/cd workflow for databricks, using github actions. There seems to be a couple of main choices. \n\n* Either executing the code using the runNotebook action, so the code runs on my existing databricks cluster. I am worried that this will be a bit hacky, as I would have to rely on running scripts from the notebook instead of just regular old python scripts. Getting results back and forth from the notebook to the vm running from github actions also seeems like a hurdle to implementing a good script.\n* Or setting up the yaml to replicate the databricks environment on the virtual machine that github actions spins up. I have tried the latter, but I struggle to get all the versions of java, scala, spark, pyton, packages etc to be identical an have my code execute without issue.\n\nI mainly have access to using databricks to write code, but I could consider using vscode on a local or virtual machine, or even coder as a web based alternative. Setting up a workflow using vscode and dbx is an option, but I do not want to use conda at the moment, which is what the quickstart seems to require.   \n\n\nWhat approach would you recommend? running on the existing cluster or setting up an identical environment in the yaml that fires up a vm in github actions?", "author_fullname": "t2_lqsp3ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a databricks ci/cd pipeline with github actions - executing code on databricks cluster vs github VM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1120wo2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676369168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to set up a ci/cd workflow for databricks, using github actions. There seems to be a couple of main choices. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Either executing the code using the runNotebook action, so the code runs on my existing databricks cluster. I am worried that this will be a bit hacky, as I would have to rely on running scripts from the notebook instead of just regular old python scripts. Getting results back and forth from the notebook to the vm running from github actions also seeems like a hurdle to implementing a good script.&lt;/li&gt;\n&lt;li&gt;Or setting up the yaml to replicate the databricks environment on the virtual machine that github actions spins up. I have tried the latter, but I struggle to get all the versions of java, scala, spark, pyton, packages etc to be identical an have my code execute without issue.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I mainly have access to using databricks to write code, but I could consider using vscode on a local or virtual machine, or even coder as a web based alternative. Setting up a workflow using vscode and dbx is an option, but I do not want to use conda at the moment, which is what the quickstart seems to require.   &lt;/p&gt;\n\n&lt;p&gt;What approach would you recommend? running on the existing cluster or setting up an identical environment in the yaml that fires up a vm in github actions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1120wo2", "is_robot_indexable": true, "report_reasons": null, "author": "Tullsokk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1120wo2/setting_up_a_databricks_cicd_pipeline_with_github/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1120wo2/setting_up_a_databricks_cicd_pipeline_with_github/", "subreddit_subscribers": 847683, "created_utc": 1676369168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nAnswering  questions based on domain knowledge (like internal documentation,  contracts, books, etc.) is challenging because of the size of the domain  knowledge.\n\nAs you might know,  Transformer-based models are limited in terms of input size (most of the  time it's hard to use more than 2048 tokens). So you cannot really do  question answering on a huge knowledge base with these models.\n\nYou  can try to fine-tune your own text generation model for that (like  GPT-3, GPT-J, etc.) but results are sometimes disappointing as the  fine-tuned models tends to forget some of the facts mentioned in the  dataset...\n\nAt NLP Cloud, the  strategy we prefer is about mixing semantic search and question  answering. Here is a quick article about how to do it:\n\n[https://nlpcloud.com/question-answering-on-domain-knowledge-with-semantic-search-and-text-generation.html](https://nlpcloud.com/question-answering-on-domain-knowledge-with-semantic-search-and-text-generation.html?utm_source=reddit&amp;utm_campaign=h5d7a9cc-3816-11ed-a261-0242ac140006)\n\nAs far as we know, this is the best way to get both a fluent and user friendly answer while being very accurate.\n\nI would love to hear your thoughts on this. Can you think of a better strategy for question answering on very large documents?\n\nJulien", "author_fullname": "t2_4z4m2qcs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question Answering On Domain Knowledge With Semantic Search And Text Generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1120br9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676368470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Answering  questions based on domain knowledge (like internal documentation,  contracts, books, etc.) is challenging because of the size of the domain  knowledge.&lt;/p&gt;\n\n&lt;p&gt;As you might know,  Transformer-based models are limited in terms of input size (most of the  time it&amp;#39;s hard to use more than 2048 tokens). So you cannot really do  question answering on a huge knowledge base with these models.&lt;/p&gt;\n\n&lt;p&gt;You  can try to fine-tune your own text generation model for that (like  GPT-3, GPT-J, etc.) but results are sometimes disappointing as the  fine-tuned models tends to forget some of the facts mentioned in the  dataset...&lt;/p&gt;\n\n&lt;p&gt;At NLP Cloud, the  strategy we prefer is about mixing semantic search and question  answering. Here is a quick article about how to do it:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://nlpcloud.com/question-answering-on-domain-knowledge-with-semantic-search-and-text-generation.html?utm_source=reddit&amp;amp;utm_campaign=h5d7a9cc-3816-11ed-a261-0242ac140006\"&gt;https://nlpcloud.com/question-answering-on-domain-knowledge-with-semantic-search-and-text-generation.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As far as we know, this is the best way to get both a fluent and user friendly answer while being very accurate.&lt;/p&gt;\n\n&lt;p&gt;I would love to hear your thoughts on this. Can you think of a better strategy for question answering on very large documents?&lt;/p&gt;\n\n&lt;p&gt;Julien&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mHYCtcmhwvyno0-uAta4G_T1My3-otrPscUd2OjvMes.jpg?auto=webp&amp;v=enabled&amp;s=d3760fef3f1ab6bf1d1dbe5461b5e2df024b7dd7", "width": 1024, "height": 683}, "resolutions": [{"url": "https://external-preview.redd.it/mHYCtcmhwvyno0-uAta4G_T1My3-otrPscUd2OjvMes.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f32d166764978c9cd7cea214169c858b7f31626", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/mHYCtcmhwvyno0-uAta4G_T1My3-otrPscUd2OjvMes.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=330e05b09bb97fe6ac21be1ffc757f8fa7cf101a", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/mHYCtcmhwvyno0-uAta4G_T1My3-otrPscUd2OjvMes.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb05b30bd6775eca0fdf8ab431ffd783c78a3c3d", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/mHYCtcmhwvyno0-uAta4G_T1My3-otrPscUd2OjvMes.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d3e65e7ececfa70c71c17229f1689ccf7e4e577", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/mHYCtcmhwvyno0-uAta4G_T1My3-otrPscUd2OjvMes.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49bce8862474499c1b35d696de560211bd6ba757", "width": 960, "height": 640}], "variants": {}, "id": "jQb1GRiS1-tsk2CyjgZgae0Y0DSRywZFiupWPpk_E6M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1120br9", "is_robot_indexable": true, "report_reasons": null, "author": "juliensalinas", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1120br9/question_answering_on_domain_knowledge_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1120br9/question_answering_on_domain_knowledge_with/", "subreddit_subscribers": 847683, "created_utc": 1676368470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys,\n\nRecently I have been interviewing with some companies for DS roles, and there were many case studies involved. When given the case and not much data (just might be a basic graph), how do you come up with the hypotheses and draw conclusions quickly? The tasks are generally interesting given the time and the data, but having to answer on spot throws me off.", "author_fullname": "t2_venbqmo3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS interviews (case study)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111xlvi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676362955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;Recently I have been interviewing with some companies for DS roles, and there were many case studies involved. When given the case and not much data (just might be a basic graph), how do you come up with the hypotheses and draw conclusions quickly? The tasks are generally interesting given the time and the data, but having to answer on spot throws me off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111xlvi", "is_robot_indexable": true, "report_reasons": null, "author": "shajiegdvmd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111xlvi/ds_interviews_case_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111xlvi/ds_interviews_case_study/", "subreddit_subscribers": 847683, "created_utc": 1676362955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I need to choose between 2 laptops. The laptop I choose is the one I'm going to use for the next 3-4 years. I need it to be practical and fast to setup and easy to use. I need praticality and speed. I know how to use computers, even linux but I'm at a point I just need praticality and speed. I want to code locally and be able to run simple supervised machine-learning models locally. Any deep learning is going to be made in the could. I don't care about money but it has to be a laptop.\n\nNeed to choose between: \n\n* m2 Macbook Air 16gb ram, 512gb storage\n* 16\" Huawei Matebook with Ryzen 7 5800H processor\n\nNeed to choose between one of those, it just needs to work and have good battery and perfomance when not plugged into a wall.", "author_fullname": "t2_hl3fb0fu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which one should I get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111w0b2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676356626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I need to choose between 2 laptops. The laptop I choose is the one I&amp;#39;m going to use for the next 3-4 years. I need it to be practical and fast to setup and easy to use. I need praticality and speed. I know how to use computers, even linux but I&amp;#39;m at a point I just need praticality and speed. I want to code locally and be able to run simple supervised machine-learning models locally. Any deep learning is going to be made in the could. I don&amp;#39;t care about money but it has to be a laptop.&lt;/p&gt;\n\n&lt;p&gt;Need to choose between: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;m2 Macbook Air 16gb ram, 512gb storage&lt;/li&gt;\n&lt;li&gt;16&amp;quot; Huawei Matebook with Ryzen 7 5800H processor&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Need to choose between one of those, it just needs to work and have good battery and perfomance when not plugged into a wall.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111w0b2", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Appearance-73", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111w0b2/which_one_should_i_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111w0b2/which_one_should_i_get/", "subreddit_subscribers": 847683, "created_utc": 1676356626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Network Science Related] How much percentage of the actual network nodes (and its corresponding links) need to be in the data in order to \"properly\" describe the network?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111qxiw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676340373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111qxiw", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111qxiw/network_science_related_how_much_percentage_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111qxiw/network_science_related_how_much_percentage_of/", "subreddit_subscribers": 847683, "created_utc": 1676340373.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}