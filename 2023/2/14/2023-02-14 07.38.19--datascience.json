{"kind": "Listing", "data": {"after": "t3_111gk2x", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7b3mpu85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "you're an angel!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_111dnva", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 762, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 762, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fRQStMinHGCEMfJNW1fDwAN-WrKdyQIPa4hoDaumscM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676306633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/539jft6hizha1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/539jft6hizha1.jpg?auto=webp&amp;v=enabled&amp;s=698d7256b3867119033c96be6c48e90c971650b9", "width": 1062, "height": 1092}, "resolutions": [{"url": "https://preview.redd.it/539jft6hizha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9031897f37c64ce7b642c25d702717308dafef7b", "width": 108, "height": 111}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4278d9727a077b0eec3dd838b5759170f3e1bcbb", "width": 216, "height": 222}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbe1515e438b8985385afc2b17623cd1496bebbd", "width": 320, "height": 329}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abc1e4cf33111e2851e27c8e3e5b19cdc2514374", "width": 640, "height": 658}, {"url": "https://preview.redd.it/539jft6hizha1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cfe01728f0e30a84798781b0537d8933e19b77b", "width": 960, "height": 987}], "variants": {}, "id": "-z2k3VtrVjUY1u9BHjb7MXRvRdYqO9kNH74Rp_GmBbw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "111dnva", "is_robot_indexable": true, "report_reasons": null, "author": "BrilliantCashew", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111dnva/youre_an_angel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/539jft6hizha1.jpg", "subreddit_subscribers": 847615, "created_utc": 1676306633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So, I started using ChatGPT to gather literature references for my scientific project. Love the information it gives me, clear, accurate and so far correct. It will also give me papers supporting these findings when asked. \n\nHOWEVER, none of these papers actually exist. I can't find them on google scholar, google, or anywhere else. They can't be found by title or author names. When I ask it for a DOI it happily provides one, but it either is not taken or leads to a different paper that has nothing to do with the topic. I thought translations from different languages could be the cause and it was actually a thing for some papers, but not even the english ones could be traced anywhere online.\n\nDoes ChatGPR just generate random papers that look damn much like real ones?\n\nhttps://preview.redd.it/s8sa42mzixha1.png?width=824&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70dfc38d58b6219ea4d494142e5f9e4b75e92a7a", "author_fullname": "t2_847qh0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ghost papers provided by ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s8sa42mzixha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/s8sa42mzixha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8cf62f95b4939e92302401ac3415ff1031dc3973"}, {"y": 159, "x": 216, "u": "https://preview.redd.it/s8sa42mzixha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a79390ea24a28023e4f9dbf264aa475bd4bb449"}, {"y": 236, "x": 320, "u": "https://preview.redd.it/s8sa42mzixha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0506a6e16d52c4d02b690625d2a821aff58c78b2"}, {"y": 472, "x": 640, "u": "https://preview.redd.it/s8sa42mzixha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e6a5f9c05bc37fd6b3153fe836906febb459633"}], "s": {"y": 608, "x": 824, "u": "https://preview.redd.it/s8sa42mzixha1.png?width=824&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70dfc38d58b6219ea4d494142e5f9e4b75e92a7a"}, "id": "s8sa42mzixha1"}}, "name": "t3_1114q7e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 339, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 339, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6L710ySDHoGPiIdfzIbF4wZzzb7RjK356OL7gPAbEvY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676282549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I started using ChatGPT to gather literature references for my scientific project. Love the information it gives me, clear, accurate and so far correct. It will also give me papers supporting these findings when asked. &lt;/p&gt;\n\n&lt;p&gt;HOWEVER, none of these papers actually exist. I can&amp;#39;t find them on google scholar, google, or anywhere else. They can&amp;#39;t be found by title or author names. When I ask it for a DOI it happily provides one, but it either is not taken or leads to a different paper that has nothing to do with the topic. I thought translations from different languages could be the cause and it was actually a thing for some papers, but not even the english ones could be traced anywhere online.&lt;/p&gt;\n\n&lt;p&gt;Does ChatGPR just generate random papers that look damn much like real ones?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s8sa42mzixha1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=70dfc38d58b6219ea4d494142e5f9e4b75e92a7a\"&gt;https://preview.redd.it/s8sa42mzixha1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=70dfc38d58b6219ea4d494142e5f9e4b75e92a7a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1114q7e", "is_robot_indexable": true, "report_reasons": null, "author": "flexeltheman", "discussion_type": null, "num_comments": 148, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1114q7e/ghost_papers_provided_by_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1114q7e/ghost_papers_provided_by_chatgpt/", "subreddit_subscribers": 847615, "created_utc": 1676282549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "One very useful benefit I found with ChatGPT for job seekers is asking it to serve as a Hiring Manager that asks you questions for the specific role you're interested in.\n\nTo help it guide you, you need to provide it with some context based on the job description. For example, you list out the job description to ChatGPT and say that you want to be asked questions on causality and experimentation.\n\nHere's the response from ChatGPT based on the following prompt I provided it after sharing the job description:\n\n===**PROMPT BY ME===**\n\n&gt;I would like you to be a hiring manager. I'll be soon interviewing with you for a Data Science role, and you are going to ask me questions related to causality and experimentation. What are the questions you would ask to test my knowledge?\n\n**==RESPONSE BY CHATGPT===**\n\nSure! As a hiring manager for a data science role, I would like to assess your understanding of causality and experimentation. Here are a few questions I might ask:\n\n1. Can you explain the difference between correlation and causation, and give an example of each?\n2. How do you design an experiment to test a causal hypothesis?\n3. How do you ensure that the results of an experiment are valid and reliable?\n4. Can you explain the concept of confounding variables, and how do you control for them in an experiment?\n5. Can you explain the difference between a randomized controlled trial (RCT) and an observational study?\n6. How do you measure the impact of a treatment or intervention in an experiment?\n7. Have you worked with any causal inference methods such as propensity score matching or instrumental variables? Can you give an example of when you used them?\n8. How do you determine which variables to include in an experiment, and how do you avoid overfitting in the modeling process?\n9. Can you discuss any challenges or limitations you have encountered in designing and conducting experiments, and how you overcame them?\n10. Can you give an example of a real-world scenario in which you applied causal inference or experimentation methods to solve a business problem?\n\n====\n\nYou can ask ChatGPT to provide a summary answer for each. However, I would highly recommend you validate the answers by researching as well as ChatGPT can give confident, wrong answers.\n\nHope this helps others!", "author_fullname": "t2_48648", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Hunting Tip: Practice Interviewing with ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111h9pp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 125, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 125, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676315601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One very useful benefit I found with ChatGPT for job seekers is asking it to serve as a Hiring Manager that asks you questions for the specific role you&amp;#39;re interested in.&lt;/p&gt;\n\n&lt;p&gt;To help it guide you, you need to provide it with some context based on the job description. For example, you list out the job description to ChatGPT and say that you want to be asked questions on causality and experimentation.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the response from ChatGPT based on the following prompt I provided it after sharing the job description:&lt;/p&gt;\n\n&lt;p&gt;===&lt;strong&gt;PROMPT BY ME===&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I would like you to be a hiring manager. I&amp;#39;ll be soon interviewing with you for a Data Science role, and you are going to ask me questions related to causality and experimentation. What are the questions you would ask to test my knowledge?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;==RESPONSE BY CHATGPT===&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Sure! As a hiring manager for a data science role, I would like to assess your understanding of causality and experimentation. Here are a few questions I might ask:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Can you explain the difference between correlation and causation, and give an example of each?&lt;/li&gt;\n&lt;li&gt;How do you design an experiment to test a causal hypothesis?&lt;/li&gt;\n&lt;li&gt;How do you ensure that the results of an experiment are valid and reliable?&lt;/li&gt;\n&lt;li&gt;Can you explain the concept of confounding variables, and how do you control for them in an experiment?&lt;/li&gt;\n&lt;li&gt;Can you explain the difference between a randomized controlled trial (RCT) and an observational study?&lt;/li&gt;\n&lt;li&gt;How do you measure the impact of a treatment or intervention in an experiment?&lt;/li&gt;\n&lt;li&gt;Have you worked with any causal inference methods such as propensity score matching or instrumental variables? Can you give an example of when you used them?&lt;/li&gt;\n&lt;li&gt;How do you determine which variables to include in an experiment, and how do you avoid overfitting in the modeling process?&lt;/li&gt;\n&lt;li&gt;Can you discuss any challenges or limitations you have encountered in designing and conducting experiments, and how you overcame them?&lt;/li&gt;\n&lt;li&gt;Can you give an example of a real-world scenario in which you applied causal inference or experimentation methods to solve a business problem?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;You can ask ChatGPT to provide a summary answer for each. However, I would highly recommend you validate the answers by researching as well as ChatGPT can give confident, wrong answers.&lt;/p&gt;\n\n&lt;p&gt;Hope this helps others!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111h9pp", "is_robot_indexable": true, "report_reasons": null, "author": "forbiscuit", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111h9pp/job_hunting_tip_practice_interviewing_with_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111h9pp/job_hunting_tip_practice_interviewing_with_chatgpt/", "subreddit_subscribers": 847615, "created_utc": 1676315601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I had an interview with an interesting question. I had a solution that would have worked which would have required a recursive query, but the interviewer said there is a simpler solution. I was wondering if anyone here could figure it out\n\nFour column table:\n\n     1. UserID- unique string\n     2. Device Id- unique string\n     3. session_start_time- timestamp\n     4. session_end_time- timestamp\n\nIt's possible for users with different deviceIds to have overlapping sessions. \nThe goal is to have one row per user per overlapping session.\nSo let's say that I have a session on my phone from 10:30-11:30, and on my computer from 10:45-11:45. The goal would be to aggregate this into a single row:\n\n     UserId|session_start|session_end\n     aaquad|10:30|11:45\n\nMy strategy was to use a lag to bring up the next row in order to be able to compare them- and the recursion would have been required in cases where a user had 3 or more overlapping sessions. But according to the interviewer, there is a different solution (I asked and did not get the answer).\n\nAny ideas?", "author_fullname": "t2_13b1f3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interesting SQL Interview Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111232h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676272042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an interview with an interesting question. I had a solution that would have worked which would have required a recursive query, but the interviewer said there is a simpler solution. I was wondering if anyone here could figure it out&lt;/p&gt;\n\n&lt;p&gt;Four column table:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; 1. UserID- unique string\n 2. Device Id- unique string\n 3. session_start_time- timestamp\n 4. session_end_time- timestamp\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It&amp;#39;s possible for users with different deviceIds to have overlapping sessions. \nThe goal is to have one row per user per overlapping session.\nSo let&amp;#39;s say that I have a session on my phone from 10:30-11:30, and on my computer from 10:45-11:45. The goal would be to aggregate this into a single row:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; UserId|session_start|session_end\n aaquad|10:30|11:45\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My strategy was to use a lag to bring up the next row in order to be able to compare them- and the recursion would have been required in cases where a user had 3 or more overlapping sessions. But according to the interviewer, there is a different solution (I asked and did not get the answer).&lt;/p&gt;\n\n&lt;p&gt;Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111232h", "is_robot_indexable": true, "report_reasons": null, "author": "aaquad", "discussion_type": null, "num_comments": 38, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111232h/interesting_sql_interview_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111232h/interesting_sql_interview_question/", "subreddit_subscribers": 847615, "created_utc": 1676272042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was not happy in my job for a while. Dead-end project, under-resourced, people leaving constantly.\n\nI spoke up a number of times in how unhappy I am but my boss took it semi-seriously and didn't really take much action.\n\nI recently got a new offer, although only 12% higher total package. Seems like this offer is for more interesting work and generally in a more dynamic environment. It's a downgrade in title and offers a bit less security though.\n\nI attempted to resign today but my boss promised me a new project and a new team. While it will take maybe a couple of months to tie up lose ends, my boss is generally trustworthy. He seemed genuinely distressed and sad that I am thinking of leaving.\n\nI don't know what to do.", "author_fullname": "t2_vw0djvfd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tried to resign but my boss talked me out of it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1113cce", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676276983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was not happy in my job for a while. Dead-end project, under-resourced, people leaving constantly.&lt;/p&gt;\n\n&lt;p&gt;I spoke up a number of times in how unhappy I am but my boss took it semi-seriously and didn&amp;#39;t really take much action.&lt;/p&gt;\n\n&lt;p&gt;I recently got a new offer, although only 12% higher total package. Seems like this offer is for more interesting work and generally in a more dynamic environment. It&amp;#39;s a downgrade in title and offers a bit less security though.&lt;/p&gt;\n\n&lt;p&gt;I attempted to resign today but my boss promised me a new project and a new team. While it will take maybe a couple of months to tie up lose ends, my boss is generally trustworthy. He seemed genuinely distressed and sad that I am thinking of leaving.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1113cce", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Potato-9181", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1113cce/tried_to_resign_but_my_boss_talked_me_out_of_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1113cce/tried_to_resign_but_my_boss_talked_me_out_of_it/", "subreddit_subscribers": 847615, "created_utc": 1676276983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a3gidxwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use tools for automatic EDA? Which ones? Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 124, "top_awarded_type": null, "hide_score": false, "name": "t3_111bdx7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0n-XkGoK9DGBd2apAPMmt0LMqEZD_mgZ2OiYHWzsGVQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676300959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ebndmkmo1zha1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ebndmkmo1zha1.png?auto=webp&amp;v=enabled&amp;s=361760fbb896149ca6627a89a02e5c114c1e3d37", "width": 573, "height": 511}, "resolutions": [{"url": "https://preview.redd.it/ebndmkmo1zha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=daeee7c14463aaf455054d332f8070a5890259c3", "width": 108, "height": 96}, {"url": "https://preview.redd.it/ebndmkmo1zha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9652ddc761884172b566f646aab38498570190d", "width": 216, "height": 192}, {"url": "https://preview.redd.it/ebndmkmo1zha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74a098147bdc4010d91343568db317ee8b207aae", "width": 320, "height": 285}], "variants": {}, "id": "zdkAAF1c8bR8rK7ISrL8SFsPBgatJO54h2qCQfS2lRU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "111bdx7", "is_robot_indexable": true, "report_reasons": null, "author": "malirkan", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111bdx7/do_you_use_tools_for_automatic_eda_which_ones_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ebndmkmo1zha1.png", "subreddit_subscribers": 847615, "created_utc": 1676300959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Been getting a tad annoyed with Conda lately, at least as a package manager. So I wanted to hear what everyone else likes to use.", "author_fullname": "t2_dqghhocz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use to manage your Python packages and environments? Do you prefer Conda or something like virtualenv + pip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111djvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676306360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been getting a tad annoyed with Conda lately, at least as a package manager. So I wanted to hear what everyone else likes to use.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111djvu", "is_robot_indexable": true, "report_reasons": null, "author": "CardboardBoxPlot", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111djvu/what_do_you_use_to_manage_your_python_packages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111djvu/what_do_you_use_to_manage_your_python_packages/", "subreddit_subscribers": 847615, "created_utc": 1676306360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I'm currently in my 6 month journey learning DS( I come from a mechanical engineering background), and I was wondering what tools/topics should I focus on if I want to get an entry level job in data science in the field of cancer research? \nIf anyone had an idea about this topic I would really appreciate your insight. Thank you!", "author_fullname": "t2_8773zq8m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going into cancer research in data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111m24k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676327352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m currently in my 6 month journey learning DS( I come from a mechanical engineering background), and I was wondering what tools/topics should I focus on if I want to get an entry level job in data science in the field of cancer research? \nIf anyone had an idea about this topic I would really appreciate your insight. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111m24k", "is_robot_indexable": true, "report_reasons": null, "author": "baydati", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111m24k/going_into_cancer_research_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111m24k/going_into_cancer_research_in_data_science/", "subreddit_subscribers": 847615, "created_utc": 1676327352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Should job posting with mentions of these tools be ignored or are they useful for a data scientists?", "author_fullname": "t2_17yq29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What role does Tableau and Power BI play in your analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111lroh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676330349.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676326649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should job posting with mentions of these tools be ignored or are they useful for a data scientists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111lroh", "is_robot_indexable": true, "report_reasons": null, "author": "shastaslacker", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111lroh/what_role_does_tableau_and_power_bi_play_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111lroh/what_role_does_tableau_and_power_bi_play_in_your/", "subreddit_subscribers": 847615, "created_utc": 1676326649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'd love some clarification on these dimensionality reduction methods. I see PCA/PCR recommended as the go-to method for this type of task and almost never see people talk about using PLS to get principal components. It's my understanding that the key difference is that PLS considers the target when making components. \n\nIntuitively, I feel like this would make PLS stronger in many datasets, as it uses more of the available information to better inform the creation of components. I'm not an expert here and don't actually have a ton of expertise with how either of these methods work on a deep level, so any feedback from more experienced data scientists would be much appreciated. Thanks in advance for any thoughts or opinions here!", "author_fullname": "t2_vj9xwd07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is PCR recommended much more often than PLS for dimension reduction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111kw8i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676324549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;d love some clarification on these dimensionality reduction methods. I see PCA/PCR recommended as the go-to method for this type of task and almost never see people talk about using PLS to get principal components. It&amp;#39;s my understanding that the key difference is that PLS considers the target when making components. &lt;/p&gt;\n\n&lt;p&gt;Intuitively, I feel like this would make PLS stronger in many datasets, as it uses more of the available information to better inform the creation of components. I&amp;#39;m not an expert here and don&amp;#39;t actually have a ton of expertise with how either of these methods work on a deep level, so any feedback from more experienced data scientists would be much appreciated. Thanks in advance for any thoughts or opinions here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111kw8i", "is_robot_indexable": true, "report_reasons": null, "author": "NDVGuy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111kw8i/why_is_pcr_recommended_much_more_often_than_pls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111kw8i/why_is_pcr_recommended_much_more_often_than_pls/", "subreddit_subscribers": 847615, "created_utc": 1676324549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I need to choose between 2 laptops. The laptop I choose is the one I'm going to use for the next 3-4 years. I need it to be practical and fast to setup and easy to use. I need praticality and speed. I know how to use computers, even linux but I'm at a point I just need praticality and speed. I want to code locally and be able to run simple supervised machine-learning models locally. Any deep learning is going to be made in the could. I don't care about money but it has to be a laptop.\n\nNeed to choose between: \n\n* m2 Macbook Air 16gb ram, 512gb storage\n* 16\" Huawei Matebook with Ryzen 7 5800H processor\n\nNeed to choose between one of those, it just needs to work and have good battery and perfomance when not plugged into a wall.", "author_fullname": "t2_hl3fb0fu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which one should I get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_111w0b2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676356626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I need to choose between 2 laptops. The laptop I choose is the one I&amp;#39;m going to use for the next 3-4 years. I need it to be practical and fast to setup and easy to use. I need praticality and speed. I know how to use computers, even linux but I&amp;#39;m at a point I just need praticality and speed. I want to code locally and be able to run simple supervised machine-learning models locally. Any deep learning is going to be made in the could. I don&amp;#39;t care about money but it has to be a laptop.&lt;/p&gt;\n\n&lt;p&gt;Need to choose between: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;m2 Macbook Air 16gb ram, 512gb storage&lt;/li&gt;\n&lt;li&gt;16&amp;quot; Huawei Matebook with Ryzen 7 5800H processor&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Need to choose between one of those, it just needs to work and have good battery and perfomance when not plugged into a wall.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111w0b2", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Appearance-73", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111w0b2/which_one_should_i_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111w0b2/which_one_should_i_get/", "subreddit_subscribers": 847615, "created_utc": 1676356626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I barely cared about internships and work experience. Im in my fourth year of college and I realized I should have done more. So I found out how to make my own projects and portfolio. Finally, landed my first unpaid DS + machine learning intern spot. Excited to learn on the job, but how should I be climbing up the ladder to better positions with better pay. The internship is a small commitment under 10 hours a week. What should I be doing on the side?", "author_fullname": "t2_1bswoe14", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move up in the industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_111vvya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676356192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I barely cared about internships and work experience. Im in my fourth year of college and I realized I should have done more. So I found out how to make my own projects and portfolio. Finally, landed my first unpaid DS + machine learning intern spot. Excited to learn on the job, but how should I be climbing up the ladder to better positions with better pay. The internship is a small commitment under 10 hours a week. What should I be doing on the side?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111vvya", "is_robot_indexable": true, "report_reasons": null, "author": "RebornMoki", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111vvya/how_to_move_up_in_the_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111vvya/how_to_move_up_in_the_industry/", "subreddit_subscribers": 847615, "created_utc": 1676356192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[This paper](https://arxiv.org/abs/2112.13299) is super interesting, and by the same author who introduced CUPED.\n\nIn short, experiments with one sided non-compliance can result in a watered down ATE due to few users actually engaging with the treatment.  This paper builds on the instrumental variables literature (which we leverage a lot for these types of experiments) to use cuped to further reduce variance.\n\nIts super interesting and really useful for those of us who run experiments and need to increase precision of our estimates.  Best of all, there is some fairly food code attached as a footnote.", "author_fullname": "t2_131vu3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zero to Hero: Exploiting Null Effects to Achieve Variance Reduction in Experiments with One-sided Triggering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111rr5g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "modflair", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676342770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2112.13299\"&gt;This paper&lt;/a&gt; is super interesting, and by the same author who introduced CUPED.&lt;/p&gt;\n\n&lt;p&gt;In short, experiments with one sided non-compliance can result in a watered down ATE due to few users actually engaging with the treatment.  This paper builds on the instrumental variables literature (which we leverage a lot for these types of experiments) to use cuped to further reduce variance.&lt;/p&gt;\n\n&lt;p&gt;Its super interesting and really useful for those of us who run experiments and need to increase precision of our estimates.  Best of all, there is some fairly food code attached as a footnote.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111rr5g", "is_robot_indexable": true, "report_reasons": null, "author": "__compactsupport__", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/111rr5g/zero_to_hero_exploiting_null_effects_to_achieve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111rr5g/zero_to_hero_exploiting_null_effects_to_achieve/", "subreddit_subscribers": 847615, "created_utc": 1676342770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nI am a beginner in data analytics and I find it very interesting. For a school project we have a case where company A only has a ERP system where all the transactions happen and it is connected to a single Database(SQL). The question in this case is what solution would be appropiate because the company doesn't have the manpower and budget to maintain data analytics.\n\nWe have learned the basics of ETL, Data warehousing with the Visual Studio tool and connection with SSIS package. But I did some research and found out you can do ETL in powerBI. But how reliable is this? And do you have to keep changing the relationsships in PowerBI everytime you want to make a dashboard or does it stay like that? I need to make a few KPI and dasboards for this school project.\n\nThanks for advance responses", "author_fullname": "t2_1m0nn2zz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "School", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1113ep1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676277222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am a beginner in data analytics and I find it very interesting. For a school project we have a case where company A only has a ERP system where all the transactions happen and it is connected to a single Database(SQL). The question in this case is what solution would be appropiate because the company doesn&amp;#39;t have the manpower and budget to maintain data analytics.&lt;/p&gt;\n\n&lt;p&gt;We have learned the basics of ETL, Data warehousing with the Visual Studio tool and connection with SSIS package. But I did some research and found out you can do ETL in powerBI. But how reliable is this? And do you have to keep changing the relationsships in PowerBI everytime you want to make a dashboard or does it stay like that? I need to make a few KPI and dasboards for this school project.&lt;/p&gt;\n\n&lt;p&gt;Thanks for advance responses&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1113ep1", "is_robot_indexable": true, "report_reasons": null, "author": "superarcady", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1113ep1/school/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1113ep1/school/", "subreddit_subscribers": 847615, "created_utc": 1676277222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I knew that I wanted to do something data-related (or computer science-based) for a long time, but unfortunately my undergrad major was completely unrelated to DS. I self-taught myself Python/R while working a marketing job and was suuper fortunate to get into a DS/DA Master\u2019s Program at one of the Ivy League schools. I worked 24/7 to learn a ton about best data practices/tools, had 2 DS co-ops, generated a ton of material for my GitHub, etc.\n\nObviously, just finishing a Master\u2019s program isn\u2019t enough to enter DS, so my career counselor recommended me to \u201ctake a data analyst job at a low-maturity company, use your skills to upgrade the way they use data, become a data rockstar and you\u2019ll get all kinds of doors opened in no time.\u201d **Spoiler** \\- *I did not become a data rockstar*.\n\nI landed a marketing analyst position at a large insurance company, and, long story short, this job is just a mess. My boss is a lifer that recently celebrated 20 years in the same position at the same company and got promoted to manager right as I joined. 30% of my work is shuffling around Excel sheets/making ugly (because the style is regulated) PowerPoints, another 30% is attending useless meetings that she sets up for me to \u201chear what conversations are happening around here\u201d; 20% is making sense of the broken SPSS code that someone wrote for her 15 years ago; the rest of the time is allocated to using Power BI, SQL and R to pull data from various sources and create reports for the internal stakeholders.\n\nNothing at my job is Googlable because all of the processes are either created by someone from 20 years ago or my manager herself; each process can require up to 100 little steps of moving data between million Excel spreadsheets populated with billion formulas making it impossible to comprehend or even load. The documentation she provides me with is always incorrect/incomplete, forcing me to ask for her help, as it would take me many hours to figure this out myself. When I try to do things my way, she gets clearly upset and sometimes starts micromanaging me, making us miss a ton of deadlines just for the sake of making everything look perfect according to the internal standards.\u00a0\n\nI spend a ton of time outside of work hours (the last one to leave the office every single day) trying to make time for extra DS projects but the amount of \u201chigh-priority\u201d tasks I get assigned is too overwhelming. The most technical thing I've done is used VDI to automate an R script that imports a bunch of files, anti-joins them and outputs certain distributions into an Excel doc, but that was a one-time project. I told my manager that I enjoy coding the most and she promised to land me a \u201ccoding-heavy\u201d project with the DS team. However, I followed up 3 times throughout a month and she ended up telling me that the person who asked for help decided to leave the company (jeez I wonder why.)\u00a0\n\nI get so much stress from a seemingly \u201ceasy\u201d job (no one seems to care that everything is 2+ weeks past due or indefinitely undelivered) because I\u2019m trying so hard to get a chance to make an impact and have at least something I can use when applying for DS roles after hitting that 1-year mark. I\u2019m afraid that the advanced skills I acquired are slowly fading away and I\u2019m learning a lot of bad habits from my manager.\n\nWhat would you recommend in my situation to stay on the right track? I usually work through my lunch breaks too (I know, I know), but I had an idea of maybe using that time to read a stats book or something DS-related - any suggestions? Any advice is appreciated :)\n\nEdit: the reason I put \"Ivy League School\" is because, a couple of months ago, my manager gave me an impossible task; I asked for a suggestion on how to better approach this task, and she responded with \"well, I'm not the one with the Master's Degree from the Ivy Leagues.\" I got so butthurt from this because I haven't brought it up once since joining the company lol.\n\nTLDR: want to break into DS through low-maturity analytics job but feeling like it has a negative effect on my career potential - tips?\u00a0", "author_fullname": "t2_6kb6g3y1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a mistake to enter DS through low-maturity Data Analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_111wcmt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676358583.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676357925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I knew that I wanted to do something data-related (or computer science-based) for a long time, but unfortunately my undergrad major was completely unrelated to DS. I self-taught myself Python/R while working a marketing job and was suuper fortunate to get into a DS/DA Master\u2019s Program at one of the Ivy League schools. I worked 24/7 to learn a ton about best data practices/tools, had 2 DS co-ops, generated a ton of material for my GitHub, etc.&lt;/p&gt;\n\n&lt;p&gt;Obviously, just finishing a Master\u2019s program isn\u2019t enough to enter DS, so my career counselor recommended me to \u201ctake a data analyst job at a low-maturity company, use your skills to upgrade the way they use data, become a data rockstar and you\u2019ll get all kinds of doors opened in no time.\u201d &lt;strong&gt;Spoiler&lt;/strong&gt; - &lt;em&gt;I did not become a data rockstar&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;I landed a marketing analyst position at a large insurance company, and, long story short, this job is just a mess. My boss is a lifer that recently celebrated 20 years in the same position at the same company and got promoted to manager right as I joined. 30% of my work is shuffling around Excel sheets/making ugly (because the style is regulated) PowerPoints, another 30% is attending useless meetings that she sets up for me to \u201chear what conversations are happening around here\u201d; 20% is making sense of the broken SPSS code that someone wrote for her 15 years ago; the rest of the time is allocated to using Power BI, SQL and R to pull data from various sources and create reports for the internal stakeholders.&lt;/p&gt;\n\n&lt;p&gt;Nothing at my job is Googlable because all of the processes are either created by someone from 20 years ago or my manager herself; each process can require up to 100 little steps of moving data between million Excel spreadsheets populated with billion formulas making it impossible to comprehend or even load. The documentation she provides me with is always incorrect/incomplete, forcing me to ask for her help, as it would take me many hours to figure this out myself. When I try to do things my way, she gets clearly upset and sometimes starts micromanaging me, making us miss a ton of deadlines just for the sake of making everything look perfect according to the internal standards.\u00a0&lt;/p&gt;\n\n&lt;p&gt;I spend a ton of time outside of work hours (the last one to leave the office every single day) trying to make time for extra DS projects but the amount of \u201chigh-priority\u201d tasks I get assigned is too overwhelming. The most technical thing I&amp;#39;ve done is used VDI to automate an R script that imports a bunch of files, anti-joins them and outputs certain distributions into an Excel doc, but that was a one-time project. I told my manager that I enjoy coding the most and she promised to land me a \u201ccoding-heavy\u201d project with the DS team. However, I followed up 3 times throughout a month and she ended up telling me that the person who asked for help decided to leave the company (jeez I wonder why.)\u00a0&lt;/p&gt;\n\n&lt;p&gt;I get so much stress from a seemingly \u201ceasy\u201d job (no one seems to care that everything is 2+ weeks past due or indefinitely undelivered) because I\u2019m trying so hard to get a chance to make an impact and have at least something I can use when applying for DS roles after hitting that 1-year mark. I\u2019m afraid that the advanced skills I acquired are slowly fading away and I\u2019m learning a lot of bad habits from my manager.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend in my situation to stay on the right track? I usually work through my lunch breaks too (I know, I know), but I had an idea of maybe using that time to read a stats book or something DS-related - any suggestions? Any advice is appreciated :)&lt;/p&gt;\n\n&lt;p&gt;Edit: the reason I put &amp;quot;Ivy League School&amp;quot; is because, a couple of months ago, my manager gave me an impossible task; I asked for a suggestion on how to better approach this task, and she responded with &amp;quot;well, I&amp;#39;m not the one with the Master&amp;#39;s Degree from the Ivy Leagues.&amp;quot; I got so butthurt from this because I haven&amp;#39;t brought it up once since joining the company lol.&lt;/p&gt;\n\n&lt;p&gt;TLDR: want to break into DS through low-maturity analytics job but feeling like it has a negative effect on my career potential - tips?\u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111wcmt", "is_robot_indexable": true, "report_reasons": null, "author": "busy_bee_movie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111wcmt/is_it_a_mistake_to_enter_ds_through_lowmaturity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111wcmt/is_it_a_mistake_to_enter_ds_through_lowmaturity/", "subreddit_subscribers": 847615, "created_utc": 1676357925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My understanding is that -\n\n`cross_val_predict` yields the actual predictions from the CV. E.g., if there are 5 folds, then the output of each sample is its predicted value when it was in the test fold (trained on the other 4 folds of data).\n\n`cross_val_score` calculates the predictions in the exact same way as `cross_val_predict`. However, it does not return the predictions - only the averaged score across the test folds.\n\nI\u2019m not completely sure though. Could anyone please confirm or correct my understanding?\n\nThank you!", "author_fullname": "t2_kxtyr690", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "sklearn - cross_val_predict VS cross_val_score?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111uv80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676352610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My understanding is that -&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;cross_val_predict&lt;/code&gt; yields the actual predictions from the CV. E.g., if there are 5 folds, then the output of each sample is its predicted value when it was in the test fold (trained on the other 4 folds of data).&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;cross_val_score&lt;/code&gt; calculates the predictions in the exact same way as &lt;code&gt;cross_val_predict&lt;/code&gt;. However, it does not return the predictions - only the averaged score across the test folds.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not completely sure though. Could anyone please confirm or correct my understanding?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111uv80", "is_robot_indexable": true, "report_reasons": null, "author": "user149162536", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111uv80/sklearn_cross_val_predict_vs_cross_val_score/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111uv80/sklearn_cross_val_predict_vs_cross_val_score/", "subreddit_subscribers": 847615, "created_utc": 1676352610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys,\n\nI'm working on a pet-project where I'm trying to predict rental prices in a city. I web scrapped data from Facebook Marketplace, but now I'm facing a problem. When I'm trying to extract the areas of the properties, I'm getting a lot of NaN values (\\~80%). I'm not sure how to deal with them correctly.\n\nI was thinking about using the mean or median values based on specific property type, number of bedrooms, and bathrooms. But I'm not sure if that's the best approach. I would appreciate any suggestions or advice on how to handle these NaN values.\n\nThanks in advance for your help!", "author_fullname": "t2_9spnm15l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with handling NaN values in rental price prediction project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111h0jz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676314978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a pet-project where I&amp;#39;m trying to predict rental prices in a city. I web scrapped data from Facebook Marketplace, but now I&amp;#39;m facing a problem. When I&amp;#39;m trying to extract the areas of the properties, I&amp;#39;m getting a lot of NaN values (~80%). I&amp;#39;m not sure how to deal with them correctly.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about using the mean or median values based on specific property type, number of bedrooms, and bathrooms. But I&amp;#39;m not sure if that&amp;#39;s the best approach. I would appreciate any suggestions or advice on how to handle these NaN values.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111h0jz", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant_Bad1311", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111h0jz/need_help_with_handling_nan_values_in_rental/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111h0jz/need_help_with_handling_nan_values_in_rental/", "subreddit_subscribers": 847615, "created_utc": 1676314978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'm attempting to experiment with BERTopic on a small set of review site data (less than 1000 small text passages). I'm on an Intel Macbook air that is a couple years old, running python v 3.9.13. I've worked through some initial errors related to hdbscan, and now am getting the following error.\n\n    segmentation fault  python ml_bertopic.py\n\nAfter some googling, I [found this](https://github.com/MaartenGr/BERTopic/issues/112) (but for M1 chip Mac) --I wonder if I'm stuck. Is this laptop just not up for the job of working with sentence transformers? Appreciate your advice\n\nHere's the full code. When I run this in Jupyter Notebook, the kernal dies\n\n    import csv\n    import json\n    from bertopic import BERTopic\n    import pandas as pd\n\n    comm_list = []\n\n    with open(\"comments_short.csv\", \"r\") as f:\n        for i in f:\n            comm_list.append(i)\n\n\n    print(comm_list[:5])\n\n    topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", verbose=True)\n\n    topics, probs = topic_model.fit_transform(comm_list)\n\n    topic_model.get_topic_info()", "author_fullname": "t2_z075s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sentence transformers (BERTopic) on a Macbook Air", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ghwu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676318158.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676313715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m attempting to experiment with BERTopic on a small set of review site data (less than 1000 small text passages). I&amp;#39;m on an Intel Macbook air that is a couple years old, running python v 3.9.13. I&amp;#39;ve worked through some initial errors related to hdbscan, and now am getting the following error.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;segmentation fault  python ml_bertopic.py\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;After some googling, I &lt;a href=\"https://github.com/MaartenGr/BERTopic/issues/112\"&gt;found this&lt;/a&gt; (but for M1 chip Mac) --I wonder if I&amp;#39;m stuck. Is this laptop just not up for the job of working with sentence transformers? Appreciate your advice&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the full code. When I run this in Jupyter Notebook, the kernal dies&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import csv\nimport json\nfrom bertopic import BERTopic\nimport pandas as pd\n\ncomm_list = []\n\nwith open(&amp;quot;comments_short.csv&amp;quot;, &amp;quot;r&amp;quot;) as f:\n    for i in f:\n        comm_list.append(i)\n\n\nprint(comm_list[:5])\n\ntopic_model = BERTopic(embedding_model=&amp;quot;all-MiniLM-L6-v2&amp;quot;, verbose=True)\n\ntopics, probs = topic_model.fit_transform(comm_list)\n\ntopic_model.get_topic_info()\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?auto=webp&amp;v=enabled&amp;s=bbf226a9c6b0720dd81923963fd2f24a15641dc4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=227d45a3f306bcfac12c4ceda1049ff3a0ff71c9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c6e4812556d1a69709eadc8eeccde110b39f43f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9df78f0a92990abc3e8a390953bef7ba97b11609", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae675f42c933fce8c3cc320c4c9e6b5dbff2492f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=118a9b11bdd6f7776c173b903a98b220803d900d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/bQYcKp3txeUoQ7HHj5ruZ4vI4lqlfoFkBT0nIMKovCA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26b9b660367bfc5b57f6ccc2a5c92fe433a8f70a", "width": 1080, "height": 540}], "variants": {}, "id": "S1XXqx7VXEl0y3YXL_GIXCAq06KFO9bt-xoKMSyC1Gc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111ghwu", "is_robot_indexable": true, "report_reasons": null, "author": "mogla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111ghwu/sentence_transformers_bertopic_on_a_macbook_air/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111ghwu/sentence_transformers_bertopic_on_a_macbook_air/", "subreddit_subscribers": 847615, "created_utc": 1676313715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_vuozxz2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reverse causality (reverse causation): an important concept when analyzing time series data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111duqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1676307109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dsdaily.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://dsdaily.substack.com/p/ds-daily-reverse-causality?sd=pf", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111duqu", "is_robot_indexable": true, "report_reasons": null, "author": "RAFisherman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111duqu/reverse_causality_reverse_causation_an_important/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dsdaily.substack.com/p/ds-daily-reverse-causality?sd=pf", "subreddit_subscribers": 847615, "created_utc": 1676307109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am plotting my result, and the surface plot look really off. I know I am plotting right just its shape is not the best for clearness in 3D. Should I rotate or something or just use a heatmap instead of a surface plot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "name": "t3_11188bo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_70mnmect", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kex9KJDZoklIwlieJpu0bii6k_Jz0vJrrSlqScJ07QQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "labrats", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8\n\nI used another dataset for similar shape so that my PI wont kill me", "author_fullname": "t2_70mnmect", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am plotting my result, and the surface plot look really off. I know I am plotting right just its shape is not the best for clearness in 3D. Should I rotate or something or just use a heatmap instead of a surface plot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/labrats", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0in8x9m7jyha1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d95166fa29b40fe643aa853bc1fa0abfa561165"}, {"y": 161, "x": 216, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fe0b4b6cd770c5d458aa872c92d5375a6b10447"}, {"y": 239, "x": 320, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c159efdd5754aaa58d36b9f9bde00a7be5eaab7"}, {"y": 479, "x": 640, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e9715f18e6d2e63f27b9a61eb3a1fd4f18865e3"}, {"y": 719, "x": 960, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eeb73c76ef5e6f72f1e9b5ac092bea4880c96e0a"}, {"y": 809, "x": 1080, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d54e1c5906168529d31e4c7e8a4bba13f4c4e6f3"}], "s": {"y": 834, "x": 1113, "u": "https://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8"}, "id": "0in8x9m7jyha1"}}, "name": "t3_111864p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "36fb6490-02ad-11e1-bcca-12313d192a33", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kex9KJDZoklIwlieJpu0bii6k_Jz0vJrrSlqScJ07QQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676294700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.labrats", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8\"&gt;https://preview.redd.it/0in8x9m7jyha1.png?width=1113&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e50fe556f69570e5ea2c50973d440584fc6a6a8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I used another dataset for similar shape so that my PI wont kill me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "labrat that is in control group ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2srnv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111864p", "is_robot_indexable": true, "report_reasons": null, "author": "Striking-Warning9533", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "subreddit_subscribers": 604941, "created_utc": 1676294700.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1676294885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.labrats", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "11188bo", "is_robot_indexable": true, "report_reasons": null, "author": "Striking-Warning9533", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_111864p", "author_flair_text_color": null, "permalink": "/r/datascience/comments/11188bo/i_am_plotting_my_result_and_the_surface_plot_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/labrats/comments/111864p/i_am_plotting_my_result_and_the_surface_plot_look/", "subreddit_subscribers": 847615, "created_utc": 1676294885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nI am working with a binary classification dataset with many features, and not much info about them. I made an extra trees classifier with an F1 of 0.99. I believe the data originated from a synthetic source, so I\u2019ll take that 0.99 as viable. \n\nAnyway, the many features give the most important ones about a 0.02 influence by itself. My question is, how can I see how these features may work together to influence the outcome? For example , perhaps features 3, 21, 78 create a 0.15 impact together. \n\nAny insight is appreciated!", "author_fullname": "t2_2f2zz72s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Model interpretation with many features", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111884j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676294869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a binary classification dataset with many features, and not much info about them. I made an extra trees classifier with an F1 of 0.99. I believe the data originated from a synthetic source, so I\u2019ll take that 0.99 as viable. &lt;/p&gt;\n\n&lt;p&gt;Anyway, the many features give the most important ones about a 0.02 influence by itself. My question is, how can I see how these features may work together to influence the outcome? For example , perhaps features 3, 21, 78 create a 0.15 impact together. &lt;/p&gt;\n\n&lt;p&gt;Any insight is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111884j", "is_robot_indexable": true, "report_reasons": null, "author": "anon67543", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111884j/model_interpretation_with_many_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111884j/model_interpretation_with_many_features/", "subreddit_subscribers": 847615, "created_utc": 1676294869.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Network Science Related] How much percentage of the actual network nodes (and its corresponding links) need to be in the data in order to \"properly\" describe the network?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111qxiw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676340373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111qxiw", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111qxiw/network_science_related_how_much_percentage_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111qxiw/network_science_related_how_much_percentage_of/", "subreddit_subscribers": 847615, "created_utc": 1676340373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_9fjoqhty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning from Machine Learning | Maarten Grootendorst: BERTopic, Data Science, Psychology", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_111qiz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pZRnHxmbgEfUBFtjXKz7mV-b92W4pedM5bfWbvpv_Ec.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676339210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/learning-from-machine-learning-maarten-grootendorst-bertopic-data-science-psychology-9ed9b9b2921", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?auto=webp&amp;v=enabled&amp;s=2af3de0a635567916d800533a55cd697c2e49709", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac44d9743c4c3701ddcba20ed12007d32035b414", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef2dc42af09e18b3c440be824a655fe8b8e8b76f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbde90fcc329d0cfbded61f4c62f7ec86a70fdc3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a9ccf33aaa4b63f4c23022aa7b056517e3b9c8f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8430bc16a5c60ccbee8a158f1e13c87576300a3f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/yCA78lSuJjzSJ8YW7E6FfgT3vxyShTpjN4VUUKHSh2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2419d66fe039ef54b334eecb4b3c7721b766165", "width": 1080, "height": 607}], "variants": {}, "id": "Px4aCnL6kflFq7V1b0qLJPfpjZKoXMS83ENgNHOLdIA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "111qiz7", "is_robot_indexable": true, "report_reasons": null, "author": "NLPnerd", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111qiz7/learning_from_machine_learning_maarten/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/learning-from-machine-learning-maarten-grootendorst-bertopic-data-science-psychology-9ed9b9b2921", "subreddit_subscribers": 847615, "created_utc": 1676339210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Dear Data Scientists, \n\nToday I got the amazing news that I am accepted for the MSc in Data Science. \nAs someone with a Bachelors in Finance (also included Stats,Maths and Programming electives). I wanted to know would you recommend being data scientist in the banking or finance sector? (With this I also mean are the working hours decent and do you have a social life, and are you allowed to WFH)\n\nThank you in advance for your answers\n\nKind Regards, \n\nA Future Data Scientist", "author_fullname": "t2_53y185cuu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Scientists in the Banking/Finance sector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111inqf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676319055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Scientists, &lt;/p&gt;\n\n&lt;p&gt;Today I got the amazing news that I am accepted for the MSc in Data Science. \nAs someone with a Bachelors in Finance (also included Stats,Maths and Programming electives). I wanted to know would you recommend being data scientist in the banking or finance sector? (With this I also mean are the working hours decent and do you have a social life, and are you allowed to WFH)&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your answers&lt;/p&gt;\n\n&lt;p&gt;Kind Regards, &lt;/p&gt;\n\n&lt;p&gt;A Future Data Scientist&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111inqf", "is_robot_indexable": true, "report_reasons": null, "author": "NotGettingFazed", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111inqf/data_scientists_in_the_bankingfinance_sector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111inqf/data_scientists_in_the_bankingfinance_sector/", "subreddit_subscribers": 847615, "created_utc": 1676319055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_uzoqx6k0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to load and use a pytorch model (.pth) without knowing the model class?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111gk2x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676313861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "111gk2x", "is_robot_indexable": true, "report_reasons": null, "author": "Haritha37", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/111gk2x/is_it_possible_to_load_and_use_a_pytorch_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/111gk2x/is_it_possible_to_load_and_use_a_pytorch_model/", "subreddit_subscribers": 847615, "created_utc": 1676313861.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}