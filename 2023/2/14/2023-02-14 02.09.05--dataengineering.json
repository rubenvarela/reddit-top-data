{"kind": "Listing", "data": {"after": "t3_111btrw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nCurrently my org is making a strategic move from a onprem solution to the Azure Public Cloud.\n\nThe current onprem datawarehouse has a few limited usecases and can be identified as small. Its based on SQL SP + SSIS, with views on top of the datamart which joins all the needed data together for PowerBI.\n\nThe datawarehouse will probaly grow in the longer term to more different kind of usecases then only reporting. Think about statistiscal models in Python &amp; scenario simulations &amp; other analysis not known yet.\n\nIn regards of sheer datasize its all really limited( Max  5 mil records a day) Types of Datafeeds are API\"s or SFTP's.\n\n So what is a good dataplatform setup for these kind of usecases in the Azure Cloud? Powerbi is the chosen reporting tool already.\n\n1. Snowflake: Not an option due to regulatory compliance\n2: ADL gen 2 + Databricks: maybe overkill for the given amount of data? But python and R can be excuted on the same dataset as our reports which is a + in terms of auditablility and one dataplatform for the organisation.\n3. Synapse: No clue, same overkill?\n4. Azure SQL server + ADF: is this futute proof for also more potential advanced usecases?\n\nI would like to have 1 generic envoirement where we can implement all our dataproducts and make then available for the organisation.\n\nP.s. there is no technical knowledge currently avalaible in the org. So in terms of choosing the stack it doesnt matter", "author_fullname": "t2_mv3d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving OnPrem to Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1112tdq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676274940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Currently my org is making a strategic move from a onprem solution to the Azure Public Cloud.&lt;/p&gt;\n\n&lt;p&gt;The current onprem datawarehouse has a few limited usecases and can be identified as small. Its based on SQL SP + SSIS, with views on top of the datamart which joins all the needed data together for PowerBI.&lt;/p&gt;\n\n&lt;p&gt;The datawarehouse will probaly grow in the longer term to more different kind of usecases then only reporting. Think about statistiscal models in Python &amp;amp; scenario simulations &amp;amp; other analysis not known yet.&lt;/p&gt;\n\n&lt;p&gt;In regards of sheer datasize its all really limited( Max  5 mil records a day) Types of Datafeeds are API&amp;quot;s or SFTP&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;So what is a good dataplatform setup for these kind of usecases in the Azure Cloud? Powerbi is the chosen reporting tool already.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Snowflake: Not an option due to regulatory compliance\n2: ADL gen 2 + Databricks: maybe overkill for the given amount of data? But python and R can be excuted on the same dataset as our reports which is a + in terms of auditablility and one dataplatform for the organisation.&lt;/li&gt;\n&lt;li&gt;Synapse: No clue, same overkill?&lt;/li&gt;\n&lt;li&gt;Azure SQL server + ADF: is this futute proof for also more potential advanced usecases?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would like to have 1 generic envoirement where we can implement all our dataproducts and make then available for the organisation.&lt;/p&gt;\n\n&lt;p&gt;P.s. there is no technical knowledge currently avalaible in the org. So in terms of choosing the stack it doesnt matter&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1112tdq", "is_robot_indexable": true, "report_reasons": null, "author": "artopaper", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1112tdq/moving_onprem_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1112tdq/moving_onprem_to_cloud/", "subreddit_subscribers": 89504, "created_utc": 1676274940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am a junior data engineer has been working for about half a year now and I would like to work on some interesting open-source project in my spare time. Partly so I can develop faster as well as just do something for fun. I mainly work with Python, docker and Linux. \n\nMaybe one of you is active in some such project ? I would be happy to help \n\nGreetings", "author_fullname": "t2_uqgrebp4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an open-source project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ha7n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676315633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am a junior data engineer has been working for about half a year now and I would like to work on some interesting open-source project in my spare time. Partly so I can develop faster as well as just do something for fun. I mainly work with Python, docker and Linux. &lt;/p&gt;\n\n&lt;p&gt;Maybe one of you is active in some such project ? I would be happy to help &lt;/p&gt;\n\n&lt;p&gt;Greetings&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "111ha7n", "is_robot_indexable": true, "report_reasons": null, "author": "Scyzoryk881", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111ha7n/looking_for_an_opensource_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111ha7n/looking_for_an_opensource_project/", "subreddit_subscribers": 89504, "created_utc": 1676315633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen them floating around the industry. They strike me as a slightly modern Palantir/IBM consultant oriented shop. But Google is partnering with them to use their GPT-3 competitor in their products so I was curious what their products even are/do.\n\nAnyone have experience with it?", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use any c3.ai products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1111vos", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676271268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen them floating around the industry. They strike me as a slightly modern Palantir/IBM consultant oriented shop. But Google is partnering with them to use their GPT-3 competitor in their products so I was curious what their products even are/do.&lt;/p&gt;\n\n&lt;p&gt;Anyone have experience with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1111vos", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1111vos/anyone_use_any_c3ai_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1111vos/anyone_use_any_c3ai_products/", "subreddit_subscribers": 89504, "created_utc": 1676271268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have experience with or have heard about specific data engineering certificates/ courses, could you share? Which one is the best in terms of preparing you and landing a job?\n\nI\u2019m aware of Udacity nano degree, MIT xPro,  and Coursera IBM professional certificates.\n\nWould love input, especially for programs tailored more at the beginner level to start.", "author_fullname": "t2_bqvtlw0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing online certs and courses- which one is the best?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111j3kb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676320195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have experience with or have heard about specific data engineering certificates/ courses, could you share? Which one is the best in terms of preparing you and landing a job?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m aware of Udacity nano degree, MIT xPro,  and Coursera IBM professional certificates.&lt;/p&gt;\n\n&lt;p&gt;Would love input, especially for programs tailored more at the beginner level to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111j3kb", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy_Pop_6966", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111j3kb/comparing_online_certs_and_courses_which_one_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111j3kb/comparing_online_certs_and_courses_which_one_is/", "subreddit_subscribers": 89504, "created_utc": 1676320195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I come from a DE batch project where Events trigger Lambdas, which trigger Step Functions to run all the batch pipelines. I was thinking how to replace Step Functions with Apache Airflow, so I was looking for an example architecture which combines events and Lambda functions to trigger Airflow pipelines. Do you have any reference which I could find useful? Many thanks", "author_fullname": "t2_v3ifff74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(AWS) Events, lambda and Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1115ab2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676284660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I come from a DE batch project where Events trigger Lambdas, which trigger Step Functions to run all the batch pipelines. I was thinking how to replace Step Functions with Apache Airflow, so I was looking for an example architecture which combines events and Lambda functions to trigger Airflow pipelines. Do you have any reference which I could find useful? Many thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1115ab2", "is_robot_indexable": true, "report_reasons": null, "author": "kaismd", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1115ab2/aws_events_lambda_and_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1115ab2/aws_events_lambda_and_airflow/", "subreddit_subscribers": 89504, "created_utc": 1676284660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[GeoSpatial Analysis Using GeoPandas In Python](https://preview.redd.it/1gryl6sc8zha1.jpg?width=334&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4ab2a6e45dc5c29b8d26b3d414d1c9e5dc9d3390)\n\n[GeoSpatial Analysis Using GeoPandas In Python](https://spatial-dev.guru/2023/02/05/geospatial-analysis-using-geopandas-in-python/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GeoSpatial Analysis Using GeoPandas In Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1gryl6sc8zha1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/1gryl6sc8zha1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21fc9f983962fe0bdab3349db1eaa52b7aa3f71a"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/1gryl6sc8zha1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aaa4437dd3694e6b4978ba205b5e4ffba8283635"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/1gryl6sc8zha1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f0e047ee2725c279ca99d8678bbc4eb5e6304e3"}], "s": {"y": 692, "x": 334, "u": "https://preview.redd.it/1gryl6sc8zha1.jpg?width=334&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4ab2a6e45dc5c29b8d26b3d414d1c9e5dc9d3390"}, "id": "1gryl6sc8zha1"}}, "name": "t3_111c8wl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Wcr2xuzP5RtAv0uH2P0-jD5kaFDnhB3qSNHhvxU9aq4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676303150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1gryl6sc8zha1.jpg?width=334&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4ab2a6e45dc5c29b8d26b3d414d1c9e5dc9d3390\"&gt;GeoSpatial Analysis Using GeoPandas In Python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/02/05/geospatial-analysis-using-geopandas-in-python/\"&gt;GeoSpatial Analysis Using GeoPandas In Python&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "111c8wl", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111c8wl/geospatial_analysis_using_geopandas_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111c8wl/geospatial_analysis_using_geopandas_in_python/", "subreddit_subscribers": 89504, "created_utc": 1676303150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is your opinion about following data architecture?\n\n1. Raw JSON/CSV data extracted everyday using Python apps\n1b. Source data from databases extracted everyday using python apps\n2. Raw data stored in S3\n\n2b. Data from database sources goes straight into the next (3) step\n\n3. Python apps (polars/duckdb/?) performing transformations/cleansing on the raw data and saving into the next \u201cstage\u201d of S3 using parquet file format\n\n4. Parquet files registered as Hive tables using Trino\n\n5. Aggregations/joins etc. jobs run as SQL scripts using Trino saved as \u201cdatabase\u201d tables (again in the Hive catalog) in Iceberg table format\n\n6. End users (and apps like Metabase and others) access the \u201cparquet\u201d and \u201cIceberg\u201d tables using Trino\n\nAll jobs orchestrated by Airflow. \n\nQuestions:\n\na. I guess it does not make sense to extract data from databases and dump them into files such as csv and then take these files and convert into parquet - thats why I want to grab db data and turn in directly into parquet. Is that good approach?\n\nb. Should I transform ALL tables into the Iceberg, even if some of them would be totally the same? Or leave \u201cfinal\u201d tables as they are and create only \u201caggregate\u201d tables as new (Iceberg) layer? Somehow it is more appealing to me to convert ALL the tables into Iceberg so it is all stored in the same way. Also reading of such data should be faster than reading pure parquet I think. \n\nc. Which stage should include data validations (check for missings, deduplicate etc.)? I guess step 3(?)\n\nd. Do I miss some important component? Maybe something like semantical layer on top of that?\n\ne. Do you see any pitfalls? Or do you have any recommendations what (not) to do?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture opinion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ioyr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676319450.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676319144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is your opinion about following data architecture?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Raw JSON/CSV data extracted everyday using Python apps\n1b. Source data from databases extracted everyday using python apps&lt;/li&gt;\n&lt;li&gt;Raw data stored in S3&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;2b. Data from database sources goes straight into the next (3) step&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Python apps (polars/duckdb/?) performing transformations/cleansing on the raw data and saving into the next \u201cstage\u201d of S3 using parquet file format&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Parquet files registered as Hive tables using Trino&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Aggregations/joins etc. jobs run as SQL scripts using Trino saved as \u201cdatabase\u201d tables (again in the Hive catalog) in Iceberg table format&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;End users (and apps like Metabase and others) access the \u201cparquet\u201d and \u201cIceberg\u201d tables using Trino&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;All jobs orchestrated by Airflow. &lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;p&gt;a. I guess it does not make sense to extract data from databases and dump them into files such as csv and then take these files and convert into parquet - thats why I want to grab db data and turn in directly into parquet. Is that good approach?&lt;/p&gt;\n\n&lt;p&gt;b. Should I transform ALL tables into the Iceberg, even if some of them would be totally the same? Or leave \u201cfinal\u201d tables as they are and create only \u201caggregate\u201d tables as new (Iceberg) layer? Somehow it is more appealing to me to convert ALL the tables into Iceberg so it is all stored in the same way. Also reading of such data should be faster than reading pure parquet I think. &lt;/p&gt;\n\n&lt;p&gt;c. Which stage should include data validations (check for missings, deduplicate etc.)? I guess step 3(?)&lt;/p&gt;\n\n&lt;p&gt;d. Do I miss some important component? Maybe something like semantical layer on top of that?&lt;/p&gt;\n\n&lt;p&gt;e. Do you see any pitfalls? Or do you have any recommendations what (not) to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111ioyr", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111ioyr/data_architecture_opinion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111ioyr/data_architecture_opinion/", "subreddit_subscribers": 89504, "created_utc": 1676319144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have a DW running on On Prem MS SQL. It's a very small setup, around 50 tables, less than 10 sources and no more than 15GB of data at the moment. I'm extracting with python and some IaaS. I load the data in source db, transform in stage db and then load to prod db.\n\nThe DW is still in development, mostly because there is a complete lack of governance when it comes to business process best practices (Lost battle for now, don't even get me started). So once in a while, someone would manage to get out of their way to break some business logic. When this happens, I would just manually fix the affected rows if they are just a few, or just rebuild the table if they are many. And of course I would change my code to include this new case.\n\nThe question is, how can I logically roll back to the previous batch?  I couldn't find anything online.\n\nI hope this made sense.", "author_fullname": "t2_83poggkq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse roll back table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1114jeb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676281807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have a DW running on On Prem MS SQL. It&amp;#39;s a very small setup, around 50 tables, less than 10 sources and no more than 15GB of data at the moment. I&amp;#39;m extracting with python and some IaaS. I load the data in source db, transform in stage db and then load to prod db.&lt;/p&gt;\n\n&lt;p&gt;The DW is still in development, mostly because there is a complete lack of governance when it comes to business process best practices (Lost battle for now, don&amp;#39;t even get me started). So once in a while, someone would manage to get out of their way to break some business logic. When this happens, I would just manually fix the affected rows if they are just a few, or just rebuild the table if they are many. And of course I would change my code to include this new case.&lt;/p&gt;\n\n&lt;p&gt;The question is, how can I logically roll back to the previous batch?  I couldn&amp;#39;t find anything online.&lt;/p&gt;\n\n&lt;p&gt;I hope this made sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1114jeb", "is_robot_indexable": true, "report_reasons": null, "author": "Ancient-Entry-6436", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1114jeb/data_warehouse_roll_back_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1114jeb/data_warehouse_roll_back_table/", "subreddit_subscribers": 89504, "created_utc": 1676281807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m pretty new to this, so I apologise if I\u2019m not asking the question in the right way.  But say I have a Parquet table, and in storage, it\u2019s saved in 3 parts. Next, I have a Databricks compute with autoscaling that is reading data from the table. For one read activity on that single table from Databricks, will that count as 1 read, or would it be 3 reads (one for each parquet part file)? Also, since Databricks uses distributed compute, could there be multiple caller IPs for one read activity, or does it count as one based on driver node? Does this mean, if Databricks was scaled to 3 workers, (plus a driver), one read on one table could count as 3 part files x 4 compute = 12 read activities?", "author_fullname": "t2_59ggcmrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Log Analytics count read activities to Parquet files in ADLSg2 from distributed compute, such as Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11118td", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676268880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m pretty new to this, so I apologise if I\u2019m not asking the question in the right way.  But say I have a Parquet table, and in storage, it\u2019s saved in 3 parts. Next, I have a Databricks compute with autoscaling that is reading data from the table. For one read activity on that single table from Databricks, will that count as 1 read, or would it be 3 reads (one for each parquet part file)? Also, since Databricks uses distributed compute, could there be multiple caller IPs for one read activity, or does it count as one based on driver node? Does this mean, if Databricks was scaled to 3 workers, (plus a driver), one read on one table could count as 3 part files x 4 compute = 12 read activities?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11118td", "is_robot_indexable": true, "report_reasons": null, "author": "jinbe-san", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11118td/how_does_log_analytics_count_read_activities_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11118td/how_does_log_analytics_count_read_activities_to/", "subreddit_subscribers": 89504, "created_utc": 1676268880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to learn more about lakehouse design. We don\u2019t use it at work, and I have an 11 month old baby at home so not too much time for self study. \n\nThis question might be an easy answer\u2026\n\nI understand the three lakehouse approaches can hold semi-structured data, and also store data in parquet files. My understanding is that parquet, as a column format file needs structured data.\n\nHow exactly are json or images stored within a lakehouse? Is it in a variant type column such as is the case with Snowflake, or is is just saved down raw with a pointer to its location and metadata about it?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unstructured/Semi-structured data in Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111j1me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676320055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to learn more about lakehouse design. We don\u2019t use it at work, and I have an 11 month old baby at home so not too much time for self study. &lt;/p&gt;\n\n&lt;p&gt;This question might be an easy answer\u2026&lt;/p&gt;\n\n&lt;p&gt;I understand the three lakehouse approaches can hold semi-structured data, and also store data in parquet files. My understanding is that parquet, as a column format file needs structured data.&lt;/p&gt;\n\n&lt;p&gt;How exactly are json or images stored within a lakehouse? Is it in a variant type column such as is the case with Snowflake, or is is just saved down raw with a pointer to its location and metadata about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111j1me", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111j1me/unstructuredsemistructured_data_in_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111j1me/unstructuredsemistructured_data_in_lakehouse/", "subreddit_subscribers": 89504, "created_utc": 1676320055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does Trino handle incremental updates of the table when used with S3 as a storage layer?\n\nMy scenario is that I have S3 bucket with some tables stored as parquet files. Every day I would get raw JSON data from source, run ETL pipeline and save new data into parquet adding it to the S3 bucket. \n\nDoes Trino automatically load this new data or I have to somehow manually update it?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino/S3 - incremental loads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111i3ld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676317680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does Trino handle incremental updates of the table when used with S3 as a storage layer?&lt;/p&gt;\n\n&lt;p&gt;My scenario is that I have S3 bucket with some tables stored as parquet files. Every day I would get raw JSON data from source, run ETL pipeline and save new data into parquet adding it to the S3 bucket. &lt;/p&gt;\n\n&lt;p&gt;Does Trino automatically load this new data or I have to somehow manually update it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111i3ld", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111i3ld/trinos3_incremental_loads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111i3ld/trinos3_incremental_loads/", "subreddit_subscribers": 89504, "created_utc": 1676317680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a very security-minded org that has very poor environment parity across all of our source systems.  We have very sparse/non-existent dev data and I'm looking for some solutions and wanted to see if this community has any tooling recommendations to help.\n\nThe requirement is to build out synthetic data with very similar size and shape to our production data and ideally have a framework to do this, maybe at some level use our data and \"de-productionize\" it.  [gretel.ai](https://gretel.ai) looks like it may be a fit, but from what I see I need to upload production data to their environment and that's a no-go.\n\n&amp;#x200B;\n\nHas anyone come across a similar problem and what are some solutions?\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_3q5hfpq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generating Synthetic Data from Prod systems into Dev/UAT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111djgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676306331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a very security-minded org that has very poor environment parity across all of our source systems.  We have very sparse/non-existent dev data and I&amp;#39;m looking for some solutions and wanted to see if this community has any tooling recommendations to help.&lt;/p&gt;\n\n&lt;p&gt;The requirement is to build out synthetic data with very similar size and shape to our production data and ideally have a framework to do this, maybe at some level use our data and &amp;quot;de-productionize&amp;quot; it.  &lt;a href=\"https://gretel.ai\"&gt;gretel.ai&lt;/a&gt; looks like it may be a fit, but from what I see I need to upload production data to their environment and that&amp;#39;s a no-go.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across a similar problem and what are some solutions?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?auto=webp&amp;v=enabled&amp;s=2ba92a939a095501fb155c357863e8a0c95ca82b", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4f34c328c74a03183c562084539022e0e4db596", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cce563ca19a1be89ed5a347ea1b639594e3bded4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8588b7410b33d54a1d311f88be9432dbe4c23f34", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5efcd2d79f439ba042ed5abeaf109eb20084d6d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed601ca17686ef686d064a42f9a105f384094833", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Fabxw_Az9CPdZ2ialGX1IUMozM8LSYUqVHUDf2Ln2aA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7d17ee8c3bb78f99e891c45ad65d95cd31d8450", "width": 1080, "height": 567}], "variants": {}, "id": "o0qqAq5goPMeGx3o_KWtBt4Uz4NUKAsNYfk7pyjJwGM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111djgk", "is_robot_indexable": true, "report_reasons": null, "author": "timewarp80", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111djgk/generating_synthetic_data_from_prod_systems_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111djgk/generating_synthetic_data_from_prod_systems_into/", "subreddit_subscribers": 89504, "created_utc": 1676306331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/1118668)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which big data file formats do you query in your data lake / lakehouse for most of your analytical workloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1118668", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676294704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1118668\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1118668", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1676726704169, "options": [{"text": "primarily Parquet", "id": "21578485"}, {"text": "primarily ORC", "id": "21578486"}, {"text": "primarily AVRO", "id": "21578487"}, {"text": "primarily some other format", "id": "21578488"}, {"text": "we frequently query more than one format", "id": "21578489"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 252, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1118668/which_big_data_file_formats_do_you_query_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1118668/which_big_data_file_formats_do_you_query_in_your/", "subreddit_subscribers": 89504, "created_utc": 1676294704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to come up with a data model where customer can have multiple subscriptions and  subscription is just a type of many other products, so when customer buys  subscription then product with type subscription is being added to the  order and order has a payment fk. When payment is successful then  subscription is added to the subscription table with (user fk, order fk,  start date, end date, price, is\\_active).\n\nBut how to approach subscription recurring payment?\n\n1. Since  subscription is a product that is part of the order, should new ordered  and new payment be created every time when subscription recurring payment happens?\n2. Or  should subscription have 1:m relationship with payment tbl, and only  new payment should be created on the subscription renewal? but this  approach seem little weird since initial subscription purchase payment  was in the order table.\n\nHow to go about it?", "author_fullname": "t2_glxz8l6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you model multiple subscriptions if subscription is a type of a product?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111171s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676268685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to come up with a data model where customer can have multiple subscriptions and  subscription is just a type of many other products, so when customer buys  subscription then product with type subscription is being added to the  order and order has a payment fk. When payment is successful then  subscription is added to the subscription table with (user fk, order fk,  start date, end date, price, is_active).&lt;/p&gt;\n\n&lt;p&gt;But how to approach subscription recurring payment?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Since  subscription is a product that is part of the order, should new ordered  and new payment be created every time when subscription recurring payment happens?&lt;/li&gt;\n&lt;li&gt;Or  should subscription have 1:m relationship with payment tbl, and only  new payment should be created on the subscription renewal? but this  approach seem little weird since initial subscription purchase payment  was in the order table.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How to go about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111171s", "is_robot_indexable": true, "report_reasons": null, "author": "No-Race8789", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111171s/how_do_you_model_multiple_subscriptions_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111171s/how_do_you_model_multiple_subscriptions_if/", "subreddit_subscribers": 89504, "created_utc": 1676268685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, for people that use spark/pyspark to build ETLs, do you always go for spark even when it's not for a huge load (example: csv with 50k rows, small db tables ..) ?\nI used to work with sqlalchemy/pandas when the data is manageable without spark, but nowadays spark got so ahead and clean, it just feels simpler to use it instead of the regular sqlalchemy/pandas.\n\nWhat you guys think ? Do you use spark for your low load pipelines too ?", "author_fullname": "t2_9k40zzr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is spark always your go to solution ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111ijh9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676318757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, for people that use spark/pyspark to build ETLs, do you always go for spark even when it&amp;#39;s not for a huge load (example: csv with 50k rows, small db tables ..) ?\nI used to work with sqlalchemy/pandas when the data is manageable without spark, but nowadays spark got so ahead and clean, it just feels simpler to use it instead of the regular sqlalchemy/pandas.&lt;/p&gt;\n\n&lt;p&gt;What you guys think ? Do you use spark for your low load pipelines too ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111ijh9", "is_robot_indexable": true, "report_reasons": null, "author": "mohaidoss", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111ijh9/is_spark_always_your_go_to_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111ijh9/is_spark_always_your_go_to_solution/", "subreddit_subscribers": 89504, "created_utc": 1676318757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to create a python ETL pipelines transforming raw data (JSONs, CSV etc.) to parquet. \n\nI guess the most obvious approach is to use pandas or pyspark. \n\nWhat are your approaches? Are there any better alternatives? Or does it even make sense to look for alternatives? Or is it just enough to use pandas/polars and forget about other stuff?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python library for storing parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111i04f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676317433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to create a python ETL pipelines transforming raw data (JSONs, CSV etc.) to parquet. &lt;/p&gt;\n\n&lt;p&gt;I guess the most obvious approach is to use pandas or pyspark. &lt;/p&gt;\n\n&lt;p&gt;What are your approaches? Are there any better alternatives? Or does it even make sense to look for alternatives? Or is it just enough to use pandas/polars and forget about other stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111i04f", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111i04f/python_library_for_storing_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111i04f/python_library_for_storing_parquet/", "subreddit_subscribers": 89504, "created_utc": 1676317433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, can someone help me with the meaning of \u201cucd\u201d in the DataFrame class in the vaex 4.16.0.\n\nNot very well versed in data engineering but couldn\u2019t find anything with google searches.", "author_fullname": "t2_306r5wtz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vaex documentation help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111hz5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676317363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, can someone help me with the meaning of \u201cucd\u201d in the DataFrame class in the vaex 4.16.0.&lt;/p&gt;\n\n&lt;p&gt;Not very well versed in data engineering but couldn\u2019t find anything with google searches.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111hz5q", "is_robot_indexable": true, "report_reasons": null, "author": "AbbuBumPhodo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111hz5q/vaex_documentation_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111hz5q/vaex_documentation_help/", "subreddit_subscribers": 89504, "created_utc": 1676317363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company we use jdbcoperator to transform and upload data.\nHowever it would be very useful to see in the logs information like \u201cupdated rows #\u201d after merge or \u201c# of rows\u201d for newly created tables (in particular temporary tables).\n\nI know I can extract those info through a query and output on the logs but is there a more Airflowic way to do so?", "author_fullname": "t2_e4qv08k8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow JdbcOperator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11169ft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676288474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company we use jdbcoperator to transform and upload data.\nHowever it would be very useful to see in the logs information like \u201cupdated rows #\u201d after merge or \u201c# of rows\u201d for newly created tables (in particular temporary tables).&lt;/p&gt;\n\n&lt;p&gt;I know I can extract those info through a query and output on the logs but is there a more Airflowic way to do so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11169ft", "is_robot_indexable": true, "report_reasons": null, "author": "Busy_Elderberry8650", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11169ft/airflow_jdbcoperator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11169ft/airflow_jdbcoperator/", "subreddit_subscribers": 89504, "created_utc": 1676288474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi - am looking at ingesting data to s3 from tools like Salesforce.  \n\nI was looking at Airbyte, and then also saw AppFlow.  Any thoughts on tradeoffs?  I was keen on also trying dbt, how do you do that with AppFlow?", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AppFlow vs Airbyte?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1115zay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676287394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi - am looking at ingesting data to s3 from tools like Salesforce.  &lt;/p&gt;\n\n&lt;p&gt;I was looking at Airbyte, and then also saw AppFlow.  Any thoughts on tradeoffs?  I was keen on also trying dbt, how do you do that with AppFlow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1115zay", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1115zay/appflow_vs_airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1115zay/appflow_vs_airbyte/", "subreddit_subscribers": 89504, "created_utc": 1676287394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " So, as title, I have to create an ETL from a Server that overwrites data twice a day. The only solution I have for now to be sure of not losing any data is to run the ETL twice for every overwriting, so 4 times a day. How would you approach such a case?", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with data that gets overwrited every time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1115wp6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676287122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, as title, I have to create an ETL from a Server that overwrites data twice a day. The only solution I have for now to be sure of not losing any data is to run the ETL twice for every overwriting, so 4 times a day. How would you approach such a case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1115wp6", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1115wp6/how_to_deal_with_data_that_gets_overwrited_every/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1115wp6/how_to_deal_with_data_that_gets_overwrited_every/", "subreddit_subscribers": 89504, "created_utc": 1676287122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, Is it a good practice to have side effects in the delta live table pipeline? I would like to write data into Postgres DB as a last step. This will be continuously running DLT\n\nSomething like this\n\n    @dlt.table\n    def table():\n        df = read stream data\n        df.save -- to db\n        return df", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to write a database (RDS) from within delta live table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_111opk0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676334132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, Is it a good practice to have side effects in the delta live table pipeline? I would like to write data into Postgres DB as a last step. This will be continuously running DLT&lt;/p&gt;\n\n&lt;p&gt;Something like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@dlt.table\ndef table():\n    df = read stream data\n    df.save -- to db\n    return df\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111opk0", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111opk0/is_it_possible_to_write_a_database_rds_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111opk0/is_it_possible_to_write_a_database_rds_from/", "subreddit_subscribers": 89504, "created_utc": 1676334132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How time consuming is building this in-house vs. using a third party consumption based pricing solution? Do you think the legacy SaaS subscription/pricing automation vendors can easily get into doing the same for consumption based pricing, or is the technology inherently different?\n\n&amp;#x200B;\n\nOr if any of you have used the outsourced vendors, how has your experience been? Do you feel you're just cutting costs or is there incremental revenue capture because of the ability to customize how you price your solutions?", "author_fullname": "t2_rcx1cqyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have any of you worked on the infrastructure for building usage based / consumption based pricing systems? How does this work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111jwmv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676322555.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676322151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How time consuming is building this in-house vs. using a third party consumption based pricing solution? Do you think the legacy SaaS subscription/pricing automation vendors can easily get into doing the same for consumption based pricing, or is the technology inherently different?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or if any of you have used the outsourced vendors, how has your experience been? Do you feel you&amp;#39;re just cutting costs or is there incremental revenue capture because of the ability to customize how you price your solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "111jwmv", "is_robot_indexable": true, "report_reasons": null, "author": "----bubba----", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111jwmv/have_any_of_you_worked_on_the_infrastructure_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111jwmv/have_any_of_you_worked_on_the_infrastructure_for/", "subreddit_subscribers": 89504, "created_utc": 1676322151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen a few presentations (such as [this](https://www.youtube.com/watch?v=LB8WV94JQTY)) saying that they are dynamically adding new rules without restarting and redeploying the Flink job/ app? I cannot find more resources on this. Is there anywhere I can look at, maybe some examples of good documentation on this?", "author_fullname": "t2_77bjhopm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink CEP - dynamically add new rules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111h6xm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676315409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a few presentations (such as &lt;a href=\"https://www.youtube.com/watch?v=LB8WV94JQTY\"&gt;this&lt;/a&gt;) saying that they are dynamically adding new rules without restarting and redeploying the Flink job/ app? I cannot find more resources on this. Is there anywhere I can look at, maybe some examples of good documentation on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CLW4Wza2w5OktWJqnlRLHT2qedJXvDP3psZROXBof78.jpg?auto=webp&amp;v=enabled&amp;s=591ab4f15cb6367350d7f966ef1e1e8c1c467701", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/CLW4Wza2w5OktWJqnlRLHT2qedJXvDP3psZROXBof78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acb0036e36fcd0c5e2335aa763d72addde99857a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/CLW4Wza2w5OktWJqnlRLHT2qedJXvDP3psZROXBof78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a475fd8195067d66307f6da0027ab164ae14a0b9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/CLW4Wza2w5OktWJqnlRLHT2qedJXvDP3psZROXBof78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed94ffb962dc4d5b3febc43d8c7f379f3e786dbf", "width": 320, "height": 240}], "variants": {}, "id": "I56cJpbHReIG8gFEifHgj9uFvJne0vAig7MVcWEpJvI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111h6xm", "is_robot_indexable": true, "report_reasons": null, "author": "Overall-Pen-4797", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111h6xm/flink_cep_dynamically_add_new_rules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111h6xm/flink_cep_dynamically_add_new_rules/", "subreddit_subscribers": 89504, "created_utc": 1676315409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1bnhotlu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 118, "top_awarded_type": null, "hide_score": false, "name": "t3_111cvow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/53iBsor-oCzGVT3z-ZeCYV_HUqbP5gTxrnUzkNfWHCU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676304692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kanger.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kanger.dev/data-engineering-skills/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4CW8qHkdLuH2At66g-OGyUfAf7CPGBCbrB1h9CGRXWY.jpg?auto=webp&amp;v=enabled&amp;s=39b3c642d8baa96e804371f84d46c64bbc43d0f5", "width": 1024, "height": 870}, "resolutions": [{"url": "https://external-preview.redd.it/4CW8qHkdLuH2At66g-OGyUfAf7CPGBCbrB1h9CGRXWY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36556b16af3c8c71596eef0273de1be6cf0ab0fa", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/4CW8qHkdLuH2At66g-OGyUfAf7CPGBCbrB1h9CGRXWY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b379b8cb4d088556a744dfd45c4ed2379dc29de6", "width": 216, "height": 183}, {"url": "https://external-preview.redd.it/4CW8qHkdLuH2At66g-OGyUfAf7CPGBCbrB1h9CGRXWY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d367d29b2011774284236d9221ae74f41db0303", "width": 320, "height": 271}, {"url": "https://external-preview.redd.it/4CW8qHkdLuH2At66g-OGyUfAf7CPGBCbrB1h9CGRXWY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87cab2269fda0e71af3f30aa5ab61ee134612728", "width": 640, "height": 543}, {"url": "https://external-preview.redd.it/4CW8qHkdLuH2At66g-OGyUfAf7CPGBCbrB1h9CGRXWY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40c2c7a489b2e0f0fdda12859bae1894512c7969", "width": 960, "height": 815}], "variants": {}, "id": "vGNbo-f1adpCvC9ZQ3eQqDvJuX41B2__MhB_agwlzNk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "111cvow", "is_robot_indexable": true, "report_reasons": null, "author": "skj8", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111cvow/data_engineering_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kanger.dev/data-engineering-skills/", "subreddit_subscribers": 89504, "created_utc": 1676304692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, I am trying to understand the real effort behind running Stitch in production and I have some questions  \n\n\n1. Any maintenance events that take time? If yes, please describe, i wanna understand the effort over a year of running the tool.  \n\n2. Schema evolution supported? including nested fields? I am wondering how it handles multiple levels of nesting (from mongo to BQ) and if it can evolve the schema, or if I need to full reload.  \n\n3. Any SLA considerations? They say they offer SLA but not what it is (contact sales). Do their pipelines break? (ofc they do) what response time do they have?\n\n&amp;#x200B;\n\nMuch appreciated! If you have experience regarding these points with a similar tool, I would be happy to hear about that too.", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stitch Schema migration and maintenance burden?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_111btrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676302066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, I am trying to understand the real effort behind running Stitch in production and I have some questions  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Any maintenance events that take time? If yes, please describe, i wanna understand the effort over a year of running the tool.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Schema evolution supported? including nested fields? I am wondering how it handles multiple levels of nesting (from mongo to BQ) and if it can evolve the schema, or if I need to full reload.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any SLA considerations? They say they offer SLA but not what it is (contact sales). Do their pipelines break? (ofc they do) what response time do they have?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Much appreciated! If you have experience regarding these points with a similar tool, I would be happy to hear about that too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "111btrw", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/111btrw/stitch_schema_migration_and_maintenance_burden/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/111btrw/stitch_schema_migration_and_maintenance_burden/", "subreddit_subscribers": 89504, "created_utc": 1676302066.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}