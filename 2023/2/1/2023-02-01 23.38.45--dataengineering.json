{"kind": "Listing", "data": {"after": "t3_10r0w1g", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I'm sorry if this post is long...there's a **tldr** at the bottom. I just want a place to vent and get feedback - I feel discouraged and frustrated. About three weeks ago, I had an initial screening with a recruiter from a tech start-up based in Europe. It led to a technical interview for a remote data engineering role (US) with them. The first technical interview went well, and I was informed that the next step would be a take-home assignment. The assignment asked me to do pretty much what I would be doing if I received the position:\n\n1. Write an API that connects to one of their datasets\n2. include multiple parameters for aggregation, querying, and filtering\n3. put it on the cloud\n4. write documentation for it\n5. Write a report of why you chose the tools you did and what you would do differently.\n\nThe time limit on the assignment was approximately three hours. This struck me as a red flag because I felt like this was a lot of work to accomplish in less than half a work day (I would love your thoughts on that, actually. Maybe I'm just slow.)? The assignment was also very open-ended. There was no mention of which dataset to connect to, how to connect to it, which tools to use, or really any expectations on their end.\n\nI'm not a fan of take-home assignments because I've heard horror stories of people getting used for their free work. Since it was so open-ended, only three hours long, and the interviewer was super chill when talking about the assignment (\"We just need something to get a feel for your Python skills\") I assumed it was ok to build a small application showcasing the fundamentals of API development.\n\nI used one of their smaller datasets, coded my API in Python using Django and Pandas, hosted it on an EC2 instance, and wrote markdown documentation. In my report, I mention how I would use Spark, tokenization (I was working with text data), and a proper database if I were to make this app production grade. I included all the drawbacks of my current application and how I would fix them - because of course - the assignment was only supposed to be three hours long, and there was only so much I was willing to do for a take-home assignment. I felt like this was a lot of work as it is, and it took me roughly 6.5 hours to complete.\n\nAfter submitting my assignment, I got an email the next day telling me I did a great job and they would like to proceed with the second technical interview. The second interview ended well, with the interviewer telling me they would schedule a final interview with the CTO. Instead, I received another email three days later stating that the company decided to terminate my interview process. They said it was because my app was not using a large enough dataset, my code wasn't efficient enough, pretty much telling me that it wasn't applicable for a large-scale system. ????????. Of course not? This is a take-home assignment that was supposed to take 3 hours to finish. I am honestly baffled. Is this normal? I understand there are people out there who probably can write an enterprise-level API in 3 hours, but even so, why do it for free during the interview process? At this point, I feel a bit sketched out and like I dodged a bullet. It's still discouraging to make it so far in the interview process and not get an offer, though.\n\n&amp;#x200B;\n\n**TLDR**; *I completed a take-home assignment, and I feel kind of shitty about it. Considering avoiding companies that ask me to do this in the future...*", "author_fullname": "t2_kju9dvol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feel a bit burned from a take-home assignment I completed during an interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qdxjr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 96, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 96, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675262425.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675207533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I&amp;#39;m sorry if this post is long...there&amp;#39;s a &lt;strong&gt;tldr&lt;/strong&gt; at the bottom. I just want a place to vent and get feedback - I feel discouraged and frustrated. About three weeks ago, I had an initial screening with a recruiter from a tech start-up based in Europe. It led to a technical interview for a remote data engineering role (US) with them. The first technical interview went well, and I was informed that the next step would be a take-home assignment. The assignment asked me to do pretty much what I would be doing if I received the position:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write an API that connects to one of their datasets&lt;/li&gt;\n&lt;li&gt;include multiple parameters for aggregation, querying, and filtering&lt;/li&gt;\n&lt;li&gt;put it on the cloud&lt;/li&gt;\n&lt;li&gt;write documentation for it&lt;/li&gt;\n&lt;li&gt;Write a report of why you chose the tools you did and what you would do differently.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The time limit on the assignment was approximately three hours. This struck me as a red flag because I felt like this was a lot of work to accomplish in less than half a work day (I would love your thoughts on that, actually. Maybe I&amp;#39;m just slow.)? The assignment was also very open-ended. There was no mention of which dataset to connect to, how to connect to it, which tools to use, or really any expectations on their end.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not a fan of take-home assignments because I&amp;#39;ve heard horror stories of people getting used for their free work. Since it was so open-ended, only three hours long, and the interviewer was super chill when talking about the assignment (&amp;quot;We just need something to get a feel for your Python skills&amp;quot;) I assumed it was ok to build a small application showcasing the fundamentals of API development.&lt;/p&gt;\n\n&lt;p&gt;I used one of their smaller datasets, coded my API in Python using Django and Pandas, hosted it on an EC2 instance, and wrote markdown documentation. In my report, I mention how I would use Spark, tokenization (I was working with text data), and a proper database if I were to make this app production grade. I included all the drawbacks of my current application and how I would fix them - because of course - the assignment was only supposed to be three hours long, and there was only so much I was willing to do for a take-home assignment. I felt like this was a lot of work as it is, and it took me roughly 6.5 hours to complete.&lt;/p&gt;\n\n&lt;p&gt;After submitting my assignment, I got an email the next day telling me I did a great job and they would like to proceed with the second technical interview. The second interview ended well, with the interviewer telling me they would schedule a final interview with the CTO. Instead, I received another email three days later stating that the company decided to terminate my interview process. They said it was because my app was not using a large enough dataset, my code wasn&amp;#39;t efficient enough, pretty much telling me that it wasn&amp;#39;t applicable for a large-scale system. ????????. Of course not? This is a take-home assignment that was supposed to take 3 hours to finish. I am honestly baffled. Is this normal? I understand there are people out there who probably can write an enterprise-level API in 3 hours, but even so, why do it for free during the interview process? At this point, I feel a bit sketched out and like I dodged a bullet. It&amp;#39;s still discouraging to make it so far in the interview process and not get an offer, though.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;; &lt;em&gt;I completed a take-home assignment, and I feel kind of shitty about it. Considering avoiding companies that ask me to do this in the future...&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10qdxjr", "is_robot_indexable": true, "report_reasons": null, "author": "LusciousPigeon", "discussion_type": null, "num_comments": 102, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qdxjr/feel_a_bit_burned_from_a_takehome_assignment_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qdxjr/feel_a_bit_burned_from_a_takehome_assignment_i/", "subreddit_subscribers": 88185, "created_utc": 1675207533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the challenges I hear about over and over in the data space is the giant gap that exists between those working on the data operation and those having a stake in the data operation's deliverables.  \n\n\nThis often results in a tumultuous first few weeks in a new technical data role, as one weighs the need to deliver value quickly, agains the desire to build something with a strong foundation.  \n\n\nI started a mini-series of medium articles that seeks to strike a balance between the two opposing forces - designed to go over a viable path that quickly delivers on a fictional data ask, in a cloud environment, on a shoestring budget:  \n\n\n[part I - we set the stage](https://medium.com/@syampols/data-pipeline-on-a-shoestring-part-i-where-to-begin-a085bb397470) \u2014 whether you\u2019re a hobbyist looking to build a fun data pipeline or you just landed in a startup that needs data served yesterday and you're looking for a perspective on how to deliver, this summarizes a good problem statement.  \n\n\n[part II - we pose a data need](https://medium.com/@syampols/data-pipeline-on-a-shoestring-part-ii-the-data-need-and-the-architecture-to-solve-for-it-bed5f208de36) \u2014 we describe the fictional ask from our imaginary stakeholder, and propose the architecture that will solve for the need, from ingestion, to transformation, to serving the data, on a shoestring, of course.  \n\n\n[part III - we lay the foundation for the work](https://medium.com/@syampols/data-pipeline-on-a-shoestring-part-iii-laying-the-foundation-d74014112f0) \u2014 using a combination of service oriented architecture principles in combination with infrastructure as code, we prime our cloud environments and describe a framework to deploy the resources needed to solve for the fictional data problem.  \n\n\nIn part IV (coming soon), we build the pipeline \u2014 using the foundation and deployment process outlined in part III, we continue to deploy the resources that make the pipeline run, as outlined in the architecture in part II.  \n\n\nIn part V (coming soon), we wrap \u2014 Using the prior framework laid out in the prior four articles, we talk about pathways to scale the solution, as this is, after all, a pipeline on a budget, and will/should not be used beyond a solid proof of concept (unless ye seek technical debt).  \n\n\nEnjoy! I would give you rotten tomatoes to throw my way but then it would end up on your monitor, that said, I would love any feedback you may have!", "author_fullname": "t2_daui6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline on a Shoestring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qm36f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675231129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the challenges I hear about over and over in the data space is the giant gap that exists between those working on the data operation and those having a stake in the data operation&amp;#39;s deliverables.  &lt;/p&gt;\n\n&lt;p&gt;This often results in a tumultuous first few weeks in a new technical data role, as one weighs the need to deliver value quickly, agains the desire to build something with a strong foundation.  &lt;/p&gt;\n\n&lt;p&gt;I started a mini-series of medium articles that seeks to strike a balance between the two opposing forces - designed to go over a viable path that quickly delivers on a fictional data ask, in a cloud environment, on a shoestring budget:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@syampols/data-pipeline-on-a-shoestring-part-i-where-to-begin-a085bb397470\"&gt;part I - we set the stage&lt;/a&gt; \u2014 whether you\u2019re a hobbyist looking to build a fun data pipeline or you just landed in a startup that needs data served yesterday and you&amp;#39;re looking for a perspective on how to deliver, this summarizes a good problem statement.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@syampols/data-pipeline-on-a-shoestring-part-ii-the-data-need-and-the-architecture-to-solve-for-it-bed5f208de36\"&gt;part II - we pose a data need&lt;/a&gt; \u2014 we describe the fictional ask from our imaginary stakeholder, and propose the architecture that will solve for the need, from ingestion, to transformation, to serving the data, on a shoestring, of course.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@syampols/data-pipeline-on-a-shoestring-part-iii-laying-the-foundation-d74014112f0\"&gt;part III - we lay the foundation for the work&lt;/a&gt; \u2014 using a combination of service oriented architecture principles in combination with infrastructure as code, we prime our cloud environments and describe a framework to deploy the resources needed to solve for the fictional data problem.  &lt;/p&gt;\n\n&lt;p&gt;In part IV (coming soon), we build the pipeline \u2014 using the foundation and deployment process outlined in part III, we continue to deploy the resources that make the pipeline run, as outlined in the architecture in part II.  &lt;/p&gt;\n\n&lt;p&gt;In part V (coming soon), we wrap \u2014 Using the prior framework laid out in the prior four articles, we talk about pathways to scale the solution, as this is, after all, a pipeline on a budget, and will/should not be used beyond a solid proof of concept (unless ye seek technical debt).  &lt;/p&gt;\n\n&lt;p&gt;Enjoy! I would give you rotten tomatoes to throw my way but then it would end up on your monitor, that said, I would love any feedback you may have!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X1UjVNIpjNDAip8AKmYwRjmbXYy06CldchWMsB0Spig.jpg?auto=webp&amp;v=enabled&amp;s=f3f0d8bb3a99ce5b99739efd8864c8c1983bbd4d", "width": 800, "height": 1088}, "resolutions": [{"url": "https://external-preview.redd.it/X1UjVNIpjNDAip8AKmYwRjmbXYy06CldchWMsB0Spig.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cad18c9f8041c9b317470b7d68352b6d50b19dbe", "width": 108, "height": 146}, {"url": "https://external-preview.redd.it/X1UjVNIpjNDAip8AKmYwRjmbXYy06CldchWMsB0Spig.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36f906a4b6545525dea99f0ec146006ccfe33416", "width": 216, "height": 293}, {"url": "https://external-preview.redd.it/X1UjVNIpjNDAip8AKmYwRjmbXYy06CldchWMsB0Spig.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7418e2a1e9f58e4bed983544bb241d9a1d496db", "width": 320, "height": 435}, {"url": "https://external-preview.redd.it/X1UjVNIpjNDAip8AKmYwRjmbXYy06CldchWMsB0Spig.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c16cdcb7d1d29145a69c84315cef39c3a3f2fe9d", "width": 640, "height": 870}], "variants": {}, "id": "jDcIGIjerCPbG0NdHcBNB5qc0vI-g436N3O6fz4iSQQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10qm36f", "is_robot_indexable": true, "report_reasons": null, "author": "JaJ_Judy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qm36f/data_pipeline_on_a_shoestring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qm36f/data_pipeline_on_a_shoestring/", "subreddit_subscribers": 88185, "created_utc": 1675231129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with Uber and had 3 rounds with them:\n\n1. DSA - Graph based problem\n2. Spark/SQL/Scaling - Asked to write a query to find number of users who went to a same group of cities (order matters, records need to be ordered by time). Asked to give time complexity of SQL query. Asked to port that to spark, lot of cross questioning about optimisations, large amount of data handling in spark with limited resources etc.\n3. System Design - Asked to design bookmyshow. Lot of cross questioning around concurrency, fault tolerance, CAP theorem, how to choose data sources etc.\n\nMy interviews didn't went the way I hoped, so wanted to understand from more experienced folks here, how do I prepare for:\n\n1. Big O notation complexity calculation on a sql query\n2. Prepare of system design, data modeling for system design. I was stumped on choosing data sources for specific purposes (like which data source to use for storing seats availability)", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uber Interview Experience/Asking Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qzicp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675271957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675270322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with Uber and had 3 rounds with them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DSA - Graph based problem&lt;/li&gt;\n&lt;li&gt;Spark/SQL/Scaling - Asked to write a query to find number of users who went to a same group of cities (order matters, records need to be ordered by time). Asked to give time complexity of SQL query. Asked to port that to spark, lot of cross questioning about optimisations, large amount of data handling in spark with limited resources etc.&lt;/li&gt;\n&lt;li&gt;System Design - Asked to design bookmyshow. Lot of cross questioning around concurrency, fault tolerance, CAP theorem, how to choose data sources etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My interviews didn&amp;#39;t went the way I hoped, so wanted to understand from more experienced folks here, how do I prepare for:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Big O notation complexity calculation on a sql query&lt;/li&gt;\n&lt;li&gt;Prepare of system design, data modeling for system design. I was stumped on choosing data sources for specific purposes (like which data source to use for storing seats availability)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10qzicp", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qzicp/uber_interview_experienceasking_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qzicp/uber_interview_experienceasking_suggestions/", "subreddit_subscribers": 88185, "created_utc": 1675270322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you provide access to end users to the RDBMS (say Postgres) warehouse if they want to use (only) python? Do you build custom python SDK which include predefined queries? Or maybe use something like PostgREST? Or SQLAlchemy? Or any other  way?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Providing end users access to the database using Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qv2x3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675259128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you provide access to end users to the RDBMS (say Postgres) warehouse if they want to use (only) python? Do you build custom python SDK which include predefined queries? Or maybe use something like PostgREST? Or SQLAlchemy? Or any other  way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qv2x3", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qv2x3/providing_end_users_access_to_the_database_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qv2x3/providing_end_users_access_to_the_database_using/", "subreddit_subscribers": 88185, "created_utc": 1675259128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, wanted to ask the group what are the most common \"data engineering\" tools used today. i have a background on informatica ETL, back 20 years ago, and today work a lot with customers that use azure data factory, but i'm curious to know what are the most common tools used today (as in 2023).", "author_fullname": "t2_imia197u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most common data engineering tools used today?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qgu0f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675215159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, wanted to ask the group what are the most common &amp;quot;data engineering&amp;quot; tools used today. i have a background on informatica ETL, back 20 years ago, and today work a lot with customers that use azure data factory, but i&amp;#39;m curious to know what are the most common tools used today (as in 2023).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qgu0f", "is_robot_indexable": true, "report_reasons": null, "author": "Pty_Rick", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qgu0f/most_common_data_engineering_tools_used_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qgu0f/most_common_data_engineering_tools_used_today/", "subreddit_subscribers": 88185, "created_utc": 1675215159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I think the general idea of Airflow is to orchestrate but I came across two deployments in the last week that are running thousands of tasks concurrently using dynamic tasks - a sort of map reduce process using airflow.\n\nThe data is being passed around using the file system. We can also see many examples online of carrying the heavy lifting inside Airflow.\n\nHave you noticed that? It\u2019s kind of tempting - especially with the dynamic tasks I\u2019m thinking yeah sure why not? Then I don\u2019t have to deal with any other tooling to process my data.\n\nEspecially if you deploy to Kubernetes you can keep scaling without too much issues", "author_fullname": "t2_htptc13q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Airflow Anti-Patterns the Norm Now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qtv21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675255548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think the general idea of Airflow is to orchestrate but I came across two deployments in the last week that are running thousands of tasks concurrently using dynamic tasks - a sort of map reduce process using airflow.&lt;/p&gt;\n\n&lt;p&gt;The data is being passed around using the file system. We can also see many examples online of carrying the heavy lifting inside Airflow.&lt;/p&gt;\n\n&lt;p&gt;Have you noticed that? It\u2019s kind of tempting - especially with the dynamic tasks I\u2019m thinking yeah sure why not? Then I don\u2019t have to deal with any other tooling to process my data.&lt;/p&gt;\n\n&lt;p&gt;Especially if you deploy to Kubernetes you can keep scaling without too much issues&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qtv21", "is_robot_indexable": true, "report_reasons": null, "author": "Main_Tap_1256", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qtv21/are_airflow_antipatterns_the_norm_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qtv21/are_airflow_antipatterns_the_norm_now/", "subreddit_subscribers": 88185, "created_utc": 1675255548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the actual use cases of the data contracts? I just read about them and from what I understand, it is used mainly for defining data schema for streaming data? I.e. you define schema you expect your data to adhere when reading such data from e.g. Kafka? And it is useful because e.g. protobuf can be implemented using different languages? Or are data contracts used in some other way? Do they make any sense for batch data processing?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts actual use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qrefc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675250797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the actual use cases of the data contracts? I just read about them and from what I understand, it is used mainly for defining data schema for streaming data? I.e. you define schema you expect your data to adhere when reading such data from e.g. Kafka? And it is useful because e.g. protobuf can be implemented using different languages? Or are data contracts used in some other way? Do they make any sense for batch data processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qrefc", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qrefc/data_contracts_actual_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qrefc/data_contracts_actual_use_cases/", "subreddit_subscribers": 88185, "created_utc": 1675250797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Happened to stumble upon this today\u2026 [https://www.fivetran.com/blog/fivetran-pricing](https://www.fivetran.com/blog/fivetran-pricing)\n\n*February 1, 2023 update: We\u2019re offering a Free Plan, replacing the Standard Select tier. On top of that, we lowered pay-as-you-go pricing by 11 percent.* [*Check out the announcement.*](https://www.fivetran.com/fivetran-free-plan)\n\nThe announcement link is currently a 404. Maybe it will be up tomorrow.", "author_fullname": "t2_8fr7j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran launching a free tier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qh91r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675216314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happened to stumble upon this today\u2026 &lt;a href=\"https://www.fivetran.com/blog/fivetran-pricing\"&gt;https://www.fivetran.com/blog/fivetran-pricing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;February 1, 2023 update: We\u2019re offering a Free Plan, replacing the Standard Select tier. On top of that, we lowered pay-as-you-go pricing by 11 percent.&lt;/em&gt; &lt;a href=\"https://www.fivetran.com/fivetran-free-plan\"&gt;&lt;em&gt;Check out the announcement.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The announcement link is currently a 404. Maybe it will be up tomorrow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?auto=webp&amp;v=enabled&amp;s=6f76ad332b9778f9e5c8b936f874ac86d6b803a5", "width": 2000, "height": 1001}, "resolutions": [{"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3856080cf62bd92071ebc9f672d05a051a8bcaf4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=506ff27238e7e3b069d4ce2480b79701a9683d76", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a1c7c5e73f51b9dd57bccd28f3504341244b39f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d33f1659cd8f7cfe30c21ed294c91b712389b68", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=388e9e8cd9d4f86495d58145de24d40764151151", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/B9XwF2PzIiw_ZW3rcoAXVLgUZ9hL8BA0u9-EUUjgBtk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0109a6302cd8c6284059bcf54a763c1db7ac347e", "width": 1080, "height": 540}], "variants": {}, "id": "PJzWrjCOMY508Zf959hjx2rzPDC0ORXbiulae7FDjho"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qh91r", "is_robot_indexable": true, "report_reasons": null, "author": "CEOnnor", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qh91r/fivetran_launching_a_free_tier/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qh91r/fivetran_launching_a_free_tier/", "subreddit_subscribers": 88185, "created_utc": 1675216314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Share Data with Snowflake Data Sharing (with examples)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_10qwozi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1q8IakFKMNzTQCObe9Q7ElwGo3r5_mvFu3G_w6BEk5c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675263429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "prequel.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.prequel.co/blog/how-to-share-data-with-snowflake-data-sharing", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?auto=webp&amp;v=enabled&amp;s=da978d0a443e9b734d24029be2a75603b0c9768d", "width": 1920, "height": 820}, "resolutions": [{"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebc0cf3491161978a928a0bd40b30156c504f74a", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf0dd251abed2b4a5f6d434eaedd8d107064b7d8", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=015a9589470108d9aad7758462d684bf484f4ca0", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=692b96c58baac0764e69d3cb39f2e9fa756f91c0", "width": 640, "height": 273}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8efad1fbb093e4f14fd609c95bff616808dd1e4f", "width": 960, "height": 410}, {"url": "https://external-preview.redd.it/7q6yVBHNGFOlZHU6w068YfVV4i5lmqMuf3yoem7V8LM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d813c4a516adb779c9a2b40df18a63c70e83584", "width": 1080, "height": 461}], "variants": {}, "id": "hW2pCvJeKg5crzWPDHlylm0VFGaSzyon859YfBjDf8M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10qwozi", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/10qwozi/how_to_share_data_with_snowflake_data_sharing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.prequel.co/blog/how-to-share-data-with-snowflake-data-sharing", "subreddit_subscribers": 88185, "created_utc": 1675263429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you handle a destructive change to the table used in production? Let's say we have a Postgres as production warehouse and applications query the data from it (obviously). And now we want to change some table in the WH and let's say we want to delete some columns. How do you manage this process? You create a clone table with the same data and give some period to all applications to adapt to the new table and then discontinue the old one? Or what is the best practice in such scenario?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling table schema change in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qs3or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675252031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you handle a destructive change to the table used in production? Let&amp;#39;s say we have a Postgres as production warehouse and applications query the data from it (obviously). And now we want to change some table in the WH and let&amp;#39;s say we want to delete some columns. How do you manage this process? You create a clone table with the same data and give some period to all applications to adapt to the new table and then discontinue the old one? Or what is the best practice in such scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qs3or", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qs3or/handling_table_schema_change_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qs3or/handling_table_schema_change_in_production/", "subreddit_subscribers": 88185, "created_utc": 1675252031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today I found out about AWS Quicksight Q which helps answering business questions using NLP based on reports created in it (like what was my sales for the last week?)\n\nI want to know if there is a solution could be integrated via slack bots to help people get quick answers to such queries. Currently Quicksight Q doesn\u2019t allow slack integration I think. \n\nFeel like this feature can be helpful for quick answers rather than searching through a report.", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NLP support for reports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qp8dc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675243625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today I found out about AWS Quicksight Q which helps answering business questions using NLP based on reports created in it (like what was my sales for the last week?)&lt;/p&gt;\n\n&lt;p&gt;I want to know if there is a solution could be integrated via slack bots to help people get quick answers to such queries. Currently Quicksight Q doesn\u2019t allow slack integration I think. &lt;/p&gt;\n\n&lt;p&gt;Feel like this feature can be helpful for quick answers rather than searching through a report.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10qp8dc", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qp8dc/nlp_support_for_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qp8dc/nlp_support_for_reports/", "subreddit_subscribers": 88185, "created_utc": 1675243625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1675270813.475, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qzpp1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675270813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qzpp1", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qzpp1/monthly_general_discussion/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/10qzpp1/monthly_general_discussion/", "subreddit_subscribers": 88185, "created_utc": 1675270813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently a new DE at my company and we\u2019re designing and implementing a data lakehouse using Databricks and Unity Catalog. \n\nMy coworkers were telling me about the design of the landing container. We have several source systems and they will transfer their data to the ADLS bucket on an hourly basis to folders like source_system/domain/item/YYYY/MM/DD/HH. The data are CDC logs in JSON format. \n\nThis part is straightforward and I can understand it.\n\nThe next part is the part I have questions about: we have jobs that will read the raw files from the landing container and basically turn them into delta format. We have a design that involves 3 folders \u201clanding\u201d, \u201cworking\u201d, \u201carchived\u201d. Whenever you start a job that involves that file, you would move that file on the landing container from a \u201clanding\u201d folder to a \u201cworking\u201d folder. Then once the job is completed, we are supposed to move the file from the \u201cworking\u201d folder to an \u201carchived\u201d folder.\n\nA problem arises when we have several jobs that involves the same file. Let\u2019s say both job A and job B uses that raw file, and they\u2019re sequential. When job A is done with the file, it moves it to an \u201carchived\u201d folder. Now job B, which expects the file to be in the \u201clanding\u201d folder, can no longer locate the file, because it\u2019s been moved to the \u201carchived\u201d folder. It is even a bigger problem when both job A and job B are happening at the same time. \n\nMy coworkers tell me that this is intentional and what should actually happen is that we are supposed to create 2 copies of the raw file, or in the case of 3 jobs being dependent on the file, then 3 copies. We would have to rename the item to something like item_copy1, item_copy2. \n\nThe reason is that this is a best practice because now the state of where the file is will reflect the status of the job. Upon a job success, files are always moved to the \u201carchived\u201d folder. The \u201clanding\u201d and \u201cworking\u201d folders should always be empty after a job. If they are not empty, it will help indicate to us that some error happened to the job.\n\nI\u2019ve never seen a design like this before and I\u2019m used to the design where once a file is in the landing container, we don\u2019t move it and we only read from it. We use Airflow to track the success of individual jobs instead of the file location. But my coworkers told me this would be brittle design as now I am too reliant of Airflow for information on the status of the job. \n\nI was wondering if anyone could educate me or point me to resources that explain this type of design to me more. Or correct me if this is some well-known best practice that I\u2019m misunderstanding and butchering. I want to understand the rationale behind creating copies and moving files to an \u201carchived\u201d folder and its benefits.", "author_fullname": "t2_13lfsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lakehouse architecture - design of landing container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qzlma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675270728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675270547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently a new DE at my company and we\u2019re designing and implementing a data lakehouse using Databricks and Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;My coworkers were telling me about the design of the landing container. We have several source systems and they will transfer their data to the ADLS bucket on an hourly basis to folders like source_system/domain/item/YYYY/MM/DD/HH. The data are CDC logs in JSON format. &lt;/p&gt;\n\n&lt;p&gt;This part is straightforward and I can understand it.&lt;/p&gt;\n\n&lt;p&gt;The next part is the part I have questions about: we have jobs that will read the raw files from the landing container and basically turn them into delta format. We have a design that involves 3 folders \u201clanding\u201d, \u201cworking\u201d, \u201carchived\u201d. Whenever you start a job that involves that file, you would move that file on the landing container from a \u201clanding\u201d folder to a \u201cworking\u201d folder. Then once the job is completed, we are supposed to move the file from the \u201cworking\u201d folder to an \u201carchived\u201d folder.&lt;/p&gt;\n\n&lt;p&gt;A problem arises when we have several jobs that involves the same file. Let\u2019s say both job A and job B uses that raw file, and they\u2019re sequential. When job A is done with the file, it moves it to an \u201carchived\u201d folder. Now job B, which expects the file to be in the \u201clanding\u201d folder, can no longer locate the file, because it\u2019s been moved to the \u201carchived\u201d folder. It is even a bigger problem when both job A and job B are happening at the same time. &lt;/p&gt;\n\n&lt;p&gt;My coworkers tell me that this is intentional and what should actually happen is that we are supposed to create 2 copies of the raw file, or in the case of 3 jobs being dependent on the file, then 3 copies. We would have to rename the item to something like item_copy1, item_copy2. &lt;/p&gt;\n\n&lt;p&gt;The reason is that this is a best practice because now the state of where the file is will reflect the status of the job. Upon a job success, files are always moved to the \u201carchived\u201d folder. The \u201clanding\u201d and \u201cworking\u201d folders should always be empty after a job. If they are not empty, it will help indicate to us that some error happened to the job.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve never seen a design like this before and I\u2019m used to the design where once a file is in the landing container, we don\u2019t move it and we only read from it. We use Airflow to track the success of individual jobs instead of the file location. But my coworkers told me this would be brittle design as now I am too reliant of Airflow for information on the status of the job. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone could educate me or point me to resources that explain this type of design to me more. Or correct me if this is some well-known best practice that I\u2019m misunderstanding and butchering. I want to understand the rationale behind creating copies and moving files to an \u201carchived\u201d folder and its benefits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qzlma", "is_robot_indexable": true, "report_reasons": null, "author": "pizzanub", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qzlma/data_lakehouse_architecture_design_of_landing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qzlma/data_lakehouse_architecture_design_of_landing/", "subreddit_subscribers": 88185, "created_utc": 1675270547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " tldr: joined company 3 weeks ago, lots of red flags, possibly no work for months, jump ship or be patient?\n\nI joined a huge well known company as data engineer 3 weeks ago. As it turned out, all the work is being done by an outsourced company, communication is a mess and overall the whole project is a chaos. I have been asking to be put on the work I am most interested in but nothing yet. We suppose to take over the project in a few months, but there is no chance it will be done by that time.\n\nThe team is new and as I understood people are just trying to find somebody from the external team to try and cooperate with. I joined with another guy, he basically feels the same way...\n\nAnother thing is that made me decide to accept the offer is the migration from one cloud provider to another, that I prefer much more. Well, it turned out this is not going to happen anytime soon...\n\nHas anybody been in a similar situation before? I am not sure if I should be patient and stay or jump ship. I left my previous work because I was basically not doing anything and felt like I was not getting any valuable experience, I don't want be stuck in another place like that.", "author_fullname": "t2_4jwhs15h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New work - lots of redflags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qocoq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675239879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tldr: joined company 3 weeks ago, lots of red flags, possibly no work for months, jump ship or be patient?&lt;/p&gt;\n\n&lt;p&gt;I joined a huge well known company as data engineer 3 weeks ago. As it turned out, all the work is being done by an outsourced company, communication is a mess and overall the whole project is a chaos. I have been asking to be put on the work I am most interested in but nothing yet. We suppose to take over the project in a few months, but there is no chance it will be done by that time.&lt;/p&gt;\n\n&lt;p&gt;The team is new and as I understood people are just trying to find somebody from the external team to try and cooperate with. I joined with another guy, he basically feels the same way...&lt;/p&gt;\n\n&lt;p&gt;Another thing is that made me decide to accept the offer is the migration from one cloud provider to another, that I prefer much more. Well, it turned out this is not going to happen anytime soon...&lt;/p&gt;\n\n&lt;p&gt;Has anybody been in a similar situation before? I am not sure if I should be patient and stay or jump ship. I left my previous work because I was basically not doing anything and felt like I was not getting any valuable experience, I don&amp;#39;t want be stuck in another place like that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10qocoq", "is_robot_indexable": true, "report_reasons": null, "author": "blukitteh", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qocoq/new_work_lots_of_redflags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qocoq/new_work_lots_of_redflags/", "subreddit_subscribers": 88185, "created_utc": 1675239879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please help decide:\n8 YOE + H1-B Visa\nBoth remote jobs\nGiven the current Layoffs and possible recession, unable to decide which is better.\n\nCurrent Company (Staff Data Engineer): \n170K base + 40K bonus at year-end.\nGreat work-life balance (&lt; 6 hrs a day of work) and rest for personal skills.\nThe job is 100% secure and working with the team for 3+ years.\nSkills: Airflow, Meltano, dbt, Snowflake, AWS\n\nSeries A Start-Up (Senior Analytics Engineer):\n190K base + 0.05% (~250K) vested over 4 years.\nInsurance Industry. Revenue ~30 Mil and raised ~30 Mil in 2022.\n8-10 hrs work depending on the learning needed.\nThe Hiring Manager says the job is secure for at least 2 years.\nSkills: Dagster, GCP, dbt, python", "author_fullname": "t2_pzyrjzd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current company vs Series A Startup (almost Series B)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qggo2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675214179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please help decide:\n8 YOE + H1-B Visa\nBoth remote jobs\nGiven the current Layoffs and possible recession, unable to decide which is better.&lt;/p&gt;\n\n&lt;p&gt;Current Company (Staff Data Engineer): \n170K base + 40K bonus at year-end.\nGreat work-life balance (&amp;lt; 6 hrs a day of work) and rest for personal skills.\nThe job is 100% secure and working with the team for 3+ years.\nSkills: Airflow, Meltano, dbt, Snowflake, AWS&lt;/p&gt;\n\n&lt;p&gt;Series A Start-Up (Senior Analytics Engineer):\n190K base + 0.05% (~250K) vested over 4 years.\nInsurance Industry. Revenue ~30 Mil and raised ~30 Mil in 2022.\n8-10 hrs work depending on the learning needed.\nThe Hiring Manager says the job is secure for at least 2 years.\nSkills: Dagster, GCP, dbt, python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10qggo2", "is_robot_indexable": true, "report_reasons": null, "author": "mothikikm", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qggo2/current_company_vs_series_a_startup_almost_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qggo2/current_company_vs_series_a_startup_almost_series/", "subreddit_subscribers": 88185, "created_utc": 1675214179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I'm currently enrolled a Master's in Software Engineering and have been very interested in DE since my undergrad and completed various projects and an internship within the space, and I definitely plan on pursuing more internships if possible. \n\nI've read a lot that getting a job as a DE right out of university with no professional experience is very difficult due to the nature of the job. From what I've read on this sub, people recommend going through a different role in Software Engineering and then transitioning into a DE role. \n\nIs this accurate? If so, what other role is the best suited for this transition? Backend development is something I've seen mentioned a lot.\n\nThanks!", "author_fullname": "t2_49e7c0kc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Entering DE as a grad with no experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qgc2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675213835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m currently enrolled a Master&amp;#39;s in Software Engineering and have been very interested in DE since my undergrad and completed various projects and an internship within the space, and I definitely plan on pursuing more internships if possible. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read a lot that getting a job as a DE right out of university with no professional experience is very difficult due to the nature of the job. From what I&amp;#39;ve read on this sub, people recommend going through a different role in Software Engineering and then transitioning into a DE role. &lt;/p&gt;\n\n&lt;p&gt;Is this accurate? If so, what other role is the best suited for this transition? Backend development is something I&amp;#39;ve seen mentioned a lot.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10qgc2m", "is_robot_indexable": true, "report_reasons": null, "author": "infiniteAggression-", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qgc2m/entering_de_as_a_grad_with_no_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qgc2m/entering_de_as_a_grad_with_no_experience/", "subreddit_subscribers": 88185, "created_utc": 1675213835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a CPRA analyst and data governance analyst. Is PE and DE related, or separate? \n\nHow would someone be a PE?", "author_fullname": "t2_6qmgdlvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Privacy Engineering Related to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r6h9g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675286300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a CPRA analyst and data governance analyst. Is PE and DE related, or separate? &lt;/p&gt;\n\n&lt;p&gt;How would someone be a PE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10r6h9g", "is_robot_indexable": true, "report_reasons": null, "author": "OkScientist96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r6h9g/is_privacy_engineering_related_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r6h9g/is_privacy_engineering_related_to_de/", "subreddit_subscribers": 88185, "created_utc": 1675286300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gone through the [Getting started with dbt Core](https://docs.getdbt.com/docs/get-started/getting-started-dbt-core) tutorial and I've connected my BigQuery instance to dbt through the use of the JSON file in the relevant directory.\n\nI've made changes to tables/models and see those reflected in the warehouse each time I run `dbt run`, but I cannot see where some of the base tables exist, namely orders.sql and customers.sql, both of which were used to define the models that were similarly named, and also the staging models.\n\nSurely, I looked under the `seeds` directory since that would make sense as they're seeding the project, but I don't see anything there.\n\nFor reference, the two staging tables toward the end of the tutorial, `stg_customers.sql` and `stg_orders.sql` are defined to source from\n\n    `dbt-tutorial`.jaffle_shop.customers    and\n    `dbt-tutorial`.jaffle_shop.orders\n\nrespectively.\n\nI've tried moving on and doing other stuff to further my learning, but I can't scratch this and it's irritating me.\n\nI figured that I probably deleted it so I ran `dbt init` again and there's nothing present that's just \\`orders.sql\\` or anything.", "author_fullname": "t2_svn12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Very much a noob question, but for dbt's tutorial, where are the order and customer tables kept?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r5wrc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675285747.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675284986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gone through the &lt;a href=\"https://docs.getdbt.com/docs/get-started/getting-started-dbt-core\"&gt;Getting started with dbt Core&lt;/a&gt; tutorial and I&amp;#39;ve connected my BigQuery instance to dbt through the use of the JSON file in the relevant directory.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made changes to tables/models and see those reflected in the warehouse each time I run &lt;code&gt;dbt run&lt;/code&gt;, but I cannot see where some of the base tables exist, namely orders.sql and customers.sql, both of which were used to define the models that were similarly named, and also the staging models.&lt;/p&gt;\n\n&lt;p&gt;Surely, I looked under the &lt;code&gt;seeds&lt;/code&gt; directory since that would make sense as they&amp;#39;re seeding the project, but I don&amp;#39;t see anything there.&lt;/p&gt;\n\n&lt;p&gt;For reference, the two staging tables toward the end of the tutorial, &lt;code&gt;stg_customers.sql&lt;/code&gt; and &lt;code&gt;stg_orders.sql&lt;/code&gt; are defined to source from&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;`dbt-tutorial`.jaffle_shop.customers    and\n`dbt-tutorial`.jaffle_shop.orders\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;respectively.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried moving on and doing other stuff to further my learning, but I can&amp;#39;t scratch this and it&amp;#39;s irritating me.&lt;/p&gt;\n\n&lt;p&gt;I figured that I probably deleted it so I ran &lt;code&gt;dbt init&lt;/code&gt; again and there&amp;#39;s nothing present that&amp;#39;s just `orders.sql` or anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r5wrc", "is_robot_indexable": true, "report_reasons": null, "author": "paxmlank", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r5wrc/very_much_a_noob_question_but_for_dbts_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r5wrc/very_much_a_noob_question_but_for_dbts_tutorial/", "subreddit_subscribers": 88185, "created_utc": 1675284986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a compsci student who's trying to get into this field and I really don't like the idea of using low-code/no-code tools (i.e. Tableu or Power BI) \u2014 I'm a programmer after all.  A few months ago I was planning to learn Plotly Dash because I wanted to start a carreer working with data, but most roles which involve analysis and dashboarding have as prerequisite low code tools like Excel and those aforementioned.\n\nStraight to the point, now: I want to become a DE because I like to build stuff with code and I would love to make some dashboards with Python here and there, but:\n\n1) I don't even know if dashboarding skills is something needed to this role.\n\n2) Assuming it is, how useful is to learn a \"low level\" tool like Plotly Dash since the market usually asks for low-code stuff?", "author_fullname": "t2_pme9byci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should a data engineer aspirant learn dashboarding tools like Tableu and/or Plotly Dash?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10r72qx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675287654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a compsci student who&amp;#39;s trying to get into this field and I really don&amp;#39;t like the idea of using low-code/no-code tools (i.e. Tableu or Power BI) \u2014 I&amp;#39;m a programmer after all.  A few months ago I was planning to learn Plotly Dash because I wanted to start a carreer working with data, but most roles which involve analysis and dashboarding have as prerequisite low code tools like Excel and those aforementioned.&lt;/p&gt;\n\n&lt;p&gt;Straight to the point, now: I want to become a DE because I like to build stuff with code and I would love to make some dashboards with Python here and there, but:&lt;/p&gt;\n\n&lt;p&gt;1) I don&amp;#39;t even know if dashboarding skills is something needed to this role.&lt;/p&gt;\n\n&lt;p&gt;2) Assuming it is, how useful is to learn a &amp;quot;low level&amp;quot; tool like Plotly Dash since the market usually asks for low-code stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10r72qx", "is_robot_indexable": true, "report_reasons": null, "author": "mr_tellok", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r72qx/should_a_data_engineer_aspirant_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r72qx/should_a_data_engineer_aspirant_learn/", "subreddit_subscribers": 88185, "created_utc": 1675287654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to this field. Would really appreciate a book or something with quite a few end to end projects explained covering all corners of DE in business today. Just to get my hands dirty and concepts clearer.\n\nSomething like this:\nData Science Projects with Python: A case study approach to gaining valuable insights from real data with machine learning, 2nd Edition https://amzn.eu/d/4ZXQwH6\n\nAny help will be appreciated!", "author_fullname": "t2_m8ebmzsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any hands-on project book in this field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r69sh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675285821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to this field. Would really appreciate a book or something with quite a few end to end projects explained covering all corners of DE in business today. Just to get my hands dirty and concepts clearer.&lt;/p&gt;\n\n&lt;p&gt;Something like this:\nData Science Projects with Python: A case study approach to gaining valuable insights from real data with machine learning, 2nd Edition &lt;a href=\"https://amzn.eu/d/4ZXQwH6\"&gt;https://amzn.eu/d/4ZXQwH6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any help will be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r69sh", "is_robot_indexable": true, "report_reasons": null, "author": "riderx65", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r69sh/any_handson_project_book_in_this_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r69sh/any_handson_project_book_in_this_field/", "subreddit_subscribers": 88185, "created_utc": 1675285821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I recently graduated with a Master's degree in bioinformatics and I'm looking to break in to the field of data engineering.  I'm based in the UK with a 2-year graduate visa and have intermediate skills in Python and R. I don't have any job experience, because I did my master's straight after my undergrad (biochemistry).\n\nAfter researching this subreddit, I have a clear idea of what skills to pick up and how to do it. However, I'm not sure which approach in terms of job hunting to take in order to make it more realistic.\n\nShould I:\n\n1. Focus on learning and simultaneously applying for junior DE positions until I land a role?\n2. Start by finding a data analyst or similar data-related job to gain experience?\n3. Target a specific role that will help me make the transition to data engineering more easily?\n\nI would appreciate your thoughts and suggestions. Thank you!", "author_fullname": "t2_v7lrnxcl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career path for a Bioinformatician", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10qhf4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675216797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I recently graduated with a Master&amp;#39;s degree in bioinformatics and I&amp;#39;m looking to break in to the field of data engineering.  I&amp;#39;m based in the UK with a 2-year graduate visa and have intermediate skills in Python and R. I don&amp;#39;t have any job experience, because I did my master&amp;#39;s straight after my undergrad (biochemistry).&lt;/p&gt;\n\n&lt;p&gt;After researching this subreddit, I have a clear idea of what skills to pick up and how to do it. However, I&amp;#39;m not sure which approach in terms of job hunting to take in order to make it more realistic.&lt;/p&gt;\n\n&lt;p&gt;Should I:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Focus on learning and simultaneously applying for junior DE positions until I land a role?&lt;/li&gt;\n&lt;li&gt;Start by finding a data analyst or similar data-related job to gain experience?&lt;/li&gt;\n&lt;li&gt;Target a specific role that will help me make the transition to data engineering more easily?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would appreciate your thoughts and suggestions. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10qhf4n", "is_robot_indexable": true, "report_reasons": null, "author": "LivingOnTheMullet", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10qhf4n/career_path_for_a_bioinformatician/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10qhf4n/career_path_for_a_bioinformatician/", "subreddit_subscribers": 88185, "created_utc": 1675216797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "...what would you suggest?\n\nFocussing on:\n\n* Data Engineering processes (Databricks + AWS)\n* Adoption step by step\n* Best practice", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need to become an expert on enterprise databricks adoption but nothing to do with code...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r3rrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675280872.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675280111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...what would you suggest?&lt;/p&gt;\n\n&lt;p&gt;Focussing on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Engineering processes (Databricks + AWS)&lt;/li&gt;\n&lt;li&gt;Adoption step by step&lt;/li&gt;\n&lt;li&gt;Best practice&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10r3rrr", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r3rrr/i_need_to_become_an_expert_on_enterprise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r3rrr/i_need_to_become_an_expert_on_enterprise/", "subreddit_subscribers": 88185, "created_utc": 1675280111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lease remote resources? Kinda going over my head. Can someone give a real time example?", "author_fullname": "t2_5lve5av0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the use case of nimbus IAAS tool kit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r2504", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675276388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lease remote resources? Kinda going over my head. Can someone give a real time example?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r2504", "is_robot_indexable": true, "report_reasons": null, "author": "noobmastersmaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r2504/what_is_the_use_case_of_nimbus_iaas_tool_kit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r2504/what_is_the_use_case_of_nimbus_iaas_tool_kit/", "subreddit_subscribers": 88185, "created_utc": 1675276388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all! I\u2019m currently a 90% data analyst 10% DE in my role. I have a good opportunity to gain and practise data engineering skills through new projects this year. My employer is fully behind me learning this and has offered to pay for some courses - as long as I do the research and find one.\n\nI am looking for any recommendations that will take me from basically nothing (I\u2019m comfortable with SQL and Python but no warehousing, cloud, pipeline experience) and give me the tools required to use in new projects in my role that will eventually help me fully transition.\n\nAny free course/book recommendations are also appreciated!\n\nThank you!\n\nEdit: the wiki has some good learning resources but I\u2019m looking for sort of an all-encompassing course with lectures, exercises etc. and ideally not an inordinate cost.", "author_fullname": "t2_az62upwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r1xk3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675276672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675275919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! I\u2019m currently a 90% data analyst 10% DE in my role. I have a good opportunity to gain and practise data engineering skills through new projects this year. My employer is fully behind me learning this and has offered to pay for some courses - as long as I do the research and find one.&lt;/p&gt;\n\n&lt;p&gt;I am looking for any recommendations that will take me from basically nothing (I\u2019m comfortable with SQL and Python but no warehousing, cloud, pipeline experience) and give me the tools required to use in new projects in my role that will eventually help me fully transition.&lt;/p&gt;\n\n&lt;p&gt;Any free course/book recommendations are also appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;Edit: the wiki has some good learning resources but I\u2019m looking for sort of an all-encompassing course with lectures, exercises etc. and ideally not an inordinate cost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r1xk3", "is_robot_indexable": true, "report_reasons": null, "author": "hektar9987", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r1xk3/course_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r1xk3/course_recommendations/", "subreddit_subscribers": 88185, "created_utc": 1675275919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For me, analyze the data set, create the data models, define the architecture, set up the environment, design the pipelines, deploy , migrate to production. I have listed the phases but having a hard time to estimate the efforts, I know the complexity might be the factor which is the key. But what could be the worst case scenarios.", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you crafted a project plan for big data ? What are the parameters that were considered ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10r0w1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675273542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me, analyze the data set, create the data models, define the architecture, set up the environment, design the pipelines, deploy , migrate to production. I have listed the phases but having a hard time to estimate the efforts, I know the complexity might be the factor which is the key. But what could be the worst case scenarios.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10r0w1g", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10r0w1g/have_you_crafted_a_project_plan_for_big_data_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10r0w1g/have_you_crafted_a_project_plan_for_big_data_what/", "subreddit_subscribers": 88185, "created_utc": 1675273542.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}