{"kind": "Listing", "data": {"after": "t3_10tj322", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Here is the link to the article: https://towardsdatascience.com/today-i-quit-data-sciences-here-are-7-reasons-why-15c29e51d032", "author_fullname": "t2_mv20pew7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectations for Data Scientists, a very interesting perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_10tnpsw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 264, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 264, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QkJnpthDPFBmmUR3V94UIbp1ZfZ2xZ1wSW30oz5-9dw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675535970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the link to the article: &lt;a href=\"https://towardsdatascience.com/today-i-quit-data-sciences-here-are-7-reasons-why-15c29e51d032\"&gt;https://towardsdatascience.com/today-i-quit-data-sciences-here-are-7-reasons-why-15c29e51d032&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/j6jjlhvoc9ga1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?auto=webp&amp;v=enabled&amp;s=723856c1f3a56e4f57869e84c989d5046ccfe54d", "width": 1179, "height": 1637}, "resolutions": [{"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a2fdfcd36388505ebc77874a2573ed827abc7c3", "width": 108, "height": 149}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f02a59fab63b11a67f9fa8e9f651f2eb95c82bf", "width": 216, "height": 299}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b89ae0e07f114d4e669dc55221599cf6b40d600", "width": 320, "height": 444}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ea54189da0b48e9084736b2e2bd7308d53b4c4e", "width": 640, "height": 888}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e44e0dfdf6d476c7a066a556df5d689dca96f693", "width": 960, "height": 1332}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=914eed27bbee9e3dcf13d3587831c89d21f2b566", "width": 1080, "height": 1499}], "variants": {}, "id": "ALeTGh-u0ujUsoTVmsd6yjnZOItqRA0QfPgcJq3IeKw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tnpsw", "is_robot_indexable": true, "report_reasons": null, "author": "Calm_Inky", "discussion_type": null, "num_comments": 93, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tnpsw/expectations_for_data_scientists_a_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/j6jjlhvoc9ga1.jpg", "subreddit_subscribers": 844838, "created_utc": 1675535970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Has anyone experienced this? I'm 2 weeks into a new job and had a meeting with my manager where he said he was concerned I was working slowly.  However I've finished the two tasks he's assigned me each a day early..", "author_fullname": "t2_2d6rhe2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Completing Tasks before the finish date but manager says I'm slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tjqwi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 218, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 218, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675526359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone experienced this? I&amp;#39;m 2 weeks into a new job and had a meeting with my manager where he said he was concerned I was working slowly.  However I&amp;#39;ve finished the two tasks he&amp;#39;s assigned me each a day early..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tjqwi", "is_robot_indexable": true, "report_reasons": null, "author": "HappyKoalaCub", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tjqwi/completing_tasks_before_the_finish_date_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tjqwi/completing_tasks_before_the_finish_date_but/", "subreddit_subscribers": 844838, "created_utc": 1675526359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been promoted twice to manager now of data science and I find myself in meetings talking more than I spend (if any) working on technical stuff\n\nIs this normal? I get paid almost $200k to say stuff \ud83e\udd14", "author_fullname": "t2_aji4iba3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal for data scientists to move to non technical roles as they move up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10trfwo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 133, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 133, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675545142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been promoted twice to manager now of data science and I find myself in meetings talking more than I spend (if any) working on technical stuff&lt;/p&gt;\n\n&lt;p&gt;Is this normal? I get paid almost $200k to say stuff \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10trfwo", "is_robot_indexable": true, "report_reasons": null, "author": "whowasphones", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10trfwo/is_it_normal_for_data_scientists_to_move_to_non/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10trfwo/is_it_normal_for_data_scientists_to_move_to_non/", "subreddit_subscribers": 844838, "created_utc": 1675545142.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work for a small engineering firm. I have been tasked by my CEO to train an AI to solve what is essentially a regression problem (although he doesn't know that, he just wants it to \"make predictions.\" AI/ML is not his expertise). There are only 4 features (all numerical) to this dataset, but unfortunately there are also only 25 samples. Collecting test samples for this application is expensive, and no relevant public data exists. In a few months, we should be able to collect 25-30 more samples. There will not be another chance after that to collect more data before the contract ends. It also doesn't help that I'm not even sure we can trust that the data we do have was collected properly (there are some serious anomalies) but that's besides the point I guess.\n\nI've tried explaining to my CEO why this is extremely difficult to work with and why it is hard to trust the predictions of the model. He says that we get paid to do the impossible. I cannot seem to convince him or get him to understand how absurdly small 25 samples is for training an AI model. He originally wanted us to use a deep neural net. Right now I'm trying a simple ANN (mostly to placate him) and also a support vector machine. \n\nAny advice on how to handle this, whether technically or professionally? Are there better models or any standard practices for when working with such limited data? Any way I can explain to my boss when this inevitably fails why it's not my fault?", "author_fullname": "t2_ll6fgzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with extremely limited data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10u61v7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675579221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a small engineering firm. I have been tasked by my CEO to train an AI to solve what is essentially a regression problem (although he doesn&amp;#39;t know that, he just wants it to &amp;quot;make predictions.&amp;quot; AI/ML is not his expertise). There are only 4 features (all numerical) to this dataset, but unfortunately there are also only 25 samples. Collecting test samples for this application is expensive, and no relevant public data exists. In a few months, we should be able to collect 25-30 more samples. There will not be another chance after that to collect more data before the contract ends. It also doesn&amp;#39;t help that I&amp;#39;m not even sure we can trust that the data we do have was collected properly (there are some serious anomalies) but that&amp;#39;s besides the point I guess.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried explaining to my CEO why this is extremely difficult to work with and why it is hard to trust the predictions of the model. He says that we get paid to do the impossible. I cannot seem to convince him or get him to understand how absurdly small 25 samples is for training an AI model. He originally wanted us to use a deep neural net. Right now I&amp;#39;m trying a simple ANN (mostly to placate him) and also a support vector machine. &lt;/p&gt;\n\n&lt;p&gt;Any advice on how to handle this, whether technically or professionally? Are there better models or any standard practices for when working with such limited data? Any way I can explain to my boss when this inevitably fails why it&amp;#39;s not my fault?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u61v7", "is_robot_indexable": true, "report_reasons": null, "author": "CyanDean", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u61v7/working_with_extremely_limited_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10u61v7/working_with_extremely_limited_data/", "subreddit_subscribers": 844838, "created_utc": 1675579221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How hard/easy will it be for me to get a data science job having this under my belt:\n\n\\- 2 years working as a Data Analyst\n\n\\- 2 years as Computational biologist in undergrad\n\n\\- Masters in Data Science", "author_fullname": "t2_6xabempy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some perspective (related to the DS job market):", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tnx8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675536470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How hard/easy will it be for me to get a data science job having this under my belt:&lt;/p&gt;\n\n&lt;p&gt;- 2 years working as a Data Analyst&lt;/p&gt;\n\n&lt;p&gt;- 2 years as Computational biologist in undergrad&lt;/p&gt;\n\n&lt;p&gt;- Masters in Data Science&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tnx8m", "is_robot_indexable": true, "report_reasons": null, "author": "marjose2", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tnx8m/i_need_some_perspective_related_to_the_ds_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tnx8m/i_need_some_perspective_related_to_the_ds_job/", "subreddit_subscribers": 844838, "created_utc": 1675536470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to see which house prices in my dataset are outliers based on other feature columns like sqft, waterfront or year built. \n\nSome of these are more important and play significant role on deciding if a data point is an outlier. \n\nWhat is the logic/literature behind this? How can I go about building this code?", "author_fullname": "t2_m826ekr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multivariate Outlier Detection in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10u7wfw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675586319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to see which house prices in my dataset are outliers based on other feature columns like sqft, waterfront or year built. &lt;/p&gt;\n\n&lt;p&gt;Some of these are more important and play significant role on deciding if a data point is an outlier. &lt;/p&gt;\n\n&lt;p&gt;What is the logic/literature behind this? How can I go about building this code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u7wfw", "is_robot_indexable": true, "report_reasons": null, "author": "Utterizi", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u7wfw/multivariate_outlier_detection_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10u7wfw/multivariate_outlier_detection_in_python/", "subreddit_subscribers": 844838, "created_utc": 1675586319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I read before on Reddit as well as on blog posts about data science professionals landing a job and thinking that they finally get to apply their skills to do something cool, when they had to instead do analysis on excel. \n\nAlso data scientists in some companies who aren\u2019t sure what goal they are hired to achieve. Or they don\u2019t even have the right data to work on. Or their work never makes it to production because of some reason or another. And they get disillusioned.\n\nRecently I came across this post on data science in finance. And the description looks pretty interesting. \n\nhttps://careerfoundry.com/en/blog/data-analytics/data-science-in-finance/\n\nAre any current Data science professionals who are in the financial industry. Who can shed some light on this? \n\nIs the work really as described? Or is it better? \n\nDo you get interesting problems to work on? \n\nDo you feel you get an opportunity to make an impact with your work? \n\nDo your models make it to production?", "author_fullname": "t2_kxuu98cn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do data scientists in the financial industry really get to do this cool stuff?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10u8jio", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675588918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I read before on Reddit as well as on blog posts about data science professionals landing a job and thinking that they finally get to apply their skills to do something cool, when they had to instead do analysis on excel. &lt;/p&gt;\n\n&lt;p&gt;Also data scientists in some companies who aren\u2019t sure what goal they are hired to achieve. Or they don\u2019t even have the right data to work on. Or their work never makes it to production because of some reason or another. And they get disillusioned.&lt;/p&gt;\n\n&lt;p&gt;Recently I came across this post on data science in finance. And the description looks pretty interesting. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://careerfoundry.com/en/blog/data-analytics/data-science-in-finance/\"&gt;https://careerfoundry.com/en/blog/data-analytics/data-science-in-finance/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Are any current Data science professionals who are in the financial industry. Who can shed some light on this? &lt;/p&gt;\n\n&lt;p&gt;Is the work really as described? Or is it better? &lt;/p&gt;\n\n&lt;p&gt;Do you get interesting problems to work on? &lt;/p&gt;\n\n&lt;p&gt;Do you feel you get an opportunity to make an impact with your work? &lt;/p&gt;\n\n&lt;p&gt;Do your models make it to production?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?auto=webp&amp;v=enabled&amp;s=b1f342a687094de675f38a3e1341bf42c650e76d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4727ec8b4b2e5fc969314fc8266ea394739d8b72", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=308b97f9105b3d61c1643e7574f2d47ab264a8f8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63f7f446d229be98f5fe796d53520d6fee17a829", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=893f0a3204b4464fd58384ebee7bc9c1412a49ab", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90b27af958090d65f4f044916ece584e64c0d794", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0UoHBT4PuSsR_Vm260X6fpPbhrlkDLmvfR0c6vik5YY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb3198f892353fe206a82b36e271cb6653aff306", "width": 1080, "height": 540}], "variants": {}, "id": "pxpxKEZtpLgEYvmWS0rEj_u-yqmAkUOclPNab-nMANI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u8jio", "is_robot_indexable": true, "report_reasons": null, "author": "lnfrarad", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u8jio/do_data_scientists_in_the_financial_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10u8jio/do_data_scientists_in_the_financial_industry/", "subreddit_subscribers": 844838, "created_utc": 1675588918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There are a lot of things going on in the field, how to you keep track of all the new stuff coming up. \n\nMy main resource at the moment is my linkedIn feed with focused research after finding something interesting.", "author_fullname": "t2_ds7pgpee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staying up to date after graduation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tu44b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675551885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are a lot of things going on in the field, how to you keep track of all the new stuff coming up. &lt;/p&gt;\n\n&lt;p&gt;My main resource at the moment is my linkedIn feed with focused research after finding something interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tu44b", "is_robot_indexable": true, "report_reasons": null, "author": "Tukdu", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tu44b/staying_up_to_date_after_graduation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tu44b/staying_up_to_date_after_graduation/", "subreddit_subscribers": 844838, "created_utc": 1675551885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I've watched a few tutorials on topic modeling in Python, with some open source libraries (gensim, spacy, etc). What I want to be able to do is tag a large group of reviews (such as yelp reviews) with one or more appropriately named topics. In the [above mentioned tutorials](https://youtu.be/UEn3xHNBXJU?t=867) I see the results as clusters that contain common terms, but I don't yet see how to go about adding an appropriate name to these clusters, and then ultimately assigning this name, or tag, back to individual reviews. How do I get from the initial analysis (LDA or what have you), to actually tagging a review with one or more appropriate categories? For example, a review talking about how expensive a hamburger is, might get assigned a \"price\" tag/category. Maybe LDA isn't the right method? Thanks for your tips and/or recommended tutorials!", "author_fullname": "t2_z075s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NLP: naming topics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tr7zr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675544875.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675544583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;ve watched a few tutorials on topic modeling in Python, with some open source libraries (gensim, spacy, etc). What I want to be able to do is tag a large group of reviews (such as yelp reviews) with one or more appropriately named topics. In the &lt;a href=\"https://youtu.be/UEn3xHNBXJU?t=867\"&gt;above mentioned tutorials&lt;/a&gt; I see the results as clusters that contain common terms, but I don&amp;#39;t yet see how to go about adding an appropriate name to these clusters, and then ultimately assigning this name, or tag, back to individual reviews. How do I get from the initial analysis (LDA or what have you), to actually tagging a review with one or more appropriate categories? For example, a review talking about how expensive a hamburger is, might get assigned a &amp;quot;price&amp;quot; tag/category. Maybe LDA isn&amp;#39;t the right method? Thanks for your tips and/or recommended tutorials!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jVTARUg3iTtvmYrVAujuKvGg4Gzsxj-WRpJgwnO7_XI.jpg?auto=webp&amp;v=enabled&amp;s=3658bcd4877612422d23235b8db678090d990d6a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/jVTARUg3iTtvmYrVAujuKvGg4Gzsxj-WRpJgwnO7_XI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69631c225060a3f0e6103f258394901054dfa227", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/jVTARUg3iTtvmYrVAujuKvGg4Gzsxj-WRpJgwnO7_XI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d80d05728c007c6050747177975e48bc6878803", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/jVTARUg3iTtvmYrVAujuKvGg4Gzsxj-WRpJgwnO7_XI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a047e862301da93e9c39613205b9a2b505c09166", "width": 320, "height": 240}], "variants": {}, "id": "pi2z2Pelp3SX_k0jIvGaPYOqkKb7lrZb4a1lg9essRw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tr7zr", "is_robot_indexable": true, "report_reasons": null, "author": "mogla", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tr7zr/nlp_naming_topics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tr7zr/nlp_naming_topics/", "subreddit_subscribers": 844838, "created_utc": 1675544583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a problem where I'm looking at API calls and downstream impact. Not really a specific question to answer, but predicting performance of the calls based on path through the audit trail would be interesting to do. Or, anomaly detection (i.e, this API is defective in 10% of transactions)  \n\n\nI have thought of two approaches. One of them is regarding the data contained within the request/response payloads. The issue is, it's all text based, or changes depending on the specific API, so the data transformation aspect is very tough. Encoding one API's response to compare to a different API's response would be challenging. Perhaps there is an opportunity for deep learning here.  \n\n\nThe other approach I'm thinking about, is treating this as a graph problem. The majority of these transactions will form a tree-like structure, but some of them don't have a true root, so its more of a directed, acyclic graph (think it's called a polytree?). My approach would be to use graph theory principals such as degrees, closeness, vertex distance, etc. as features instead of parsing the very dynamic payloads. Since these are rules-based systems, I would love to be able to model the underlying relationships between these calls, that way when one transaction chain doesn't match the model, it's likely an anomaly.   \n\n\nDoes this make sense to do? Has anyone here done something similar? Any recommended literature? I've tried googling it, but when you search \"machine learning approach to tree-based systems\", you get a lot of papers or articles referencing tree-based ML models, which is not exactly what I'm looking for yet. Thoughts?", "author_fullname": "t2_1wcsnihu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applying Graph Theory in Machine Learning problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10to95b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675537246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a problem where I&amp;#39;m looking at API calls and downstream impact. Not really a specific question to answer, but predicting performance of the calls based on path through the audit trail would be interesting to do. Or, anomaly detection (i.e, this API is defective in 10% of transactions)  &lt;/p&gt;\n\n&lt;p&gt;I have thought of two approaches. One of them is regarding the data contained within the request/response payloads. The issue is, it&amp;#39;s all text based, or changes depending on the specific API, so the data transformation aspect is very tough. Encoding one API&amp;#39;s response to compare to a different API&amp;#39;s response would be challenging. Perhaps there is an opportunity for deep learning here.  &lt;/p&gt;\n\n&lt;p&gt;The other approach I&amp;#39;m thinking about, is treating this as a graph problem. The majority of these transactions will form a tree-like structure, but some of them don&amp;#39;t have a true root, so its more of a directed, acyclic graph (think it&amp;#39;s called a polytree?). My approach would be to use graph theory principals such as degrees, closeness, vertex distance, etc. as features instead of parsing the very dynamic payloads. Since these are rules-based systems, I would love to be able to model the underlying relationships between these calls, that way when one transaction chain doesn&amp;#39;t match the model, it&amp;#39;s likely an anomaly.   &lt;/p&gt;\n\n&lt;p&gt;Does this make sense to do? Has anyone here done something similar? Any recommended literature? I&amp;#39;ve tried googling it, but when you search &amp;quot;machine learning approach to tree-based systems&amp;quot;, you get a lot of papers or articles referencing tree-based ML models, which is not exactly what I&amp;#39;m looking for yet. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10to95b", "is_robot_indexable": true, "report_reasons": null, "author": "tigerclaw468", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10to95b/applying_graph_theory_in_machine_learning_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10to95b/applying_graph_theory_in_machine_learning_problem/", "subreddit_subscribers": 844838, "created_utc": 1675537246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/10uefx9)", "author_fullname": "t2_cr8bri51", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you get into datascience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10uefx9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675609185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10uefx9\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "10uefx9", "is_robot_indexable": true, "report_reasons": null, "author": "PPC_GodToBe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1676213985763, "options": [{"text": "Directly by university studies", "id": "21440823"}, {"text": "Emerged from an area of CS", "id": "21440824"}, {"text": "Autodidact", "id": "21440825"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 37, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10uefx9/how_did_you_get_into_datascience/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/10uefx9/how_did_you_get_into_datascience/", "subreddit_subscribers": 844838, "created_utc": 1675609185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_cdrji8bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "isn't this just too much for a take home assignment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 128, "top_awarded_type": null, "hide_score": true, "name": "t3_10ueevu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XnRi7AnPeriE520osDIV_l4HSuSyW6V4Pqmgzwssu3U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675609107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jdtrxsz2efga1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?auto=webp&amp;v=enabled&amp;s=0985c44cfb4efe1d0d8d1b2de411f169d1397444", "width": 1080, "height": 988}, "resolutions": [{"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9778e6b16ef078d8351f11b5bc596c767f347c61", "width": 108, "height": 98}, {"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e324d275de5af34fb37faf87ebba87c31511359e", "width": 216, "height": 197}, {"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=877115ce24bf001cacfab00f83aa24e614986e24", "width": 320, "height": 292}, {"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c83730ae4f0e3084cc0fd0199364a1152fbdba7a", "width": 640, "height": 585}, {"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=796a00f9173ceee148da202d9b210376e44d156d", "width": 960, "height": 878}, {"url": "https://preview.redd.it/jdtrxsz2efga1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66778d27c72b94c31cc1cf62b358cfa6a6060739", "width": 1080, "height": 988}], "variants": {}, "id": "E6xrcS-8woaqKln1AI0fuEzDUlz5ZTUf60J2TsPbvy4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10ueevu", "is_robot_indexable": true, "report_reasons": null, "author": "questionaboutpsy", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10ueevu/isnt_this_just_too_much_for_a_take_home_assignment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jdtrxsz2efga1.jpg", "subreddit_subscribers": 844838, "created_utc": 1675609107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am learning to use SQL server. I am trying to create a catalog but get the following highlighted error. Have tried searching for the solution couldn't solve it. Please HELP.\n\nhttps://preview.redd.it/4muv1k1brcga1.png?width=1689&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4fe922dea4efa6fe772f498751deda348c86755", "author_fullname": "t2_7d59jl16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with MS SQL Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4muv1k1brcga1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef1f77a3df412e367407909de948c593ce004d7f"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=897522d4750e603fa0e99ee04c22facaf1fc2b53"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0e31d574314b709a63102eb4d6ee8977c62b2e4"}, {"y": 281, "x": 640, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1767a0cec2de7ffd4b9437c9b5315b66d8e0fb5c"}, {"y": 421, "x": 960, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8da764bbfe7f013436d7e79c101aab3e907f112f"}, {"y": 474, "x": 1080, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ac1815401aa272a9385ae472ff1b93775b3acc9"}], "s": {"y": 742, "x": 1689, "u": "https://preview.redd.it/4muv1k1brcga1.png?width=1689&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4fe922dea4efa6fe772f498751deda348c86755"}, "id": "4muv1k1brcga1"}}, "name": "t3_10ua3yf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WvJ-7fpbXA6W6HNNu2RWjt0i9hzdATaA0DJOPllYGhU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675595270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning to use SQL server. I am trying to create a catalog but get the following highlighted error. Have tried searching for the solution couldn&amp;#39;t solve it. Please HELP.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4muv1k1brcga1.png?width=1689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c4fe922dea4efa6fe772f498751deda348c86755\"&gt;https://preview.redd.it/4muv1k1brcga1.png?width=1689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c4fe922dea4efa6fe772f498751deda348c86755&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10ua3yf", "is_robot_indexable": true, "report_reasons": null, "author": "yujshr", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10ua3yf/need_help_with_ms_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10ua3yf/need_help_with_ms_sql_server/", "subreddit_subscribers": 844838, "created_utc": 1675595270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently we're just using google sheets to document stuff and while its working, it's been a hassle keeping it up. A miss in the updates can break the whole thing, which can be frustrating rather than focusing on my tests. Also it becomes tedious too when someone checks up on it and it takes a long time to backtrack on everything.\n\nWe've tried using the timeline feature or tasking in Asana but its quite tedious as well to track and keep tickets or are we just doing it wrong? Lately we've been planning on trying out to setup like a kanban thing but just wondering if there are any best practices for this?", "author_fullname": "t2_7u37sy3v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What practices/tools/platforms are best on documenting undergoing tests (AB tests, and etc.) and strategies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10u9kkw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675593106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we&amp;#39;re just using google sheets to document stuff and while its working, it&amp;#39;s been a hassle keeping it up. A miss in the updates can break the whole thing, which can be frustrating rather than focusing on my tests. Also it becomes tedious too when someone checks up on it and it takes a long time to backtrack on everything.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve tried using the timeline feature or tasking in Asana but its quite tedious as well to track and keep tickets or are we just doing it wrong? Lately we&amp;#39;ve been planning on trying out to setup like a kanban thing but just wondering if there are any best practices for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u9kkw", "is_robot_indexable": true, "report_reasons": null, "author": "Aggressive-Pup-28", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u9kkw/what_practicestoolsplatforms_are_best_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10u9kkw/what_practicestoolsplatforms_are_best_on/", "subreddit_subscribers": 844838, "created_utc": 1675593106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m assuming network science and data science share some ties so asking here. I know it\u2019s a newer field but I\u2019m still surprised it isn\u2019t more prominent. Ex there\u2019s not even a functioning subreddit for it", "author_fullname": "t2_10l3hp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know why network science isn\u2019t a more popular discipline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tyivq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675563981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m assuming network science and data science share some ties so asking here. I know it\u2019s a newer field but I\u2019m still surprised it isn\u2019t more prominent. Ex there\u2019s not even a functioning subreddit for it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tyivq", "is_robot_indexable": true, "report_reasons": null, "author": "tropicalparzival", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tyivq/anyone_know_why_network_science_isnt_a_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tyivq/anyone_know_why_network_science_isnt_a_more/", "subreddit_subscribers": 844838, "created_utc": 1675563981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ukrwuk8c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone got an example of training a foundational model using python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ucm8c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675604162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10ucm8c", "is_robot_indexable": true, "report_reasons": null, "author": "Lumpy_Tangerine_4208", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10ucm8c/has_anyone_got_an_example_of_training_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10ucm8c/has_anyone_got_an_example_of_training_a/", "subreddit_subscribers": 844838, "created_utc": 1675604162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Autonomous Driving Off-Road | Swaayatt Robots | Dense Fog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_10u8ucx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_3pnnjkp9", "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Autonomous Driving Off-Roads Through Dense Fog in India", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "author_name": "Swaayatt Robots", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/JrpHdtgT0dI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@swaayattrobots1827"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10u8ucx", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VvTgbBxthoCNezCQrEB3r5bHbgN7WdNfDO6_Q-iHFJw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "AutonomousVehicles", "selftext": "", "author_fullname": "t2_3pnnjkp9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Autonomous Driving Off-Road | Swaayatt Robots | Dense Fog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/AutonomousVehicles", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_10u8pd0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Autonomous Driving Off-Roads Through Dense Fog in India", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "author_name": "Swaayatt Robots", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/JrpHdtgT0dI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@swaayattrobots1827"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/10u8pd0", "height": 200}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "image", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "mod_note": null, "created": 1675589592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/JrpHdtgT0dI", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?auto=webp&amp;v=enabled&amp;s=b9014d85f9a9d9b5000fddc6bc26981cb43e8daa", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bcf0719e3cf9ee62845195d3bf62fed78adefe6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=386cbc0e54b0e3346cea16f7027a4037ff2aa20a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e26531cc12944796291e47fb79072657a687f667", "width": 320, "height": 240}], "variants": {}, "id": "cfNJ612Bi1hKtEAIxesv6TXtsrP-lsK2EQfhPpcvDVE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_320vf", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u8pd0", "is_robot_indexable": true, "report_reasons": null, "author": "shani_786", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/AutonomousVehicles/comments/10u8pd0/autonomous_driving_offroad_swaayatt_robots_dense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/JrpHdtgT0dI", "subreddit_subscribers": 3307, "created_utc": 1675589592.0, "num_crossposts": 6, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Autonomous Driving Off-Roads Through Dense Fog in India", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "author_name": "Swaayatt Robots", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/JrpHdtgT0dI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@swaayattrobots1827"}}, "is_video": false}], "created": 1675590160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/JrpHdtgT0dI", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?auto=webp&amp;v=enabled&amp;s=b9014d85f9a9d9b5000fddc6bc26981cb43e8daa", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bcf0719e3cf9ee62845195d3bf62fed78adefe6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=386cbc0e54b0e3346cea16f7027a4037ff2aa20a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/xUxELlD0SAsXjs68etGHv003jJ0z3GXXa-I2ZkeY5dY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e26531cc12944796291e47fb79072657a687f667", "width": 320, "height": 240}], "variants": {}, "id": "cfNJ612Bi1hKtEAIxesv6TXtsrP-lsK2EQfhPpcvDVE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u8ucx", "is_robot_indexable": true, "report_reasons": null, "author": "shani_786", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_10u8pd0", "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u8ucx/autonomous_driving_offroad_swaayatt_robots_dense/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/JrpHdtgT0dI", "subreddit_subscribers": 844838, "created_utc": 1675590160.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Autonomous Driving Off-Roads Through Dense Fog in India", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/JrpHdtgT0dI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Autonomous Driving Off-Roads Through Dense Fog in India\"&gt;&lt;/iframe&gt;", "author_name": "Swaayatt Robots", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/JrpHdtgT0dI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@swaayattrobots1827"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have 6 months of internship experience, 3 months as a Data scientist and 3 months as an ML researcher. I have done courses in deep learning which is my area of expertise mainly. I am good with tensorflow, and i know SQL and python well.\n\nI dont seem to crack any data science or machine learning interviews though. What are a few things you could suggest that I should add to my skillset to get a job as a Data scientist? \n\nI have an Aerospace bachelor's degree from a top college and not a data science one. The only data science education i got was from online courses. Please drop some suggestions.", "author_fullname": "t2_yn13es6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on how to proceed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10u7spk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675585895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 6 months of internship experience, 3 months as a Data scientist and 3 months as an ML researcher. I have done courses in deep learning which is my area of expertise mainly. I am good with tensorflow, and i know SQL and python well.&lt;/p&gt;\n\n&lt;p&gt;I dont seem to crack any data science or machine learning interviews though. What are a few things you could suggest that I should add to my skillset to get a job as a Data scientist? &lt;/p&gt;\n\n&lt;p&gt;I have an Aerospace bachelor&amp;#39;s degree from a top college and not a data science one. The only data science education i got was from online courses. Please drop some suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u7spk", "is_robot_indexable": true, "report_reasons": null, "author": "Nalayak_Launda", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u7spk/need_advice_on_how_to_proceed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10u7spk/need_advice_on_how_to_proceed/", "subreddit_subscribers": 844838, "created_utc": 1675585895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello there, I am trying to detect a fraud detection model which outputs risk as Low Medium or High, I have a customers id in one data frame and in another data frame i have their data that from which customer (source) id to which (target) how much money 'emt' is being transferred. Now I want to drop customer id from the initial data frame and add a new column containing a series of transaction for both sources and targets. How do i do this and is there a better way to do this?", "author_fullname": "t2_amimzgo5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to process additional data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10u69ua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675580037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there, I am trying to detect a fraud detection model which outputs risk as Low Medium or High, I have a customers id in one data frame and in another data frame i have their data that from which customer (source) id to which (target) how much money &amp;#39;emt&amp;#39; is being transferred. Now I want to drop customer id from the initial data frame and add a new column containing a series of transaction for both sources and targets. How do i do this and is there a better way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10u69ua", "is_robot_indexable": true, "report_reasons": null, "author": "Old_Stick_9560", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10u69ua/how_to_process_additional_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10u69ua/how_to_process_additional_data/", "subreddit_subscribers": 844838, "created_utc": 1675580037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI was tasked with finding videos for product releases for several companies from a very large Excel file with headlines of new articles. The Excel has only the headline of the product release article &amp; the company names. there are about 10,000 headlines.\n\nHow can I approach this problem and ultimately automate it?", "author_fullname": "t2_4jekmmq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding Products Advertisement Videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10txeas", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675560727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I was tasked with finding videos for product releases for several companies from a very large Excel file with headlines of new articles. The Excel has only the headline of the product release article &amp;amp; the company names. there are about 10,000 headlines.&lt;/p&gt;\n\n&lt;p&gt;How can I approach this problem and ultimately automate it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10txeas", "is_robot_indexable": true, "report_reasons": null, "author": "mouayedlajnef", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10txeas/finding_products_advertisement_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10txeas/finding_products_advertisement_videos/", "subreddit_subscribers": 844838, "created_utc": 1675560727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Non-data-scientist here.\n\nI came across this huggingface webpage ([https://huggingface.co/docs/transformers/main/model\\_doc/biogpt](https://huggingface.co/docs/transformers/main/model_doc/biogpt)) and saw this bullet point:\n\n&gt;BioGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\nWhat does that mean?\n\nTIA.", "author_fullname": "t2_rvu8sqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: \"to pad the inputs on the right rather than the left.\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10trq62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675545850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Non-data-scientist here.&lt;/p&gt;\n\n&lt;p&gt;I came across this huggingface webpage (&lt;a href=\"https://huggingface.co/docs/transformers/main/model_doc/biogpt\"&gt;https://huggingface.co/docs/transformers/main/model_doc/biogpt&lt;/a&gt;) and saw this bullet point:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;BioGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;What does that mean?&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?auto=webp&amp;v=enabled&amp;s=09310ab6255bb753ca4a771c0314bc214f78d2ac", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d98fc28f03459191933e04a79947277468884240", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63df823a11fd89a126d46aa7fe24a61e8802e772", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=320261cc71f0e3ceb4e3bf8a0d13183bec39c51d", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1ad1e1c2f09bcd80fcb1946c0b9fd262bd87956", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d11d529124019d86e37cdf83869dc778034a7213", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/Bixm6H31yqw0RCcD8LB0e8eIdtJeMUaF4N5ZipM_BQY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4946d1e6be18d75acabb03d041038e3d4fa8e686", "width": 1080, "height": 583}], "variants": {}, "id": "jfeVG47nZdEkz9kXfW1CcS-Sy8l4DXGb9JErx6bLKfU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10trq62", "is_robot_indexable": true, "report_reasons": null, "author": "babysharkdoodoodoo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10trq62/eli5_to_pad_the_inputs_on_the_right_rather_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10trq62/eli5_to_pad_the_inputs_on_the_right_rather_than/", "subreddit_subscribers": 844838, "created_utc": 1675545850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TL; DR:  I explore a hypothetical scenario where widget makers of varying experience are compared according to defect rate over a 5 year window.  To make things comparable (accounting for the experience problem), I inquire whether using a weighted moving average is a good strategy for balancing the problem of experience out or whether there is a better technique/solution to employ.  \n\n&amp;#x200B;\n\nThe Scenario:\n\n* Assume you were to look at the historical performance of  several widget makers who manually produce widgets.  These widget makers could be active employees at present  or left at some point during the window.\n* You want to compare the historical performance of the widget makers in terms of defect rate over the last 5 years (2018-2022).  My definition of defect rate is simply the  total number of defects/number of widgets produced (inclusive of defects)\n* Some makers have decades of experience while other makers only have 1 year of experience (minimum required for this analysis)\n* The original data is by year (2018,2019,...).  Let's assume Maker A, who started their career over a decade ago, made widgets over the entire window. Maker B has 3 years of experience but was let go sometime before 2022.  Maker C was recently hired and worked all of 2022.\n* To make things more intuitive (hopefully) and comparable, I converted the table from calendar years to years of experience within the time window. So Year 1 doesn't necessarily translate to 2018, it's simply represents the first year worked  in that time window. See table below.\n\n&amp;#x200B;\n\n|Maker|Year 1|2|3|4|5|\n|:-|:-|:-|:-|:-|:-|\n|A|2/17|2/15|3/11|1/8|2/10|\n|B|5/15|3/12|3/13|||\n|C|2/7|||||\n\n&amp;#x200B;\n\n*  Their defect rate is recorded over the last 5 years. For example Maker A produced 17 widgets of which 2 were defective in the first year of the time window, then produced 15 widgets in which 2 were defective Year 2, and so on...  Note that demand could be variable for widget production.\n* Simply looking at the defect rate above, my calculations  show Maker A would have an overall defect rate of .25, B would have .35 and C would have .43.  The problem I see  is less experience penalizes more heavily than more experience; so if this were a bar graph the 'worst performers' would likely always be the least experienced and that may not be very fair or  informative and at worst, hiding poor performers with a lot of experience.\n\nSo, this implies that some sort of smoothing or weighting is needed in order to make things more comparable. I was wondering if it made sense to use a weighted moving average.  My weighted average would look like the following:\n\n&amp;#x200B;\n\n* Each year of the window has an arbitrary weight associated to it.  For this example we want to downweight that first year  and gradually increase weights later on:\n\n&amp;#x200B;\n\n|Year |1|2|3|4|5|\n|:-|:-|:-|:-|:-|:-|\n|Weight|.6|.7|.8|.9|1|\n\n&amp;#x200B;\n\n* I obtain the sum of the product of each year's weight by  the maker's  defect rate  for each year they worked divided by the sum of the weights for the length of time that the maker worked.  Clarifying this last part - Maker A's denominator would be the sum of all 5 weights, B would be the sum of 3 weights and C would be only the Year 1 weight\n* Using the weight and formulas mentioned, I show that A has a weighted average defect rate of .17, B has .43 and C has .29, suggesting that B is producing more defects relative to their experience compared to the other two.\n\nThis results make some intuitive sense to me as B continued their career with a higher rate of defects relative to A.  My concern is that 1) weighting is arbitrary  and that 2) this method might cause an inverse problem from the one I was originally attempting to solve - giving too much leniency to the inexperienced and overly penalizing experience. I understand there is no perfect answer here but wondering if this is a sound approach or there are better, more intuitive ways to approach this. \n\nTIA", "author_fullname": "t2_2nzhcx9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is This A Good Method for Weighting Varying Levels of Experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tl1jk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675529596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL; DR:  I explore a hypothetical scenario where widget makers of varying experience are compared according to defect rate over a 5 year window.  To make things comparable (accounting for the experience problem), I inquire whether using a weighted moving average is a good strategy for balancing the problem of experience out or whether there is a better technique/solution to employ.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The Scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Assume you were to look at the historical performance of  several widget makers who manually produce widgets.  These widget makers could be active employees at present  or left at some point during the window.&lt;/li&gt;\n&lt;li&gt;You want to compare the historical performance of the widget makers in terms of defect rate over the last 5 years (2018-2022).  My definition of defect rate is simply the  total number of defects/number of widgets produced (inclusive of defects)&lt;/li&gt;\n&lt;li&gt;Some makers have decades of experience while other makers only have 1 year of experience (minimum required for this analysis)&lt;/li&gt;\n&lt;li&gt;The original data is by year (2018,2019,...).  Let&amp;#39;s assume Maker A, who started their career over a decade ago, made widgets over the entire window. Maker B has 3 years of experience but was let go sometime before 2022.  Maker C was recently hired and worked all of 2022.&lt;/li&gt;\n&lt;li&gt;To make things more intuitive (hopefully) and comparable, I converted the table from calendar years to years of experience within the time window. So Year 1 doesn&amp;#39;t necessarily translate to 2018, it&amp;#39;s simply represents the first year worked  in that time window. See table below.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Maker&lt;/th&gt;\n&lt;th align=\"left\"&gt;Year 1&lt;/th&gt;\n&lt;th align=\"left\"&gt;2&lt;/th&gt;\n&lt;th align=\"left\"&gt;3&lt;/th&gt;\n&lt;th align=\"left\"&gt;4&lt;/th&gt;\n&lt;th align=\"left\"&gt;5&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/17&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/15&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/11&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;5/15&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/12&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/13&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;C&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/7&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Their defect rate is recorded over the last 5 years. For example Maker A produced 17 widgets of which 2 were defective in the first year of the time window, then produced 15 widgets in which 2 were defective Year 2, and so on...  Note that demand could be variable for widget production.&lt;/li&gt;\n&lt;li&gt;Simply looking at the defect rate above, my calculations  show Maker A would have an overall defect rate of .25, B would have .35 and C would have .43.  The problem I see  is less experience penalizes more heavily than more experience; so if this were a bar graph the &amp;#39;worst performers&amp;#39; would likely always be the least experienced and that may not be very fair or  informative and at worst, hiding poor performers with a lot of experience.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So, this implies that some sort of smoothing or weighting is needed in order to make things more comparable. I was wondering if it made sense to use a weighted moving average.  My weighted average would look like the following:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Each year of the window has an arbitrary weight associated to it.  For this example we want to downweight that first year  and gradually increase weights later on:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Year&lt;/th&gt;\n&lt;th align=\"left\"&gt;1&lt;/th&gt;\n&lt;th align=\"left\"&gt;2&lt;/th&gt;\n&lt;th align=\"left\"&gt;3&lt;/th&gt;\n&lt;th align=\"left\"&gt;4&lt;/th&gt;\n&lt;th align=\"left\"&gt;5&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Weight&lt;/td&gt;\n&lt;td align=\"left\"&gt;.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I obtain the sum of the product of each year&amp;#39;s weight by  the maker&amp;#39;s  defect rate  for each year they worked divided by the sum of the weights for the length of time that the maker worked.  Clarifying this last part - Maker A&amp;#39;s denominator would be the sum of all 5 weights, B would be the sum of 3 weights and C would be only the Year 1 weight&lt;/li&gt;\n&lt;li&gt;Using the weight and formulas mentioned, I show that A has a weighted average defect rate of .17, B has .43 and C has .29, suggesting that B is producing more defects relative to their experience compared to the other two.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This results make some intuitive sense to me as B continued their career with a higher rate of defects relative to A.  My concern is that 1) weighting is arbitrary  and that 2) this method might cause an inverse problem from the one I was originally attempting to solve - giving too much leniency to the inexperienced and overly penalizing experience. I understand there is no perfect answer here but wondering if this is a sound approach or there are better, more intuitive ways to approach this. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tl1jk", "is_robot_indexable": true, "report_reasons": null, "author": "outskirtsofparadise", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tl1jk/is_this_a_good_method_for_weighting_varying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tl1jk/is_this_a_good_method_for_weighting_varying/", "subreddit_subscribers": 844838, "created_utc": 1675529596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\n\nI'm considering getting a post grad credential from the list in the title and looking for input from others who have done/are doing similar.  \n\nMy Background: BS in Mgmt, Concentration Info Sys; UT MBA, Concentration IM/Tech Strategy.  I'm late career (mid 50), have been in high tech (HW) about 25 years and want to work another 4-7.  After that I would consult til tired.  \n\nCurrently facing tech layoff and looking, but no immediate financial pressure for 1 - 2 years. \n\nPrograms I'm considering:\n\n1. Newly announced UT Masters in AI. Would likely do an AI cert in the meanwhile or assumed pre-reqs (coding, linear algebra).  Sounds technical and not sure I'd get accepted.  est. \\~2 years to complete.\n2. GT Masters in Analytics, with a few added AI courses.  Would likely play catchup on the Micromasters to get that out of the way.  est. \\~2 years to complete.\n3. IBM or UT AI Cert and other as needed.  6-9 mos to complete.\n\nLikely would need a Udemy in Python coding in all 3 cases.  For those who have done, or looked at doing, any of these, what would you do/do differently?  \n\nAll input welcome.  Thanks!", "author_fullname": "t2_5vkcyd9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UT Masters in AI v. GT OMA v. an AI Cert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tru1d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675546131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering getting a post grad credential from the list in the title and looking for input from others who have done/are doing similar.  &lt;/p&gt;\n\n&lt;p&gt;My Background: BS in Mgmt, Concentration Info Sys; UT MBA, Concentration IM/Tech Strategy.  I&amp;#39;m late career (mid 50), have been in high tech (HW) about 25 years and want to work another 4-7.  After that I would consult til tired.  &lt;/p&gt;\n\n&lt;p&gt;Currently facing tech layoff and looking, but no immediate financial pressure for 1 - 2 years. &lt;/p&gt;\n\n&lt;p&gt;Programs I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Newly announced UT Masters in AI. Would likely do an AI cert in the meanwhile or assumed pre-reqs (coding, linear algebra).  Sounds technical and not sure I&amp;#39;d get accepted.  est. ~2 years to complete.&lt;/li&gt;\n&lt;li&gt;GT Masters in Analytics, with a few added AI courses.  Would likely play catchup on the Micromasters to get that out of the way.  est. ~2 years to complete.&lt;/li&gt;\n&lt;li&gt;IBM or UT AI Cert and other as needed.  6-9 mos to complete.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Likely would need a Udemy in Python coding in all 3 cases.  For those who have done, or looked at doing, any of these, what would you do/do differently?  &lt;/p&gt;\n\n&lt;p&gt;All input welcome.  Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tru1d", "is_robot_indexable": true, "report_reasons": null, "author": "bart_grewup", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tru1d/ut_masters_in_ai_v_gt_oma_v_an_ai_cert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tru1d/ut_masters_in_ai_v_gt_oma_v_an_ai_cert/", "subreddit_subscribers": 844838, "created_utc": 1675546131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_b71e9j7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happiness and Meaning in What We Do", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_10tkcts", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QUsQw0AQYHzN9jfM5MniTiDOMTzeUYYRu9QeubFh41U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675527852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "flowingdata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://flowingdata.com/2023/01/11/happiness-and-meaning/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?auto=webp&amp;v=enabled&amp;s=021a7d0111ed3a520890d19700459aaf9ef6ba71", "width": 2002, "height": 1384}, "resolutions": [{"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9077316aa3da2500d96d9faac26bdcb2b03e0d11", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=285f586d50982f85347f71360137e14269ffbc6f", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c49429ab851f7ba714eb6e292e2735ff87296bc3", "width": 320, "height": 221}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d319403f4d62209f1d22d7f9b2773269d5444e4", "width": 640, "height": 442}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cc6865ef5ff462da12de89fa39069dbd6f3af55", "width": 960, "height": 663}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8a0770c97242367e386083afaf4e3eefd3176a8", "width": 1080, "height": 746}], "variants": {}, "id": "44qKXOapwlucIy6lsY36iDrLifWLEcJhv2rdvR-x3x8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tkcts", "is_robot_indexable": true, "report_reasons": null, "author": "fchung", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tkcts/happiness_and_meaning_in_what_we_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://flowingdata.com/2023/01/11/happiness-and-meaning/", "subreddit_subscribers": 844838, "created_utc": 1675527852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm given a school assignment and one of the requirements is to visualize the important aspects and give meaningful realization of a real world dataset.\nSo I was wondering I someone could recommend me a dataset that is simple, real world, has some real use", "author_fullname": "t2_i3uokfwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please give data set recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tj322", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675524660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m given a school assignment and one of the requirements is to visualize the important aspects and give meaningful realization of a real world dataset.\nSo I was wondering I someone could recommend me a dataset that is simple, real world, has some real use&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tj322", "is_robot_indexable": true, "report_reasons": null, "author": "A_B_1_2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tj322/please_give_data_set_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tj322/please_give_data_set_recommendation/", "subreddit_subscribers": 844838, "created_utc": 1675524660.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}