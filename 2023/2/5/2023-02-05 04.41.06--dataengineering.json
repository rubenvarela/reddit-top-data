{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...\n\nBut I'm curious to know, what did ETL look like before this?\n\nWhat did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?\n\nSometimes I find tutorials like \"Build an ETL pipeline with Airflow and Pandas\" and I think \"Pandas, Really?\".\n\nIn other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?\n\nPlease do share your experience!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What did ETL look like before the \"modern data stack\" was a thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tjhve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675525724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m curious to know, what did ETL look like before this?&lt;/p&gt;\n\n&lt;p&gt;What did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?&lt;/p&gt;\n\n&lt;p&gt;Sometimes I find tutorials like &amp;quot;Build an ETL pipeline with Airflow and Pandas&amp;quot; and I think &amp;quot;Pandas, Really?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;In other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?&lt;/p&gt;\n\n&lt;p&gt;Please do share your experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tjhve", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 175, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/", "subreddit_subscribers": 88509, "created_utc": 1675525724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is **prohibited**. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. \n\nTo mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.", "author_fullname": "t2_eckrjl60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bundle million small json files and save to s3, later easy to process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t5mvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675487162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is &lt;strong&gt;prohibited&lt;/strong&gt;. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. &lt;/p&gt;\n\n&lt;p&gt;To mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t5mvd", "is_robot_indexable": true, "report_reasons": null, "author": "b-y-f", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "subreddit_subscribers": 88509, "created_utc": 1675487162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose", "author_fullname": "t2_7gkixkov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you're new to databases should you start with the book Database Design for Mere Mortals or SQL Queries for Mere Mortals or Head first with sql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tag8g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tag8g", "is_robot_indexable": true, "report_reasons": null, "author": "One_Valuable7049", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "subreddit_subscribers": 88509, "created_utc": 1675500898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi experts,\n\nWe are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I'm curious is there recommendation?\n\nFeature needed:\n- Can connect and query cloud database, especially BigQuery;\n\n- Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it's a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;\n\n- Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);\n\n- Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its \"AI\" but full table scan is really expensive;\n\n- Can setup threshold for the metrics, and send alerts to emails and slack channels;\n\nThat's all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don't know if it's good. It's definitely mature though, heard about it for years.\n\nThank you!", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask DE: What nice data quality monitoring tool are you using?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t4n5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675483912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts,&lt;/p&gt;\n\n&lt;p&gt;We are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I&amp;#39;m curious is there recommendation?&lt;/p&gt;\n\n&lt;p&gt;Feature needed:\n- Can connect and query cloud database, especially BigQuery;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it&amp;#39;s a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its &amp;quot;AI&amp;quot; but full table scan is really expensive;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can setup threshold for the metrics, and send alerts to emails and slack channels;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don&amp;#39;t know if it&amp;#39;s good. It&amp;#39;s definitely mature though, heard about it for years.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t4n5x", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "subreddit_subscribers": 88509, "created_utc": 1675483912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. \n\nI'm now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn't require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.\n\nI've been at my company for over a year now and mostly due to the outdated stack that I'm limited to in my work environment, I'm looking to transition to a different data engineering position. That way, I'm hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.\n\nThanks for any guidance in advance!", "author_fullname": "t2_x9ryi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Good Free Resources for Learning Data Warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t745z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675492344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn&amp;#39;t require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been at my company for over a year now and mostly due to the outdated stack that I&amp;#39;m limited to in my work environment, I&amp;#39;m looking to transition to a different data engineering position. That way, I&amp;#39;m hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any guidance in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10t745z", "is_robot_indexable": true, "report_reasons": null, "author": "papes_tv", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "subreddit_subscribers": 88509, "created_utc": 1675492344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined a huge company in November of last year. I switched careers from Data Science to engineering, so I do not know much about the typical DE environment. Previously, I worked together with one Data Engineer and I am now one of 9 in my team. However, it certainly is different from how I expected it to be. My expectations were that everything would be more structured. You get a task, there is a straightforward way to do it, and then you spend time implementing it. My current style of working is instead 90% searching where to put my 5 lines of code and then testing if this is really how it should be done.\n\nThe personal red flags are:\n\n1. Everything is exceptionally complex, from Airflow pipelines with 1000 lines config files to a vast Terraform multi repo (around 20 individual repos). They use a lot of internal libraries, most of them wrapping well-known libraries and adding some functionality. For example, the Airflow S3 hook was wrapped in a company internal one giving it like 30 more options to read data for specific use cases. It took me hours to figure this out because it's only documented inline and the file is around 5000 lines of code.\n2. There is one colleague who is the single source of truth. He has been in the company for 10 years and knows why things are implemented like they are. Others have been around 5 years but basically only refer to him. He is quite good at what he does, but I still find this hierarchy concerning. I understand that more experienced colleagues are asked for their opinion more often, but this goes as far as at least one colleague approving his pull requests without actually looking at them.\n3. Many colleagues know much about the existing infrastructure but fail to understand what's behind it or what they are doing. For example, a colleague from another department asked what kubectl port-forward is doing and a colleague responsible for our Kubernetes cluster replied, \"It gives your local user access rights to the cluster.\" This was the most extreme example, but I found many smaller ones confusing. For instance, I talked about writing a bash script for a simple renaming task and a colleague said: \"Why not implement it with Python? We shouldn't use languages that need additional installations on our servers\".\n\nWould you consider that red flags? What would you do to improve the situation, expecially the complexity of the existing code?", "author_fullname": "t2_4htedsojh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New job - red flags or just the usual nightmare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tju13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675526548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a huge company in November of last year. I switched careers from Data Science to engineering, so I do not know much about the typical DE environment. Previously, I worked together with one Data Engineer and I am now one of 9 in my team. However, it certainly is different from how I expected it to be. My expectations were that everything would be more structured. You get a task, there is a straightforward way to do it, and then you spend time implementing it. My current style of working is instead 90% searching where to put my 5 lines of code and then testing if this is really how it should be done.&lt;/p&gt;\n\n&lt;p&gt;The personal red flags are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Everything is exceptionally complex, from Airflow pipelines with 1000 lines config files to a vast Terraform multi repo (around 20 individual repos). They use a lot of internal libraries, most of them wrapping well-known libraries and adding some functionality. For example, the Airflow S3 hook was wrapped in a company internal one giving it like 30 more options to read data for specific use cases. It took me hours to figure this out because it&amp;#39;s only documented inline and the file is around 5000 lines of code.&lt;/li&gt;\n&lt;li&gt;There is one colleague who is the single source of truth. He has been in the company for 10 years and knows why things are implemented like they are. Others have been around 5 years but basically only refer to him. He is quite good at what he does, but I still find this hierarchy concerning. I understand that more experienced colleagues are asked for their opinion more often, but this goes as far as at least one colleague approving his pull requests without actually looking at them.&lt;/li&gt;\n&lt;li&gt;Many colleagues know much about the existing infrastructure but fail to understand what&amp;#39;s behind it or what they are doing. For example, a colleague from another department asked what kubectl port-forward is doing and a colleague responsible for our Kubernetes cluster replied, &amp;quot;It gives your local user access rights to the cluster.&amp;quot; This was the most extreme example, but I found many smaller ones confusing. For instance, I talked about writing a bash script for a simple renaming task and a colleague said: &amp;quot;Why not implement it with Python? We shouldn&amp;#39;t use languages that need additional installations on our servers&amp;quot;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would you consider that red flags? What would you do to improve the situation, expecially the complexity of the existing code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10tju13", "is_robot_indexable": true, "report_reasons": null, "author": "Front_Sun_8213", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tju13/new_job_red_flags_or_just_the_usual_nightmare/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tju13/new_job_red_flags_or_just_the_usual_nightmare/", "subreddit_subscribers": 88509, "created_utc": 1675526548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that this must vary by pipeline, but in my example I have an Excel file users can manually update in a SharePoint site, and I want to make sure that I want to make sure that the data passes a number of validity checks (i.e. type checking, regex checking, cross-column dependency checking).\n\nThe actual pipeline is basic, and triggered via Airflow:\n\n1. **Extract:** Read file into memory via SharePoint API &amp; Write to S3 as CSV\n2. **Load:** Read CSV into memory from S3 &amp; Write to RedShift\n\nNow I know that I have the flexibility to do this in countless ways, but for those with experience, what's the best way to validate? In memory during the Extract task? A new task between Extract &amp; Load? In memory during the Load task? Or somewhere after load like before or during Transformation?\n\nAlso unrelated, is it write for me to throw an error on invalid data, or should I attempt to automatically transform it to be valid?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When exactly should I be validating data in my ingestion pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tkcac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675527812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that this must vary by pipeline, but in my example I have an Excel file users can manually update in a SharePoint site, and I want to make sure that I want to make sure that the data passes a number of validity checks (i.e. type checking, regex checking, cross-column dependency checking).&lt;/p&gt;\n\n&lt;p&gt;The actual pipeline is basic, and triggered via Airflow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Extract:&lt;/strong&gt; Read file into memory via SharePoint API &amp;amp; Write to S3 as CSV&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Load:&lt;/strong&gt; Read CSV into memory from S3 &amp;amp; Write to RedShift&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now I know that I have the flexibility to do this in countless ways, but for those with experience, what&amp;#39;s the best way to validate? In memory during the Extract task? A new task between Extract &amp;amp; Load? In memory during the Load task? Or somewhere after load like before or during Transformation?&lt;/p&gt;\n\n&lt;p&gt;Also unrelated, is it write for me to throw an error on invalid data, or should I attempt to automatically transform it to be valid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10tkcac", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tkcac/when_exactly_should_i_be_validating_data_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tkcac/when_exactly_should_i_be_validating_data_in_my/", "subreddit_subscribers": 88509, "created_utc": 1675527812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I worked as a database and ETL developer from 1998 to 2015. I mainly used bash, sql, pl/sql, and SAP Data Services. What do I need to learn to perform a data engineering role today?", "author_fullname": "t2_7ynplafk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for Current Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tt7y7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675549586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I worked as a database and ETL developer from 1998 to 2015. I mainly used bash, sql, pl/sql, and SAP Data Services. What do I need to learn to perform a data engineering role today?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10tt7y7", "is_robot_indexable": true, "report_reasons": null, "author": "Xenos865D", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tt7y7/advice_for_current_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tt7y7/advice_for_current_data_engineering/", "subreddit_subscribers": 88509, "created_utc": 1675549586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a start-up where we will have 0-10 million of rows stored in our database daily, and about 500k users. I am very new to data engineering but have some solid background. The thing is, the database need to have high read and write capacity. Users will use search functions in our mobile app to get rows of data to their phone. At the same time it needs to take in many new rows of data.\n\nWhat are efficient ways to build a data warehouse with theese needs? I was looking at azure data lake storage and google bigTable as possible solutions.can someone give me some insight and opinion on how i should tackle this problem?", "author_fullname": "t2_6441kcer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help on best practice for building a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tmn9e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675533391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a start-up where we will have 0-10 million of rows stored in our database daily, and about 500k users. I am very new to data engineering but have some solid background. The thing is, the database need to have high read and write capacity. Users will use search functions in our mobile app to get rows of data to their phone. At the same time it needs to take in many new rows of data.&lt;/p&gt;\n\n&lt;p&gt;What are efficient ways to build a data warehouse with theese needs? I was looking at azure data lake storage and google bigTable as possible solutions.can someone give me some insight and opinion on how i should tackle this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10tmn9e", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible_Hawk8015", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tmn9e/need_help_on_best_practice_for_building_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tmn9e/need_help_on_best_practice_for_building_a_data/", "subreddit_subscribers": 88509, "created_utc": 1675533391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anybody process data (do ETL) in a \u201cstreaming\u201d fashion using python asynchronous generators? I mean e. g. use some dataset and then iterating over each row and performing some operation on it. \n\nHow common is such approach? Is there any library supporting such use case?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u201cStreaming\u201d data processing using asynchronous generators in python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ts4hl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675546873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody process data (do ETL) in a \u201cstreaming\u201d fashion using python asynchronous generators? I mean e. g. use some dataset and then iterating over each row and performing some operation on it. &lt;/p&gt;\n\n&lt;p&gt;How common is such approach? Is there any library supporting such use case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ts4hl", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ts4hl/streaming_data_processing_using_asynchronous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ts4hl/streaming_data_processing_using_asynchronous/", "subreddit_subscribers": 88509, "created_utc": 1675546873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I wrote 3 articles about Parquet file format, I'll be glad to know what you think of them:\n\n\\- [Simple Parquet Tutorial and Best Practices](https://medium.com/towards-data-science/easy-parquet-tutorial-best-practices-237955e46cb7)\n\n\\- [Parquet Best Practices: Discover your Data without loading it](https://medium.com/towards-data-science/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6)\n\n\\- [Parquet Best Practices: The Art of Filtering](https://medium.com/towards-artificial-intelligence/parquet-best-practices-the-art-of-filtering-d729357e441d)", "author_fullname": "t2_ssf8k2ti", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet files for Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10trbq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675545949.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675544851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I wrote 3 articles about Parquet file format, I&amp;#39;ll be glad to know what you think of them:&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://medium.com/towards-data-science/easy-parquet-tutorial-best-practices-237955e46cb7\"&gt;Simple Parquet Tutorial and Best Practices&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://medium.com/towards-data-science/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6\"&gt;Parquet Best Practices: Discover your Data without loading it&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://medium.com/towards-artificial-intelligence/parquet-best-practices-the-art-of-filtering-d729357e441d\"&gt;Parquet Best Practices: The Art of Filtering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?auto=webp&amp;v=enabled&amp;s=59a9119d1f36c760f49d169473d7b5782df4f158", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71aed14390c87bafeb34426dac39a2c8a900e6a6", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3e0c4053a604b9d36462a4179e4121b0253f84e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d67e2c6ef6177ff64169342aacc38aa3c3da31b", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6385303f2a43d0e0f686773e8368c6c93311bb99", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87d0f66efe22859a8aad3c20d4a4e528aa861a60", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/Dz_-iNGeqzBW5DGTGIxknp5DvXD6Lud5zNAmsbGYD6k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=072bab0af5d133dee6da3611500caf9eec5e8010", "width": 1080, "height": 720}], "variants": {}, "id": "faGU4qMn4kiyF1yjGPVr3Z7_hr821Nd5HEGdrzb-3pQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "10trbq0", "is_robot_indexable": true, "report_reasons": null, "author": "Arli84", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10trbq0/parquet_files_for_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10trbq0/parquet_files_for_big_data/", "subreddit_subscribers": 88509, "created_utc": 1675544851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Reading the  [Why do people re-invent wrappers for Airflow? : dataengineering (reddit.com)](https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/)  post I was reflecting about my DE experience and I have to admit I've introduced myself extra layers or patterns that, with all due honesty, were more a \"Let's just try Bridge pattern for the sake of it\" than a well designed and necessary solution.\n\nUnnecessary classes on top of PySpark, a complicated hierarchy of abstract Pipeline classes or YAML-based configurations parsed by another self-made solution.", "author_fullname": "t2_5a2nw4lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confiteor (or how to overcomplicate data engineering for fun)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tg72q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675516640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading the  &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/\"&gt;Why do people re-invent wrappers for Airflow? : dataengineering (reddit.com)&lt;/a&gt;  post I was reflecting about my DE experience and I have to admit I&amp;#39;ve introduced myself extra layers or patterns that, with all due honesty, were more a &amp;quot;Let&amp;#39;s just try Bridge pattern for the sake of it&amp;quot; than a well designed and necessary solution.&lt;/p&gt;\n\n&lt;p&gt;Unnecessary classes on top of PySpark, a complicated hierarchy of abstract Pipeline classes or YAML-based configurations parsed by another self-made solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tg72q", "is_robot_indexable": true, "report_reasons": null, "author": "_raskol_nikov_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tg72q/confiteor_or_how_to_overcomplicate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tg72q/confiteor_or_how_to_overcomplicate_data/", "subreddit_subscribers": 88509, "created_utc": 1675516640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!  \nI enjoyed going over the discussion and learning a lot along the way, and I finally got some courage to ask for advice and guidance. \n\nI recently graduated with my BSc in Computer science with a major in Data analytics. I enjoy building systems and working on something that would automate some tasks. I wandered around various domains in cs at one point, even Quantum Computing.  \nMy first real job during and out of college was for a company building static code analytics tools. I worked a lot around the building and maintaining their data systems. Soon, I was asked to help develop an ETL pipeline using dask(deployed to GKE  ). Ideally, the aim was to create a pipeline to train and evaluate one of our models with batches of data that our Extractor obtains. As it was a startup, I had the opportunity to work on all pipeline components, including the DevOps side. This project gave me a lot of insight into the importance of pipelines and their purpose. \n\nI am considering entering grad school as I have much more to learn and improve my theoretical base. I am looking for advice on what I should explore and learn to make me a data engineer capable of understanding my projects and designing well-engineered systems.", "author_fullname": "t2_7vv90qx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Veteran, How can a newbie prepare for a career in DE ??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10txskx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675561894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;br/&gt;\nI enjoyed going over the discussion and learning a lot along the way, and I finally got some courage to ask for advice and guidance. &lt;/p&gt;\n\n&lt;p&gt;I recently graduated with my BSc in Computer science with a major in Data analytics. I enjoy building systems and working on something that would automate some tasks. I wandered around various domains in cs at one point, even Quantum Computing.&lt;br/&gt;\nMy first real job during and out of college was for a company building static code analytics tools. I worked a lot around the building and maintaining their data systems. Soon, I was asked to help develop an ETL pipeline using dask(deployed to GKE  ). Ideally, the aim was to create a pipeline to train and evaluate one of our models with batches of data that our Extractor obtains. As it was a startup, I had the opportunity to work on all pipeline components, including the DevOps side. This project gave me a lot of insight into the importance of pipelines and their purpose. &lt;/p&gt;\n\n&lt;p&gt;I am considering entering grad school as I have much more to learn and improve my theoretical base. I am looking for advice on what I should explore and learn to make me a data engineer capable of understanding my projects and designing well-engineered systems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10txskx", "is_robot_indexable": true, "report_reasons": null, "author": "Anush_Krishna", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10txskx/veteran_how_can_a_newbie_prepare_for_a_career_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10txskx/veteran_how_can_a_newbie_prepare_for_a_career_in/", "subreddit_subscribers": 88509, "created_utc": 1675561894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Airflow seems to be one of these technologies that is everywhere used while most people that I've talked with aren't happy working with it. \n\nAlso, commercial solutions for orchestration like Astronomer and Dagster, don't feel like are threatening Airflow that much yet which again feels a bit counterintuitive considering the sentiments around Airflow in general.\n\nThis might be just be a result of selection bias in my sample of cases so I thought to ask here and see how people feel about Airflow and what makes them feel like that towards it, both positive and negative.", "author_fullname": "t2_fb1s1pke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why everybody's using Airflow while no-one seems to be happy with it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ttbvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675549864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Airflow seems to be one of these technologies that is everywhere used while most people that I&amp;#39;ve talked with aren&amp;#39;t happy working with it. &lt;/p&gt;\n\n&lt;p&gt;Also, commercial solutions for orchestration like Astronomer and Dagster, don&amp;#39;t feel like are threatening Airflow that much yet which again feels a bit counterintuitive considering the sentiments around Airflow in general.&lt;/p&gt;\n\n&lt;p&gt;This might be just be a result of selection bias in my sample of cases so I thought to ask here and see how people feel about Airflow and what makes them feel like that towards it, both positive and negative.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ttbvl", "is_robot_indexable": true, "report_reasons": null, "author": "cpardl", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ttbvl/why_everybodys_using_airflow_while_noone_seems_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ttbvl/why_everybodys_using_airflow_while_noone_seems_to/", "subreddit_subscribers": 88509, "created_utc": 1675549864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m learning about the modern data stack and have a question about when and how is the best time to add internal values. \n\nUse case is ingesting data from various advertising platforms.  On these ad platforms we have several advertising accounts. When we bring the data in we need to map the advertiser accounts to internal business units and countries.\n\nI\u2019m looking to learn what best practice looks like for the following use cases:\n\n1) lookup the advertisingid and add the columns\n2) what to do with rows from source system where we haven\u2019t maintained the mapping entry yet?  Some kind of dead letter queue setup or something?\n3) the data from the source system will be brought in with a rolling time period of 30 days, and at any point a past day could receive updated values.  For example, on Monday I get values for Sunday 100 and Monday 50. On Tuesday the values are Sunday 105 and Monday 68 and Tuesday 80.  I want to send the most up to date values into the pipeline.\n4) When to run data quality checks?\n\nAssume that the raw data is sitting in a BigQuery raw data table. \n\nJust trying to learn best practices here. \n\nThanks!", "author_fullname": "t2_wgxzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to add internal values to ingested data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tr3jh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675544290.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m learning about the modern data stack and have a question about when and how is the best time to add internal values. &lt;/p&gt;\n\n&lt;p&gt;Use case is ingesting data from various advertising platforms.  On these ad platforms we have several advertising accounts. When we bring the data in we need to map the advertiser accounts to internal business units and countries.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to learn what best practice looks like for the following use cases:&lt;/p&gt;\n\n&lt;p&gt;1) lookup the advertisingid and add the columns\n2) what to do with rows from source system where we haven\u2019t maintained the mapping entry yet?  Some kind of dead letter queue setup or something?\n3) the data from the source system will be brought in with a rolling time period of 30 days, and at any point a past day could receive updated values.  For example, on Monday I get values for Sunday 100 and Monday 50. On Tuesday the values are Sunday 105 and Monday 68 and Tuesday 80.  I want to send the most up to date values into the pipeline.\n4) When to run data quality checks?&lt;/p&gt;\n\n&lt;p&gt;Assume that the raw data is sitting in a BigQuery raw data table. &lt;/p&gt;\n\n&lt;p&gt;Just trying to learn best practices here. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10tr3jh", "is_robot_indexable": true, "report_reasons": null, "author": "garnerp", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tr3jh/when_to_add_internal_values_to_ingested_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tr3jh/when_to_add_internal_values_to_ingested_data/", "subreddit_subscribers": 88509, "created_utc": 1675544290.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone give me good resources to learn data engineering with Python ? I am already fluent with python and also a backend  engineer so what could be the best way to learn data engineering. I just started reading the book called 'Data Engineering with Python' and It only covers the basic stuff.", "author_fullname": "t2_nwphe35r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course recommendations and Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10thtnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675521382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone give me good resources to learn data engineering with Python ? I am already fluent with python and also a backend  engineer so what could be the best way to learn data engineering. I just started reading the book called &amp;#39;Data Engineering with Python&amp;#39; and It only covers the basic stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10thtnv", "is_robot_indexable": true, "report_reasons": null, "author": "golesu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10thtnv/course_recommendations_and_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10thtnv/course_recommendations_and_resources/", "subreddit_subscribers": 88509, "created_utc": 1675521382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. \n\nIs there any alternative if we use Hive metastore as catalog instead of Athena/Glue?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data wrangler with Hive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tafdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. &lt;/p&gt;\n\n&lt;p&gt;Is there any alternative if we use Hive metastore as catalog instead of Athena/Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tafdg", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "subreddit_subscribers": 88509, "created_utc": 1675500852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Everybody keeps saying that one of the advantages of Parquet is that you can query only selected columns. Well, but this is also true for traditional relational database, isnt it? I mean you can specify exact column which you want to return, or what is the point? Is it meant that row based data are PHYSICALLY stored together so one still needs to firstly load ALL columns and then filter them?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet - selecting only columns advantage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tadkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everybody keeps saying that one of the advantages of Parquet is that you can query only selected columns. Well, but this is also true for traditional relational database, isnt it? I mean you can specify exact column which you want to return, or what is the point? Is it meant that row based data are PHYSICALLY stored together so one still needs to firstly load ALL columns and then filter them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tadkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tadkz/parquet_selecting_only_columns_advantage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tadkz/parquet_selecting_only_columns_advantage/", "subreddit_subscribers": 88509, "created_utc": 1675500757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am a complete noob on ETL and could really use some help.\n\nI am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.\n\nHow do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?\n\nVery greatful for any help, thanks!", "author_fullname": "t2_qiw67sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with cluster startup time in azure databricks when having event driven data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t7xs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675495387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am a complete noob on ETL and could really use some help.&lt;/p&gt;\n\n&lt;p&gt;I am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;How do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?&lt;/p&gt;\n\n&lt;p&gt;Very greatful for any help, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10t7xs0", "is_robot_indexable": true, "report_reasons": null, "author": "aLyapunov", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "subreddit_subscribers": 88509, "created_utc": 1675495387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The AWS Certified Big Data or Databricks certs both look interesting and would fill in an experience gap for me. But I would be really concerned about running up a huge bill of hundreds of dollars with those types of services while learning. In my experience, it is really easy to forget to tear everything down within a cloud platform and miss something, and I'd probably be stressed about it constantly. \n\nFrom what I know, Databricks has a community edition and then of course there's AWS free tier. But free tier is a pittance for most \"big data\" services. Anyone have some experience with either or both for certification study? I lean towards Databricks just because I'm more interested in the Spark ecosystem. I would just like to avoid hundreds of dollars of charges in either case.", "author_fullname": "t2_4i8pl31g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS DE and Databricks Certs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tpkeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675540514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The AWS Certified Big Data or Databricks certs both look interesting and would fill in an experience gap for me. But I would be really concerned about running up a huge bill of hundreds of dollars with those types of services while learning. In my experience, it is really easy to forget to tear everything down within a cloud platform and miss something, and I&amp;#39;d probably be stressed about it constantly. &lt;/p&gt;\n\n&lt;p&gt;From what I know, Databricks has a community edition and then of course there&amp;#39;s AWS free tier. But free tier is a pittance for most &amp;quot;big data&amp;quot; services. Anyone have some experience with either or both for certification study? I lean towards Databricks just because I&amp;#39;m more interested in the Spark ecosystem. I would just like to avoid hundreds of dollars of charges in either case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tpkeq", "is_robot_indexable": true, "report_reasons": null, "author": "knowledgebass", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tpkeq/aws_de_and_databricks_certs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tpkeq/aws_de_and_databricks_certs/", "subreddit_subscribers": 88509, "created_utc": 1675540514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nPlease give your insights by the metrics bellow:\n\nStatistical programmer vs Data engineer. Who will win?\n\n1. Salary\n2. Supply-demand curve. Which one has more jobs (i.e. better burgaining power)?\n3. Work-life balance\n4. Future career growth (i.e. more recession-proof)", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Statistical programmer vs Data engineer. Which one is a better job(salary/demand/work-life balance/career growth)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tlk3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675530805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please give your insights by the metrics bellow:&lt;/p&gt;\n\n&lt;p&gt;Statistical programmer vs Data engineer. Who will win?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Salary&lt;/li&gt;\n&lt;li&gt;Supply-demand curve. Which one has more jobs (i.e. better burgaining power)?&lt;/li&gt;\n&lt;li&gt;Work-life balance&lt;/li&gt;\n&lt;li&gt;Future career growth (i.e. more recession-proof)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10tlk3j", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tlk3j/statistical_programmer_vs_data_engineer_which_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tlk3j/statistical_programmer_vs_data_engineer_which_one/", "subreddit_subscribers": 88509, "created_utc": 1675530805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What of the pandas alternatives (excl. Spark) do you use the most? Reason appreciated.\n\n[View Poll](https://www.reddit.com/poll/10trqas)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What pandas alternative (excl. Spark) you use the most? Comment/Reason appreciated.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10trqas", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675545860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What of the pandas alternatives (excl. Spark) do you use the most? Reason appreciated.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10trqas\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10trqas", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1675977860961, "options": [{"text": "Modin", "id": "21428090"}, {"text": "Dask", "id": "21428091"}, {"text": "Polars", "id": "21428092"}, {"text": "PyArrow", "id": "21428093"}, {"text": "Still mostly pandas", "id": "21428094"}, {"text": "Other - please comment which", "id": "21428095"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 268, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10trqas/what_pandas_alternative_excl_spark_you_use_the/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10trqas/what_pandas_alternative_excl_spark_you_use_the/", "subreddit_subscribers": 88509, "created_utc": 1675545860.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}