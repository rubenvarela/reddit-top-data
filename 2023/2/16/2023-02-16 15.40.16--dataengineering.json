{"kind": "Listing", "data": {"after": "t3_113qu1r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9e7m1qmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finnhub streaming data pipeline using Spark, Kafka, Kubernetes and more - Github repo &amp; more info in the comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 48, "top_awarded_type": null, "hide_score": false, "name": "t3_1131jqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 146, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 146, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CbifdmwD18eb4V2QhY84xODBLIeFazHoVdEaSRnHDXc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676477425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/44en1od0kdia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/44en1od0kdia1.png?auto=webp&amp;v=enabled&amp;s=bd40c6eb7926721714742b3fcf5941eb50684aaa", "width": 2381, "height": 822}, "resolutions": [{"url": "https://preview.redd.it/44en1od0kdia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f89886f46480a09c1ee666186e1da9f60a55824", "width": 108, "height": 37}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a93b4d80d6a39cba158604636da7593cee15584", "width": 216, "height": 74}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=230b2497b9dd9fada4a17d96681c94e94bdf070a", "width": 320, "height": 110}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69a45be536958efc184054cd2e95a21ad8e44e08", "width": 640, "height": 220}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adafa2f6b140f6fa94fb9a6c57e913b627d1f7af", "width": 960, "height": 331}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50dea1dddc719945eaf5df33b056b8db300da24b", "width": 1080, "height": 372}], "variants": {}, "id": "WBRfH8NzSNCYSGKZg1qxfeT9pcEhMHoSqNpwBZvqY6o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1131jqq", "is_robot_indexable": true, "report_reasons": null, "author": "LewWariat", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131jqq/finnhub_streaming_data_pipeline_using_spark_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/44en1od0kdia1.png", "subreddit_subscribers": 89762, "created_utc": 1676477425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's been quite a while since I dealt with managing a type 2 SCD so I wanted to get confirmation on what the typical strategies are. I've done hash columns that will hash all of the columns of a dimension table and compare that to the hash column of the incoming data. This worked pretty well but I remember hearing that there's always a chance that hash values will be the same. Is that concern still valid, and how often does that happen? \n\nWhat are other strategies/data models/queries that will identify a changed row in a type 2 SCD? I came across [this Stackoverflow answer](https://stackoverflow.com/a/35171620/1175788) for essentially the same question but the query is pretty obtuse. I want to avoid a query where it's doing comparisons across all the columns like: `WHERE source.id = incoming.id AND (source.quantity &lt;&gt; incoming.quantity OR source.date &lt;&gt; incoming.date)` because that's obviously terribly inefficient", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Type 2 slowly changing dimension. What are the common SQL queries to identify updated rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1130zot", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676475958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been quite a while since I dealt with managing a type 2 SCD so I wanted to get confirmation on what the typical strategies are. I&amp;#39;ve done hash columns that will hash all of the columns of a dimension table and compare that to the hash column of the incoming data. This worked pretty well but I remember hearing that there&amp;#39;s always a chance that hash values will be the same. Is that concern still valid, and how often does that happen? &lt;/p&gt;\n\n&lt;p&gt;What are other strategies/data models/queries that will identify a changed row in a type 2 SCD? I came across &lt;a href=\"https://stackoverflow.com/a/35171620/1175788\"&gt;this Stackoverflow answer&lt;/a&gt; for essentially the same question but the query is pretty obtuse. I want to avoid a query where it&amp;#39;s doing comparisons across all the columns like: &lt;code&gt;WHERE source.id = incoming.id AND (source.quantity &amp;lt;&amp;gt; incoming.quantity OR source.date &amp;lt;&amp;gt; incoming.date)&lt;/code&gt; because that&amp;#39;s obviously terribly inefficient&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1130zot", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130zot/type_2_slowly_changing_dimension_what_are_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130zot/type_2_slowly_changing_dimension_what_are_the/", "subreddit_subscribers": 89762, "created_utc": 1676475958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.\n\nIs there anything like this?", "author_fullname": "t2_lf4it7cs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything like Kaggle for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11349lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676484109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.&lt;/p&gt;\n\n&lt;p&gt;Is there anything like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11349lf", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Deer_8686", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/", "subreddit_subscribers": 89762, "created_utc": 1676484109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you mask the data? Have full access rights? Follow just guidance? How does it work where you are?", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your data engineering team handle ingesting PII and classified data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1130z35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676475915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you mask the data? Have full access rights? Follow just guidance? How does it work where you are?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1130z35", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130z35/how_does_your_data_engineering_team_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130z35/how_does_your_data_engineering_team_handle/", "subreddit_subscribers": 89762, "created_utc": 1676475915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.\n\nNext month, we\u2019re reading [*Data Teams: A Unified Management Model for Successful Data-Focused Teams*](https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;qid=1676346037&amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;sr=8-1) by [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/).\n\nI\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It's a book I wanna read!\n\n**Here\u2019s how the book club works:** All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.\n\n**Here\u2019s the schedule:**\n\n* March 17th: Discuss pt. 1 &amp; pt. 2\n* March 31st: Discuss pt. 3\n* April 5th: AMA w/ Author, [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/)\n* April 14th: Discuss pt. 4\n\nWe currently have dozens signed up! If you\u2019d like to join, book it [**here**](https://www.operationalanalytics.club/events/titlecase-book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams)**.**", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Data Teams: A Unified Management Model for Successful Data-Focused Teams by Jesse Anderson", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139l8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676498263.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.&lt;/p&gt;\n\n&lt;p&gt;Next month, we\u2019re reading &lt;a href=\"https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;amp;qid=1676346037&amp;amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;amp;sr=8-1\"&gt;&lt;em&gt;Data Teams: A Unified Management Model for Successful Data-Focused Teams&lt;/em&gt;&lt;/a&gt; by &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It&amp;#39;s a book I wanna read!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s how the book club works:&lt;/strong&gt; All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s the schedule:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;March 17th: Discuss pt. 1 &amp;amp; pt. 2&lt;/li&gt;\n&lt;li&gt;March 31st: Discuss pt. 3&lt;/li&gt;\n&lt;li&gt;April 5th: AMA w/ Author, &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;April 14th: Discuss pt. 4&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We currently have dozens signed up! If you\u2019d like to join, book it &lt;a href=\"https://www.operationalanalytics.club/events/titlecase-book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1139l8l", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "subreddit_subscribers": 89762, "created_utc": 1676497889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jx9xua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A data mesh for the rest of us", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_113rmqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Vo04I-HUxNZxvPo113vbsqEnkeWusBIcQjdmO-x6GK8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676557012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.loveholidays.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?auto=webp&amp;v=enabled&amp;s=f96a74cf96d249f64343eef34991ad1ccdc98edd", "width": 1162, "height": 1392}, "resolutions": [{"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb8aa1511981b78405cf1f97c5ed2765d873b593", "width": 108, "height": 129}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8894b210a20989a52cb1b49459f6e71cb47c53f", "width": 216, "height": 258}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=194b8dcd3f979379f00d3a88fbddacea92bc678e", "width": 320, "height": 383}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ac07d5030bfb285211a5f90534038ab699918d2", "width": 640, "height": 766}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5df9a8d6b0e8e771972df2a9382722402b34104b", "width": 960, "height": 1150}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dba3097170bf0efd23e8fd9432e786ea92c34bc", "width": 1080, "height": 1293}], "variants": {}, "id": "c5kxXpUajNxYr31sXoHyDiM2CcPGkWrkU4iZDryDntI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113rmqn", "is_robot_indexable": true, "report_reasons": null, "author": "dropber", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113rmqn/a_data_mesh_for_the_rest_of_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "subreddit_subscribers": 89762, "created_utc": 1676557012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. \n\nMost of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. \n\nThe current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. \n\nHaving just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. \n\nI was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).\n\nFor on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?\n\nI feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. \n\nCurious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.", "author_fullname": "t2_18xs8yle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for regional contact/directory data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113fqdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676514792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. &lt;/p&gt;\n\n&lt;p&gt;Most of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. &lt;/p&gt;\n\n&lt;p&gt;The current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. &lt;/p&gt;\n\n&lt;p&gt;Having just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. &lt;/p&gt;\n\n&lt;p&gt;I was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).&lt;/p&gt;\n\n&lt;p&gt;For on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?&lt;/p&gt;\n\n&lt;p&gt;I feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. &lt;/p&gt;\n\n&lt;p&gt;Curious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113fqdd", "is_robot_indexable": true, "report_reasons": null, "author": "wves", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "subreddit_subscribers": 89762, "created_utc": 1676514792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. \n\nThere are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in 'YYYY-MM-DDTHH:MM:SS format.\n\nIn a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. \n\nHow can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I've looked into UDFs but they aren't recommended - so i'm stumped here!\n\nThanks,", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to modularise Delta Live Tables using Pyspark in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113mxlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676540959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. &lt;/p&gt;\n\n&lt;p&gt;There are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in &amp;#39;YYYY-MM-DDTHH:MM:SS format.&lt;/p&gt;\n\n&lt;p&gt;In a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. &lt;/p&gt;\n\n&lt;p&gt;How can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I&amp;#39;ve looked into UDFs but they aren&amp;#39;t recommended - so i&amp;#39;m stumped here!&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113mxlk", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "subreddit_subscribers": 89762, "created_utc": 1676540959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don't solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?", "author_fullname": "t2_3tiq5c7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's missing in the modern data stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11388la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676494377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don&amp;#39;t solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11388la", "is_robot_indexable": true, "report_reasons": null, "author": "ownubie", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "subreddit_subscribers": 89762, "created_utc": 1676494377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i've read, adding `catchup = True` will make the task start from 3 years ago. It'll also force backfill any other task which isn't fully caught up.\n\nI've noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!\n\nThe only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?", "author_fullname": "t2_zjn67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, backfilling/re-running a single task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113i7y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676523416.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676522703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i&amp;#39;ve read, adding &lt;code&gt;catchup = True&lt;/code&gt; will make the task start from 3 years ago. It&amp;#39;ll also force backfill any other task which isn&amp;#39;t fully caught up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!&lt;/p&gt;\n\n&lt;p&gt;The only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113i7y2", "is_robot_indexable": true, "report_reasons": null, "author": "Propanoate", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "subreddit_subscribers": 89762, "created_utc": 1676522703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?", "author_fullname": "t2_bwp6e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your team\u2019s method for storing credentials for your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1137p0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676492961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1137p0i", "is_robot_indexable": true, "report_reasons": null, "author": "wild_bill34", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "subreddit_subscribers": 89762, "created_utc": 1676492961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_puwuw2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My project: A focused, personalized observability report for every PR. GitHub Actions: Would you find this useful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113pg54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b2L8gjGr2xKg0WKG-hT1Xv3tNDNk5E0rte_0BRFIMWE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676550430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8lpmzrednjia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8lpmzrednjia1.png?auto=webp&amp;v=enabled&amp;s=ba5c47aeac347b8fbbb153a3f7f2093ee537698c", "width": 618, "height": 1203}, "resolutions": [{"url": "https://preview.redd.it/8lpmzrednjia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b046a88be133029b6970105b7fdbf8f46602f554", "width": 108, "height": 210}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a7699ecf679e7ee2d2a3b7600630ac4768ea9c8", "width": 216, "height": 420}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1607a7c854fdfce57ebaaa2506b150e5638977a4", "width": 320, "height": 622}], "variants": {}, "id": "JPevZe8OIe_HvE94bD2wlM2BYu4RPMzjNqzycgXcTzU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113pg54", "is_robot_indexable": true, "report_reasons": null, "author": "observability_geek", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113pg54/my_project_a_focused_personalized_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8lpmzrednjia1.png", "subreddit_subscribers": 89762, "created_utc": 1676550430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. \n\nThe goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.\n\nThis entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.\n\nOnce enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.\n\nThe data is shown on Looker Studio's free service. \n\n**Links:** \n\n[GitHub](https://github.com/sachinlim/ebay_airflow)\n\n[Looker Studio](https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD) \n\nIt is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.\n\n*** \n\nTo do the project, I used an older eBay script I had made and adapted it for this project. \n\nI think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it's a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. \n\nAfter seeing an older [Reddit ETL Pipeline](https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/) post, there's definitely a lot more ways to improve this. The code I've written is perhaps a bit basic, not sure how scalable the script is, if it's clean, and there's SQL injection possible with the `INSERT` statement, though the function to get the price averages won't run for anything other than numbers.\n\nI'm using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on /r/HardwareSwapUK.", "author_fullname": "t2_v49pkl1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Pipeline for eBay Data Extraction - Simple project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113roc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676557141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. &lt;/p&gt;\n\n&lt;p&gt;The goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.&lt;/p&gt;\n\n&lt;p&gt;This entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.&lt;/p&gt;\n\n&lt;p&gt;Once enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.&lt;/p&gt;\n\n&lt;p&gt;The data is shown on Looker Studio&amp;#39;s free service. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sachinlim/ebay_airflow\"&gt;GitHub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD\"&gt;Looker Studio&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;It is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;To do the project, I used an older eBay script I had made and adapted it for this project. &lt;/p&gt;\n\n&lt;p&gt;I think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it&amp;#39;s a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. &lt;/p&gt;\n\n&lt;p&gt;After seeing an older &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/\"&gt;Reddit ETL Pipeline&lt;/a&gt; post, there&amp;#39;s definitely a lot more ways to improve this. The code I&amp;#39;ve written is perhaps a bit basic, not sure how scalable the script is, if it&amp;#39;s clean, and there&amp;#39;s SQL injection possible with the &lt;code&gt;INSERT&lt;/code&gt; statement, though the function to get the price averages won&amp;#39;t run for anything other than numbers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on &lt;a href=\"/r/HardwareSwapUK\"&gt;/r/HardwareSwapUK&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?auto=webp&amp;v=enabled&amp;s=2af9600bd4aa992f39d564819271899a1ab7fe0e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3026bad95d2fd621dadaa0bdfc4fce3cef771be7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64cf19660c70d1617f460ff07ecf3bfa7b330151", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35f2eecb197804bddcec656085545e631b9d3436", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77273085d988cf74ed3eb98bdde92c801a2e7a26", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d649cec23ed95938f033b634800c1129c1283dfe", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd5ce8b4111c182753abc04a0a76eefb510c852", "width": 1080, "height": 540}], "variants": {}, "id": "2lf3IYLRtQAxbb7CToJK_65KH5OHqQF9YhUq4E9VuTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CS Graduate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "113roc9", "is_robot_indexable": true, "report_reasons": null, "author": "Mapleess", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "subreddit_subscribers": 89762, "created_utc": 1676557141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I got a flow which I want to automate in Azure. For now we're running a python script on prem, but the server needs to get phased out.\n\n&amp;#x200B;\n\n\\- Everyday I get emails in Office 365 from client.\n\n\\- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.\n\n&amp;#x200B;\n\nMy question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.\n\n&amp;#x200B;\n\nI don't know which approach is best for this... Synapse, Power Automate, Functions... something else.", "author_fullname": "t2_ig8s88dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Excel data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139rhi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676498341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I got a flow which I want to automate in Azure. For now we&amp;#39;re running a python script on prem, but the server needs to get phased out.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Everyday I get emails in Office 365 from client.&lt;/p&gt;\n\n&lt;p&gt;- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know which approach is best for this... Synapse, Power Automate, Functions... something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139rhi", "is_robot_indexable": true, "report_reasons": null, "author": "Hs82H", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1139rhi/automate_excel_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139rhi/automate_excel_data/", "subreddit_subscribers": 89762, "created_utc": 1676498341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this post [https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking\\_for\\_data\\_analytics\\_engine\\_for\\_lowlatency/](https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/) I was guided towards Headless BI, specifically [Cube.dev](https://Cube.dev).\n\nWe looked into it, seemed to work for us and went with it. Nice!\n\nExcept... we've run into various edge cases where we're now questioning our choice heavily.\n\nBrowsing around, I've found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?\n\nAnd while at it, how a tool like that would be actually called? Headless BI doesn't turn up too much results. Semantic Layer? Maybe something else?", "author_fullname": "t2_4gt5e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Looker/LookML &amp; Cube.dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1131e3a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676477013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/\"&gt;https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/&lt;/a&gt; I was guided towards Headless BI, specifically &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We looked into it, seemed to work for us and went with it. Nice!&lt;/p&gt;\n\n&lt;p&gt;Except... we&amp;#39;ve run into various edge cases where we&amp;#39;re now questioning our choice heavily.&lt;/p&gt;\n\n&lt;p&gt;Browsing around, I&amp;#39;ve found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?&lt;/p&gt;\n\n&lt;p&gt;And while at it, how a tool like that would be actually called? Headless BI doesn&amp;#39;t turn up too much results. Semantic Layer? Maybe something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1131e3a", "is_robot_indexable": true, "report_reasons": null, "author": "psycketom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "subreddit_subscribers": 89762, "created_utc": 1676477013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Snowflake to store all of our data and while I'm investigating issues, I notice certain columns haven't been updated for weeks which I'd like to flag and triage. \n\nI use dbt and unsure if dbt can help with this easily. I use dbt tests and can program a macro across certain columns to catch if data isn't populating as I'd expect....but I was curious...\n\n**What approaches or tools have made your lives easier with data profiling and catching data issues?**", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data profiling tools / approaches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1130jx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676474767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Snowflake to store all of our data and while I&amp;#39;m investigating issues, I notice certain columns haven&amp;#39;t been updated for weeks which I&amp;#39;d like to flag and triage. &lt;/p&gt;\n\n&lt;p&gt;I use dbt and unsure if dbt can help with this easily. I use dbt tests and can program a macro across certain columns to catch if data isn&amp;#39;t populating as I&amp;#39;d expect....but I was curious...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What approaches or tools have made your lives easier with data profiling and catching data issues?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1130jx5", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130jx5/data_profiling_tools_approaches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130jx5/data_profiling_tools_approaches/", "subreddit_subscribers": 89762, "created_utc": 1676474767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a local data server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139kdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139kdv", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "subreddit_subscribers": 89762, "created_utc": 1676497830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&gt; Debezium --&gt; Kakfa --&gt; S3, is there any way we could avoid Kafka, we don't want to over-complicate the pipeline. Are they any other better ways to implement this?", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to fetch change logs from SQL Server to S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11397tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676499194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676496933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&amp;gt; Debezium --&amp;gt; Kakfa --&amp;gt; S3, is there any way we could avoid Kafka, we don&amp;#39;t want to over-complicate the pipeline. Are they any other better ways to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11397tu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "subreddit_subscribers": 89762, "created_utc": 1676496933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi \ud83d\udc4b guys, \n\nAs a programmer, I want to learn Linux to expand my skills and knowledge. However, with so many Linux distributions out there, I'm not sure which one to choose. I want to pick a distribution that will be most useful for programming purposes and that will help me develop my skills as a programmer.\n\nSo, which Linux distribution do you recommend for a programmer to learn, and why? Are there any particular features or tools that make a certain distribution better suited for programming tasks? I'm open to any suggestions and advice you may have. Thank you in advance!", "author_fullname": "t2_m6lk62t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Linux distribution is best to learn for a programmer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1133zm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676483410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi \ud83d\udc4b guys, &lt;/p&gt;\n\n&lt;p&gt;As a programmer, I want to learn Linux to expand my skills and knowledge. However, with so many Linux distributions out there, I&amp;#39;m not sure which one to choose. I want to pick a distribution that will be most useful for programming purposes and that will help me develop my skills as a programmer.&lt;/p&gt;\n\n&lt;p&gt;So, which Linux distribution do you recommend for a programmer to learn, and why? Are there any particular features or tools that make a certain distribution better suited for programming tasks? I&amp;#39;m open to any suggestions and advice you may have. Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1133zm4", "is_robot_indexable": true, "report_reasons": null, "author": "TelevisionDue5491", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1133zm4/which_linux_distribution_is_best_to_learn_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1133zm4/which_linux_distribution_is_best_to_learn_for_a/", "subreddit_subscribers": 89762, "created_utc": 1676483410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all  \nI have data engineer interview coming up with [Booking.com](https://Booking.com). I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.   \nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven't found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Booking.com Data Engineer Interview Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113que5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;br/&gt;\nI have data engineer interview coming up with &lt;a href=\"https://Booking.com\"&gt;Booking.com&lt;/a&gt;. I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.&lt;br/&gt;\nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven&amp;#39;t found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?auto=webp&amp;v=enabled&amp;s=e78c7f7b5ef8df799c5ab7819ad21ae28e0bf405", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=128d0c5e050d4fd44731c14d3596b793d77b0906", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c59c9ba9d4b265735403cbfede7112bd6a258057", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=379c22da3c6f868cf8b47f65272c52642a9b8cc1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1246f60c367c5e32f870ccbc849b81646f382184", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd246c83391f10392953e4abed795482897eca67", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=448a7d6b2100cd5eef4217e155caab07496d166c", "width": 1080, "height": 567}], "variants": {}, "id": "pOxqQ-Hg2p6yWf-XnRr9X7KZJkKYq4zbh2GbSAcQcJM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "113que5", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "subreddit_subscribers": 89762, "created_utc": 1676554753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_91odj59i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Real Definition Of \u201cDataOps\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_113epbc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwZvqp0Pi6NPy98aD615AfICVS7yLFY5FPqaRryQO3M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676511687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?auto=webp&amp;v=enabled&amp;s=79557af160f6d8db3e9cf8e02c16192b4e84ffe1", "width": 580, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ecd095287fb884429bce38b5fc59dc1c1c0a0d1", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e66defec08dfd300a5d2b44634be101482141b5", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef8678ff9e75aba71c4f8d768567077ca613caf1", "width": 320, "height": 249}], "variants": {}, "id": "ORQg5WGGxt7NVpZTOkiaKxFABZ6KiuQxVSy7rHIK4ik"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113epbc", "is_robot_indexable": true, "report_reasons": null, "author": "david_ok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113epbc/the_real_definition_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "subreddit_subscribers": 89762, "created_utc": 1676511687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d0gtdepx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake CHANGE DATE FEED \u2014 How to read CDC without using Delta Log", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 57, "top_awarded_type": null, "hide_score": false, "name": "t3_1130wj8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gcQT_X76jiYTXNkvzEX7-zIUUj14WcY6xa5Jb9So8io.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676475716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@joydeep.roy/change-date-feed-how-to-read-cdc-without-using-delta-log-17e0ceb99a8e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?auto=webp&amp;v=enabled&amp;s=f97d42dbbd8e27dc2b44056b321fe4b91e3d7d9d", "width": 1002, "height": 414}, "resolutions": [{"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=716712296ffbdb9044c829dcc7a73014376063e1", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63f7f69089c1f264df96a662f8ec1bb1f6c1fe40", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=294b02119766f1b2ecd4dacada768322d17868d1", "width": 320, "height": 132}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32d70acf82831566b93ab0124a35cd18d16ad1fb", "width": 640, "height": 264}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef28f0508c14093813f44b172a31b8a160ca28b4", "width": 960, "height": 396}], "variants": {}, "id": "gVVkQGOOB1Nv48mh-zEITYt_WkTJdgoTJ9edGrmyA_I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1130wj8", "is_robot_indexable": true, "report_reasons": null, "author": "JBR_Codepen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1130wj8/delta_lake_change_date_feed_how_to_read_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@joydeep.roy/change-date-feed-how-to-read-cdc-without-using-delta-log-17e0ceb99a8e", "subreddit_subscribers": 89762, "created_utc": 1676475716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.\n\nMy challenge is to build reliable ELT pipelines around these unreliable data sources.\n\nThrough now, the rough workflow is: \n\n1. Design XLSX template w/ each page relating to a backend table in a 1-1 relationship\n2. Host XLSX template in SharePoint site, give access to select scientist to update\n3. Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake\n4. Load each CSV into RedShift tables, truncating and loading fully\n\nThis sort of works, but I'm not entirely happy with it, and I'm not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.\n\nAny advice is appreciated, interested what other groups are doing here!", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I ingest manually created XLSX files into my data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113slfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676559672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.&lt;/p&gt;\n\n&lt;p&gt;My challenge is to build reliable ELT pipelines around these unreliable data sources.&lt;/p&gt;\n\n&lt;p&gt;Through now, the rough workflow is: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Design XLSX template w/ each page relating to a backend table in a 1-1 relationship&lt;/li&gt;\n&lt;li&gt;Host XLSX template in SharePoint site, give access to select scientist to update&lt;/li&gt;\n&lt;li&gt;Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake&lt;/li&gt;\n&lt;li&gt;Load each CSV into RedShift tables, truncating and loading fully&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This sort of works, but I&amp;#39;m not entirely happy with it, and I&amp;#39;m not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp;amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated, interested what other groups are doing here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113slfq", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "subreddit_subscribers": 89762, "created_utc": 1676559672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using azure devops bash scripts with json file to create databricks cluster and jobs through build pipeline. Every time when build pipeline trigger, cluster and job is creating again and again. \n\nHow to configure to ignore cluster and job if already exists and if not created, create the cluster and job in bash cli.", "author_fullname": "t2_6iqir5tk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Azure devops", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113ro6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676557129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using azure devops bash scripts with json file to create databricks cluster and jobs through build pipeline. Every time when build pipeline trigger, cluster and job is creating again and again. &lt;/p&gt;\n\n&lt;p&gt;How to configure to ignore cluster and job if already exists and if not created, create the cluster and job in bash cli.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113ro6v", "is_robot_indexable": true, "report_reasons": null, "author": "pinky_07", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113ro6v/databricks_azure_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113ro6v/databricks_azure_devops/", "subreddit_subscribers": 89762, "created_utc": 1676557129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are them, really?", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the advantages of data lakes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113qu1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are them, really?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113qu1r", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "subreddit_subscribers": 89762, "created_utc": 1676554725.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}