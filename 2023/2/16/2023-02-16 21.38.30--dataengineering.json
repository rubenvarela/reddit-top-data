{"kind": "Listing", "data": {"after": "t3_113x0ap", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Inspired by this [post](https://www.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/) and this [comment](https://www.reddit.com/r/dataengineering/comments/11349lf/comment/j8spjrg/), would r/dataengineering be interested in a project based competition? (mainly for learning purposes)\n\nTo keep things simple, we could use reddit polls to host it. We can decide on the project (and the winner) using votes.\n\nWe can hash out the details if there's enough interest, but I'd be willing to chip in the first $500 to the winning pot. My personal preference is to donate the winnings but will also defer this decision to a poll.\n\n**Open questions:**\n\n1. What should the scope of the project be? Data Engineering is a very broad field.\n2. Do you see any downside to deciding the project using a reddit poll?\n3. Do you see any downside to deciding the winner using a reddit poll?\n4. How long should the competition run? 4 weeks should be max for building a production-ready project on the side (to account for DEs with full time jobs and give new DEs time to learn)\n\nLet me know what you think :)", "author_fullname": "t2_vri7kka6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Competition!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113x4cb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676580212.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676571379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by this &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/\"&gt;post&lt;/a&gt; and this &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/11349lf/comment/j8spjrg/\"&gt;comment&lt;/a&gt;, would &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; be interested in a project based competition? (mainly for learning purposes)&lt;/p&gt;\n\n&lt;p&gt;To keep things simple, we could use reddit polls to host it. We can decide on the project (and the winner) using votes.&lt;/p&gt;\n\n&lt;p&gt;We can hash out the details if there&amp;#39;s enough interest, but I&amp;#39;d be willing to chip in the first $500 to the winning pot. My personal preference is to donate the winnings but will also defer this decision to a poll.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Open questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What should the scope of the project be? Data Engineering is a very broad field.&lt;/li&gt;\n&lt;li&gt;Do you see any downside to deciding the project using a reddit poll?&lt;/li&gt;\n&lt;li&gt;Do you see any downside to deciding the winner using a reddit poll?&lt;/li&gt;\n&lt;li&gt;How long should the competition run? 4 weeks should be max for building a production-ready project on the side (to account for DEs with full time jobs and give new DEs time to learn)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Let me know what you think :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113x4cb", "is_robot_indexable": true, "report_reasons": null, "author": "datain30", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113x4cb/data_engineering_competition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113x4cb/data_engineering_competition/", "subreddit_subscribers": 89799, "created_utc": 1676571379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. \n\nThe goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.\n\nThis entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.\n\nOnce enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.\n\nThe data is shown on Looker Studio's free service. \n\n**Links:** \n\n[GitHub](https://github.com/sachinlim/ebay_airflow)\n\n[Looker Studio](https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD) \n\nIt is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.\n\n*** \n\nTo do the project, I used an older eBay script I had made and adapted it for this project. \n\nI think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it's a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. \n\nAfter seeing an older [Reddit ETL Pipeline](https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/) post, there's definitely a lot more ways to improve this. The code I've written is perhaps a bit basic, not sure how scalable the script is, if it's clean, and there's SQL injection possible with the `INSERT` statement, though the function to get the price averages won't run for anything other than numbers.\n\nI'm using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on /r/HardwareSwapUK.", "author_fullname": "t2_v49pkl1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Pipeline for eBay Data Extraction - Simple project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113roc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676557141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. &lt;/p&gt;\n\n&lt;p&gt;The goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.&lt;/p&gt;\n\n&lt;p&gt;This entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.&lt;/p&gt;\n\n&lt;p&gt;Once enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.&lt;/p&gt;\n\n&lt;p&gt;The data is shown on Looker Studio&amp;#39;s free service. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sachinlim/ebay_airflow\"&gt;GitHub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD\"&gt;Looker Studio&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;It is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;To do the project, I used an older eBay script I had made and adapted it for this project. &lt;/p&gt;\n\n&lt;p&gt;I think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it&amp;#39;s a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. &lt;/p&gt;\n\n&lt;p&gt;After seeing an older &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/\"&gt;Reddit ETL Pipeline&lt;/a&gt; post, there&amp;#39;s definitely a lot more ways to improve this. The code I&amp;#39;ve written is perhaps a bit basic, not sure how scalable the script is, if it&amp;#39;s clean, and there&amp;#39;s SQL injection possible with the &lt;code&gt;INSERT&lt;/code&gt; statement, though the function to get the price averages won&amp;#39;t run for anything other than numbers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on &lt;a href=\"/r/HardwareSwapUK\"&gt;/r/HardwareSwapUK&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?auto=webp&amp;v=enabled&amp;s=2af9600bd4aa992f39d564819271899a1ab7fe0e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3026bad95d2fd621dadaa0bdfc4fce3cef771be7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64cf19660c70d1617f460ff07ecf3bfa7b330151", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35f2eecb197804bddcec656085545e631b9d3436", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77273085d988cf74ed3eb98bdde92c801a2e7a26", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d649cec23ed95938f033b634800c1129c1283dfe", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd5ce8b4111c182753abc04a0a76eefb510c852", "width": 1080, "height": 540}], "variants": {}, "id": "2lf3IYLRtQAxbb7CToJK_65KH5OHqQF9YhUq4E9VuTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CS Graduate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "113roc9", "is_robot_indexable": true, "report_reasons": null, "author": "Mapleess", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "subreddit_subscribers": 89799, "created_utc": 1676557141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jx9xua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A data mesh for the rest of us", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113rmqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Vo04I-HUxNZxvPo113vbsqEnkeWusBIcQjdmO-x6GK8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676557012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.loveholidays.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?auto=webp&amp;v=enabled&amp;s=f96a74cf96d249f64343eef34991ad1ccdc98edd", "width": 1162, "height": 1392}, "resolutions": [{"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb8aa1511981b78405cf1f97c5ed2765d873b593", "width": 108, "height": 129}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8894b210a20989a52cb1b49459f6e71cb47c53f", "width": 216, "height": 258}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=194b8dcd3f979379f00d3a88fbddacea92bc678e", "width": 320, "height": 383}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ac07d5030bfb285211a5f90534038ab699918d2", "width": 640, "height": 766}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5df9a8d6b0e8e771972df2a9382722402b34104b", "width": 960, "height": 1150}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dba3097170bf0efd23e8fd9432e786ea92c34bc", "width": 1080, "height": 1293}], "variants": {}, "id": "c5kxXpUajNxYr31sXoHyDiM2CcPGkWrkU4iZDryDntI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113rmqn", "is_robot_indexable": true, "report_reasons": null, "author": "dropber", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113rmqn/a_data_mesh_for_the_rest_of_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "subreddit_subscribers": 89799, "created_utc": 1676557012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_puwuw2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My project: A focused, personalized observability report for every PR. GitHub Actions: Would you find this useful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113pg54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b2L8gjGr2xKg0WKG-hT1Xv3tNDNk5E0rte_0BRFIMWE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676550430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8lpmzrednjia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8lpmzrednjia1.png?auto=webp&amp;v=enabled&amp;s=ba5c47aeac347b8fbbb153a3f7f2093ee537698c", "width": 618, "height": 1203}, "resolutions": [{"url": "https://preview.redd.it/8lpmzrednjia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b046a88be133029b6970105b7fdbf8f46602f554", "width": 108, "height": 210}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a7699ecf679e7ee2d2a3b7600630ac4768ea9c8", "width": 216, "height": 420}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1607a7c854fdfce57ebaaa2506b150e5638977a4", "width": 320, "height": 622}], "variants": {}, "id": "JPevZe8OIe_HvE94bD2wlM2BYu4RPMzjNqzycgXcTzU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113pg54", "is_robot_indexable": true, "report_reasons": null, "author": "observability_geek", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113pg54/my_project_a_focused_personalized_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8lpmzrednjia1.png", "subreddit_subscribers": 89799, "created_utc": 1676550430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.\n\nNext month, we\u2019re reading [*Data Teams: A Unified Management Model for Successful Data-Focused Teams*](https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;qid=1676346037&amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;sr=8-1) by [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/).\n\nI\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It's a book I wanna read!\n\n**Here\u2019s how the book club works:** All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.\n\n**Here\u2019s the schedule:**\n\n* March 17th: Discuss pt. 1 &amp; pt. 2\n* March 31st: Discuss pt. 3\n* April 5th: AMA w/ Author, [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/)\n* April 14th: Discuss pt. 4\n\nWe currently have dozens signed up! If you\u2019d like to join, book it [**here**](https://www.operationalanalytics.club/events/book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams-1)**.**", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Data Teams: A Unified Management Model for Successful Data-Focused Teams by Jesse Anderson", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139l8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676569687.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.&lt;/p&gt;\n\n&lt;p&gt;Next month, we\u2019re reading &lt;a href=\"https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;amp;qid=1676346037&amp;amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;amp;sr=8-1\"&gt;&lt;em&gt;Data Teams: A Unified Management Model for Successful Data-Focused Teams&lt;/em&gt;&lt;/a&gt; by &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It&amp;#39;s a book I wanna read!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s how the book club works:&lt;/strong&gt; All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s the schedule:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;March 17th: Discuss pt. 1 &amp;amp; pt. 2&lt;/li&gt;\n&lt;li&gt;March 31st: Discuss pt. 3&lt;/li&gt;\n&lt;li&gt;April 5th: AMA w/ Author, &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;April 14th: Discuss pt. 4&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We currently have dozens signed up! If you\u2019d like to join, book it &lt;a href=\"https://www.operationalanalytics.club/events/book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams-1\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1139l8l", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "subreddit_subscribers": 89799, "created_utc": 1676497889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. \n\nThere are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in 'YYYY-MM-DDTHH:MM:SS format.\n\nIn a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. \n\nHow can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I've looked into UDFs but they aren't recommended - so i'm stumped here!\n\nThanks,", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to modularise Delta Live Tables using Pyspark in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113mxlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676540959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. &lt;/p&gt;\n\n&lt;p&gt;There are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in &amp;#39;YYYY-MM-DDTHH:MM:SS format.&lt;/p&gt;\n\n&lt;p&gt;In a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. &lt;/p&gt;\n\n&lt;p&gt;How can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I&amp;#39;ve looked into UDFs but they aren&amp;#39;t recommended - so i&amp;#39;m stumped here!&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113mxlk", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "subreddit_subscribers": 89799, "created_utc": 1676540959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. \n\nMost of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. \n\nThe current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. \n\nHaving just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. \n\nI was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).\n\nFor on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?\n\nI feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. \n\nCurious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.", "author_fullname": "t2_18xs8yle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for regional contact/directory data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113fqdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676514792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. &lt;/p&gt;\n\n&lt;p&gt;Most of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. &lt;/p&gt;\n\n&lt;p&gt;The current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. &lt;/p&gt;\n\n&lt;p&gt;Having just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. &lt;/p&gt;\n\n&lt;p&gt;I was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).&lt;/p&gt;\n\n&lt;p&gt;For on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?&lt;/p&gt;\n\n&lt;p&gt;I feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. &lt;/p&gt;\n\n&lt;p&gt;Curious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113fqdd", "is_robot_indexable": true, "report_reasons": null, "author": "wves", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "subreddit_subscribers": 89799, "created_utc": 1676514792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.\n\nMy challenge is to build reliable ELT pipelines around these unreliable data sources.\n\nThrough now, the rough workflow is: \n\n1. Design XLSX template w/ each page relating to a backend table in a 1-1 relationship\n2. Host XLSX template in SharePoint site, give access to select scientist to update\n3. Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake\n4. Load each CSV into RedShift tables, truncating and loading fully\n\nThis sort of works, but I'm not entirely happy with it, and I'm not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.\n\nAny advice is appreciated, interested what other groups are doing here!", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I ingest manually created XLSX files into my data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113slfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676559672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.&lt;/p&gt;\n\n&lt;p&gt;My challenge is to build reliable ELT pipelines around these unreliable data sources.&lt;/p&gt;\n\n&lt;p&gt;Through now, the rough workflow is: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Design XLSX template w/ each page relating to a backend table in a 1-1 relationship&lt;/li&gt;\n&lt;li&gt;Host XLSX template in SharePoint site, give access to select scientist to update&lt;/li&gt;\n&lt;li&gt;Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake&lt;/li&gt;\n&lt;li&gt;Load each CSV into RedShift tables, truncating and loading fully&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This sort of works, but I&amp;#39;m not entirely happy with it, and I&amp;#39;m not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp;amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated, interested what other groups are doing here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113slfq", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "subreddit_subscribers": 89799, "created_utc": 1676559672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i've read, adding `catchup = True` will make the task start from 3 years ago. It'll also force backfill any other task which isn't fully caught up.\n\nI've noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!\n\nThe only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?", "author_fullname": "t2_zjn67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, backfilling/re-running a single task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113i7y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676523416.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676522703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i&amp;#39;ve read, adding &lt;code&gt;catchup = True&lt;/code&gt; will make the task start from 3 years ago. It&amp;#39;ll also force backfill any other task which isn&amp;#39;t fully caught up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!&lt;/p&gt;\n\n&lt;p&gt;The only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113i7y2", "is_robot_indexable": true, "report_reasons": null, "author": "Propanoate", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "subreddit_subscribers": 89799, "created_utc": 1676522703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I got a flow which I want to automate in Azure. For now we're running a python script on prem, but the server needs to get phased out.\n\n&amp;#x200B;\n\n\\- Everyday I get emails in Office 365 from client.\n\n\\- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.\n\n&amp;#x200B;\n\nMy question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.\n\n&amp;#x200B;\n\nI don't know which approach is best for this... Synapse, Power Automate, Functions... something else.", "author_fullname": "t2_ig8s88dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Excel data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139rhi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676498341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I got a flow which I want to automate in Azure. For now we&amp;#39;re running a python script on prem, but the server needs to get phased out.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Everyday I get emails in Office 365 from client.&lt;/p&gt;\n\n&lt;p&gt;- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know which approach is best for this... Synapse, Power Automate, Functions... something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139rhi", "is_robot_indexable": true, "report_reasons": null, "author": "Hs82H", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1139rhi/automate_excel_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139rhi/automate_excel_data/", "subreddit_subscribers": 89799, "created_utc": 1676498341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I run a BI team and handle most of the SQL modeling, post-ETL (we have a Data Engineering team that gets stuff into our warehouse). Think building views and datasets, which are then either turned into dashboards and reports by myself or my analysts. It is by far my favorite aspect of my job and it comes very naturally to me.\n\nThis has led me to get more interested in pursuing data engineering in my career. What I do seems to be considered \"analytics engineering\" in the space now, but what I would love to hear from the community here is where \"analytics engineering\" ends and where data engineering begins, as well as the skills that are needed for the latter but not necessarily the former. I write SQL on a daily basis, but my scripting knowledge is limited because it just hasn't been directly relevant for my career yet. For the (simple, small-data) data pipelines I create, I use a software tool that handles all of the ETL for me.\n\nAny and all help is greatly appreciated!", "author_fullname": "t2_i2gus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does analytics engineering end, and data engineering begin?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113uttu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676565486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a BI team and handle most of the SQL modeling, post-ETL (we have a Data Engineering team that gets stuff into our warehouse). Think building views and datasets, which are then either turned into dashboards and reports by myself or my analysts. It is by far my favorite aspect of my job and it comes very naturally to me.&lt;/p&gt;\n\n&lt;p&gt;This has led me to get more interested in pursuing data engineering in my career. What I do seems to be considered &amp;quot;analytics engineering&amp;quot; in the space now, but what I would love to hear from the community here is where &amp;quot;analytics engineering&amp;quot; ends and where data engineering begins, as well as the skills that are needed for the latter but not necessarily the former. I write SQL on a daily basis, but my scripting knowledge is limited because it just hasn&amp;#39;t been directly relevant for my career yet. For the (simple, small-data) data pipelines I create, I use a software tool that handles all of the ETL for me.&lt;/p&gt;\n\n&lt;p&gt;Any and all help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "113uttu", "is_robot_indexable": true, "report_reasons": null, "author": "SteezeWhiz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113uttu/where_does_analytics_engineering_end_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113uttu/where_does_analytics_engineering_end_and_data/", "subreddit_subscribers": 89799, "created_utc": 1676565486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a local data server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139kdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139kdv", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "subreddit_subscribers": 89799, "created_utc": 1676497830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are them, really?", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the advantages of data lakes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113qu1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are them, really?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113qu1r", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "subreddit_subscribers": 89799, "created_utc": 1676554725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&gt; Debezium --&gt; Kakfa --&gt; S3, is there any way we could avoid Kafka, we don't want to over-complicate the pipeline. Are they any other better ways to implement this?", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to fetch change logs from SQL Server to S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11397tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676499194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676496933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&amp;gt; Debezium --&amp;gt; Kakfa --&amp;gt; S3, is there any way we could avoid Kafka, we don&amp;#39;t want to over-complicate the pipeline. Are they any other better ways to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11397tu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "subreddit_subscribers": 89799, "created_utc": 1676496933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Quick summary on my position and structure of company: \n\nI work in an Analytics department, and we provide most reporting and analysis for the company. IT handles some of the static reports and is works on data warehousing. We have read access to most OnPrem SQL Server DBs. Right now most of our job is creating Normalized Databases in Access and creating reports from there.  We use Access for its ability to link to the SQL Server Tables. It is extremely inefficient and I really don't want to write VBA to automate things. I am pretty good at SQL and know a good amount of Python.\n\nOur current setup:\n\n\\-2 Analyst with our own SQL Server where we want to load all of our data. We cannot have access to linked tables from other servers in our environment. \n\n\\-A shared virtual machine where we use our credentials to log in to. I was hoping an orchestration tool could run on this machine.\n\n\\-We use Windows\n\nGoals:\n\nData comes in very randomly, because we work with a lot of third parties. I would like to have a script that runs maybe every hour to check if new data sources are in and then run processes from there. Most of the processes are to extract the data and load into our SQL Server where I run some stored procedures.\n\nMy issue:\n\nI want to use Airflow to automate things, but we have even been denied access to IIS in the past. My Manager doesn't really care about using new tools, and would rather just use VBA to do this. I have used Pandas to transfer data between servers before, but issues around data types have come up. I used SQLAlchemy Create Table to make sure I get the correct table structure and then load the pandas data into the newly created table, but its just not easily usable. I've looked at DBT, but from the little I know using this with SQL Server is not as easy.\n\nI probably missed some important information, but I am looking for some guidance on tools I can implement myself to get my goals done. I cannot download Airflow myself because it requires Docker which requires WSL on windows and I have to get IT to allow this ability.", "author_fullname": "t2_704n3yha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on process workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113t82f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676561355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick summary on my position and structure of company: &lt;/p&gt;\n\n&lt;p&gt;I work in an Analytics department, and we provide most reporting and analysis for the company. IT handles some of the static reports and is works on data warehousing. We have read access to most OnPrem SQL Server DBs. Right now most of our job is creating Normalized Databases in Access and creating reports from there.  We use Access for its ability to link to the SQL Server Tables. It is extremely inefficient and I really don&amp;#39;t want to write VBA to automate things. I am pretty good at SQL and know a good amount of Python.&lt;/p&gt;\n\n&lt;p&gt;Our current setup:&lt;/p&gt;\n\n&lt;p&gt;-2 Analyst with our own SQL Server where we want to load all of our data. We cannot have access to linked tables from other servers in our environment. &lt;/p&gt;\n\n&lt;p&gt;-A shared virtual machine where we use our credentials to log in to. I was hoping an orchestration tool could run on this machine.&lt;/p&gt;\n\n&lt;p&gt;-We use Windows&lt;/p&gt;\n\n&lt;p&gt;Goals:&lt;/p&gt;\n\n&lt;p&gt;Data comes in very randomly, because we work with a lot of third parties. I would like to have a script that runs maybe every hour to check if new data sources are in and then run processes from there. Most of the processes are to extract the data and load into our SQL Server where I run some stored procedures.&lt;/p&gt;\n\n&lt;p&gt;My issue:&lt;/p&gt;\n\n&lt;p&gt;I want to use Airflow to automate things, but we have even been denied access to IIS in the past. My Manager doesn&amp;#39;t really care about using new tools, and would rather just use VBA to do this. I have used Pandas to transfer data between servers before, but issues around data types have come up. I used SQLAlchemy Create Table to make sure I get the correct table structure and then load the pandas data into the newly created table, but its just not easily usable. I&amp;#39;ve looked at DBT, but from the little I know using this with SQL Server is not as easy.&lt;/p&gt;\n\n&lt;p&gt;I probably missed some important information, but I am looking for some guidance on tools I can implement myself to get my goals done. I cannot download Airflow myself because it requires Docker which requires WSL on windows and I have to get IT to allow this ability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113t82f", "is_robot_indexable": true, "report_reasons": null, "author": "gloverb2016", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113t82f/advice_on_process_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113t82f/advice_on_process_workflow/", "subreddit_subscribers": 89799, "created_utc": 1676561355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all  \nI have data engineer interview coming up with [Booking.com](https://Booking.com). I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.   \nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven't found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Booking.com Data Engineer Interview Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113que5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;br/&gt;\nI have data engineer interview coming up with &lt;a href=\"https://Booking.com\"&gt;Booking.com&lt;/a&gt;. I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.&lt;br/&gt;\nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven&amp;#39;t found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?auto=webp&amp;v=enabled&amp;s=e78c7f7b5ef8df799c5ab7819ad21ae28e0bf405", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=128d0c5e050d4fd44731c14d3596b793d77b0906", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c59c9ba9d4b265735403cbfede7112bd6a258057", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=379c22da3c6f868cf8b47f65272c52642a9b8cc1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1246f60c367c5e32f870ccbc849b81646f382184", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd246c83391f10392953e4abed795482897eca67", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=448a7d6b2100cd5eef4217e155caab07496d166c", "width": 1080, "height": 567}], "variants": {}, "id": "pOxqQ-Hg2p6yWf-XnRr9X7KZJkKYq4zbh2GbSAcQcJM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "113que5", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "subreddit_subscribers": 89799, "created_utc": 1676554753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_91odj59i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Real Definition Of \u201cDataOps\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_113epbc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwZvqp0Pi6NPy98aD615AfICVS7yLFY5FPqaRryQO3M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676511687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?auto=webp&amp;v=enabled&amp;s=79557af160f6d8db3e9cf8e02c16192b4e84ffe1", "width": 580, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ecd095287fb884429bce38b5fc59dc1c1c0a0d1", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e66defec08dfd300a5d2b44634be101482141b5", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef8678ff9e75aba71c4f8d768567077ca613caf1", "width": 320, "height": 249}], "variants": {}, "id": "ORQg5WGGxt7NVpZTOkiaKxFABZ6KiuQxVSy7rHIK4ik"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113epbc", "is_robot_indexable": true, "report_reasons": null, "author": "david_ok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113epbc/the_real_definition_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "subreddit_subscribers": 89799, "created_utc": 1676511687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Best practice is a bit of a misnomer. There is no single best practice. It\u2019s what works best for the company.\n\nIs it appropriate to define best practice as \u2018the largest combination of good practices that can be implemented given the operating environment\u2019?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practice Definition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1140gfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676579943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Best practice is a bit of a misnomer. There is no single best practice. It\u2019s what works best for the company.&lt;/p&gt;\n\n&lt;p&gt;Is it appropriate to define best practice as \u2018the largest combination of good practices that can be implemented given the operating environment\u2019?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1140gfv", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1140gfv/best_practice_definition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1140gfv/best_practice_definition/", "subreddit_subscribers": 89799, "created_utc": 1676579943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database-like ops benchmark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11401cr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1676578856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "h2oai.github.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://h2oai.github.io/db-benchmark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11401cr", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11401cr/databaselike_ops_benchmark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://h2oai.github.io/db-benchmark/", "subreddit_subscribers": 89799, "created_utc": 1676578856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm out of a job, and going over DE postings on LinkedIn, I (of course) see some requiring experience with - in this case - Spark, Airflow, DBT, Snowflake. Most of my experience is with Hadoop, Python, SQL/MySQL, with some AWS and Azure Data Factory, so although it'd be **nice** to say I have experience with those things, it's not really true.\n\nWould it be sufficient to do some projects that involve these technologies, or is something more needed?", "author_fullname": "t2_1663zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get more experience with...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11400wf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676578822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m out of a job, and going over DE postings on LinkedIn, I (of course) see some requiring experience with - in this case - Spark, Airflow, DBT, Snowflake. Most of my experience is with Hadoop, Python, SQL/MySQL, with some AWS and Azure Data Factory, so although it&amp;#39;d be &lt;strong&gt;nice&lt;/strong&gt; to say I have experience with those things, it&amp;#39;s not really true.&lt;/p&gt;\n\n&lt;p&gt;Would it be sufficient to do some projects that involve these technologies, or is something more needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11400wf", "is_robot_indexable": true, "report_reasons": null, "author": "lengthy_preamble", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11400wf/how_to_get_more_experience_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11400wf/how_to_get_more_experience_with/", "subreddit_subscribers": 89799, "created_utc": 1676578822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm testing airbyte locally. I've deployed my first connection and I noticed that AirByte has cron option for syncing data. I see that Airflow supports AirByte using airbyte operators. Can you guys  give me an example why I should use airflow together with airbyte?", "author_fullname": "t2_ei3tpd4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why use Airbyte + Airflow since Airbyte has a cron scheduler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113zt9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676578265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m testing airbyte locally. I&amp;#39;ve deployed my first connection and I noticed that AirByte has cron option for syncing data. I see that Airflow supports AirByte using airbyte operators. Can you guys  give me an example why I should use airflow together with airbyte?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113zt9b", "is_robot_indexable": true, "report_reasons": null, "author": "OdiumPura", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113zt9b/why_use_airbyte_airflow_since_airbyte_has_a_cron/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113zt9b/why_use_airbyte_airflow_since_airbyte_has_a_cron/", "subreddit_subscribers": 89799, "created_utc": 1676578265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok, so I had an old collegue reach out to see if I was interested in doing some consulting for them. He works in accounting, high level controller. My initial meeting with him is tomorrow to go over the problem they want done. I would like some advice or good questions to ask him from you more experienced people. \n\nWhat I know of the job:\n\nHe works in private equity, and he has 12 entities (from aquistions) each has their own chart of accounts. The goal is to consolidite all of them into 1 chart of accounts. Right now its a manual process just using excel workbooks and takes them 3 days to do if they are lucky. \n\nI know this might not be \"data engineering\" but its automation and we do a lot of that in data engineering. I think I can write up some python to essentially consolidate them all. Obviously there will be some caviates I find during the meeting tomorrow. But what types of things should I look for? What should I ask? Thanks for your help guys!", "author_fullname": "t2_1afmkbx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Landed my first consulting job. Looking for advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113zq7m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676578049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, so I had an old collegue reach out to see if I was interested in doing some consulting for them. He works in accounting, high level controller. My initial meeting with him is tomorrow to go over the problem they want done. I would like some advice or good questions to ask him from you more experienced people. &lt;/p&gt;\n\n&lt;p&gt;What I know of the job:&lt;/p&gt;\n\n&lt;p&gt;He works in private equity, and he has 12 entities (from aquistions) each has their own chart of accounts. The goal is to consolidite all of them into 1 chart of accounts. Right now its a manual process just using excel workbooks and takes them 3 days to do if they are lucky. &lt;/p&gt;\n\n&lt;p&gt;I know this might not be &amp;quot;data engineering&amp;quot; but its automation and we do a lot of that in data engineering. I think I can write up some python to essentially consolidate them all. Obviously there will be some caviates I find during the meeting tomorrow. But what types of things should I look for? What should I ask? Thanks for your help guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "113zq7m", "is_robot_indexable": true, "report_reasons": null, "author": "w_savage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113zq7m/landed_my_first_consulting_job_looking_for_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113zq7m/landed_my_first_consulting_job_looking_for_advice/", "subreddit_subscribers": 89799, "created_utc": 1676578049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking to learn about best practices while transforming data using SQL:\n\n - How to structure my staging data?\n - How to structure my stored procedures?\n - Logging\n - Checkpoints\n - Alerting\n\nand so on.\n\nUntil now I only used standard ETL tools to both extract and transform the data, but now I need to start learning how to transform the data using SQL since we are shifting towards ELT. \n\nIf you have any recommendations for books or courses to learn how to deal with these aspects I will be thankful.", "author_fullname": "t2_4lcvdsdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What ELT/ETL for DWH book do you recommend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113y9k0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676574274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to learn about best practices while transforming data using SQL:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How to structure my staging data?&lt;/li&gt;\n&lt;li&gt;How to structure my stored procedures?&lt;/li&gt;\n&lt;li&gt;Logging&lt;/li&gt;\n&lt;li&gt;Checkpoints&lt;/li&gt;\n&lt;li&gt;Alerting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;and so on.&lt;/p&gt;\n\n&lt;p&gt;Until now I only used standard ETL tools to both extract and transform the data, but now I need to start learning how to transform the data using SQL since we are shifting towards ELT. &lt;/p&gt;\n\n&lt;p&gt;If you have any recommendations for books or courses to learn how to deal with these aspects I will be thankful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113y9k0", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional_Key", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113y9k0/what_eltetl_for_dwh_book_do_you_recommend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113y9k0/what_eltetl_for_dwh_book_do_you_recommend/", "subreddit_subscribers": 89799, "created_utc": 1676574274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE, I often face the following situation: We have some kind of etl process/analysis/report/dashboard in place. Leadership insists on validating the final product. Sounds good, except our stakeholders are unable to provide expectations for the resulting data and we have no point of comparison elsewhere at the company. In the end, we implement some kind of common sense data check that serves no real purpose; it is busy work to satisfy our leadership.\n\nAnyone face these kinds of issues? How do you quickly develop data validation strategies without a point of comparison or stakeholder input? And how do you do this without wasting time?", "author_fullname": "t2_v3k0dc9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Validation/Quality Strategies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113x1ro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676571185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE, I often face the following situation: We have some kind of etl process/analysis/report/dashboard in place. Leadership insists on validating the final product. Sounds good, except our stakeholders are unable to provide expectations for the resulting data and we have no point of comparison elsewhere at the company. In the end, we implement some kind of common sense data check that serves no real purpose; it is busy work to satisfy our leadership.&lt;/p&gt;\n\n&lt;p&gt;Anyone face these kinds of issues? How do you quickly develop data validation strategies without a point of comparison or stakeholder input? And how do you do this without wasting time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113x1ro", "is_robot_indexable": true, "report_reasons": null, "author": "zazzersmel", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113x1ro/data_validationquality_strategies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113x1ro/data_validationquality_strategies/", "subreddit_subscribers": 89799, "created_utc": 1676571185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings \n\nBest bootcamp to data engeenier no prior experience", "author_fullname": "t2_t3bmq210", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engeenring bootcamp guidence ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113x0ap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676571073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings &lt;/p&gt;\n\n&lt;p&gt;Best bootcamp to data engeenier no prior experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113x0ap", "is_robot_indexable": true, "report_reasons": null, "author": "haseebnawaz_803", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113x0ap/data_engeenring_bootcamp_guidence/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113x0ap/data_engeenring_bootcamp_guidence/", "subreddit_subscribers": 89799, "created_utc": 1676571073.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}