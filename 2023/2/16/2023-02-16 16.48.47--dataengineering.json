{"kind": "Listing", "data": {"after": "t3_1135fwu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9e7m1qmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finnhub streaming data pipeline using Spark, Kafka, Kubernetes and more - Github repo &amp; more info in the comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 48, "top_awarded_type": null, "hide_score": false, "name": "t3_1131jqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 148, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 148, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CbifdmwD18eb4V2QhY84xODBLIeFazHoVdEaSRnHDXc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676477425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/44en1od0kdia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/44en1od0kdia1.png?auto=webp&amp;v=enabled&amp;s=bd40c6eb7926721714742b3fcf5941eb50684aaa", "width": 2381, "height": 822}, "resolutions": [{"url": "https://preview.redd.it/44en1od0kdia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f89886f46480a09c1ee666186e1da9f60a55824", "width": 108, "height": 37}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a93b4d80d6a39cba158604636da7593cee15584", "width": 216, "height": 74}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=230b2497b9dd9fada4a17d96681c94e94bdf070a", "width": 320, "height": 110}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69a45be536958efc184054cd2e95a21ad8e44e08", "width": 640, "height": 220}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adafa2f6b140f6fa94fb9a6c57e913b627d1f7af", "width": 960, "height": 331}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50dea1dddc719945eaf5df33b056b8db300da24b", "width": 1080, "height": 372}], "variants": {}, "id": "WBRfH8NzSNCYSGKZg1qxfeT9pcEhMHoSqNpwBZvqY6o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1131jqq", "is_robot_indexable": true, "report_reasons": null, "author": "LewWariat", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131jqq/finnhub_streaming_data_pipeline_using_spark_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/44en1od0kdia1.png", "subreddit_subscribers": 89772, "created_utc": 1676477425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.\n\nIs there anything like this?", "author_fullname": "t2_lf4it7cs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything like Kaggle for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11349lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676484109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.&lt;/p&gt;\n\n&lt;p&gt;Is there anything like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11349lf", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Deer_8686", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/", "subreddit_subscribers": 89772, "created_utc": 1676484109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.\n\nNext month, we\u2019re reading [*Data Teams: A Unified Management Model for Successful Data-Focused Teams*](https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;qid=1676346037&amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;sr=8-1) by [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/).\n\nI\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It's a book I wanna read!\n\n**Here\u2019s how the book club works:** All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.\n\n**Here\u2019s the schedule:**\n\n* March 17th: Discuss pt. 1 &amp; pt. 2\n* March 31st: Discuss pt. 3\n* April 5th: AMA w/ Author, [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/)\n* April 14th: Discuss pt. 4\n\nWe currently have dozens signed up! If you\u2019d like to join, book it [**here**](https://www.operationalanalytics.club/events/titlecase-book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams)**.**", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Data Teams: A Unified Management Model for Successful Data-Focused Teams by Jesse Anderson", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139l8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676498263.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.&lt;/p&gt;\n\n&lt;p&gt;Next month, we\u2019re reading &lt;a href=\"https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;amp;qid=1676346037&amp;amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;amp;sr=8-1\"&gt;&lt;em&gt;Data Teams: A Unified Management Model for Successful Data-Focused Teams&lt;/em&gt;&lt;/a&gt; by &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It&amp;#39;s a book I wanna read!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s how the book club works:&lt;/strong&gt; All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s the schedule:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;March 17th: Discuss pt. 1 &amp;amp; pt. 2&lt;/li&gt;\n&lt;li&gt;March 31st: Discuss pt. 3&lt;/li&gt;\n&lt;li&gt;April 5th: AMA w/ Author, &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;April 14th: Discuss pt. 4&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We currently have dozens signed up! If you\u2019d like to join, book it &lt;a href=\"https://www.operationalanalytics.club/events/titlecase-book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1139l8l", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "subreddit_subscribers": 89772, "created_utc": 1676497889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jx9xua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A data mesh for the rest of us", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113rmqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Vo04I-HUxNZxvPo113vbsqEnkeWusBIcQjdmO-x6GK8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676557012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.loveholidays.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?auto=webp&amp;v=enabled&amp;s=f96a74cf96d249f64343eef34991ad1ccdc98edd", "width": 1162, "height": 1392}, "resolutions": [{"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb8aa1511981b78405cf1f97c5ed2765d873b593", "width": 108, "height": 129}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8894b210a20989a52cb1b49459f6e71cb47c53f", "width": 216, "height": 258}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=194b8dcd3f979379f00d3a88fbddacea92bc678e", "width": 320, "height": 383}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ac07d5030bfb285211a5f90534038ab699918d2", "width": 640, "height": 766}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5df9a8d6b0e8e771972df2a9382722402b34104b", "width": 960, "height": 1150}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dba3097170bf0efd23e8fd9432e786ea92c34bc", "width": 1080, "height": 1293}], "variants": {}, "id": "c5kxXpUajNxYr31sXoHyDiM2CcPGkWrkU4iZDryDntI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113rmqn", "is_robot_indexable": true, "report_reasons": null, "author": "dropber", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113rmqn/a_data_mesh_for_the_rest_of_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "subreddit_subscribers": 89772, "created_utc": 1676557012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. \n\nThere are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in 'YYYY-MM-DDTHH:MM:SS format.\n\nIn a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. \n\nHow can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I've looked into UDFs but they aren't recommended - so i'm stumped here!\n\nThanks,", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to modularise Delta Live Tables using Pyspark in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113mxlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676540959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. &lt;/p&gt;\n\n&lt;p&gt;There are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in &amp;#39;YYYY-MM-DDTHH:MM:SS format.&lt;/p&gt;\n\n&lt;p&gt;In a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. &lt;/p&gt;\n\n&lt;p&gt;How can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I&amp;#39;ve looked into UDFs but they aren&amp;#39;t recommended - so i&amp;#39;m stumped here!&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113mxlk", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "subreddit_subscribers": 89772, "created_utc": 1676540959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. \n\nMost of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. \n\nThe current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. \n\nHaving just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. \n\nI was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).\n\nFor on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?\n\nI feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. \n\nCurious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.", "author_fullname": "t2_18xs8yle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for regional contact/directory data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113fqdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676514792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. &lt;/p&gt;\n\n&lt;p&gt;Most of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. &lt;/p&gt;\n\n&lt;p&gt;The current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. &lt;/p&gt;\n\n&lt;p&gt;Having just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. &lt;/p&gt;\n\n&lt;p&gt;I was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).&lt;/p&gt;\n\n&lt;p&gt;For on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?&lt;/p&gt;\n\n&lt;p&gt;I feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. &lt;/p&gt;\n\n&lt;p&gt;Curious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113fqdd", "is_robot_indexable": true, "report_reasons": null, "author": "wves", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "subreddit_subscribers": 89772, "created_utc": 1676514792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i've read, adding `catchup = True` will make the task start from 3 years ago. It'll also force backfill any other task which isn't fully caught up.\n\nI've noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!\n\nThe only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?", "author_fullname": "t2_zjn67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, backfilling/re-running a single task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113i7y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676523416.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676522703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i&amp;#39;ve read, adding &lt;code&gt;catchup = True&lt;/code&gt; will make the task start from 3 years ago. It&amp;#39;ll also force backfill any other task which isn&amp;#39;t fully caught up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!&lt;/p&gt;\n\n&lt;p&gt;The only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113i7y2", "is_robot_indexable": true, "report_reasons": null, "author": "Propanoate", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "subreddit_subscribers": 89772, "created_utc": 1676522703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don't solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?", "author_fullname": "t2_3tiq5c7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's missing in the modern data stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11388la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676494377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don&amp;#39;t solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11388la", "is_robot_indexable": true, "report_reasons": null, "author": "ownubie", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "subreddit_subscribers": 89772, "created_utc": 1676494377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?", "author_fullname": "t2_bwp6e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your team\u2019s method for storing credentials for your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1137p0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676492961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1137p0i", "is_robot_indexable": true, "report_reasons": null, "author": "wild_bill34", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "subreddit_subscribers": 89772, "created_utc": 1676492961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_puwuw2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My project: A focused, personalized observability report for every PR. GitHub Actions: Would you find this useful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113pg54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b2L8gjGr2xKg0WKG-hT1Xv3tNDNk5E0rte_0BRFIMWE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676550430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8lpmzrednjia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8lpmzrednjia1.png?auto=webp&amp;v=enabled&amp;s=ba5c47aeac347b8fbbb153a3f7f2093ee537698c", "width": 618, "height": 1203}, "resolutions": [{"url": "https://preview.redd.it/8lpmzrednjia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b046a88be133029b6970105b7fdbf8f46602f554", "width": 108, "height": 210}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a7699ecf679e7ee2d2a3b7600630ac4768ea9c8", "width": 216, "height": 420}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1607a7c854fdfce57ebaaa2506b150e5638977a4", "width": 320, "height": 622}], "variants": {}, "id": "JPevZe8OIe_HvE94bD2wlM2BYu4RPMzjNqzycgXcTzU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113pg54", "is_robot_indexable": true, "report_reasons": null, "author": "observability_geek", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113pg54/my_project_a_focused_personalized_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8lpmzrednjia1.png", "subreddit_subscribers": 89772, "created_utc": 1676550430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. \n\nThe goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.\n\nThis entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.\n\nOnce enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.\n\nThe data is shown on Looker Studio's free service. \n\n**Links:** \n\n[GitHub](https://github.com/sachinlim/ebay_airflow)\n\n[Looker Studio](https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD) \n\nIt is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.\n\n*** \n\nTo do the project, I used an older eBay script I had made and adapted it for this project. \n\nI think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it's a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. \n\nAfter seeing an older [Reddit ETL Pipeline](https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/) post, there's definitely a lot more ways to improve this. The code I've written is perhaps a bit basic, not sure how scalable the script is, if it's clean, and there's SQL injection possible with the `INSERT` statement, though the function to get the price averages won't run for anything other than numbers.\n\nI'm using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on /r/HardwareSwapUK.", "author_fullname": "t2_v49pkl1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Pipeline for eBay Data Extraction - Simple project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113roc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676557141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. &lt;/p&gt;\n\n&lt;p&gt;The goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.&lt;/p&gt;\n\n&lt;p&gt;This entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.&lt;/p&gt;\n\n&lt;p&gt;Once enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.&lt;/p&gt;\n\n&lt;p&gt;The data is shown on Looker Studio&amp;#39;s free service. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sachinlim/ebay_airflow\"&gt;GitHub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD\"&gt;Looker Studio&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;It is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;To do the project, I used an older eBay script I had made and adapted it for this project. &lt;/p&gt;\n\n&lt;p&gt;I think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it&amp;#39;s a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. &lt;/p&gt;\n\n&lt;p&gt;After seeing an older &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/\"&gt;Reddit ETL Pipeline&lt;/a&gt; post, there&amp;#39;s definitely a lot more ways to improve this. The code I&amp;#39;ve written is perhaps a bit basic, not sure how scalable the script is, if it&amp;#39;s clean, and there&amp;#39;s SQL injection possible with the &lt;code&gt;INSERT&lt;/code&gt; statement, though the function to get the price averages won&amp;#39;t run for anything other than numbers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on &lt;a href=\"/r/HardwareSwapUK\"&gt;/r/HardwareSwapUK&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?auto=webp&amp;v=enabled&amp;s=2af9600bd4aa992f39d564819271899a1ab7fe0e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3026bad95d2fd621dadaa0bdfc4fce3cef771be7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64cf19660c70d1617f460ff07ecf3bfa7b330151", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35f2eecb197804bddcec656085545e631b9d3436", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77273085d988cf74ed3eb98bdde92c801a2e7a26", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d649cec23ed95938f033b634800c1129c1283dfe", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd5ce8b4111c182753abc04a0a76eefb510c852", "width": 1080, "height": 540}], "variants": {}, "id": "2lf3IYLRtQAxbb7CToJK_65KH5OHqQF9YhUq4E9VuTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CS Graduate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "113roc9", "is_robot_indexable": true, "report_reasons": null, "author": "Mapleess", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "subreddit_subscribers": 89772, "created_utc": 1676557141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I got a flow which I want to automate in Azure. For now we're running a python script on prem, but the server needs to get phased out.\n\n&amp;#x200B;\n\n\\- Everyday I get emails in Office 365 from client.\n\n\\- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.\n\n&amp;#x200B;\n\nMy question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.\n\n&amp;#x200B;\n\nI don't know which approach is best for this... Synapse, Power Automate, Functions... something else.", "author_fullname": "t2_ig8s88dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Excel data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139rhi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676498341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I got a flow which I want to automate in Azure. For now we&amp;#39;re running a python script on prem, but the server needs to get phased out.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Everyday I get emails in Office 365 from client.&lt;/p&gt;\n\n&lt;p&gt;- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know which approach is best for this... Synapse, Power Automate, Functions... something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139rhi", "is_robot_indexable": true, "report_reasons": null, "author": "Hs82H", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1139rhi/automate_excel_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139rhi/automate_excel_data/", "subreddit_subscribers": 89772, "created_utc": 1676498341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this post [https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking\\_for\\_data\\_analytics\\_engine\\_for\\_lowlatency/](https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/) I was guided towards Headless BI, specifically [Cube.dev](https://Cube.dev).\n\nWe looked into it, seemed to work for us and went with it. Nice!\n\nExcept... we've run into various edge cases where we're now questioning our choice heavily.\n\nBrowsing around, I've found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?\n\nAnd while at it, how a tool like that would be actually called? Headless BI doesn't turn up too much results. Semantic Layer? Maybe something else?", "author_fullname": "t2_4gt5e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Looker/LookML &amp; Cube.dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1131e3a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676477013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/\"&gt;https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/&lt;/a&gt; I was guided towards Headless BI, specifically &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We looked into it, seemed to work for us and went with it. Nice!&lt;/p&gt;\n\n&lt;p&gt;Except... we&amp;#39;ve run into various edge cases where we&amp;#39;re now questioning our choice heavily.&lt;/p&gt;\n\n&lt;p&gt;Browsing around, I&amp;#39;ve found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?&lt;/p&gt;\n\n&lt;p&gt;And while at it, how a tool like that would be actually called? Headless BI doesn&amp;#39;t turn up too much results. Semantic Layer? Maybe something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1131e3a", "is_robot_indexable": true, "report_reasons": null, "author": "psycketom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "subreddit_subscribers": 89772, "created_utc": 1676477013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a local data server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139kdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139kdv", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "subreddit_subscribers": 89772, "created_utc": 1676497830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&gt; Debezium --&gt; Kakfa --&gt; S3, is there any way we could avoid Kafka, we don't want to over-complicate the pipeline. Are they any other better ways to implement this?", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to fetch change logs from SQL Server to S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11397tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676499194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676496933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&amp;gt; Debezium --&amp;gt; Kakfa --&amp;gt; S3, is there any way we could avoid Kafka, we don&amp;#39;t want to over-complicate the pipeline. Are they any other better ways to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11397tu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "subreddit_subscribers": 89772, "created_utc": 1676496933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi \ud83d\udc4b guys, \n\nAs a programmer, I want to learn Linux to expand my skills and knowledge. However, with so many Linux distributions out there, I'm not sure which one to choose. I want to pick a distribution that will be most useful for programming purposes and that will help me develop my skills as a programmer.\n\nSo, which Linux distribution do you recommend for a programmer to learn, and why? Are there any particular features or tools that make a certain distribution better suited for programming tasks? I'm open to any suggestions and advice you may have. Thank you in advance!", "author_fullname": "t2_m6lk62t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Linux distribution is best to learn for a programmer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1133zm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676483410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi \ud83d\udc4b guys, &lt;/p&gt;\n\n&lt;p&gt;As a programmer, I want to learn Linux to expand my skills and knowledge. However, with so many Linux distributions out there, I&amp;#39;m not sure which one to choose. I want to pick a distribution that will be most useful for programming purposes and that will help me develop my skills as a programmer.&lt;/p&gt;\n\n&lt;p&gt;So, which Linux distribution do you recommend for a programmer to learn, and why? Are there any particular features or tools that make a certain distribution better suited for programming tasks? I&amp;#39;m open to any suggestions and advice you may have. Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1133zm4", "is_robot_indexable": true, "report_reasons": null, "author": "TelevisionDue5491", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1133zm4/which_linux_distribution_is_best_to_learn_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1133zm4/which_linux_distribution_is_best_to_learn_for_a/", "subreddit_subscribers": 89772, "created_utc": 1676483410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.\n\nMy challenge is to build reliable ELT pipelines around these unreliable data sources.\n\nThrough now, the rough workflow is: \n\n1. Design XLSX template w/ each page relating to a backend table in a 1-1 relationship\n2. Host XLSX template in SharePoint site, give access to select scientist to update\n3. Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake\n4. Load each CSV into RedShift tables, truncating and loading fully\n\nThis sort of works, but I'm not entirely happy with it, and I'm not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.\n\nAny advice is appreciated, interested what other groups are doing here!", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I ingest manually created XLSX files into my data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113slfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676559672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.&lt;/p&gt;\n\n&lt;p&gt;My challenge is to build reliable ELT pipelines around these unreliable data sources.&lt;/p&gt;\n\n&lt;p&gt;Through now, the rough workflow is: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Design XLSX template w/ each page relating to a backend table in a 1-1 relationship&lt;/li&gt;\n&lt;li&gt;Host XLSX template in SharePoint site, give access to select scientist to update&lt;/li&gt;\n&lt;li&gt;Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake&lt;/li&gt;\n&lt;li&gt;Load each CSV into RedShift tables, truncating and loading fully&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This sort of works, but I&amp;#39;m not entirely happy with it, and I&amp;#39;m not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp;amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated, interested what other groups are doing here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113slfq", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "subreddit_subscribers": 89772, "created_utc": 1676559672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all  \nI have data engineer interview coming up with [Booking.com](https://Booking.com). I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.   \nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven't found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Booking.com Data Engineer Interview Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113que5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;br/&gt;\nI have data engineer interview coming up with &lt;a href=\"https://Booking.com\"&gt;Booking.com&lt;/a&gt;. I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.&lt;br/&gt;\nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven&amp;#39;t found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?auto=webp&amp;v=enabled&amp;s=e78c7f7b5ef8df799c5ab7819ad21ae28e0bf405", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=128d0c5e050d4fd44731c14d3596b793d77b0906", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c59c9ba9d4b265735403cbfede7112bd6a258057", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=379c22da3c6f868cf8b47f65272c52642a9b8cc1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1246f60c367c5e32f870ccbc849b81646f382184", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd246c83391f10392953e4abed795482897eca67", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=448a7d6b2100cd5eef4217e155caab07496d166c", "width": 1080, "height": 567}], "variants": {}, "id": "pOxqQ-Hg2p6yWf-XnRr9X7KZJkKYq4zbh2GbSAcQcJM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "113que5", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "subreddit_subscribers": 89772, "created_utc": 1676554753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_91odj59i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Real Definition Of \u201cDataOps\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_113epbc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwZvqp0Pi6NPy98aD615AfICVS7yLFY5FPqaRryQO3M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676511687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?auto=webp&amp;v=enabled&amp;s=79557af160f6d8db3e9cf8e02c16192b4e84ffe1", "width": 580, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ecd095287fb884429bce38b5fc59dc1c1c0a0d1", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e66defec08dfd300a5d2b44634be101482141b5", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef8678ff9e75aba71c4f8d768567077ca613caf1", "width": 320, "height": 249}], "variants": {}, "id": "ORQg5WGGxt7NVpZTOkiaKxFABZ6KiuQxVSy7rHIK4ik"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113epbc", "is_robot_indexable": true, "report_reasons": null, "author": "david_ok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113epbc/the_real_definition_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "subreddit_subscribers": 89772, "created_utc": 1676511687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Quick summary on my position and structure of company: \n\nI work in an Analytics department, and we provide most reporting and analysis for the company. IT handles some of the static reports and is works on data warehousing. We have read access to most OnPrem SQL Server DBs. Right now most of our job is creating Normalized Databases in Access and creating reports from there.  We use Access for its ability to link to the SQL Server Tables. It is extremely inefficient and I really don't want to write VBA to automate things. I am pretty good at SQL and know a good amount of Python.\n\nOur current setup:\n\n\\-2 Analyst with our own SQL Server where we want to load all of our data. We cannot have access to linked tables from other servers in our environment. \n\n\\-A shared virtual machine where we use our credentials to log in to. I was hoping an orchestration tool could run on this machine.\n\n\\-We use Windows\n\nGoals:\n\nData comes in very randomly, because we work with a lot of third parties. I would like to have a script that runs maybe every hour to check if new data sources are in and then run processes from there. Most of the processes are to extract the data and load into our SQL Server where I run some stored procedures.\n\nMy issue:\n\nI want to use Airflow to automate things, but we have even been denied access to IIS in the past. My Manager doesn't really care about using new tools, and would rather just use VBA to do this. I have used Pandas to transfer data between servers before, but issues around data types have come up. I used SQLAlchemy Create Table to make sure I get the correct table structure and then load the pandas data into the newly created table, but its just not easily usable. I've looked at DBT, but from the little I know using this with SQL Server is not as easy.\n\nI probably missed some important information, but I am looking for some guidance on tools I can implement myself to get my goals done. I cannot download Airflow myself because it requires Docker which requires WSL on windows and I have to get IT to allow this ability.", "author_fullname": "t2_704n3yha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on process workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113t82f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676561355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick summary on my position and structure of company: &lt;/p&gt;\n\n&lt;p&gt;I work in an Analytics department, and we provide most reporting and analysis for the company. IT handles some of the static reports and is works on data warehousing. We have read access to most OnPrem SQL Server DBs. Right now most of our job is creating Normalized Databases in Access and creating reports from there.  We use Access for its ability to link to the SQL Server Tables. It is extremely inefficient and I really don&amp;#39;t want to write VBA to automate things. I am pretty good at SQL and know a good amount of Python.&lt;/p&gt;\n\n&lt;p&gt;Our current setup:&lt;/p&gt;\n\n&lt;p&gt;-2 Analyst with our own SQL Server where we want to load all of our data. We cannot have access to linked tables from other servers in our environment. &lt;/p&gt;\n\n&lt;p&gt;-A shared virtual machine where we use our credentials to log in to. I was hoping an orchestration tool could run on this machine.&lt;/p&gt;\n\n&lt;p&gt;-We use Windows&lt;/p&gt;\n\n&lt;p&gt;Goals:&lt;/p&gt;\n\n&lt;p&gt;Data comes in very randomly, because we work with a lot of third parties. I would like to have a script that runs maybe every hour to check if new data sources are in and then run processes from there. Most of the processes are to extract the data and load into our SQL Server where I run some stored procedures.&lt;/p&gt;\n\n&lt;p&gt;My issue:&lt;/p&gt;\n\n&lt;p&gt;I want to use Airflow to automate things, but we have even been denied access to IIS in the past. My Manager doesn&amp;#39;t really care about using new tools, and would rather just use VBA to do this. I have used Pandas to transfer data between servers before, but issues around data types have come up. I used SQLAlchemy Create Table to make sure I get the correct table structure and then load the pandas data into the newly created table, but its just not easily usable. I&amp;#39;ve looked at DBT, but from the little I know using this with SQL Server is not as easy.&lt;/p&gt;\n\n&lt;p&gt;I probably missed some important information, but I am looking for some guidance on tools I can implement myself to get my goals done. I cannot download Airflow myself because it requires Docker which requires WSL on windows and I have to get IT to allow this ability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113t82f", "is_robot_indexable": true, "report_reasons": null, "author": "gloverb2016", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113t82f/advice_on_process_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113t82f/advice_on_process_workflow/", "subreddit_subscribers": 89772, "created_utc": 1676561355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are them, really?", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the advantages of data lakes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113qu1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are them, really?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113qu1r", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "subreddit_subscribers": 89772, "created_utc": 1676554725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a project that requires building a solution to collect security log data from our AWS environment and our application to figure out if our network or data have been compromises. As I understand it, that's called indicators of compromise (IOC - [https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/](https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/)). The intention is to come up with patterns that I can detect in historical data to indicate if we were compromised in any way. Then implement those in a real-time solution to detect attacks when they happen.\n\nI'm wondering if anyone is aware or worked on a similar problem and what are possible solutions?", "author_fullname": "t2_vhiekvgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting indicators of compromise in security log data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113qhy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676553690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a project that requires building a solution to collect security log data from our AWS environment and our application to figure out if our network or data have been compromises. As I understand it, that&amp;#39;s called indicators of compromise (IOC - &lt;a href=\"https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/\"&gt;https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/&lt;/a&gt;). The intention is to come up with patterns that I can detect in historical data to indicate if we were compromised in any way. Then implement those in a real-time solution to detect attacks when they happen.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if anyone is aware or worked on a similar problem and what are possible solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?auto=webp&amp;v=enabled&amp;s=391364df561ed9fd027ab27560770b1e4b9e8507", "width": 421, "height": 260}, "resolutions": [{"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a69011b279ea40c4c11a133e0a5a677957b175c", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6745d5ad37fceafc70fb2f35c4229bbafd95f4bb", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c0dccddad114b72372cdc4fa293ee7c818dea90", "width": 320, "height": 197}], "variants": {}, "id": "YEXWbMD95Yu6a-9J0eJxXwJagWmhiuK3YIF_DFU5HaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113qhy2", "is_robot_indexable": true, "report_reasons": null, "author": "royondata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113qhy2/detecting_indicators_of_compromise_in_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113qhy2/detecting_indicators_of_compromise_in_security/", "subreddit_subscribers": 89772, "created_utc": 1676553690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This isn't so much a question about tech, as it is methods and processes.\n\nBackground: analyst 5 years, dwh dev 2 years, data engineer/analyst 1 year (total 8 years, earliest data job to current).\n\nI have this problem, a tendency to want to explore the entirety of a shiny, new data set -- sometimes at work, sometimes on my own projects (which are educational but time-sensitive). I know I need to plan and execute to be productive, and I do, but so often I find myself down rabbit holes of curiosity. Sometimes come out the other end with the completed objective, sometimes I don't, but I do always find something valuable.\n\nI find useful information when I do -- just basic profiling work, and some drilling on curious aspects of the structure. I enjoy pulling it all apart, cleaning and organizing it, and making it available. I usually add visuals for everything, iteratively where possible.\n\nThe problem is that I feel as though I can't give confident direction or feedback on my projects unless I've really explored the data to the point it's on my mind while I'm not working on it (which is when many answers come). At the end of this comprehensive dive through everything, I can find the interesting parts, develop questions, and answer them in a meaningful way.\n\nBut MAN, do I struggle with a fast process. Turn-around on a project at work always seems to be so much shorter than I'd estimate with fast coworkers, and I wonder why I can't cut to the requested answers more quickly.\n\nTL;DR:\nDoes anyone else seem to find themselves trying to consume the entirety of the data like an actual Python, when all that's needed is the right portion?", "author_fullname": "t2_77k8dmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytical Process Refinement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113iqt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676524521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This isn&amp;#39;t so much a question about tech, as it is methods and processes.&lt;/p&gt;\n\n&lt;p&gt;Background: analyst 5 years, dwh dev 2 years, data engineer/analyst 1 year (total 8 years, earliest data job to current).&lt;/p&gt;\n\n&lt;p&gt;I have this problem, a tendency to want to explore the entirety of a shiny, new data set -- sometimes at work, sometimes on my own projects (which are educational but time-sensitive). I know I need to plan and execute to be productive, and I do, but so often I find myself down rabbit holes of curiosity. Sometimes come out the other end with the completed objective, sometimes I don&amp;#39;t, but I do always find something valuable.&lt;/p&gt;\n\n&lt;p&gt;I find useful information when I do -- just basic profiling work, and some drilling on curious aspects of the structure. I enjoy pulling it all apart, cleaning and organizing it, and making it available. I usually add visuals for everything, iteratively where possible.&lt;/p&gt;\n\n&lt;p&gt;The problem is that I feel as though I can&amp;#39;t give confident direction or feedback on my projects unless I&amp;#39;ve really explored the data to the point it&amp;#39;s on my mind while I&amp;#39;m not working on it (which is when many answers come). At the end of this comprehensive dive through everything, I can find the interesting parts, develop questions, and answer them in a meaningful way.&lt;/p&gt;\n\n&lt;p&gt;But MAN, do I struggle with a fast process. Turn-around on a project at work always seems to be so much shorter than I&amp;#39;d estimate with fast coworkers, and I wonder why I can&amp;#39;t cut to the requested answers more quickly.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:\nDoes anyone else seem to find themselves trying to consume the entirety of the data like an actual Python, when all that&amp;#39;s needed is the right portion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113iqt2", "is_robot_indexable": true, "report_reasons": null, "author": "yeahbarry", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113iqt2/analytical_process_refinement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113iqt2/analytical_process_refinement/", "subreddit_subscribers": 89772, "created_utc": 1676524521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to figure out what the role of a \u201ccontent manager\u201d is at Google.  I\u2019m not sure I even have the right title. But essentially the role sits in the business and knows a lot about the data.  They work as a conduit between data engineering and the business. \n\nWhere I work we are called data managers. We typically have domain expertise and have usually worked in the business before.  They will also be fluent in data so they can help to build out requirements for the DEs.  It\u2019s similar to a BA but the big difference is they understand how the data is used. \n\nDo other people have this role in their company? I\u2019m really looking for tech industry examples. \n\nContext: my company is non-tech and wants to be tech and I\u2019m trying to figure out how to talk to leadership in terms they understand.", "author_fullname": "t2_7ge1tylx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Content Manager? at Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113anoe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676500594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to figure out what the role of a \u201ccontent manager\u201d is at Google.  I\u2019m not sure I even have the right title. But essentially the role sits in the business and knows a lot about the data.  They work as a conduit between data engineering and the business. &lt;/p&gt;\n\n&lt;p&gt;Where I work we are called data managers. We typically have domain expertise and have usually worked in the business before.  They will also be fluent in data so they can help to build out requirements for the DEs.  It\u2019s similar to a BA but the big difference is they understand how the data is used. &lt;/p&gt;\n\n&lt;p&gt;Do other people have this role in their company? I\u2019m really looking for tech industry examples. &lt;/p&gt;\n\n&lt;p&gt;Context: my company is non-tech and wants to be tech and I\u2019m trying to figure out how to talk to leadership in terms they understand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113anoe", "is_robot_indexable": true, "report_reasons": null, "author": "fannypackbringitback", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113anoe/content_manager_at_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113anoe/content_manager_at_google/", "subreddit_subscribers": 89772, "created_utc": 1676500594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not a data engineer and am looking for some advice. Currently at my work we create R applications pulling directly from a vendor database. The queries for our current applications aren't that large so we query the DB directly but I would like to try and speed things up. I am looking to optimize our workflow and am thinking of trying the following:\n\n- Copy the db tables into our data warehouse nightly\n- Create custom tables for our applications that run nightly in a \"data mart\"\n\nThis would allow our applications to hit the data mart tables instead of running the entire queries which would significantly speed up load times. Does this approach seem correct? Is there a better way of handling this?", "author_fullname": "t2_10g3u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on best practices for ETL process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1135fwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676487064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not a data engineer and am looking for some advice. Currently at my work we create R applications pulling directly from a vendor database. The queries for our current applications aren&amp;#39;t that large so we query the DB directly but I would like to try and speed things up. I am looking to optimize our workflow and am thinking of trying the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Copy the db tables into our data warehouse nightly&lt;/li&gt;\n&lt;li&gt;Create custom tables for our applications that run nightly in a &amp;quot;data mart&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This would allow our applications to hit the data mart tables instead of running the entire queries which would significantly speed up load times. Does this approach seem correct? Is there a better way of handling this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1135fwu", "is_robot_indexable": true, "report_reasons": null, "author": "pf903", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1135fwu/question_on_best_practices_for_etl_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1135fwu/question_on_best_practices_for_etl_process/", "subreddit_subscribers": 89772, "created_utc": 1676487064.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}