{"kind": "Listing", "data": {"after": "t3_113qhy2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jx9xua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A data mesh for the rest of us", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113rmqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Vo04I-HUxNZxvPo113vbsqEnkeWusBIcQjdmO-x6GK8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676557012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.loveholidays.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?auto=webp&amp;v=enabled&amp;s=f96a74cf96d249f64343eef34991ad1ccdc98edd", "width": 1162, "height": 1392}, "resolutions": [{"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb8aa1511981b78405cf1f97c5ed2765d873b593", "width": 108, "height": 129}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8894b210a20989a52cb1b49459f6e71cb47c53f", "width": 216, "height": 258}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=194b8dcd3f979379f00d3a88fbddacea92bc678e", "width": 320, "height": 383}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ac07d5030bfb285211a5f90534038ab699918d2", "width": 640, "height": 766}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5df9a8d6b0e8e771972df2a9382722402b34104b", "width": 960, "height": 1150}, {"url": "https://external-preview.redd.it/CLu9V6SZJ8LyL1Dr6EP6CguToTYYV9yKnx9bT4NB0Wk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dba3097170bf0efd23e8fd9432e786ea92c34bc", "width": 1080, "height": 1293}], "variants": {}, "id": "c5kxXpUajNxYr31sXoHyDiM2CcPGkWrkU4iZDryDntI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113rmqn", "is_robot_indexable": true, "report_reasons": null, "author": "dropber", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113rmqn/a_data_mesh_for_the_rest_of_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.loveholidays.com/a-data-mesh-for-the-rest-of-us-12e2c10ac128", "subreddit_subscribers": 89795, "created_utc": 1676557012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.\n\nNext month, we\u2019re reading [*Data Teams: A Unified Management Model for Successful Data-Focused Teams*](https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;qid=1676346037&amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;sr=8-1) by [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/).\n\nI\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It's a book I wanna read!\n\n**Here\u2019s how the book club works:** All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.\n\n**Here\u2019s the schedule:**\n\n* March 17th: Discuss pt. 1 &amp; pt. 2\n* March 31st: Discuss pt. 3\n* April 5th: AMA w/ Author, [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/)\n* April 14th: Discuss pt. 4\n\nWe currently have dozens signed up! If you\u2019d like to join, book it [**here**](https://www.operationalanalytics.club/events/book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams-1)**.**", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Data Teams: A Unified Management Model for Successful Data-Focused Teams by Jesse Anderson", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139l8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676569687.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.&lt;/p&gt;\n\n&lt;p&gt;Next month, we\u2019re reading &lt;a href=\"https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;amp;qid=1676346037&amp;amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;amp;sr=8-1\"&gt;&lt;em&gt;Data Teams: A Unified Management Model for Successful Data-Focused Teams&lt;/em&gt;&lt;/a&gt; by &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It&amp;#39;s a book I wanna read!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s how the book club works:&lt;/strong&gt; All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s the schedule:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;March 17th: Discuss pt. 1 &amp;amp; pt. 2&lt;/li&gt;\n&lt;li&gt;March 31st: Discuss pt. 3&lt;/li&gt;\n&lt;li&gt;April 5th: AMA w/ Author, &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;April 14th: Discuss pt. 4&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We currently have dozens signed up! If you\u2019d like to join, book it &lt;a href=\"https://www.operationalanalytics.club/events/book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams-1\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1139l8l", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "subreddit_subscribers": 89795, "created_utc": 1676497889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. \n\nThe goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.\n\nThis entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.\n\nOnce enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.\n\nThe data is shown on Looker Studio's free service. \n\n**Links:** \n\n[GitHub](https://github.com/sachinlim/ebay_airflow)\n\n[Looker Studio](https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD) \n\nIt is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.\n\n*** \n\nTo do the project, I used an older eBay script I had made and adapted it for this project. \n\nI think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it's a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. \n\nAfter seeing an older [Reddit ETL Pipeline](https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/) post, there's definitely a lot more ways to improve this. The code I've written is perhaps a bit basic, not sure how scalable the script is, if it's clean, and there's SQL injection possible with the `INSERT` statement, though the function to get the price averages won't run for anything other than numbers.\n\nI'm using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on /r/HardwareSwapUK.", "author_fullname": "t2_v49pkl1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow Pipeline for eBay Data Extraction - Simple project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113roc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676557141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Price Data is gathered from eBay using a Python web scraper called Beautiful Soup, transformed to remove outliers, and averages of the data are loaded onto a Postgres database. &lt;/p&gt;\n\n&lt;p&gt;The goal of this project is to get a snapshot for the value of used graphics cards (GPUs) today, and at some point in the future, compare how the prices have been fluctuating and how they compare to newer GPUs.&lt;/p&gt;\n\n&lt;p&gt;This entire process is hosted on the cloud through the use of an AWS EC2 instance and AWS RDS for the Postgres database. It runs fully independently on the cloud to automate the process of data extraction, transformation, and loading, every day so that a price history for the Nvidia GPUs can be collected.&lt;/p&gt;\n\n&lt;p&gt;Once enough data has been collected and stored, it can start to give a picture on the price performance of used graphics cards in the (UK) market, especially after the crypto mining boom and crash during and after the COVID-19 pandemic.&lt;/p&gt;\n\n&lt;p&gt;The data is shown on Looker Studio&amp;#39;s free service. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sachinlim/ebay_airflow\"&gt;GitHub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lookerstudio.google.com/u/0/reporting/47f510fa-6d05-4839-a984-9c3f9f790bab/page/tDaFD\"&gt;Looker Studio&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;It is pretty much a flat line right now, as there is not a lot of price movements. However, looking back on a monthly/weekly scale with 12-24 months worth of data, it would paint a very interesting picture.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;To do the project, I used an older eBay script I had made and adapted it for this project. &lt;/p&gt;\n\n&lt;p&gt;I think Airflow for a basic task like this is overkill but I got the chance to play around with Airflow (dynamic dag), AWS EC2, RDS (Postgres) and Looker Studio, so it&amp;#39;s a win for me, regardless. I was actually surprised that Looker Studio updates every day to show the changes. &lt;/p&gt;\n\n&lt;p&gt;After seeing an older &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/\"&gt;Reddit ETL Pipeline&lt;/a&gt; post, there&amp;#39;s definitely a lot more ways to improve this. The code I&amp;#39;ve written is perhaps a bit basic, not sure how scalable the script is, if it&amp;#39;s clean, and there&amp;#39;s SQL injection possible with the &lt;code&gt;INSERT&lt;/code&gt; statement, though the function to get the price averages won&amp;#39;t run for anything other than numbers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using an t2.medium instance because the t2.small instance would crash the website, but that was when I was playing around with more GPUs being searched. I was thinking of trying the t2.small instance again but decided to leave it as it is, and look for ways to learn about other services to deploy a pipeline like this, especially a free one so I can keep this project running for people to use, like on &lt;a href=\"/r/HardwareSwapUK\"&gt;/r/HardwareSwapUK&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?auto=webp&amp;v=enabled&amp;s=2af9600bd4aa992f39d564819271899a1ab7fe0e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3026bad95d2fd621dadaa0bdfc4fce3cef771be7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64cf19660c70d1617f460ff07ecf3bfa7b330151", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35f2eecb197804bddcec656085545e631b9d3436", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77273085d988cf74ed3eb98bdde92c801a2e7a26", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d649cec23ed95938f033b634800c1129c1283dfe", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9AyPvNJ3xAXs6VAX_YnVkXE-zEnDvnmqmzNSwirzauY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd5ce8b4111c182753abc04a0a76eefb510c852", "width": 1080, "height": 540}], "variants": {}, "id": "2lf3IYLRtQAxbb7CToJK_65KH5OHqQF9YhUq4E9VuTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CS Graduate", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "113roc9", "is_robot_indexable": true, "report_reasons": null, "author": "Mapleess", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113roc9/airflow_pipeline_for_ebay_data_extraction_simple/", "subreddit_subscribers": 89795, "created_utc": 1676557141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_puwuw2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My project: A focused, personalized observability report for every PR. GitHub Actions: Would you find this useful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_113pg54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b2L8gjGr2xKg0WKG-hT1Xv3tNDNk5E0rte_0BRFIMWE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676550430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8lpmzrednjia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8lpmzrednjia1.png?auto=webp&amp;v=enabled&amp;s=ba5c47aeac347b8fbbb153a3f7f2093ee537698c", "width": 618, "height": 1203}, "resolutions": [{"url": "https://preview.redd.it/8lpmzrednjia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b046a88be133029b6970105b7fdbf8f46602f554", "width": 108, "height": 210}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a7699ecf679e7ee2d2a3b7600630ac4768ea9c8", "width": 216, "height": 420}, {"url": "https://preview.redd.it/8lpmzrednjia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1607a7c854fdfce57ebaaa2506b150e5638977a4", "width": 320, "height": 622}], "variants": {}, "id": "JPevZe8OIe_HvE94bD2wlM2BYu4RPMzjNqzycgXcTzU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113pg54", "is_robot_indexable": true, "report_reasons": null, "author": "observability_geek", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113pg54/my_project_a_focused_personalized_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8lpmzrednjia1.png", "subreddit_subscribers": 89795, "created_utc": 1676550430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. \n\nThere are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in 'YYYY-MM-DDTHH:MM:SS format.\n\nIn a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. \n\nHow can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I've looked into UDFs but they aren't recommended - so i'm stumped here!\n\nThanks,", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to modularise Delta Live Tables using Pyspark in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113mxlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676540959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. &lt;/p&gt;\n\n&lt;p&gt;There are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in &amp;#39;YYYY-MM-DDTHH:MM:SS format.&lt;/p&gt;\n\n&lt;p&gt;In a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. &lt;/p&gt;\n\n&lt;p&gt;How can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I&amp;#39;ve looked into UDFs but they aren&amp;#39;t recommended - so i&amp;#39;m stumped here!&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113mxlk", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "subreddit_subscribers": 89795, "created_utc": 1676540959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Inspired by this [post](https://www.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/) and this [comment](https://www.reddit.com/r/dataengineering/comments/11349lf/comment/j8spjrg/), would r/dataengineering be interested in a project based competition? (mainly for learning purposes)\n\nTo keep things simple, we could use reddit polls to host it. We can decide on the project (and the winner) using votes.\n\nWe can hash out the details if there's enough interest, but I'd be willing to chip in the first $500 to the winning pot. My personal preference is to donate the winnings but will also defer this decision to a poll.\n\n**Open questions:**\n\n1. What should the scope of the project be? Data Engineering is a very broad field.\n2. Do you see any downside to deciding the project using a reddit poll?\n3. Do you see any downside to deciding the winner using a reddit poll?\n4. How long should the competition run? 4 weeks should be good for building a production-ready project on the side (to account for DEs with full time jobs)\n\nLet me know what you think :)", "author_fullname": "t2_vri7kka6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Competition!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113x4cb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676572294.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676571379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by this &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/\"&gt;post&lt;/a&gt; and this &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/11349lf/comment/j8spjrg/\"&gt;comment&lt;/a&gt;, would &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; be interested in a project based competition? (mainly for learning purposes)&lt;/p&gt;\n\n&lt;p&gt;To keep things simple, we could use reddit polls to host it. We can decide on the project (and the winner) using votes.&lt;/p&gt;\n\n&lt;p&gt;We can hash out the details if there&amp;#39;s enough interest, but I&amp;#39;d be willing to chip in the first $500 to the winning pot. My personal preference is to donate the winnings but will also defer this decision to a poll.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Open questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What should the scope of the project be? Data Engineering is a very broad field.&lt;/li&gt;\n&lt;li&gt;Do you see any downside to deciding the project using a reddit poll?&lt;/li&gt;\n&lt;li&gt;Do you see any downside to deciding the winner using a reddit poll?&lt;/li&gt;\n&lt;li&gt;How long should the competition run? 4 weeks should be good for building a production-ready project on the side (to account for DEs with full time jobs)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Let me know what you think :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113x4cb", "is_robot_indexable": true, "report_reasons": null, "author": "datain30", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113x4cb/data_engineering_competition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113x4cb/data_engineering_competition/", "subreddit_subscribers": 89795, "created_utc": 1676571379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. \n\nMost of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. \n\nThe current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. \n\nHaving just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. \n\nI was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).\n\nFor on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?\n\nI feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. \n\nCurious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.", "author_fullname": "t2_18xs8yle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for regional contact/directory data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113fqdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676514792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. &lt;/p&gt;\n\n&lt;p&gt;Most of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. &lt;/p&gt;\n\n&lt;p&gt;The current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. &lt;/p&gt;\n\n&lt;p&gt;Having just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. &lt;/p&gt;\n\n&lt;p&gt;I was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).&lt;/p&gt;\n\n&lt;p&gt;For on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?&lt;/p&gt;\n\n&lt;p&gt;I feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. &lt;/p&gt;\n\n&lt;p&gt;Curious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113fqdd", "is_robot_indexable": true, "report_reasons": null, "author": "wves", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "subreddit_subscribers": 89795, "created_utc": 1676514792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don't solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?", "author_fullname": "t2_3tiq5c7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's missing in the modern data stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11388la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676494377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don&amp;#39;t solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11388la", "is_robot_indexable": true, "report_reasons": null, "author": "ownubie", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "subreddit_subscribers": 89795, "created_utc": 1676494377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?", "author_fullname": "t2_bwp6e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your team\u2019s method for storing credentials for your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1137p0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676492961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1137p0i", "is_robot_indexable": true, "report_reasons": null, "author": "wild_bill34", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "subreddit_subscribers": 89795, "created_utc": 1676492961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i've read, adding `catchup = True` will make the task start from 3 years ago. It'll also force backfill any other task which isn't fully caught up.\n\nI've noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!\n\nThe only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?", "author_fullname": "t2_zjn67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, backfilling/re-running a single task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113i7y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676523416.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676522703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i&amp;#39;ve read, adding &lt;code&gt;catchup = True&lt;/code&gt; will make the task start from 3 years ago. It&amp;#39;ll also force backfill any other task which isn&amp;#39;t fully caught up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!&lt;/p&gt;\n\n&lt;p&gt;The only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113i7y2", "is_robot_indexable": true, "report_reasons": null, "author": "Propanoate", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "subreddit_subscribers": 89795, "created_utc": 1676522703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.\n\nMy challenge is to build reliable ELT pipelines around these unreliable data sources.\n\nThrough now, the rough workflow is: \n\n1. Design XLSX template w/ each page relating to a backend table in a 1-1 relationship\n2. Host XLSX template in SharePoint site, give access to select scientist to update\n3. Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake\n4. Load each CSV into RedShift tables, truncating and loading fully\n\nThis sort of works, but I'm not entirely happy with it, and I'm not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.\n\nAny advice is appreciated, interested what other groups are doing here!", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I ingest manually created XLSX files into my data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113slfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676559672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in the biotech industry, and scientists are inseparable from their XLSX sheets. They use them to record experiment data and annotate instrument data metadata.&lt;/p&gt;\n\n&lt;p&gt;My challenge is to build reliable ELT pipelines around these unreliable data sources.&lt;/p&gt;\n\n&lt;p&gt;Through now, the rough workflow is: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Design XLSX template w/ each page relating to a backend table in a 1-1 relationship&lt;/li&gt;\n&lt;li&gt;Host XLSX template in SharePoint site, give access to select scientist to update&lt;/li&gt;\n&lt;li&gt;Schedule extract task to read XLSX into memory, write each sheet as separate CSV to data lake&lt;/li&gt;\n&lt;li&gt;Load each CSV into RedShift tables, truncating and loading fully&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This sort of works, but I&amp;#39;m not entirely happy with it, and I&amp;#39;m not sure why. Part of me guesses that I should build smaller webapps to take in this data so that I can validate data in the application layer &amp;amp; read the validated data from the backend DB. But this seems overengineered. Right now validation happens in the Data Warehouse.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated, interested what other groups are doing here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113slfq", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113slfq/how_should_i_ingest_manually_created_xlsx_files/", "subreddit_subscribers": 89795, "created_utc": 1676559672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I got a flow which I want to automate in Azure. For now we're running a python script on prem, but the server needs to get phased out.\n\n&amp;#x200B;\n\n\\- Everyday I get emails in Office 365 from client.\n\n\\- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.\n\n&amp;#x200B;\n\nMy question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.\n\n&amp;#x200B;\n\nI don't know which approach is best for this... Synapse, Power Automate, Functions... something else.", "author_fullname": "t2_ig8s88dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Excel data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139rhi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676498341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I got a flow which I want to automate in Azure. For now we&amp;#39;re running a python script on prem, but the server needs to get phased out.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Everyday I get emails in Office 365 from client.&lt;/p&gt;\n\n&lt;p&gt;- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know which approach is best for this... Synapse, Power Automate, Functions... something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139rhi", "is_robot_indexable": true, "report_reasons": null, "author": "Hs82H", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1139rhi/automate_excel_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139rhi/automate_excel_data/", "subreddit_subscribers": 89795, "created_utc": 1676498341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a local data server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139kdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139kdv", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "subreddit_subscribers": 89795, "created_utc": 1676497830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&gt; Debezium --&gt; Kakfa --&gt; S3, is there any way we could avoid Kafka, we don't want to over-complicate the pipeline. Are they any other better ways to implement this?", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to fetch change logs from SQL Server to S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11397tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676499194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676496933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&amp;gt; Debezium --&amp;gt; Kakfa --&amp;gt; S3, is there any way we could avoid Kafka, we don&amp;#39;t want to over-complicate the pipeline. Are they any other better ways to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11397tu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "subreddit_subscribers": 89795, "created_utc": 1676496933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Quick summary on my position and structure of company: \n\nI work in an Analytics department, and we provide most reporting and analysis for the company. IT handles some of the static reports and is works on data warehousing. We have read access to most OnPrem SQL Server DBs. Right now most of our job is creating Normalized Databases in Access and creating reports from there.  We use Access for its ability to link to the SQL Server Tables. It is extremely inefficient and I really don't want to write VBA to automate things. I am pretty good at SQL and know a good amount of Python.\n\nOur current setup:\n\n\\-2 Analyst with our own SQL Server where we want to load all of our data. We cannot have access to linked tables from other servers in our environment. \n\n\\-A shared virtual machine where we use our credentials to log in to. I was hoping an orchestration tool could run on this machine.\n\n\\-We use Windows\n\nGoals:\n\nData comes in very randomly, because we work with a lot of third parties. I would like to have a script that runs maybe every hour to check if new data sources are in and then run processes from there. Most of the processes are to extract the data and load into our SQL Server where I run some stored procedures.\n\nMy issue:\n\nI want to use Airflow to automate things, but we have even been denied access to IIS in the past. My Manager doesn't really care about using new tools, and would rather just use VBA to do this. I have used Pandas to transfer data between servers before, but issues around data types have come up. I used SQLAlchemy Create Table to make sure I get the correct table structure and then load the pandas data into the newly created table, but its just not easily usable. I've looked at DBT, but from the little I know using this with SQL Server is not as easy.\n\nI probably missed some important information, but I am looking for some guidance on tools I can implement myself to get my goals done. I cannot download Airflow myself because it requires Docker which requires WSL on windows and I have to get IT to allow this ability.", "author_fullname": "t2_704n3yha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on process workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113t82f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676561355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick summary on my position and structure of company: &lt;/p&gt;\n\n&lt;p&gt;I work in an Analytics department, and we provide most reporting and analysis for the company. IT handles some of the static reports and is works on data warehousing. We have read access to most OnPrem SQL Server DBs. Right now most of our job is creating Normalized Databases in Access and creating reports from there.  We use Access for its ability to link to the SQL Server Tables. It is extremely inefficient and I really don&amp;#39;t want to write VBA to automate things. I am pretty good at SQL and know a good amount of Python.&lt;/p&gt;\n\n&lt;p&gt;Our current setup:&lt;/p&gt;\n\n&lt;p&gt;-2 Analyst with our own SQL Server where we want to load all of our data. We cannot have access to linked tables from other servers in our environment. &lt;/p&gt;\n\n&lt;p&gt;-A shared virtual machine where we use our credentials to log in to. I was hoping an orchestration tool could run on this machine.&lt;/p&gt;\n\n&lt;p&gt;-We use Windows&lt;/p&gt;\n\n&lt;p&gt;Goals:&lt;/p&gt;\n\n&lt;p&gt;Data comes in very randomly, because we work with a lot of third parties. I would like to have a script that runs maybe every hour to check if new data sources are in and then run processes from there. Most of the processes are to extract the data and load into our SQL Server where I run some stored procedures.&lt;/p&gt;\n\n&lt;p&gt;My issue:&lt;/p&gt;\n\n&lt;p&gt;I want to use Airflow to automate things, but we have even been denied access to IIS in the past. My Manager doesn&amp;#39;t really care about using new tools, and would rather just use VBA to do this. I have used Pandas to transfer data between servers before, but issues around data types have come up. I used SQLAlchemy Create Table to make sure I get the correct table structure and then load the pandas data into the newly created table, but its just not easily usable. I&amp;#39;ve looked at DBT, but from the little I know using this with SQL Server is not as easy.&lt;/p&gt;\n\n&lt;p&gt;I probably missed some important information, but I am looking for some guidance on tools I can implement myself to get my goals done. I cannot download Airflow myself because it requires Docker which requires WSL on windows and I have to get IT to allow this ability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113t82f", "is_robot_indexable": true, "report_reasons": null, "author": "gloverb2016", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113t82f/advice_on_process_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113t82f/advice_on_process_workflow/", "subreddit_subscribers": 89795, "created_utc": 1676561355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all  \nI have data engineer interview coming up with [Booking.com](https://Booking.com). I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.   \nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven't found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Booking.com Data Engineer Interview Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113que5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;br/&gt;\nI have data engineer interview coming up with &lt;a href=\"https://Booking.com\"&gt;Booking.com&lt;/a&gt;. I want advice if somebody has already gone through the hoops on how to prepare, I think they have 2 rounds technically coding and system desgin.&lt;br/&gt;\nHow should I prepare for them, from what I have searched online they have streaming coding round for coding round and system round is general system design and data modeling round on a give scenario. I am not sure what is streaming coding round. Also how do people prepare for system design round as data engineer, there are a lot of sources for SWE positions but I haven&amp;#39;t found anything like that for data engineering, would appreciate if somebody can share any resources for preparation on these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?auto=webp&amp;v=enabled&amp;s=e78c7f7b5ef8df799c5ab7819ad21ae28e0bf405", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=128d0c5e050d4fd44731c14d3596b793d77b0906", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c59c9ba9d4b265735403cbfede7112bd6a258057", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=379c22da3c6f868cf8b47f65272c52642a9b8cc1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1246f60c367c5e32f870ccbc849b81646f382184", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd246c83391f10392953e4abed795482897eca67", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JScxGhJAtnCffNUMMsG-IHXgofOOGPYqUHUx0ieSAf8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=448a7d6b2100cd5eef4217e155caab07496d166c", "width": 1080, "height": 567}], "variants": {}, "id": "pOxqQ-Hg2p6yWf-XnRr9X7KZJkKYq4zbh2GbSAcQcJM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "113que5", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113que5/bookingcom_data_engineer_interview_advice/", "subreddit_subscribers": 89795, "created_utc": 1676554753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are them, really?", "author_fullname": "t2_82dwrpz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the advantages of data lakes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113qu1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676554725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are them, really?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113qu1r", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-8440", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113qu1r/what_are_the_advantages_of_data_lakes/", "subreddit_subscribers": 89795, "created_utc": 1676554725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_91odj59i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Real Definition Of \u201cDataOps\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_113epbc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwZvqp0Pi6NPy98aD615AfICVS7yLFY5FPqaRryQO3M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676511687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?auto=webp&amp;v=enabled&amp;s=79557af160f6d8db3e9cf8e02c16192b4e84ffe1", "width": 580, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ecd095287fb884429bce38b5fc59dc1c1c0a0d1", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e66defec08dfd300a5d2b44634be101482141b5", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef8678ff9e75aba71c4f8d768567077ca613caf1", "width": 320, "height": 249}], "variants": {}, "id": "ORQg5WGGxt7NVpZTOkiaKxFABZ6KiuQxVSy7rHIK4ik"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113epbc", "is_robot_indexable": true, "report_reasons": null, "author": "david_ok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113epbc/the_real_definition_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "subreddit_subscribers": 89795, "created_utc": 1676511687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE, I often face the following situation: We have some kind of etl process/analysis/report/dashboard in place. Leadership insists on validating the final product. Sounds good, except our stakeholders are unable to provide expectations for the resulting data and we have no point of comparison elsewhere at the company. In the end, we implement some kind of common sense data check that serves no real purpose; it is busy work to satisfy our leadership.\n\nAnyone face these kinds of issues? How do you quickly develop data validation strategies without a point of comparison or stakeholder input? And how do you do this without wasting time?", "author_fullname": "t2_v3k0dc9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Validation/Quality Strategies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113x1ro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676571185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE, I often face the following situation: We have some kind of etl process/analysis/report/dashboard in place. Leadership insists on validating the final product. Sounds good, except our stakeholders are unable to provide expectations for the resulting data and we have no point of comparison elsewhere at the company. In the end, we implement some kind of common sense data check that serves no real purpose; it is busy work to satisfy our leadership.&lt;/p&gt;\n\n&lt;p&gt;Anyone face these kinds of issues? How do you quickly develop data validation strategies without a point of comparison or stakeholder input? And how do you do this without wasting time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113x1ro", "is_robot_indexable": true, "report_reasons": null, "author": "zazzersmel", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113x1ro/data_validationquality_strategies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113x1ro/data_validationquality_strategies/", "subreddit_subscribers": 89795, "created_utc": 1676571185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings \n\nBest bootcamp to data engeenier no prior experience", "author_fullname": "t2_t3bmq210", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engeenring bootcamp guidence ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113x0ap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676571073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings &lt;/p&gt;\n\n&lt;p&gt;Best bootcamp to data engeenier no prior experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113x0ap", "is_robot_indexable": true, "report_reasons": null, "author": "haseebnawaz_803", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113x0ap/data_engeenring_bootcamp_guidence/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113x0ap/data_engeenring_bootcamp_guidence/", "subreddit_subscribers": 89795, "created_utc": 1676571073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR How do you find the right front end dev partner to build out projects if you don't know any?\n\nMy background is in data science, and then like many data scientists, because of poor hiring or bad planning I ended up doing a lot of data engineering. Like a **lot**. What's neat is that between the two domains I'm able to build reasonably end to end little projects. But one place I want to improve on is I usually prototype front ends using streamlit/dash. I know very little about web dev and front end stuff.  Has anyone in the past worked on a project where they needed a front end dev? How did you go about finding them? I'm looking for someone scrappy, entrepreneurial, knowledgeable about which technical debt is worth accumulating early on in the interest of speed, vs which stuff should be handled up front. I've considered freelance websites like upwork, but I'm looking for slightly more than a gun for hire. I'm happy to pay a bit for someone's time, but I'd like some degree of their commitment as well. My worry for the freelance crowd is they might make things work, but they don't really have an incentive to write maintainable or readable code. Especially if they know I'm clueless about anything they make.", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Search for Side Project Partners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113wr7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676570428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR How do you find the right front end dev partner to build out projects if you don&amp;#39;t know any?&lt;/p&gt;\n\n&lt;p&gt;My background is in data science, and then like many data scientists, because of poor hiring or bad planning I ended up doing a lot of data engineering. Like a &lt;strong&gt;lot&lt;/strong&gt;. What&amp;#39;s neat is that between the two domains I&amp;#39;m able to build reasonably end to end little projects. But one place I want to improve on is I usually prototype front ends using streamlit/dash. I know very little about web dev and front end stuff.  Has anyone in the past worked on a project where they needed a front end dev? How did you go about finding them? I&amp;#39;m looking for someone scrappy, entrepreneurial, knowledgeable about which technical debt is worth accumulating early on in the interest of speed, vs which stuff should be handled up front. I&amp;#39;ve considered freelance websites like upwork, but I&amp;#39;m looking for slightly more than a gun for hire. I&amp;#39;m happy to pay a bit for someone&amp;#39;s time, but I&amp;#39;d like some degree of their commitment as well. My worry for the freelance crowd is they might make things work, but they don&amp;#39;t really have an incentive to write maintainable or readable code. Especially if they know I&amp;#39;m clueless about anything they make.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113wr7w", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113wr7w/the_search_for_side_project_partners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113wr7w/the_search_for_side_project_partners/", "subreddit_subscribers": 89795, "created_utc": 1676570428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a data analyst with a smaller analytics consulting agency in the US and have been using DBT cloud since I started in the data field 7 months ago. I'm looking to apply for data engineering roles after 1.5 - 2yrs of experience in my current role. \n\n\\- If you have taken the exam, what're you thoughts on the exam itself and has it been beneficial with any job searching or in everyday work?\n\n\\- In your opinion, would you prioritize this exam over an AWS associate level cert like the solutions architect or developer?\n\nAny thoughts/feedback is appreciated!", "author_fullname": "t2_71weyfry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the DBT Analytics Engineering Certification worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_113wjwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676569903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a data analyst with a smaller analytics consulting agency in the US and have been using DBT cloud since I started in the data field 7 months ago. I&amp;#39;m looking to apply for data engineering roles after 1.5 - 2yrs of experience in my current role. &lt;/p&gt;\n\n&lt;p&gt;- If you have taken the exam, what&amp;#39;re you thoughts on the exam itself and has it been beneficial with any job searching or in everyday work?&lt;/p&gt;\n\n&lt;p&gt;- In your opinion, would you prioritize this exam over an AWS associate level cert like the solutions architect or developer?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts/feedback is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113wjwy", "is_robot_indexable": true, "report_reasons": null, "author": "DrunkenWhaler136", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113wjwy/is_the_dbt_analytics_engineering_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113wjwy/is_the_dbt_analytics_engineering_certification/", "subreddit_subscribers": 89795, "created_utc": 1676569903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I run a BI team and handle most of the SQL modeling, post-ETL (we have a Data Engineering team that gets stuff into our warehouse). Think building views and datasets, which are then either turned into dashboards and reports by myself or my analysts. It is by far my favorite aspect of my job and it comes very naturally to me.\n\nThis has led me to get more interested in pursuing data engineering in my career. What I do seems to be considered \"analytics engineering\" in the space now, but what I would love to hear from the community here is where \"analytics engineering\" ends and where data engineering begins, as well as the skills that are needed for the latter but not necessarily the former. I write SQL on a daily basis, but my scripting knowledge is limited because it just hasn't been directly relevant for my career yet. For the (simple, small-data) data pipelines I create, I use a software tool that handles all of the ETL for me.\n\nAny and all help is greatly appreciated!", "author_fullname": "t2_i2gus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does analytics engineering end, and data engineering begin?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113uttu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676565486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a BI team and handle most of the SQL modeling, post-ETL (we have a Data Engineering team that gets stuff into our warehouse). Think building views and datasets, which are then either turned into dashboards and reports by myself or my analysts. It is by far my favorite aspect of my job and it comes very naturally to me.&lt;/p&gt;\n\n&lt;p&gt;This has led me to get more interested in pursuing data engineering in my career. What I do seems to be considered &amp;quot;analytics engineering&amp;quot; in the space now, but what I would love to hear from the community here is where &amp;quot;analytics engineering&amp;quot; ends and where data engineering begins, as well as the skills that are needed for the latter but not necessarily the former. I write SQL on a daily basis, but my scripting knowledge is limited because it just hasn&amp;#39;t been directly relevant for my career yet. For the (simple, small-data) data pipelines I create, I use a software tool that handles all of the ETL for me.&lt;/p&gt;\n\n&lt;p&gt;Any and all help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "113uttu", "is_robot_indexable": true, "report_reasons": null, "author": "SteezeWhiz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113uttu/where_does_analytics_engineering_end_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113uttu/where_does_analytics_engineering_end_and_data/", "subreddit_subscribers": 89795, "created_utc": 1676565486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, i'm a newly hired data engineering intern at a company which i'll later work for, my internship project is to create an ETL pipeline with a GUI based ETL tool.\n\nMy manager conducted me to do some research on who's better for future use between Talend or Pentaho, now i know this is a biased question, but if anyone has done some benchmark testing or worked with both to get some conslusions on things such as ( Performence speed, Scalability, Data integration, easy-to-use, more documentation ...).\n\n&amp;#x200B;\n\nPS: I know you guys really hate this GUI based tools haha (me too), but the manager doesn't require a technical guy for this, thanks!", "author_fullname": "t2_bfls8zod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Talend or Pentaho ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113sb1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676558884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i&amp;#39;m a newly hired data engineering intern at a company which i&amp;#39;ll later work for, my internship project is to create an ETL pipeline with a GUI based ETL tool.&lt;/p&gt;\n\n&lt;p&gt;My manager conducted me to do some research on who&amp;#39;s better for future use between Talend or Pentaho, now i know this is a biased question, but if anyone has done some benchmark testing or worked with both to get some conslusions on things such as ( Performence speed, Scalability, Data integration, easy-to-use, more documentation ...).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;PS: I know you guys really hate this GUI based tools haha (me too), but the manager doesn&amp;#39;t require a technical guy for this, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113sb1g", "is_robot_indexable": true, "report_reasons": null, "author": "infiiniterotation", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113sb1g/talend_or_pentaho/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113sb1g/talend_or_pentaho/", "subreddit_subscribers": 89795, "created_utc": 1676558884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a project that requires building a solution to collect security log data from our AWS environment and our application to figure out if our network or data have been compromises. As I understand it, that's called indicators of compromise (IOC - [https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/](https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/)). The intention is to come up with patterns that I can detect in historical data to indicate if we were compromised in any way. Then implement those in a real-time solution to detect attacks when they happen.\n\nI'm wondering if anyone is aware or worked on a similar problem and what are possible solutions?", "author_fullname": "t2_vhiekvgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting indicators of compromise in security log data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113qhy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676553690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a project that requires building a solution to collect security log data from our AWS environment and our application to figure out if our network or data have been compromises. As I understand it, that&amp;#39;s called indicators of compromise (IOC - &lt;a href=\"https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/\"&gt;https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/&lt;/a&gt;). The intention is to come up with patterns that I can detect in historical data to indicate if we were compromised in any way. Then implement those in a real-time solution to detect attacks when they happen.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if anyone is aware or worked on a similar problem and what are possible solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?auto=webp&amp;v=enabled&amp;s=391364df561ed9fd027ab27560770b1e4b9e8507", "width": 421, "height": 260}, "resolutions": [{"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a69011b279ea40c4c11a133e0a5a677957b175c", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6745d5ad37fceafc70fb2f35c4229bbafd95f4bb", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/7RM_9DTbffSKVQT6k9BFtbV7FiIvDZ1MacrP_iq1KZM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c0dccddad114b72372cdc4fa293ee7c818dea90", "width": 320, "height": 197}], "variants": {}, "id": "YEXWbMD95Yu6a-9J0eJxXwJagWmhiuK3YIF_DFU5HaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113qhy2", "is_robot_indexable": true, "report_reasons": null, "author": "royondata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113qhy2/detecting_indicators_of_compromise_in_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113qhy2/detecting_indicators_of_compromise_in_security/", "subreddit_subscribers": 89795, "created_utc": 1676553690.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}