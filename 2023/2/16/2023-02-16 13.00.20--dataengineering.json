{"kind": "Listing", "data": {"after": "t3_11365yo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9e7m1qmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finnhub streaming data pipeline using Spark, Kafka, Kubernetes and more - Github repo &amp; more info in the comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 48, "top_awarded_type": null, "hide_score": false, "name": "t3_1131jqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 134, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 134, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CbifdmwD18eb4V2QhY84xODBLIeFazHoVdEaSRnHDXc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676477425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/44en1od0kdia1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/44en1od0kdia1.png?auto=webp&amp;v=enabled&amp;s=bd40c6eb7926721714742b3fcf5941eb50684aaa", "width": 2381, "height": 822}, "resolutions": [{"url": "https://preview.redd.it/44en1od0kdia1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f89886f46480a09c1ee666186e1da9f60a55824", "width": 108, "height": 37}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a93b4d80d6a39cba158604636da7593cee15584", "width": 216, "height": 74}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=230b2497b9dd9fada4a17d96681c94e94bdf070a", "width": 320, "height": 110}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69a45be536958efc184054cd2e95a21ad8e44e08", "width": 640, "height": 220}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adafa2f6b140f6fa94fb9a6c57e913b627d1f7af", "width": 960, "height": 331}, {"url": "https://preview.redd.it/44en1od0kdia1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50dea1dddc719945eaf5df33b056b8db300da24b", "width": 1080, "height": 372}], "variants": {}, "id": "WBRfH8NzSNCYSGKZg1qxfeT9pcEhMHoSqNpwBZvqY6o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1131jqq", "is_robot_indexable": true, "report_reasons": null, "author": "LewWariat", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131jqq/finnhub_streaming_data_pipeline_using_spark_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/44en1od0kdia1.png", "subreddit_subscribers": 89747, "created_utc": 1676477425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's been quite a while since I dealt with managing a type 2 SCD so I wanted to get confirmation on what the typical strategies are. I've done hash columns that will hash all of the columns of a dimension table and compare that to the hash column of the incoming data. This worked pretty well but I remember hearing that there's always a chance that hash values will be the same. Is that concern still valid, and how often does that happen? \n\nWhat are other strategies/data models/queries that will identify a changed row in a type 2 SCD? I came across [this Stackoverflow answer](https://stackoverflow.com/a/35171620/1175788) for essentially the same question but the query is pretty obtuse. I want to avoid a query where it's doing comparisons across all the columns like: `WHERE source.id = incoming.id AND (source.quantity &lt;&gt; incoming.quantity OR source.date &lt;&gt; incoming.date)` because that's obviously terribly inefficient", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Type 2 slowly changing dimension. What are the common SQL queries to identify updated rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1130zot", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676475958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been quite a while since I dealt with managing a type 2 SCD so I wanted to get confirmation on what the typical strategies are. I&amp;#39;ve done hash columns that will hash all of the columns of a dimension table and compare that to the hash column of the incoming data. This worked pretty well but I remember hearing that there&amp;#39;s always a chance that hash values will be the same. Is that concern still valid, and how often does that happen? &lt;/p&gt;\n\n&lt;p&gt;What are other strategies/data models/queries that will identify a changed row in a type 2 SCD? I came across &lt;a href=\"https://stackoverflow.com/a/35171620/1175788\"&gt;this Stackoverflow answer&lt;/a&gt; for essentially the same question but the query is pretty obtuse. I want to avoid a query where it&amp;#39;s doing comparisons across all the columns like: &lt;code&gt;WHERE source.id = incoming.id AND (source.quantity &amp;lt;&amp;gt; incoming.quantity OR source.date &amp;lt;&amp;gt; incoming.date)&lt;/code&gt; because that&amp;#39;s obviously terribly inefficient&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1130zot", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130zot/type_2_slowly_changing_dimension_what_are_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130zot/type_2_slowly_changing_dimension_what_are_the/", "subreddit_subscribers": 89747, "created_utc": 1676475958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.\n\nIs there anything like this?", "author_fullname": "t2_lf4it7cs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything like Kaggle for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11349lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676484109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.&lt;/p&gt;\n\n&lt;p&gt;Is there anything like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11349lf", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Deer_8686", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/", "subreddit_subscribers": 89747, "created_utc": 1676484109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.\n\nNext month, we\u2019re reading [*Data Teams: A Unified Management Model for Successful Data-Focused Teams*](https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;qid=1676346037&amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;sr=8-1) by [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/).\n\nI\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It's a book I wanna read!\n\n**Here\u2019s how the book club works:** All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.\n\n**Here\u2019s the schedule:**\n\n* March 17th: Discuss pt. 1 &amp; pt. 2\n* March 31st: Discuss pt. 3\n* April 5th: AMA w/ Author, [Jesse Anderson](https://www.linkedin.com/in/jessetanderson/)\n* April 14th: Discuss pt. 4\n\nWe currently have dozens signed up! If you\u2019d like to join, book it [**here**](https://www.operationalanalytics.club/events/titlecase-book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams)**.**", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Data Teams: A Unified Management Model for Successful Data-Focused Teams by Jesse Anderson", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139l8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676498263.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve hosted a few rounds of a data book club, and it\u2019s been a fantastic way to me and 100+ data professional actually read (not just passively skim \ud83e\udd23) many books on our lists.&lt;/p&gt;\n\n&lt;p&gt;Next month, we\u2019re reading &lt;a href=\"https://www.amazon.com/Data-Teams-Management-Successful-Data-Focused/dp/1484262271/ref=sr_1_1?crid=12ST07D9VZNUB&amp;amp;keywords=Data+Teams%3A+A+Unified+Management+Model+for+Successful+Data-Focused+Teams&amp;amp;qid=1676346037&amp;amp;sprefix=data+teams+a+unified+management+model+for+successful+data-focused+teams%2Caps%2C198&amp;amp;sr=8-1\"&gt;&lt;em&gt;Data Teams: A Unified Management Model for Successful Data-Focused Teams&lt;/em&gt;&lt;/a&gt; by &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not being paid to promote this book in any way, and I have no affiliation with the author. It&amp;#39;s a book I wanna read!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s how the book club works:&lt;/strong&gt; All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here\u2019s the schedule:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;March 17th: Discuss pt. 1 &amp;amp; pt. 2&lt;/li&gt;\n&lt;li&gt;March 31st: Discuss pt. 3&lt;/li&gt;\n&lt;li&gt;April 5th: AMA w/ Author, &lt;a href=\"https://www.linkedin.com/in/jessetanderson/\"&gt;Jesse Anderson&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;April 14th: Discuss pt. 4&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We currently have dozens signed up! If you\u2019d like to join, book it &lt;a href=\"https://www.operationalanalytics.club/events/titlecase-book-club-data-teams-a-unified-management-model-for-successful-data-focused-teams\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1139l8l", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139l8l/book_club_data_teams_a_unified_management_model/", "subreddit_subscribers": 89747, "created_utc": 1676497889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you mask the data? Have full access rights? Follow just guidance? How does it work where you are?", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your data engineering team handle ingesting PII and classified data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1130z35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676475915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you mask the data? Have full access rights? Follow just guidance? How does it work where you are?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1130z35", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130z35/how_does_your_data_engineering_team_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130z35/how_does_your_data_engineering_team_handle/", "subreddit_subscribers": 89747, "created_utc": 1676475915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. \n\nMost of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. \n\nThe current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. \n\nHaving just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. \n\nI was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).\n\nFor on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?\n\nI feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. \n\nCurious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.", "author_fullname": "t2_18xs8yle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a database for regional contact/directory data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113fqdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676514792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an analyst but started a new role where I have the most data engineering knowledge for a small team struggling to manage datasets for reporting. Also, not much of a budget for cloud offerings. &lt;/p&gt;\n\n&lt;p&gt;Most of the budget goes to the third party software subscription that generates a lot of supplier and provider data based on user input (geocoding especially). Naturally, they charge too much for our team to afford  web services offerings, so the data we receive for our reports are very large xlsx files. They also have some large regional data as sharepoint on prem lists. &lt;/p&gt;\n\n&lt;p&gt;The current reporting process involves a lot of power excel books with various other excel dependencies and it\u2019s tough to look at and all so manual. &lt;/p&gt;\n\n&lt;p&gt;Having just started, I made some python ETL scripts that (very) slowly interact with the saas software and gathers sharepoint tables then loads into a postgresql database - a temporary favor for the tableau guy. &lt;/p&gt;\n\n&lt;p&gt;I was able to get an on prem server for host a database, but now I\u2019m trying to think of the best approach to serve as a repo to replace xlsx files on the network drive (and sharepoint).&lt;/p&gt;\n\n&lt;p&gt;For on prem ETL pipelines of about 5 million rows with data that consists of contacts, suppliers, address, region/geo, and a bunch of associated flags that do ultimately need some joins \u2014 is a relational sql db the obvious move?&lt;/p&gt;\n\n&lt;p&gt;I feel like a graph db could be in the ballpark but I\u2019m not sure if that\u2019s just me being frustrated by data types and schemas in my load pipelines. Graph and document structure has always been more intuitive in my head, plus the sharepoint lists are Json rest responses already. &lt;/p&gt;\n\n&lt;p&gt;Curious to know anyone else\u2019s experience or approach and I\u2019m glad to add more details if this isn\u2019t enough to judge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113fqdd", "is_robot_indexable": true, "report_reasons": null, "author": "wves", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113fqdd/choosing_a_database_for_regional_contactdirectory/", "subreddit_subscribers": 89747, "created_utc": 1676514792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don't solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?", "author_fullname": "t2_3tiq5c7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's missing in the modern data stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11388la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676494377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am exploring some ideas in the data space and wonder what are some of the pain points that current solutions don&amp;#39;t solve for you? I am assuming ETL(fivetran) + Warehouse(snowflake) + Modeling(DBT) + RETL(Hightouch/Census) is the usual setup at organizations when they are revamping their modern data stack. Am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11388la", "is_robot_indexable": true, "report_reasons": null, "author": "ownubie", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11388la/whats_missing_in_the_modern_data_stack/", "subreddit_subscribers": 89747, "created_utc": 1676494377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i've read, adding `catchup = True` will make the task start from 3 years ago. It'll also force backfill any other task which isn't fully caught up.\n\nI've noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!\n\nThe only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?", "author_fullname": "t2_zjn67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, backfilling/re-running a single task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113i7y2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676523416.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676522703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I added a new task to our 3 year old DAG, but I only need to backfill it for the past 2 months. From what i&amp;#39;ve read, adding &lt;code&gt;catchup = True&lt;/code&gt; will make the task start from 3 years ago. It&amp;#39;ll also force backfill any other task which isn&amp;#39;t fully caught up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed I can just run the task manually using the GUI, but was wondering if there was a way to backfill a certain amount of time only. Thanks!&lt;/p&gt;\n\n&lt;p&gt;The only other option I was thinking about was adding a new DAG which starts 2 months ago. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113i7y2", "is_robot_indexable": true, "report_reasons": null, "author": "Propanoate", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113i7y2/airflow_backfillingrerunning_a_single_task/", "subreddit_subscribers": 89747, "created_utc": 1676522703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?", "author_fullname": "t2_bwp6e1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your team\u2019s method for storing credentials for your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1137p0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676492961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is pretty strict about using config files to store a variety of credentials tied to our data pipelines - especially variables making up the connection string for our DW. Is this a common thing or do other teams use other methods such as .env files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1137p0i", "is_robot_indexable": true, "report_reasons": null, "author": "wild_bill34", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1137p0i/what_is_your_teams_method_for_storing_credentials/", "subreddit_subscribers": 89747, "created_utc": 1676492961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I got a flow which I want to automate in Azure. For now we're running a python script on prem, but the server needs to get phased out.\n\n&amp;#x200B;\n\n\\- Everyday I get emails in Office 365 from client.\n\n\\- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.\n\n&amp;#x200B;\n\nMy question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.\n\n&amp;#x200B;\n\nI don't know which approach is best for this... Synapse, Power Automate, Functions... something else.", "author_fullname": "t2_ig8s88dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Excel data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139rhi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676498341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I got a flow which I want to automate in Azure. For now we&amp;#39;re running a python script on prem, but the server needs to get phased out.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Everyday I get emails in Office 365 from client.&lt;/p&gt;\n\n&lt;p&gt;- Every email holds a single Excel file with data on a single (the 2nd) row. The format is always the same.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is how can I get that data from Excel, so I can enrich and manipulate it and process it to another system.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know which approach is best for this... Synapse, Power Automate, Functions... something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139rhi", "is_robot_indexable": true, "report_reasons": null, "author": "Hs82H", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1139rhi/automate_excel_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139rhi/automate_excel_data/", "subreddit_subscribers": 89747, "created_utc": 1676498341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this post [https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking\\_for\\_data\\_analytics\\_engine\\_for\\_lowlatency/](https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/) I was guided towards Headless BI, specifically [Cube.dev](https://Cube.dev).\n\nWe looked into it, seemed to work for us and went with it. Nice!\n\nExcept... we've run into various edge cases where we're now questioning our choice heavily.\n\nBrowsing around, I've found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?\n\nAnd while at it, how a tool like that would be actually called? Headless BI doesn't turn up too much results. Semantic Layer? Maybe something else?", "author_fullname": "t2_4gt5e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any alternatives to Looker/LookML &amp; Cube.dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1131e3a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1676477013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/\"&gt;https://www.reddit.com/r/dataengineering/comments/y8wbh0/looking_for_data_analytics_engine_for_lowlatency/&lt;/a&gt; I was guided towards Headless BI, specifically &lt;a href=\"https://Cube.dev\"&gt;Cube.dev&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We looked into it, seemed to work for us and went with it. Nice!&lt;/p&gt;\n\n&lt;p&gt;Except... we&amp;#39;ve run into various edge cases where we&amp;#39;re now questioning our choice heavily.&lt;/p&gt;\n\n&lt;p&gt;Browsing around, I&amp;#39;ve found Cube to be somewhat inspired by Looker and Looker being, seemingly, more powerful, but was thinking, maybe there are other tools on the block?&lt;/p&gt;\n\n&lt;p&gt;And while at it, how a tool like that would be actually called? Headless BI doesn&amp;#39;t turn up too much results. Semantic Layer? Maybe something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1131e3a", "is_robot_indexable": true, "report_reasons": null, "author": "psycketom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1131e3a/any_alternatives_to_lookerlookml_cubedev/", "subreddit_subscribers": 89747, "created_utc": 1676477013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Snowflake to store all of our data and while I'm investigating issues, I notice certain columns haven't been updated for weeks which I'd like to flag and triage. \n\nI use dbt and unsure if dbt can help with this easily. I use dbt tests and can program a macro across certain columns to catch if data isn't populating as I'd expect....but I was curious...\n\n**What approaches or tools have made your lives easier with data profiling and catching data issues?**", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data profiling tools / approaches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1130jx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676474767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Snowflake to store all of our data and while I&amp;#39;m investigating issues, I notice certain columns haven&amp;#39;t been updated for weeks which I&amp;#39;d like to flag and triage. &lt;/p&gt;\n\n&lt;p&gt;I use dbt and unsure if dbt can help with this easily. I use dbt tests and can program a macro across certain columns to catch if data isn&amp;#39;t populating as I&amp;#39;d expect....but I was curious...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What approaches or tools have made your lives easier with data profiling and catching data issues?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1130jx5", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1130jx5/data_profiling_tools_approaches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1130jx5/data_profiling_tools_approaches/", "subreddit_subscribers": 89747, "created_utc": 1676474767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. \n\nThere are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in 'YYYY-MM-DDTHH:MM:SS format.\n\nIn a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. \n\nHow can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I've looked into UDFs but they aren't recommended - so i'm stumped here!\n\nThanks,", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to modularise Delta Live Tables using Pyspark in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113mxlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676540959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using the Databricks platform to build out our Lakehouse infrastructure and have been advised to use Delta Live Tables. &lt;/p&gt;\n\n&lt;p&gt;There are lots of common processes to be run for each of our 300+ silver tables, one of these is to ensure the DateTime format is in &amp;#39;YYYY-MM-DDTHH:MM:SS format.&lt;/p&gt;\n\n&lt;p&gt;In a typical Python environment I would define that function once and then call it from various scripts, meaning if I ever needed to change that function I would only do so in one place. &lt;/p&gt;\n\n&lt;p&gt;How can I modularise my delta live tables so I can point a notebook to this function, rather than defining it at the top of every single notebook? I&amp;#39;ve looked into UDFs but they aren&amp;#39;t recommended - so i&amp;#39;m stumped here!&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "113mxlk", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113mxlk/how_to_modularise_delta_live_tables_using_pyspark/", "subreddit_subscribers": 89747, "created_utc": 1676540959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a local data server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1139kdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676497830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to ask few questions regarding setting up a free data server. Right now I work in a tech company but my department does not have any data server that I can use to extract data. This is extremely uncomfortable and make my work process very inefficient. Since they said they do not have a budget to get up a data server (I think the upper management level is just lazy). I got a permission to make my own data server on my local environment just so I can use it to do my work. Is it possible to do that using free SQL server? I have access to the raw data from a 3rd party website (csv and excel format). Would it be possible to manually load those data to the sql server and update when there is new data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1139kdv", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1139kdv/setting_up_a_local_data_server/", "subreddit_subscribers": 89747, "created_utc": 1676497830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&gt; Debezium --&gt; Kakfa --&gt; S3, is there any way we could avoid Kafka, we don't want to over-complicate the pipeline. Are they any other better ways to implement this?", "author_fullname": "t2_2adeipr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to fetch change logs from SQL Server to S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11397tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1676499194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676496933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, How would you fetch the transaction log from SQL Server into S3? We are currently using AWS DMS to fetch change logs to S3 but DMS is very bad it unreliable. Another solution I looked into is using RDS --&amp;gt; Debezium --&amp;gt; Kakfa --&amp;gt; S3, is there any way we could avoid Kafka, we don&amp;#39;t want to over-complicate the pipeline. Are they any other better ways to implement this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11397tu", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Outlandishness-74", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11397tu/how_to_fetch_change_logs_from_sql_server_to_s3/", "subreddit_subscribers": 89747, "created_utc": 1676496933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi \ud83d\udc4b guys, \n\nAs a programmer, I want to learn Linux to expand my skills and knowledge. However, with so many Linux distributions out there, I'm not sure which one to choose. I want to pick a distribution that will be most useful for programming purposes and that will help me develop my skills as a programmer.\n\nSo, which Linux distribution do you recommend for a programmer to learn, and why? Are there any particular features or tools that make a certain distribution better suited for programming tasks? I'm open to any suggestions and advice you may have. Thank you in advance!", "author_fullname": "t2_m6lk62t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Linux distribution is best to learn for a programmer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1133zm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676483410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi \ud83d\udc4b guys, &lt;/p&gt;\n\n&lt;p&gt;As a programmer, I want to learn Linux to expand my skills and knowledge. However, with so many Linux distributions out there, I&amp;#39;m not sure which one to choose. I want to pick a distribution that will be most useful for programming purposes and that will help me develop my skills as a programmer.&lt;/p&gt;\n\n&lt;p&gt;So, which Linux distribution do you recommend for a programmer to learn, and why? Are there any particular features or tools that make a certain distribution better suited for programming tasks? I&amp;#39;m open to any suggestions and advice you may have. Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1133zm4", "is_robot_indexable": true, "report_reasons": null, "author": "TelevisionDue5491", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1133zm4/which_linux_distribution_is_best_to_learn_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1133zm4/which_linux_distribution_is_best_to_learn_for_a/", "subreddit_subscribers": 89747, "created_utc": 1676483410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This isn't so much a question about tech, as it is methods and processes.\n\nBackground: analyst 5 years, dwh dev 2 years, data engineer/analyst 1 year (total 8 years, earliest data job to current).\n\nI have this problem, a tendency to want to explore the entirety of a shiny, new data set -- sometimes at work, sometimes on my own projects (which are educational but time-sensitive). I know I need to plan and execute to be productive, and I do, but so often I find myself down rabbit holes of curiosity. Sometimes come out the other end with the completed objective, sometimes I don't, but I do always find something valuable.\n\nI find useful information when I do -- just basic profiling work, and some drilling on curious aspects of the structure. I enjoy pulling it all apart, cleaning and organizing it, and making it available. I usually add visuals for everything, iteratively where possible.\n\nThe problem is that I feel as though I can't give confident direction or feedback on my projects unless I've really explored the data to the point it's on my mind while I'm not working on it (which is when many answers come). At the end of this comprehensive dive through everything, I can find the interesting parts, develop questions, and answer them in a meaningful way.\n\nBut MAN, do I struggle with a fast process. Turn-around on a project at work always seems to be so much shorter than I'd estimate with fast coworkers, and I wonder why I can't cut to the requested answers more quickly.\n\nTL;DR:\nDoes anyone else seem to find themselves trying to consume the entirety of the data like an actual Python, when all that's needed is the right portion?", "author_fullname": "t2_77k8dmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytical Process Refinement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113iqt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676524521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This isn&amp;#39;t so much a question about tech, as it is methods and processes.&lt;/p&gt;\n\n&lt;p&gt;Background: analyst 5 years, dwh dev 2 years, data engineer/analyst 1 year (total 8 years, earliest data job to current).&lt;/p&gt;\n\n&lt;p&gt;I have this problem, a tendency to want to explore the entirety of a shiny, new data set -- sometimes at work, sometimes on my own projects (which are educational but time-sensitive). I know I need to plan and execute to be productive, and I do, but so often I find myself down rabbit holes of curiosity. Sometimes come out the other end with the completed objective, sometimes I don&amp;#39;t, but I do always find something valuable.&lt;/p&gt;\n\n&lt;p&gt;I find useful information when I do -- just basic profiling work, and some drilling on curious aspects of the structure. I enjoy pulling it all apart, cleaning and organizing it, and making it available. I usually add visuals for everything, iteratively where possible.&lt;/p&gt;\n\n&lt;p&gt;The problem is that I feel as though I can&amp;#39;t give confident direction or feedback on my projects unless I&amp;#39;ve really explored the data to the point it&amp;#39;s on my mind while I&amp;#39;m not working on it (which is when many answers come). At the end of this comprehensive dive through everything, I can find the interesting parts, develop questions, and answer them in a meaningful way.&lt;/p&gt;\n\n&lt;p&gt;But MAN, do I struggle with a fast process. Turn-around on a project at work always seems to be so much shorter than I&amp;#39;d estimate with fast coworkers, and I wonder why I can&amp;#39;t cut to the requested answers more quickly.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:\nDoes anyone else seem to find themselves trying to consume the entirety of the data like an actual Python, when all that&amp;#39;s needed is the right portion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113iqt2", "is_robot_indexable": true, "report_reasons": null, "author": "yeahbarry", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113iqt2/analytical_process_refinement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113iqt2/analytical_process_refinement/", "subreddit_subscribers": 89747, "created_utc": 1676524521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_91odj59i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Real Definition Of \u201cDataOps\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_113epbc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwZvqp0Pi6NPy98aD615AfICVS7yLFY5FPqaRryQO3M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676511687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?auto=webp&amp;v=enabled&amp;s=79557af160f6d8db3e9cf8e02c16192b4e84ffe1", "width": 580, "height": 453}, "resolutions": [{"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ecd095287fb884429bce38b5fc59dc1c1c0a0d1", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e66defec08dfd300a5d2b44634be101482141b5", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/xPRVdoUSZhxe8rncqq5In7OmzwJRFhL9GSrHNkqAOqo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef8678ff9e75aba71c4f8d768567077ca613caf1", "width": 320, "height": 249}], "variants": {}, "id": "ORQg5WGGxt7NVpZTOkiaKxFABZ6KiuQxVSy7rHIK4ik"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113epbc", "is_robot_indexable": true, "report_reasons": null, "author": "david_ok", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/113epbc/the_real_definition_of_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/weareservian/the-real-definition-of-dataops-9016ccee2f1b", "subreddit_subscribers": 89747, "created_utc": 1676511687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d0gtdepx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake CHANGE DATE FEED \u2014 How to read CDC without using Delta Log", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 57, "top_awarded_type": null, "hide_score": false, "name": "t3_1130wj8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gcQT_X76jiYTXNkvzEX7-zIUUj14WcY6xa5Jb9So8io.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676475716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@joydeep.roy/change-date-feed-how-to-read-cdc-without-using-delta-log-17e0ceb99a8e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?auto=webp&amp;v=enabled&amp;s=f97d42dbbd8e27dc2b44056b321fe4b91e3d7d9d", "width": 1002, "height": 414}, "resolutions": [{"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=716712296ffbdb9044c829dcc7a73014376063e1", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63f7f69089c1f264df96a662f8ec1bb1f6c1fe40", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=294b02119766f1b2ecd4dacada768322d17868d1", "width": 320, "height": 132}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32d70acf82831566b93ab0124a35cd18d16ad1fb", "width": 640, "height": 264}, {"url": "https://external-preview.redd.it/FQh0qHIqWY4sZK_iDxisWEBeDlYXpY_A-QEaD6nwoS4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef28f0508c14093813f44b172a31b8a160ca28b4", "width": 960, "height": 396}], "variants": {}, "id": "gVVkQGOOB1Nv48mh-zEITYt_WkTJdgoTJ9edGrmyA_I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1130wj8", "is_robot_indexable": true, "report_reasons": null, "author": "JBR_Codepen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1130wj8/delta_lake_change_date_feed_how_to_read_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@joydeep.roy/change-date-feed-how-to-read-cdc-without-using-delta-log-17e0ceb99a8e", "subreddit_subscribers": 89747, "created_utc": 1676475716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to figure out what the role of a \u201ccontent manager\u201d is at Google.  I\u2019m not sure I even have the right title. But essentially the role sits in the business and knows a lot about the data.  They work as a conduit between data engineering and the business. \n\nWhere I work we are called data managers. We typically have domain expertise and have usually worked in the business before.  They will also be fluent in data so they can help to build out requirements for the DEs.  It\u2019s similar to a BA but the big difference is they understand how the data is used. \n\nDo other people have this role in their company? I\u2019m really looking for tech industry examples. \n\nContext: my company is non-tech and wants to be tech and I\u2019m trying to figure out how to talk to leadership in terms they understand.", "author_fullname": "t2_7ge1tylx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Content Manager? at Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113anoe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676500594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to figure out what the role of a \u201ccontent manager\u201d is at Google.  I\u2019m not sure I even have the right title. But essentially the role sits in the business and knows a lot about the data.  They work as a conduit between data engineering and the business. &lt;/p&gt;\n\n&lt;p&gt;Where I work we are called data managers. We typically have domain expertise and have usually worked in the business before.  They will also be fluent in data so they can help to build out requirements for the DEs.  It\u2019s similar to a BA but the big difference is they understand how the data is used. &lt;/p&gt;\n\n&lt;p&gt;Do other people have this role in their company? I\u2019m really looking for tech industry examples. &lt;/p&gt;\n\n&lt;p&gt;Context: my company is non-tech and wants to be tech and I\u2019m trying to figure out how to talk to leadership in terms they understand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "113anoe", "is_robot_indexable": true, "report_reasons": null, "author": "fannypackbringitback", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113anoe/content_manager_at_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113anoe/content_manager_at_google/", "subreddit_subscribers": 89747, "created_utc": 1676500594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not a data engineer and am looking for some advice. Currently at my work we create R applications pulling directly from a vendor database. The queries for our current applications aren't that large so we query the DB directly but I would like to try and speed things up. I am looking to optimize our workflow and am thinking of trying the following:\n\n- Copy the db tables into our data warehouse nightly\n- Create custom tables for our applications that run nightly in a \"data mart\"\n\nThis would allow our applications to hit the data mart tables instead of running the entire queries which would significantly speed up load times. Does this approach seem correct? Is there a better way of handling this?", "author_fullname": "t2_10g3u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on best practices for ETL process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1135fwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676487064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not a data engineer and am looking for some advice. Currently at my work we create R applications pulling directly from a vendor database. The queries for our current applications aren&amp;#39;t that large so we query the DB directly but I would like to try and speed things up. I am looking to optimize our workflow and am thinking of trying the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Copy the db tables into our data warehouse nightly&lt;/li&gt;\n&lt;li&gt;Create custom tables for our applications that run nightly in a &amp;quot;data mart&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This would allow our applications to hit the data mart tables instead of running the entire queries which would significantly speed up load times. Does this approach seem correct? Is there a better way of handling this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1135fwu", "is_robot_indexable": true, "report_reasons": null, "author": "pf903", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1135fwu/question_on_best_practices_for_etl_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1135fwu/question_on_best_practices_for_etl_process/", "subreddit_subscribers": 89747, "created_utc": 1676487064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_nxu067bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to read fixed width files in Apache Nifi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_113n7nc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/a8-m3vUKCrY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to read fixed width files in Apache Nifi\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to read fixed width files in Apache Nifi", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/a8-m3vUKCrY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to read fixed width files in Apache Nifi\"&gt;&lt;/iframe&gt;", "author_name": "InsightByte", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/a8-m3vUKCrY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@InsightByte"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/a8-m3vUKCrY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to read fixed width files in Apache Nifi\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/113n7nc", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hE9tchDZIxmSLMdYbc2Dx5bzgXBi7qQLn0z5zjkBLow.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676542048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=a8-m3vUKCrY&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8Fp-zaW4kR3msDDPSAFCPjOqJqVt4EewVMvQUlb7c2M.jpg?auto=webp&amp;v=enabled&amp;s=01bcb6f0f896f55f5eaf6850a7f8a4786ffd491c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/8Fp-zaW4kR3msDDPSAFCPjOqJqVt4EewVMvQUlb7c2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a30d65398d64a48cc681985c133bf6757499d16d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/8Fp-zaW4kR3msDDPSAFCPjOqJqVt4EewVMvQUlb7c2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e0033b90a2e57ce5204019962e9b2e03fcc2747", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/8Fp-zaW4kR3msDDPSAFCPjOqJqVt4EewVMvQUlb7c2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=707fc5db3b38fd1edb371e397ae6a81d68358ec0", "width": 320, "height": 240}], "variants": {}, "id": "1GJmyzyQTYb4Azd2wp02wwUDpIzvhTKpeOl7FUmjuMk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "113n7nc", "is_robot_indexable": true, "report_reasons": null, "author": "InsightByte", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113n7nc/how_to_read_fixed_width_files_in_apache_nifi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=a8-m3vUKCrY&amp;feature=share", "subreddit_subscribers": 89747, "created_utc": 1676542048.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to read fixed width files in Apache Nifi", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/a8-m3vUKCrY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to read fixed width files in Apache Nifi\"&gt;&lt;/iframe&gt;", "author_name": "InsightByte", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/a8-m3vUKCrY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@InsightByte"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as question. Would appreciate if you could attach the screenshots too.\n\nTIA.", "author_fullname": "t2_i8pf583u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF : How to load n tables, taking in specific columns using a single copy activity, dynamically.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_113mrfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676540248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as question. Would appreciate if you could attach the screenshots too.&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "113mrfb", "is_robot_indexable": true, "report_reasons": null, "author": "titbitwit", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113mrfb/adf_how_to_load_n_tables_taking_in_specific/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/113mrfb/adf_how_to_load_n_tables_taking_in_specific/", "subreddit_subscribers": 89747, "created_utc": 1676540248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_putfris0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_113jt38", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/7N5RaBedmjo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)\"&gt;&lt;/iframe&gt;", "width": 267, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 267, "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/7N5RaBedmjo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)\"&gt;&lt;/iframe&gt;", "author_name": "UiPath Video Tutorials made by Cristian Negulescu", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7N5RaBedmjo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CristianNegulescu"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/7N5RaBedmjo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)\"&gt;&lt;/iframe&gt;", "width": 267, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/113jt38", "height": 200}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JDNyKEy4zhTuQV8JANgW5w5_KpwpObTXZAonOiFOek4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1676528313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/7N5RaBedmjo", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U39QJ0GjEoSZ62scREvzAhlsXwFORmORdIApDIw277w.jpg?auto=webp&amp;v=enabled&amp;s=1b5080a661fb1144527e6042775762090feca05d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/U39QJ0GjEoSZ62scREvzAhlsXwFORmORdIApDIw277w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7946e016b98c55950cb4e73b8132fcb8e81e8d64", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/U39QJ0GjEoSZ62scREvzAhlsXwFORmORdIApDIw277w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=920b63ab44368c184babdba769761a9e9a051b4f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/U39QJ0GjEoSZ62scREvzAhlsXwFORmORdIApDIw277w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b43304e84cc387c11769f604da239b9dfb56bb47", "width": 320, "height": 240}], "variants": {}, "id": "3yMzRF2CAF_SWry0hQAbVtIt_i7oJ8lunLZ5MTmpXuE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "113jt38", "is_robot_indexable": true, "report_reasons": null, "author": "BiguUiPath", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/113jt38/uipath_and_chatgpt_extract_tables_from_pdf_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/7N5RaBedmjo", "subreddit_subscribers": 89747, "created_utc": 1676528313.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 267, "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/7N5RaBedmjo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"UiPath and ChatGPT extract Tables from PDF (use case) (PDF table) (ChatGPT prompts)\"&gt;&lt;/iframe&gt;", "author_name": "UiPath Video Tutorials made by Cristian Negulescu", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7N5RaBedmjo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CristianNegulescu"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The first version of SQL, SQL-86, was released in 1986 by the American National Standards Institute (ANSI) and the International Organization for Standardization (ISO). Since then, there have been several updates to the standard, including SQL-92, SQL-99, and SQL-2003.\n\nIf you're interested in learning about the evolution of SQL over years and how it got to its current place, please check my article published in Level Up Coding publication. In this article, I am going to take a look at the SQL Standard, as well as popular implementations of SQL such as MySQL, PostgreSQL, MS SQL Server, Oracle, and SQLite. I will explore the features and capabilities of each, as well as any limitations or drawbacks.\n\n[https://levelup.gitconnected.com/the-evolution-of-sql-a-look-at-the-past-present-and-future-of-sql-standards-2326cddf7a45](https://levelup.gitconnected.com/the-evolution-of-sql-a-look-at-the-past-present-and-future-of-sql-standards-2326cddf7a45)\n\nhttps://preview.redd.it/w953o0sukeia1.jpg?width=3136&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7faa01ad856bb983564cc05c649d755d9d07bc5d", "author_fullname": "t2_vacizcrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Evolution of SQL: A Look at the Past, Present, and Future of SQL Standards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"w953o0sukeia1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7a69ed3036aa697781bff8a8ca53307ec3a7284"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4455daad9ff1a72855a158605f2f80698db2e93b"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5302bd6af2f3bc780c0acc47297d5a9bc0dd33e0"}, {"y": 320, "x": 640, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f948110d61eae32aac5e59c64ded82abe3aa7f44"}, {"y": 480, "x": 960, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ebb0d7356cd5701a6e04f5e6e6d02845d03936"}, {"y": 540, "x": 1080, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf45919883bd7338cebc0ca4337986789b6e4553"}], "s": {"y": 1568, "x": 3136, "u": "https://preview.redd.it/w953o0sukeia1.jpg?width=3136&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7faa01ad856bb983564cc05c649d755d9d07bc5d"}, "id": "w953o0sukeia1"}}, "name": "t3_11365yo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/bC5Cu5pGGZi2AOMbTc7_wH3X0LYw6z7ehQMCnXEtS80.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1676488958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The first version of SQL, SQL-86, was released in 1986 by the American National Standards Institute (ANSI) and the International Organization for Standardization (ISO). Since then, there have been several updates to the standard, including SQL-92, SQL-99, and SQL-2003.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in learning about the evolution of SQL over years and how it got to its current place, please check my article published in Level Up Coding publication. In this article, I am going to take a look at the SQL Standard, as well as popular implementations of SQL such as MySQL, PostgreSQL, MS SQL Server, Oracle, and SQLite. I will explore the features and capabilities of each, as well as any limitations or drawbacks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://levelup.gitconnected.com/the-evolution-of-sql-a-look-at-the-past-present-and-future-of-sql-standards-2326cddf7a45\"&gt;https://levelup.gitconnected.com/the-evolution-of-sql-a-look-at-the-past-present-and-future-of-sql-standards-2326cddf7a45&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w953o0sukeia1.jpg?width=3136&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7faa01ad856bb983564cc05c649d755d9d07bc5d\"&gt;https://preview.redd.it/w953o0sukeia1.jpg?width=3136&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7faa01ad856bb983564cc05c649d755d9d07bc5d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11365yo", "is_robot_indexable": true, "report_reasons": null, "author": "sdmohajer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11365yo/the_evolution_of_sql_a_look_at_the_past_present/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11365yo/the_evolution_of_sql_a_look_at_the_past_present/", "subreddit_subscribers": 89747, "created_utc": 1676488958.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}