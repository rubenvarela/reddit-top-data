{"kind": "Listing", "data": {"after": "t3_11aef80", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A day in the life of centralized IT...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11a3ecv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PGEguyXn6VqV0TOrBaPj1_BRDZvB_Ym2V_GhwQ2t8CQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677173420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1puqfv734zja1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1puqfv734zja1.jpg?auto=webp&amp;v=enabled&amp;s=de267645d8420ae9b14da400b8d3bd42fc98c44d", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8163c8aeb9edc556e46b2977b7e05a4f8053618", "width": 108, "height": 108}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93991da2ca07bfdcd3ced3ec2e13f072a4e1214a", "width": 216, "height": 216}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d8390e6ca8bd81cee8512c97f5479f6692d185", "width": 320, "height": 320}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e45703f97977f05a0ddf779a9785e13fc440726", "width": 640, "height": 640}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=568f2a2dcd0cf1d4df6215cab06a129ea7c3aa1f", "width": 960, "height": 960}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ab649d8539e8ffda47f1ff8dfd94b3468b3f657", "width": 1080, "height": 1080}], "variants": {}, "id": "p1GSJSfAihfxQGzlZqxswANtTs13sdu2gw0ZCmq9rDg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11a3ecv", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3ecv/a_day_in_the_life_of_centralized_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1puqfv734zja1.jpg", "subreddit_subscribers": 90928, "created_utc": 1677173420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code autoformatter for SQL in VSCode that plays nicely with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7f4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677183120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7f4f", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "subreddit_subscribers": 90928, "created_utc": 1677183120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building language model powered pipelines with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11ackof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uuZwMK_nbNKEYKYujUACRrINOjep3qB_WH-jNYNuY3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677195889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.fal.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?auto=webp&amp;v=enabled&amp;s=5543fed33667c60dfa9b68fec40bc0db4ca4dbe6", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b544d928e82249086452b75125a9619262e2d70c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c397544223dbf986eea3575d64ba752fbabf3e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c03754374081e4865cd6aac9483acc511dc8c1d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=099525a73aeb4bbf03a161566969d9cc19566ba2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5a9a18b59f17938935585e8006ba219e7ec479b", "width": 960, "height": 960}], "variants": {}, "id": "LD_PEcrSvnsrXVCk_-8W_Qkjy50rlUh4TVFaOcEKjHU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ackof", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ackof/building_language_model_powered_pipelines_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "subreddit_subscribers": 90928, "created_utc": 1677195889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create \\*actual\\* streaming pipelines. \n\nBut no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)\n\nAs such, we\u2019d love to have both your **warm** and **snarky** feedback on our baby (beta platform).\n\nThe most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)\n\n**The Pitch:**  \nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. \n\nA free account up to 25gb/mo in data movement can be had here: [www.estuary.dev](https://www.estuary.dev/?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=reddit_feedback&amp;utm_id=18681982783)\n\n**The Details:**\n\nEstuary Flow is built on top of an open-source streaming framework ([Gazette](http://gazette.dev/)) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.\n\nBeyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:\n\n**\\*Collections instead of Buffers.** When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history *and* ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.\n\n**\\*Continuous Views instead of Sinks.** Materialized views update *in-place.* Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make *any* database a \u201creal time\u201d database.  \n\n\n**\\*Completely Incremental, Exactly-Once.** Flow uses a continuous processing model, which propagates transactional data *changes* through your processing graph. This helps keep costs low while maintaining exact copies across different systems.\n\n\\***Turnkey batch and streaming connectors.** Both real-time data as well as historical data supported through one tool and access to pre-built connectors to \\~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.\n\n**\\*Transformations.** We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.\n\n**Managed CDC.**  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  \n\nWe have thick skin and welcome all feedback on our newborn.\n\nSo thick a phlebotomist uses a hammer and nail to take our blood :)  \nBut we also love hugs if that is what you have for us!  \n\n\n[a quick video of our baby, Fivka \\(Estuary Flow\\)](https://reddit.com/link/11a3vga/video/t566u66qlyja1/player)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If Fivetran and Kafka had a baby...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t566u66qlyja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/DASHPlaylist.mpd?a=1679827285%2CZWVkNDI3MWFhMmEwYTExZTJkNGQzNWFjNTJhZDRmODg5NzA3MWJjMjUwMjQxMDY4YzVjOWJmNDc2MTUxMmI5OQ%3D%3D&amp;v=1&amp;f=sd", "x": 1080, "y": 720, "hlsUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/HLSPlaylist.m3u8?a=1679827285%2CMjU0ZWVhMWY0NzNhOTdkOGNkMTQ1ZGIwODlhOGMyNzg0MWUxMzNhZmViYmY3MWMwODMxOTllOWI5ODg3NTliZQ%3D%3D&amp;v=1&amp;f=sd", "id": "t566u66qlyja1", "isGif": false}}, "name": "t3_11a3vga", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hcqEDMEJ6eCMTtHlsTHEcAGVk3iLOY4l6Pt3zg-23AQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677174561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create *actual* streaming pipelines. &lt;/p&gt;\n\n&lt;p&gt;But no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)&lt;/p&gt;\n\n&lt;p&gt;As such, we\u2019d love to have both your &lt;strong&gt;warm&lt;/strong&gt; and &lt;strong&gt;snarky&lt;/strong&gt; feedback on our baby (beta platform).&lt;/p&gt;\n\n&lt;p&gt;The most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Pitch:&lt;/strong&gt;&lt;br/&gt;\nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. &lt;/p&gt;\n\n&lt;p&gt;A free account up to 25gb/mo in data movement can be had here: &lt;a href=\"https://www.estuary.dev/?utm_source=social&amp;amp;utm_medium=reddit&amp;amp;utm_campaign=reddit_feedback&amp;amp;utm_id=18681982783\"&gt;www.estuary.dev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Estuary Flow is built on top of an open-source streaming framework (&lt;a href=\"http://gazette.dev/\"&gt;Gazette&lt;/a&gt;) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.&lt;/p&gt;\n\n&lt;p&gt;Beyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Collections instead of Buffers.&lt;/strong&gt; When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history &lt;em&gt;and&lt;/em&gt; ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Continuous Views instead of Sinks.&lt;/strong&gt; Materialized views update &lt;em&gt;in-place.&lt;/em&gt; Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make &lt;em&gt;any&lt;/em&gt; database a \u201creal time\u201d database.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Completely Incremental, Exactly-Once.&lt;/strong&gt; Flow uses a continuous processing model, which propagates transactional data &lt;em&gt;changes&lt;/em&gt; through your processing graph. This helps keep costs low while maintaining exact copies across different systems.&lt;/p&gt;\n\n&lt;p&gt;*&lt;strong&gt;Turnkey batch and streaming connectors.&lt;/strong&gt; Both real-time data as well as historical data supported through one tool and access to pre-built connectors to ~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Transformations.&lt;/strong&gt; We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed CDC.&lt;/strong&gt;  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  &lt;/p&gt;\n\n&lt;p&gt;We have thick skin and welcome all feedback on our newborn.&lt;/p&gt;\n\n&lt;p&gt;So thick a phlebotomist uses a hammer and nail to take our blood :)&lt;br/&gt;\nBut we also love hugs if that is what you have for us!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/11a3vga/video/t566u66qlyja1/player\"&gt;a quick video of our baby, Fivka (Estuary Flow)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?auto=webp&amp;v=enabled&amp;s=70ac2549cbcee50babf14c4348696590af422bb9", "width": 1024, "height": 542}, "resolutions": [{"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d4aa52afef2755d5d67cba98872d25b5fa321a6", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22d039df909086dd8040909466aec6b242932759", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67d17a7aa8da8d0faa193b11b00fce364340cf6d", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7effc9be0656e11886240c621f136698e2b36fda", "width": 640, "height": 338}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=237b5d6191866d30915c9b83efa0184390c18c6f", "width": 960, "height": 508}], "variants": {}, "id": "gYr1fOXcV-SPELTio4np6ONxmyr0lk2IGzDEuzijt1A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11a3vga", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "subreddit_subscribers": 90928, "created_utc": 1677174561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?\n\nWould it work and make sense?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone already used dbt with polars?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9mkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?&lt;/p&gt;\n\n&lt;p&gt;Would it work and make sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9mkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "subreddit_subscribers": 90928, "created_utc": 1677188542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is more a hypothetical. I don\u2019t think I\u2019d do it. \n\nWe spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.\n\nOur pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.\n\nImagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB &amp; Parquet \u2014&gt; Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is more a hypothetical. I don\u2019t think I\u2019d do it. &lt;/p&gt;\n\n&lt;p&gt;We spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.&lt;/p&gt;\n\n&lt;p&gt;Our pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.&lt;/p&gt;\n\n&lt;p&gt;Imagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mzl", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "subreddit_subscribers": 90928, "created_utc": 1677176357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)\n\nThey run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &lt;&gt; \u2014profile-dir &lt;&gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?\n\n(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)\n\nThanks", "author_fullname": "t2_6csnaw5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core from Windows Task Scheduler or SQL Server Agent?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11adgji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677198235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)&lt;/p&gt;\n\n&lt;p&gt;They run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &amp;lt;&amp;gt; \u2014profile-dir &amp;lt;&amp;gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11adgji", "is_robot_indexable": true, "report_reasons": null, "author": "Material-Resource-19", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "subreddit_subscribers": 90928, "created_utc": 1677198235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\n I got might get a scholarship from [Turing College's Data Engineering course](https://www.turingcollege.com/data-engineering) \\- which comes with an obligation to accept a DE job from their partner companies. If no offer given within 1 month after finishing the course (around 8 months duration), you can apply to other companies as well. Fair, but though. I would need to accept a Data Engineering position with the pay levels they advertise (50k\u20ac), after graduation.\n\nWould you rather\u2026\n\n* ...stick to [self-curated curriculum](https://binchentso.notion.site/My-learning-path-516014e8d523457cbb277c9e2f00e409) aligned with a DE mentor, using the slack time at my current role to work on portfolio projects?\n* ...or follow up with the Turing College course, for 6\u20138 months and afterward have the chance to land a job with their hiring partners?", "author_fullname": "t2_9v9dakww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-curated curriculum vs. Turing College | Data Engineer in training", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a13ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677167900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I got might get a scholarship from &lt;a href=\"https://www.turingcollege.com/data-engineering\"&gt;Turing College&amp;#39;s Data Engineering course&lt;/a&gt; - which comes with an obligation to accept a DE job from their partner companies. If no offer given within 1 month after finishing the course (around 8 months duration), you can apply to other companies as well. Fair, but though. I would need to accept a Data Engineering position with the pay levels they advertise (50k\u20ac), after graduation.&lt;/p&gt;\n\n&lt;p&gt;Would you rather\u2026&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;...stick to &lt;a href=\"https://binchentso.notion.site/My-learning-path-516014e8d523457cbb277c9e2f00e409\"&gt;self-curated curriculum&lt;/a&gt; aligned with a DE mentor, using the slack time at my current role to work on portfolio projects?&lt;/li&gt;\n&lt;li&gt;...or follow up with the Turing College course, for 6\u20138 months and afterward have the chance to land a job with their hiring partners?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?auto=webp&amp;v=enabled&amp;s=df68ae1816dc7afdf03ee42537ebb0b71aacabcb", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bb2164dd8b8974933cbe4b231a8477df4f32097", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccd665a3aea84b67300a43e71c0a49fb8b49a715", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=beffcdca2b7995bee20a409769a64d46f43ff5ce", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11ea4960f04d10d36ba6bd388fada31e58ba1825", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31bfc7b9868fcd83a8c8798e05af9af474a2f682", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82c9ac111fe09001c54f00087a00654e84a211e8", "width": 1080, "height": 564}], "variants": {}, "id": "XqCbOvusBXQkcj77ZU8r51MvvlCHrNaKfTmnqOakYNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data &amp; Analytics Engineer in training", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11a13ge", "is_robot_indexable": true, "report_reasons": null, "author": "binchentso", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11a13ge/selfcurated_curriculum_vs_turing_college_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a13ge/selfcurated_curriculum_vs_turing_college_data/", "subreddit_subscribers": 90928, "created_utc": 1677167900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious if there\u2019s roles out there that are a hybrid of both.", "author_fullname": "t2_l1vnoo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering roles that involve both technical and managerial skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119x4bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious if there\u2019s roles out there that are a hybrid of both.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119x4bq", "is_robot_indexable": true, "report_reasons": null, "author": "Paulythress", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119x4bq/data_engineering_roles_that_involve_both/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119x4bq/data_engineering_roles_that_involve_both/", "subreddit_subscribers": 90928, "created_utc": 1677157209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I'm thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I'm specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?\n\nAlso, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I'm not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.\n\nCurious if anyone has other ideas of how to show off analytics engineering skills, as well.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Healthcare Analytics Engineering Project Ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abfi3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677192976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I&amp;#39;m thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I&amp;#39;m specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?&lt;/p&gt;\n\n&lt;p&gt;Also, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I&amp;#39;m not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has other ideas of how to show off analytics engineering skills, as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abfi3", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "subreddit_subscribers": 90928, "created_utc": 1677192976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I'm learning python and so far have been pretty frustrated with learning it. I'm doing as many projects as I can, as well as thinking of use cases but there's so much to learn I'm not sure how to tie everything together.\n\nI had an interview today with what looked like a simple technical questions involving lists yet I couldn't figure it out for the life of me. \n\nSo how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?", "author_fullname": "t2_pcb7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not all programmers are equal, how much programming is actually needed for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9pep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I&amp;#39;m learning python and so far have been pretty frustrated with learning it. I&amp;#39;m doing as many projects as I can, as well as thinking of use cases but there&amp;#39;s so much to learn I&amp;#39;m not sure how to tie everything together.&lt;/p&gt;\n\n&lt;p&gt;I had an interview today with what looked like a simple technical questions involving lists yet I couldn&amp;#39;t figure it out for the life of me. &lt;/p&gt;\n\n&lt;p&gt;So how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9pep", "is_robot_indexable": true, "report_reasons": null, "author": "IceStallion", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "subreddit_subscribers": 90928, "created_utc": 1677188740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data Engineers,\n\nYou might have gotten many cold emails from many different vendors. And as a new guy in this field, I don't want to send you another spam message. \n\nWhat problems do you commonly see in cold messaging? \n\nHow would you write a cold email/message if you were in their shoe?  \n\n\nAny feedback and/or example will be super helpful--thanks!", "author_fullname": "t2_udfajs4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What problems do you commonly see in cold messaging to data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a2wvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677172238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;You might have gotten many cold emails from many different vendors. And as a new guy in this field, I don&amp;#39;t want to send you another spam message. &lt;/p&gt;\n\n&lt;p&gt;What problems do you commonly see in cold messaging? &lt;/p&gt;\n\n&lt;p&gt;How would you write a cold email/message if you were in their shoe?  &lt;/p&gt;\n\n&lt;p&gt;Any feedback and/or example will be super helpful--thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a2wvf", "is_robot_indexable": true, "report_reasons": null, "author": "jun_dagster", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a2wvf/what_problems_do_you_commonly_see_in_cold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a2wvf/what_problems_do_you_commonly_see_in_cold/", "subreddit_subscribers": 90928, "created_utc": 1677172238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:\n\nDuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?\n\nHope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.\n\nCan it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7418", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677182400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:&lt;/p&gt;\n\n&lt;p&gt;DuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?&lt;/p&gt;\n\n&lt;p&gt;Hope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.&lt;/p&gt;\n\n&lt;p&gt;Can it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7418", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7418/duckdb_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7418/duckdb_question/", "subreddit_subscribers": 90928, "created_utc": 1677182400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table with complex type such as array of dictionaries . I would need to flatten the table  in such a way that every dictionary item value in a array shows as a separate column. How do I achieve this in spark or hive without exploding(and thereby increasing the rows) to multiple rows.\n\nTldr: sample data is A,B,[dict(key1=\"Col3\",value1=\"col3_value\"),dict(key1=\"Col4\",value1=\"col4_value\")].\n\nFinal expected result:\nCol1,col2,col3,col4\nA,B,col3_value,col4_value", "author_fullname": "t2_9fhhwjm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to solve this with hive or spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119xrc4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677159046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table with complex type such as array of dictionaries . I would need to flatten the table  in such a way that every dictionary item value in a array shows as a separate column. How do I achieve this in spark or hive without exploding(and thereby increasing the rows) to multiple rows.&lt;/p&gt;\n\n&lt;p&gt;Tldr: sample data is A,B,[dict(key1=&amp;quot;Col3&amp;quot;,value1=&amp;quot;col3_value&amp;quot;),dict(key1=&amp;quot;Col4&amp;quot;,value1=&amp;quot;col4_value&amp;quot;)].&lt;/p&gt;\n\n&lt;p&gt;Final expected result:\nCol1,col2,col3,col4\nA,B,col3_value,col4_value&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119xrc4", "is_robot_indexable": true, "report_reasons": null, "author": "cieloskyg", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119xrc4/how_to_solve_this_with_hive_or_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119xrc4/how_to_solve_this_with_hive_or_spark/", "subreddit_subscribers": 90928, "created_utc": 1677159046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[This article](https://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7) is a nice summary of different patterns for dealing with DQ issues. It got me to wondering: what (or even *if*) people tend to do with data quality errors? \n\nDo you use Write-Audit-Publish? Just ignore the errors? Not even check for DQ and wait until users start to scream?\u2026", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do *you* implement data quality in your pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119x43k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7\"&gt;This article&lt;/a&gt; is a nice summary of different patterns for dealing with DQ issues. It got me to wondering: what (or even &lt;em&gt;if&lt;/em&gt;) people tend to do with data quality errors? &lt;/p&gt;\n\n&lt;p&gt;Do you use Write-Audit-Publish? Just ignore the errors? Not even check for DQ and wait until users start to scream?\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?auto=webp&amp;v=enabled&amp;s=39aec5dfc16d1e4ae17045f2d7d70d73355a13af", "width": 785, "height": 728}, "resolutions": [{"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7294aa8fb39ec073993e20336f03ccb896c909ad", "width": 108, "height": 100}, {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d81961ca0665d92b74e24fc5026f076840b0233d", "width": 216, "height": 200}, {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=639b9bb187d5aee4fc8cfeb506ce07ec645c6d95", "width": 320, "height": 296}, {"url": "https://external-preview.redd.it/gYWTe07z3ri27fN9NarOSIXF6PfDL2jjCtQnEHyFRwA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f96b3288b28b4a13363a07f45117094b4671108", "width": 640, "height": 593}], "variants": {}, "id": "EtMVakfqU2sDntG3RUUPf0qPToPTaAd3rB7HkaZsRIE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119x43k", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119x43k/how_do_you_implement_data_quality_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119x43k/how_do_you_implement_data_quality_in_your/", "subreddit_subscribers": 90928, "created_utc": 1677157190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have little experience myself with DAGs so please correct me if I'm wrong, but I get the feeling that there's a good amount of DE practitioners that doesn't like DAGs.\n\nThe feeling is reinforced by my current experience with Prefect, which seems to make the fact that it doesn't use DAGs one of their selling points (e.g. https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/)\n\nWhat's the deal? What am I missing?", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's wrong with DAGs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119wyu1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677156735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have little experience myself with DAGs so please correct me if I&amp;#39;m wrong, but I get the feeling that there&amp;#39;s a good amount of DE practitioners that doesn&amp;#39;t like DAGs.&lt;/p&gt;\n\n&lt;p&gt;The feeling is reinforced by my current experience with Prefect, which seems to make the fact that it doesn&amp;#39;t use DAGs one of their selling points (e.g. &lt;a href=\"https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/\"&gt;https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the deal? What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?auto=webp&amp;v=enabled&amp;s=547594bb7ad1461e2bf376971cdf95a74f1db4d2", "width": 6930, "height": 4220}, "resolutions": [{"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a957033059ab0398722232920683d08838f596a", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84decadd5dfee4b9d747e77e10a678f0bfab0d9d", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad3d118771a2f5516054663ffee041741d83b000", "width": 320, "height": 194}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=285b88fd76ca6cb55d0eaee34d6d8326d1eeba32", "width": 640, "height": 389}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=119a69e1370db6e473c0f3edbbbea4890b69e281", "width": 960, "height": 584}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521052318c5d7fd560d892a96b70506c103b8440", "width": 1080, "height": 657}], "variants": {}, "id": "i8z4NJBB0EQA17Rwbx0Uu43Y6kWtvPxTeve2tIChgrI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119wyu1", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119wyu1/whats_wrong_with_dags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119wyu1/whats_wrong_with_dags/", "subreddit_subscribers": 90928, "created_utc": 1677156735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To Improve Data Availability, Think 'Right-Time'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "name": "t3_11agvwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Aatb8m8r8sWEdd3FJNZkDzbgLZjtuViJSUAkJe6laN0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677208096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanami.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datanami.com/2023/02/13/to-improve-data-availability-think-right-time-not-real-time/?blaid=4188838", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?auto=webp&amp;v=enabled&amp;s=d2e030605948155e6c89a2165ddb11fbc337394a", "width": 1389, "height": 531}, "resolutions": [{"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=056b77a7baf445b1ece21ffaf1f9dab33ee2e773", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=deae2bc83e4a0e90fee97da2162d58701b23ad29", "width": 216, "height": 82}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb6904a096734a504bb0928368e153250964a89b", "width": 320, "height": 122}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f24b0323e479cca28b1fe73c408bbdaeaaef9fc", "width": 640, "height": 244}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b978c9cfb64ecd553b5f69046b5f870e20c665d", "width": 960, "height": 366}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8db7b40710eb0d916a52d49d89f677b17a6c3a1", "width": 1080, "height": 412}], "variants": {}, "id": "MTsFQhBimbyfpw5k3sB_a4-MwBM2wdWShDOgTrqsMVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11agvwv", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11agvwv/to_improve_data_availability_think_righttime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datanami.com/2023/02/13/to-improve-data-availability-think-right-time-not-real-time/?blaid=4188838", "subreddit_subscribers": 90928, "created_utc": 1677208096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is loosely related to dataengineering, but as this is a problem for my data engineering team, I figured I'd ask here.\n\nWe need to generate lots of statements and dump them as files in S3. But we also need to serve / expose them via a RESTful microservice. \n\nQuestions: \n\n\\- What are the best practices for that? \n\n\\- Should there be a limit of the file size?\n\n\\- Could the contents be read and then supplied in the response?\n\n\\- Or, is there a way to offer the whole file as a file?\n\nI would love you advice on this one team. Please and thank you.\n\nNOTE: for this scenario, we can assume that security is solved (as it already would be in reality).", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serve S3 files from REST API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abg1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677193012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is loosely related to dataengineering, but as this is a problem for my data engineering team, I figured I&amp;#39;d ask here.&lt;/p&gt;\n\n&lt;p&gt;We need to generate lots of statements and dump them as files in S3. But we also need to serve / expose them via a RESTful microservice. &lt;/p&gt;\n\n&lt;p&gt;Questions: &lt;/p&gt;\n\n&lt;p&gt;- What are the best practices for that? &lt;/p&gt;\n\n&lt;p&gt;- Should there be a limit of the file size?&lt;/p&gt;\n\n&lt;p&gt;- Could the contents be read and then supplied in the response?&lt;/p&gt;\n\n&lt;p&gt;- Or, is there a way to offer the whole file as a file?&lt;/p&gt;\n\n&lt;p&gt;I would love you advice on this one team. Please and thank you.&lt;/p&gt;\n\n&lt;p&gt;NOTE: for this scenario, we can assume that security is solved (as it already would be in reality).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abg1k", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abg1k/serve_s3_files_from_rest_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abg1k/serve_s3_files_from_rest_api/", "subreddit_subscribers": 90928, "created_utc": 1677193012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?", "author_fullname": "t2_bigv1te1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way of merging tables in snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4zio", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677177188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4zio", "is_robot_indexable": true, "report_reasons": null, "author": "SD_strange", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "subreddit_subscribers": 90928, "created_utc": 1677177188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After having worked at different companies, at different sizes and needs. The larger companies seem to keep building pipeline, with lots of duplicated code, deployment scripts, versioning scripts, CI pipelines etc for each process in their business.\n\nThere is a common theme in places where I have worked, where people tend to think of things as either a batch or a stream. Now, given Kafka is embraced at your company and event streaming is implemented heavily. Why not just treat the batch data sources as a stream? By this I mean, just model the pipeline as a producer to a topic and then go on about your transformations. (Confluent's kSQL fb and Connectors help a lot with this).\n\nTime and time again, I keep seeing a lot of re inventing because of the decision to separate the two. It seems much easier, (perhaps with hindsight) to treat data as a first-class citizen and write less code to just produce and consume messages with the data itself, at a record level and use fan-out accordingly. This way the workflow or orchestration is embedded within the services and how they operate. This requires no need for DAGs or step function configurations. Granted this is usually best for one domain where it is appropriate.\n\nWhat is your take? There are of course edge cases and different approaches based on you business needs. But I think simply embracing the architecture of kafka and allowing services to easily plug in to a topic and do a certain transformation at the record level, makes things much easier to reason about.\n\nFor example, SFTP files to an athena table can be simplified greatly.\n\nSpecifically from:\n\n\\-&gt; Poll (x source) (Python Application) -&gt; S3 -&gt; Spark (EMR) Hudi Table -&gt; Athena -&gt; Consumption\n\nExtra configuration (Check state of source, Metrics, Obersvability, Workflow Code (Airflow DAGs, Deployment Code etc)\n\nto:\n\n\\-&gt; Kafka Connector (source) -&gt; Topic -&gt; kSQL DB/Consumer -&gt; Sink\n\nPerhaps I am missing some details but for simpler integrations it seems easier to follow this logic. Where your data is pushed through and is masked (PII). Instead of pushing references to data and then orchestrating each workflow and for each case.\n\nEDIT: (I really should have added a good contextual reference)\n\nAssumptions:\n\n* Kafka (Confluent Managed Instance - replicated) - Observability, Kafka Lag Exporter, Metrics, DLQ, kSQLDB (and connectors), and Replay ability with a well designed schema evolution and centralised schema registry (Avro)\n* Hudi Tables for incremental datalake house integrations\n* Most business really do not need real-time but this can be updated to handle that (Correctness is more important Initially)\n* Moderate costs but justifiable if other factors are beneficial (speed of deployment, agility and correctness, speed of integrations)\n* Most pipelines are stateless\n* Some pipelines being stateful but fairly simple sliding window aggregations (eg. Update User's preferences for GDPR)", "author_fullname": "t2_7jhnfjx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on thinking of everything as a stream and not a batch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119xan6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677186943.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After having worked at different companies, at different sizes and needs. The larger companies seem to keep building pipeline, with lots of duplicated code, deployment scripts, versioning scripts, CI pipelines etc for each process in their business.&lt;/p&gt;\n\n&lt;p&gt;There is a common theme in places where I have worked, where people tend to think of things as either a batch or a stream. Now, given Kafka is embraced at your company and event streaming is implemented heavily. Why not just treat the batch data sources as a stream? By this I mean, just model the pipeline as a producer to a topic and then go on about your transformations. (Confluent&amp;#39;s kSQL fb and Connectors help a lot with this).&lt;/p&gt;\n\n&lt;p&gt;Time and time again, I keep seeing a lot of re inventing because of the decision to separate the two. It seems much easier, (perhaps with hindsight) to treat data as a first-class citizen and write less code to just produce and consume messages with the data itself, at a record level and use fan-out accordingly. This way the workflow or orchestration is embedded within the services and how they operate. This requires no need for DAGs or step function configurations. Granted this is usually best for one domain where it is appropriate.&lt;/p&gt;\n\n&lt;p&gt;What is your take? There are of course edge cases and different approaches based on you business needs. But I think simply embracing the architecture of kafka and allowing services to easily plug in to a topic and do a certain transformation at the record level, makes things much easier to reason about.&lt;/p&gt;\n\n&lt;p&gt;For example, SFTP files to an athena table can be simplified greatly.&lt;/p&gt;\n\n&lt;p&gt;Specifically from:&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; Poll (x source) (Python Application) -&amp;gt; S3 -&amp;gt; Spark (EMR) Hudi Table -&amp;gt; Athena -&amp;gt; Consumption&lt;/p&gt;\n\n&lt;p&gt;Extra configuration (Check state of source, Metrics, Obersvability, Workflow Code (Airflow DAGs, Deployment Code etc)&lt;/p&gt;\n\n&lt;p&gt;to:&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; Kafka Connector (source) -&amp;gt; Topic -&amp;gt; kSQL DB/Consumer -&amp;gt; Sink&lt;/p&gt;\n\n&lt;p&gt;Perhaps I am missing some details but for simpler integrations it seems easier to follow this logic. Where your data is pushed through and is masked (PII). Instead of pushing references to data and then orchestrating each workflow and for each case.&lt;/p&gt;\n\n&lt;p&gt;EDIT: (I really should have added a good contextual reference)&lt;/p&gt;\n\n&lt;p&gt;Assumptions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kafka (Confluent Managed Instance - replicated) - Observability, Kafka Lag Exporter, Metrics, DLQ, kSQLDB (and connectors), and Replay ability with a well designed schema evolution and centralised schema registry (Avro)&lt;/li&gt;\n&lt;li&gt;Hudi Tables for incremental datalake house integrations&lt;/li&gt;\n&lt;li&gt;Most business really do not need real-time but this can be updated to handle that (Correctness is more important Initially)&lt;/li&gt;\n&lt;li&gt;Moderate costs but justifiable if other factors are beneficial (speed of deployment, agility and correctness, speed of integrations)&lt;/li&gt;\n&lt;li&gt;Most pipelines are stateless&lt;/li&gt;\n&lt;li&gt;Some pipelines being stateful but fairly simple sliding window aggregations (eg. Update User&amp;#39;s preferences for GDPR)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119xan6", "is_robot_indexable": true, "report_reasons": null, "author": "the_real_tobo", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119xan6/what_are_your_thoughts_on_thinking_of_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119xan6/what_are_your_thoughts_on_thinking_of_everything/", "subreddit_subscribers": 90928, "created_utc": 1677157717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!   \n\n\nI work on a logistics startup that currently relies on Superset and Power BI as viz toolings. Superset works as self-service BI, while Power BI has an extra layer of validation by the BI team. \n\nI would like to know if you use tools like **Redash**, **Superset** or **Metabase**, what do you do to extract the most from them? Do you have a routine in place to keep the tools clean? Or somehow certify Dashboards official?", "author_fullname": "t2_ijp90vxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How your company uses Superset? Is it on a big scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119wc2e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677154698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!   &lt;/p&gt;\n\n&lt;p&gt;I work on a logistics startup that currently relies on Superset and Power BI as viz toolings. Superset works as self-service BI, while Power BI has an extra layer of validation by the BI team. &lt;/p&gt;\n\n&lt;p&gt;I would like to know if you use tools like &lt;strong&gt;Redash&lt;/strong&gt;, &lt;strong&gt;Superset&lt;/strong&gt; or &lt;strong&gt;Metabase&lt;/strong&gt;, what do you do to extract the most from them? Do you have a routine in place to keep the tools clean? Or somehow certify Dashboards official?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119wc2e", "is_robot_indexable": true, "report_reasons": null, "author": "CzarSantos98", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119wc2e/how_your_company_uses_superset_is_it_on_a_big/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119wc2e/how_your_company_uses_superset_is_it_on_a_big/", "subreddit_subscribers": 90928, "created_utc": 1677154698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI\u2019m the son of a restaurant owner. I basically wait on tables, clean dishes, and maybe chop up some of the veggies.\n\nI did go university and study maths or computer science.\n\nAnyways, I\u2019m kind of making a unified data platform. So we can understand all of our costs, customers, and marketing we do.\n\nI\u2019m trying to automate as much as possible (I\u2019m not an amazing coder).\n\nSo far I\u2019ve got AirByte doing the pulling of data via these things called APIs, I got the data going into a storage place Heroku, and I got this process running by itself with a thing called Dagster.\n\nI got a bunch of sources like deliveroo, facebook, wordpress, and accounting systems.\n\nI\u2019m getting to the stage I gotta do data architecture/modelling and I was wondering if there anything out there that can help make recommendation of what the data model should look like and I can review and edit it.\n\nThe plan afterwards is to get Grafana and h20 so I can represent the current state of the restaurant and maybe predict future revenue.\n\nI ask people who I went to school with and they keep telling me to leave and become an engineer. I\u2019m a restaurant boy but just wanna get this bit right!\n\nThank you everyone", "author_fullname": "t2_5i7ldi97v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything out there that can automate data architecture modelling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119uj2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677148253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m the son of a restaurant owner. I basically wait on tables, clean dishes, and maybe chop up some of the veggies.&lt;/p&gt;\n\n&lt;p&gt;I did go university and study maths or computer science.&lt;/p&gt;\n\n&lt;p&gt;Anyways, I\u2019m kind of making a unified data platform. So we can understand all of our costs, customers, and marketing we do.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to automate as much as possible (I\u2019m not an amazing coder).&lt;/p&gt;\n\n&lt;p&gt;So far I\u2019ve got AirByte doing the pulling of data via these things called APIs, I got the data going into a storage place Heroku, and I got this process running by itself with a thing called Dagster.&lt;/p&gt;\n\n&lt;p&gt;I got a bunch of sources like deliveroo, facebook, wordpress, and accounting systems.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m getting to the stage I gotta do data architecture/modelling and I was wondering if there anything out there that can help make recommendation of what the data model should look like and I can review and edit it.&lt;/p&gt;\n\n&lt;p&gt;The plan afterwards is to get Grafana and h20 so I can represent the current state of the restaurant and maybe predict future revenue.&lt;/p&gt;\n\n&lt;p&gt;I ask people who I went to school with and they keep telling me to leave and become an engineer. I\u2019m a restaurant boy but just wanna get this bit right!&lt;/p&gt;\n\n&lt;p&gt;Thank you everyone&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119uj2h", "is_robot_indexable": true, "report_reasons": null, "author": "biltor-pol", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119uj2h/is_there_anything_out_there_that_can_automate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119uj2h/is_there_anything_out_there_that_can_automate/", "subreddit_subscribers": 90928, "created_utc": 1677148253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently a rockstar senior business systems analyst at fortune 500 with SQL and big query stack. Either I go up into something like data engineering, or I find a new branch career. We have lots of other analyst jobs and many data engineer jobs.\n\n\nWhat's your typical day? What tools do you use? Do I need to know hardcore python programming or something? Don't want to jump into something and then realize I'm drowning cause it's so complex I can't keep up.", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does a data engineer do every day? How do I know if I'm good enough to do that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11amg63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677227305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently a rockstar senior business systems analyst at fortune 500 with SQL and big query stack. Either I go up into something like data engineering, or I find a new branch career. We have lots of other analyst jobs and many data engineer jobs.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your typical day? What tools do you use? Do I need to know hardcore python programming or something? Don&amp;#39;t want to jump into something and then realize I&amp;#39;m drowning cause it&amp;#39;s so complex I can&amp;#39;t keep up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11amg63", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11amg63/what_does_a_data_engineer_do_every_day_how_do_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11amg63/what_does_a_data_engineer_do_every_day_how_do_i/", "subreddit_subscribers": 90928, "created_utc": 1677227305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2rku02rf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Techniques You Should Know as a Kafka Streams Developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11al0xa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BFrngw6jOK9dADX8e-L5TlbuBGJlcGfbzA1Ili3TGIg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677221851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kestra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kestra.io/blogs/2023-02-23-techniques-kafka-streams-developer.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?auto=webp&amp;v=enabled&amp;s=ddf81e77be4d6b79c49db7779725f5235ec9771d", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfc0cab571adc9ba69d078c698cd351f8631c9ba", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c27156262e4d20ef542d9d7bd7e0b10c3e58dd03", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7287209d8ee2c4db4e3fbb60ab42031dd788f63c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bef29cd382b91822c871aa448b00d3c2215efc22", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff52541263d0b94d4959bce5922b72f6e7f12bab", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7f391870b5d7d874b3ab0d2ddcc1c69a2437e2c", "width": 1080, "height": 607}], "variants": {}, "id": "jfTPXGQiNOKhMii9w6fkPVeGpyxL1ZT1lqC_p0_TZTY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11al0xa", "is_robot_indexable": true, "report_reasons": null, "author": "tchiotludo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11al0xa/techniques_you_should_know_as_a_kafka_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kestra.io/blogs/2023-02-23-techniques-kafka-streams-developer.html", "subreddit_subscribers": 90928, "created_utc": 1677221851.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone has insights on how to move data from Hyperion to Snowflake?  Fivetran, ADF, Matillion Data Loader, Stitch etc. don't have connector for Hyperion. There has to be a better way than extracting data from Hyperion as files and load files into Snowflake.", "author_fullname": "t2_2y4jk308", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move data from Hyperion to Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11aef80", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677200948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone has insights on how to move data from Hyperion to Snowflake?  Fivetran, ADF, Matillion Data Loader, Stitch etc. don&amp;#39;t have connector for Hyperion. There has to be a better way than extracting data from Hyperion as files and load files into Snowflake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11aef80", "is_robot_indexable": true, "report_reasons": null, "author": "mr2711", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11aef80/how_to_move_data_from_hyperion_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11aef80/how_to_move_data_from_hyperion_to_snowflake/", "subreddit_subscribers": 90928, "created_utc": 1677200948.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}