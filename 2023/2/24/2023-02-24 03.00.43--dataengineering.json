{"kind": "Listing", "data": {"after": "t3_11a0cmf", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "dbt is getting pretty popular recently, but is it really that \u201cnecessary\u201d? I mean what are the added benefit of introducing new tool when you can do all transformations using python (polars, duckDB\u2026) + in python you can also do the \u201cextract\u201d step so basically you are able to cover entire ETL lifecycle with one tool? Also you can unit test your code better. As python disadvantage I see the dependency management. The only advantage of dbt I can see is you do not have to explicitly create tables as it creates it for you.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt really necessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119s7yv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 68, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677139328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;dbt is getting pretty popular recently, but is it really that \u201cnecessary\u201d? I mean what are the added benefit of introducing new tool when you can do all transformations using python (polars, duckDB\u2026) + in python you can also do the \u201cextract\u201d step so basically you are able to cover entire ETL lifecycle with one tool? Also you can unit test your code better. As python disadvantage I see the dependency management. The only advantage of dbt I can see is you do not have to explicitly create tables as it creates it for you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119s7yv", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119s7yv/is_dbt_really_necessary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119s7yv/is_dbt_really_necessary/", "subreddit_subscribers": 90888, "created_utc": 1677139328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))\n\nI love VS Code and other local IDEs, but they don\u2019t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick [demo](https://www.loom.com/share/8db10268612d4769893123b00500ad35) below).\n\nNext, we\u2019re planning to explore column-level lineage and code/yaml autocomplete using AI. I\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!\n\n[https://www.loom.com/share/8db10268612d4769893123b00500ad35](https://www.loom.com/share/8db10268612d4769893123b00500ad35)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a better local dbt experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119oxil", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677130632.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677128125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone \ud83d\udc4b I\u2019m Ian \u2014 I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable (&lt;a href=\"https://www.turntable.so/\"&gt;https://www.turntable.so/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;I love VS Code and other local IDEs, but they don\u2019t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick &lt;a href=\"https://www.loom.com/share/8db10268612d4769893123b00500ad35\"&gt;demo&lt;/a&gt; below).&lt;/p&gt;\n\n&lt;p&gt;Next, we\u2019re planning to explore column-level lineage and code/yaml autocomplete using AI. I\u2019d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM\u2026 thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.loom.com/share/8db10268612d4769893123b00500ad35\"&gt;https://www.loom.com/share/8db10268612d4769893123b00500ad35&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "119oxil", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/", "subreddit_subscribers": 90888, "created_utc": 1677128125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create \\*actual\\* streaming pipelines. \n\nBut no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)\n\nAs such, we\u2019d love to have both your **warm** and **snarky** feedback on our baby (beta platform).\n\nThe most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)\n\n**The Pitch:**  \nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. \n\nA free account up to 25gb/mo in data movement can be had here: [www.estuary.dev](https://www.estuary.dev/?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=reddit_feedback&amp;utm_id=18681982783)\n\n**The Details:**\n\nEstuary Flow is built on top of an open-source streaming framework ([Gazette](http://gazette.dev/)) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.\n\nBeyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:\n\n**\\*Collections instead of Buffers.** When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history *and* ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.\n\n**\\*Continuous Views instead of Sinks.** Materialized views update *in-place.* Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make *any* database a \u201creal time\u201d database.  \n\n\n**\\*Completely Incremental, Exactly-Once.** Flow uses a continuous processing model, which propagates transactional data *changes* through your processing graph. This helps keep costs low while maintaining exact copies across different systems.\n\n\\***Turnkey batch and streaming connectors.** Both real-time data as well as historical data supported through one tool and access to pre-built connectors to \\~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.\n\n**\\*Transformations.** We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.\n\n**Managed CDC.**  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  \n\nWe have thick skin and welcome all feedback on our newborn.\n\nSo thick a phlebotomist uses a hammer and nail to take our blood :)  \nBut we also love hugs if that is what you have for us!  \n\n\n[a quick video of our baby, Fivka \\(Estuary Flow\\)](https://reddit.com/link/11a3vga/video/t566u66qlyja1/player)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If Fivetran and Kafka had a baby...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t566u66qlyja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/DASHPlaylist.mpd?a=1679799643%2CNjE5ZDIwNjRmZGU0MmRlOThjOTkyM2Y2Y2YzMGIyMTg1NjY4NzM0MWRmYmYzYjM3ODg3ZTZlMDJlZWYyOTI0OQ%3D%3D&amp;v=1&amp;f=sd", "x": 1080, "y": 720, "hlsUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/HLSPlaylist.m3u8?a=1679799643%2COGE3MzdiMWExOGQzMjY3MzVlZjc1ZGU0NDA3ZWUyN2Q5NTEzYThhODZmNGViZjEzNzAwYWI4ODEyYTQ5ODVlNA%3D%3D&amp;v=1&amp;f=sd", "id": "t566u66qlyja1", "isGif": false}}, "name": "t3_11a3vga", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hcqEDMEJ6eCMTtHlsTHEcAGVk3iLOY4l6Pt3zg-23AQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677174561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create *actual* streaming pipelines. &lt;/p&gt;\n\n&lt;p&gt;But no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)&lt;/p&gt;\n\n&lt;p&gt;As such, we\u2019d love to have both your &lt;strong&gt;warm&lt;/strong&gt; and &lt;strong&gt;snarky&lt;/strong&gt; feedback on our baby (beta platform).&lt;/p&gt;\n\n&lt;p&gt;The most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Pitch:&lt;/strong&gt;&lt;br/&gt;\nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. &lt;/p&gt;\n\n&lt;p&gt;A free account up to 25gb/mo in data movement can be had here: &lt;a href=\"https://www.estuary.dev/?utm_source=social&amp;amp;utm_medium=reddit&amp;amp;utm_campaign=reddit_feedback&amp;amp;utm_id=18681982783\"&gt;www.estuary.dev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Estuary Flow is built on top of an open-source streaming framework (&lt;a href=\"http://gazette.dev/\"&gt;Gazette&lt;/a&gt;) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.&lt;/p&gt;\n\n&lt;p&gt;Beyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Collections instead of Buffers.&lt;/strong&gt; When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history &lt;em&gt;and&lt;/em&gt; ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Continuous Views instead of Sinks.&lt;/strong&gt; Materialized views update &lt;em&gt;in-place.&lt;/em&gt; Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make &lt;em&gt;any&lt;/em&gt; database a \u201creal time\u201d database.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Completely Incremental, Exactly-Once.&lt;/strong&gt; Flow uses a continuous processing model, which propagates transactional data &lt;em&gt;changes&lt;/em&gt; through your processing graph. This helps keep costs low while maintaining exact copies across different systems.&lt;/p&gt;\n\n&lt;p&gt;*&lt;strong&gt;Turnkey batch and streaming connectors.&lt;/strong&gt; Both real-time data as well as historical data supported through one tool and access to pre-built connectors to ~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Transformations.&lt;/strong&gt; We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed CDC.&lt;/strong&gt;  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  &lt;/p&gt;\n\n&lt;p&gt;We have thick skin and welcome all feedback on our newborn.&lt;/p&gt;\n\n&lt;p&gt;So thick a phlebotomist uses a hammer and nail to take our blood :)&lt;br/&gt;\nBut we also love hugs if that is what you have for us!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/11a3vga/video/t566u66qlyja1/player\"&gt;a quick video of our baby, Fivka (Estuary Flow)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?auto=webp&amp;v=enabled&amp;s=70ac2549cbcee50babf14c4348696590af422bb9", "width": 1024, "height": 542}, "resolutions": [{"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d4aa52afef2755d5d67cba98872d25b5fa321a6", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22d039df909086dd8040909466aec6b242932759", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67d17a7aa8da8d0faa193b11b00fce364340cf6d", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7effc9be0656e11886240c621f136698e2b36fda", "width": 640, "height": 338}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=237b5d6191866d30915c9b83efa0184390c18c6f", "width": 960, "height": 508}], "variants": {}, "id": "gYr1fOXcV-SPELTio4np6ONxmyr0lk2IGzDEuzijt1A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11a3vga", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "subreddit_subscribers": 90888, "created_utc": 1677174561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A day in the life of centralized IT...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11a3ecv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PGEguyXn6VqV0TOrBaPj1_BRDZvB_Ym2V_GhwQ2t8CQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677173420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1puqfv734zja1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1puqfv734zja1.jpg?auto=webp&amp;v=enabled&amp;s=de267645d8420ae9b14da400b8d3bd42fc98c44d", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8163c8aeb9edc556e46b2977b7e05a4f8053618", "width": 108, "height": 108}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93991da2ca07bfdcd3ced3ec2e13f072a4e1214a", "width": 216, "height": 216}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d8390e6ca8bd81cee8512c97f5479f6692d185", "width": 320, "height": 320}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e45703f97977f05a0ddf779a9785e13fc440726", "width": 640, "height": 640}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=568f2a2dcd0cf1d4df6215cab06a129ea7c3aa1f", "width": 960, "height": 960}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ab649d8539e8ffda47f1ff8dfd94b3468b3f657", "width": 1080, "height": 1080}], "variants": {}, "id": "p1GSJSfAihfxQGzlZqxswANtTs13sdu2gw0ZCmq9rDg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11a3ecv", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3ecv/a_day_in_the_life_of_centralized_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1puqfv734zja1.jpg", "subreddit_subscribers": 90888, "created_utc": 1677173420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code autoformatter for SQL in VSCode that plays nicely with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7f4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677183120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7f4f", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "subreddit_subscribers": 90888, "created_utc": 1677183120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is more a hypothetical. I don\u2019t think I\u2019d do it. \n\nWe spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.\n\nOur pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.\n\nImagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB &amp; Parquet \u2014&gt; Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is more a hypothetical. I don\u2019t think I\u2019d do it. &lt;/p&gt;\n\n&lt;p&gt;We spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.&lt;/p&gt;\n\n&lt;p&gt;Our pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.&lt;/p&gt;\n\n&lt;p&gt;Imagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mzl", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "subreddit_subscribers": 90888, "created_utc": 1677176357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?\n\nWould it work and make sense?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone already used dbt with polars?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9mkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?&lt;/p&gt;\n\n&lt;p&gt;Would it work and make sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9mkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "subreddit_subscribers": 90888, "created_utc": 1677188542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building language model powered pipelines with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11ackof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uuZwMK_nbNKEYKYujUACRrINOjep3qB_WH-jNYNuY3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677195889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.fal.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?auto=webp&amp;v=enabled&amp;s=5543fed33667c60dfa9b68fec40bc0db4ca4dbe6", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b544d928e82249086452b75125a9619262e2d70c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c397544223dbf986eea3575d64ba752fbabf3e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c03754374081e4865cd6aac9483acc511dc8c1d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=099525a73aeb4bbf03a161566969d9cc19566ba2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5a9a18b59f17938935585e8006ba219e7ec479b", "width": 960, "height": 960}], "variants": {}, "id": "LD_PEcrSvnsrXVCk_-8W_Qkjy50rlUh4TVFaOcEKjHU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ackof", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ackof/building_language_model_powered_pipelines_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "subreddit_subscribers": 90888, "created_utc": 1677195889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious if there\u2019s roles out there that are a hybrid of both.", "author_fullname": "t2_l1vnoo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering roles that involve both technical and managerial skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119x4bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious if there\u2019s roles out there that are a hybrid of both.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119x4bq", "is_robot_indexable": true, "report_reasons": null, "author": "Paulythress", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119x4bq/data_engineering_roles_that_involve_both/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119x4bq/data_engineering_roles_that_involve_both/", "subreddit_subscribers": 90888, "created_utc": 1677157209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)\n\nThey run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &lt;&gt; \u2014profile-dir &lt;&gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?\n\n(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)\n\nThanks", "author_fullname": "t2_6csnaw5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core from Windows Task Scheduler or SQL Server Agent?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11adgji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677198235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)&lt;/p&gt;\n\n&lt;p&gt;They run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &amp;lt;&amp;gt; \u2014profile-dir &amp;lt;&amp;gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11adgji", "is_robot_indexable": true, "report_reasons": null, "author": "Material-Resource-19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "subreddit_subscribers": 90888, "created_utc": 1677198235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:\n\nDuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?\n\nHope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.\n\nCan it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7418", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677182400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:&lt;/p&gt;\n\n&lt;p&gt;DuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?&lt;/p&gt;\n\n&lt;p&gt;Hope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.&lt;/p&gt;\n\n&lt;p&gt;Can it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7418", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7418/duckdb_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7418/duckdb_question/", "subreddit_subscribers": 90888, "created_utc": 1677182400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\n I got might get a scholarship from [Turing College's Data Engineering course](https://www.turingcollege.com/data-engineering) \\- which comes with an obligation to accept a DE job from their partner companies. If no offer given within 1 month after finishing the course (around 8 months duration), you can apply to other companies as well. Fair, but though. I would need to accept a Data Engineering position with the pay levels they advertise (50k\u20ac), after graduation.\n\nWould you rather\u2026\n\n* ...stick to [self-curated curriculum](https://binchentso.notion.site/My-learning-path-516014e8d523457cbb277c9e2f00e409) aligned with a DE mentor, using the slack time at my current role to work on portfolio projects?\n* ...or follow up with the Turing College course, for 6\u20138 months and afterward have the chance to land a job with their hiring partners?", "author_fullname": "t2_9v9dakww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-curated curriculum vs. Turing College | Data Engineer in training", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a13ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677167900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I got might get a scholarship from &lt;a href=\"https://www.turingcollege.com/data-engineering\"&gt;Turing College&amp;#39;s Data Engineering course&lt;/a&gt; - which comes with an obligation to accept a DE job from their partner companies. If no offer given within 1 month after finishing the course (around 8 months duration), you can apply to other companies as well. Fair, but though. I would need to accept a Data Engineering position with the pay levels they advertise (50k\u20ac), after graduation.&lt;/p&gt;\n\n&lt;p&gt;Would you rather\u2026&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;...stick to &lt;a href=\"https://binchentso.notion.site/My-learning-path-516014e8d523457cbb277c9e2f00e409\"&gt;self-curated curriculum&lt;/a&gt; aligned with a DE mentor, using the slack time at my current role to work on portfolio projects?&lt;/li&gt;\n&lt;li&gt;...or follow up with the Turing College course, for 6\u20138 months and afterward have the chance to land a job with their hiring partners?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?auto=webp&amp;v=enabled&amp;s=df68ae1816dc7afdf03ee42537ebb0b71aacabcb", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bb2164dd8b8974933cbe4b231a8477df4f32097", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccd665a3aea84b67300a43e71c0a49fb8b49a715", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=beffcdca2b7995bee20a409769a64d46f43ff5ce", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11ea4960f04d10d36ba6bd388fada31e58ba1825", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31bfc7b9868fcd83a8c8798e05af9af474a2f682", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/KPEAGoZ2QYkiiV9psiCaDhpnUCbzJ_Y1e4Lytbi71a4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82c9ac111fe09001c54f00087a00654e84a211e8", "width": 1080, "height": 564}], "variants": {}, "id": "XqCbOvusBXQkcj77ZU8r51MvvlCHrNaKfTmnqOakYNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data &amp; Analytics Engineer in training", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11a13ge", "is_robot_indexable": true, "report_reasons": null, "author": "binchentso", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11a13ge/selfcurated_curriculum_vs_turing_college_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a13ge/selfcurated_curriculum_vs_turing_college_data/", "subreddit_subscribers": 90888, "created_utc": 1677167900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table with complex type such as array of dictionaries . I would need to flatten the table  in such a way that every dictionary item value in a array shows as a separate column. How do I achieve this in spark or hive without exploding(and thereby increasing the rows) to multiple rows.\n\nTldr: sample data is A,B,[dict(key1=\"Col3\",value1=\"col3_value\"),dict(key1=\"Col4\",value1=\"col4_value\")].\n\nFinal expected result:\nCol1,col2,col3,col4\nA,B,col3_value,col4_value", "author_fullname": "t2_9fhhwjm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to solve this with hive or spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119xrc4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677159046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table with complex type such as array of dictionaries . I would need to flatten the table  in such a way that every dictionary item value in a array shows as a separate column. How do I achieve this in spark or hive without exploding(and thereby increasing the rows) to multiple rows.&lt;/p&gt;\n\n&lt;p&gt;Tldr: sample data is A,B,[dict(key1=&amp;quot;Col3&amp;quot;,value1=&amp;quot;col3_value&amp;quot;),dict(key1=&amp;quot;Col4&amp;quot;,value1=&amp;quot;col4_value&amp;quot;)].&lt;/p&gt;\n\n&lt;p&gt;Final expected result:\nCol1,col2,col3,col4\nA,B,col3_value,col4_value&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119xrc4", "is_robot_indexable": true, "report_reasons": null, "author": "cieloskyg", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119xrc4/how_to_solve_this_with_hive_or_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119xrc4/how_to_solve_this_with_hive_or_spark/", "subreddit_subscribers": 90888, "created_utc": 1677159046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My new favorite website -https://status.snowflake.com :(", "author_fullname": "t2_6l3ghhxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new favorite website -&lt;a href=\"https://status.snowflake.com\"&gt;https://status.snowflake.com&lt;/a&gt; :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mhv", "is_robot_indexable": true, "report_reasons": null, "author": "MRWH35", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mhv/snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mhv/snowflake/", "subreddit_subscribers": 90888, "created_utc": 1677176324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data Engineers,\n\nYou might have gotten many cold emails from many different vendors. And as a new guy in this field, I don't want to send you another spam message. \n\nWhat problems do you commonly see in cold messaging? \n\nHow would you write a cold email/message if you were in their shoe?  \n\n\nAny feedback and/or example will be super helpful--thanks!", "author_fullname": "t2_udfajs4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What problems do you commonly see in cold messaging to data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a2wvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677172238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;You might have gotten many cold emails from many different vendors. And as a new guy in this field, I don&amp;#39;t want to send you another spam message. &lt;/p&gt;\n\n&lt;p&gt;What problems do you commonly see in cold messaging? &lt;/p&gt;\n\n&lt;p&gt;How would you write a cold email/message if you were in their shoe?  &lt;/p&gt;\n\n&lt;p&gt;Any feedback and/or example will be super helpful--thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a2wvf", "is_robot_indexable": true, "report_reasons": null, "author": "jun_dagster", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a2wvf/what_problems_do_you_commonly_see_in_cold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a2wvf/what_problems_do_you_commonly_see_in_cold/", "subreddit_subscribers": 90888, "created_utc": 1677172238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After having worked at different companies, at different sizes and needs. The larger companies seem to keep building pipeline, with lots of duplicated code, deployment scripts, versioning scripts, CI pipelines etc for each process in their business.\n\nThere is a common theme in places where I have worked, where people tend to think of things as either a batch or a stream. Now, given Kafka is embraced at your company and event streaming is implemented heavily. Why not just treat the batch data sources as a stream? By this I mean, just model the pipeline as a producer to a topic and then go on about your transformations. (Confluent's kSQL fb and Connectors help a lot with this).\n\nTime and time again, I keep seeing a lot of re inventing because of the decision to separate the two. It seems much easier, (perhaps with hindsight) to treat data as a first-class citizen and write less code to just produce and consume messages with the data itself, at a record level and use fan-out accordingly. This way the workflow or orchestration is embedded within the services and how they operate. This requires no need for DAGs or step function configurations. Granted this is usually best for one domain where it is appropriate.\n\nWhat is your take? There are of course edge cases and different approaches based on you business needs. But I think simply embracing the architecture of kafka and allowing services to easily plug in to a topic and do a certain transformation at the record level, makes things much easier to reason about.\n\nFor example, SFTP files to an athena table can be simplified greatly.\n\nSpecifically from:\n\n\\-&gt; Poll (x source) (Python Application) -&gt; S3 -&gt; Spark (EMR) Hudi Table -&gt; Athena -&gt; Consumption\n\nExtra configuration (Check state of source, Metrics, Obersvability, Workflow Code (Airflow DAGs, Deployment Code etc)\n\nto:\n\n\\-&gt; Kafka Connector (source) -&gt; Topic -&gt; kSQL DB/Consumer -&gt; Sink\n\nPerhaps I am missing some details but for simpler integrations it seems easier to follow this logic. Where your data is pushed through and is masked (PII). Instead of pushing references to data and then orchestrating each workflow and for each case.\n\nEDIT: (I really should have added a good contextual reference)\n\nAssumptions:\n\n* Kafka (Confluent Managed Instance - replicated) - Observability, Kafka Lag Exporter, Metrics, DLQ, kSQLDB (and connectors), and Replay ability with a well designed schema evolution and centralised schema registry (Avro)\n* Hudi Tables for incremental datalake house integrations\n* Most business really do not need real-time but this can be updated to handle that (Correctness is more important Initially)\n* Moderate costs but justifiable if other factors are beneficial (speed of deployment, agility and correctness, speed of integrations)\n* Most pipelines are stateless\n* Some pipelines being stateful but fairly simple sliding window aggregations (eg. Update User's preferences for GDPR)", "author_fullname": "t2_7jhnfjx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on thinking of everything as a stream and not a batch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119xan6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677186943.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677157717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After having worked at different companies, at different sizes and needs. The larger companies seem to keep building pipeline, with lots of duplicated code, deployment scripts, versioning scripts, CI pipelines etc for each process in their business.&lt;/p&gt;\n\n&lt;p&gt;There is a common theme in places where I have worked, where people tend to think of things as either a batch or a stream. Now, given Kafka is embraced at your company and event streaming is implemented heavily. Why not just treat the batch data sources as a stream? By this I mean, just model the pipeline as a producer to a topic and then go on about your transformations. (Confluent&amp;#39;s kSQL fb and Connectors help a lot with this).&lt;/p&gt;\n\n&lt;p&gt;Time and time again, I keep seeing a lot of re inventing because of the decision to separate the two. It seems much easier, (perhaps with hindsight) to treat data as a first-class citizen and write less code to just produce and consume messages with the data itself, at a record level and use fan-out accordingly. This way the workflow or orchestration is embedded within the services and how they operate. This requires no need for DAGs or step function configurations. Granted this is usually best for one domain where it is appropriate.&lt;/p&gt;\n\n&lt;p&gt;What is your take? There are of course edge cases and different approaches based on you business needs. But I think simply embracing the architecture of kafka and allowing services to easily plug in to a topic and do a certain transformation at the record level, makes things much easier to reason about.&lt;/p&gt;\n\n&lt;p&gt;For example, SFTP files to an athena table can be simplified greatly.&lt;/p&gt;\n\n&lt;p&gt;Specifically from:&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; Poll (x source) (Python Application) -&amp;gt; S3 -&amp;gt; Spark (EMR) Hudi Table -&amp;gt; Athena -&amp;gt; Consumption&lt;/p&gt;\n\n&lt;p&gt;Extra configuration (Check state of source, Metrics, Obersvability, Workflow Code (Airflow DAGs, Deployment Code etc)&lt;/p&gt;\n\n&lt;p&gt;to:&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; Kafka Connector (source) -&amp;gt; Topic -&amp;gt; kSQL DB/Consumer -&amp;gt; Sink&lt;/p&gt;\n\n&lt;p&gt;Perhaps I am missing some details but for simpler integrations it seems easier to follow this logic. Where your data is pushed through and is masked (PII). Instead of pushing references to data and then orchestrating each workflow and for each case.&lt;/p&gt;\n\n&lt;p&gt;EDIT: (I really should have added a good contextual reference)&lt;/p&gt;\n\n&lt;p&gt;Assumptions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Kafka (Confluent Managed Instance - replicated) - Observability, Kafka Lag Exporter, Metrics, DLQ, kSQLDB (and connectors), and Replay ability with a well designed schema evolution and centralised schema registry (Avro)&lt;/li&gt;\n&lt;li&gt;Hudi Tables for incremental datalake house integrations&lt;/li&gt;\n&lt;li&gt;Most business really do not need real-time but this can be updated to handle that (Correctness is more important Initially)&lt;/li&gt;\n&lt;li&gt;Moderate costs but justifiable if other factors are beneficial (speed of deployment, agility and correctness, speed of integrations)&lt;/li&gt;\n&lt;li&gt;Most pipelines are stateless&lt;/li&gt;\n&lt;li&gt;Some pipelines being stateful but fairly simple sliding window aggregations (eg. Update User&amp;#39;s preferences for GDPR)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119xan6", "is_robot_indexable": true, "report_reasons": null, "author": "the_real_tobo", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119xan6/what_are_your_thoughts_on_thinking_of_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119xan6/what_are_your_thoughts_on_thinking_of_everything/", "subreddit_subscribers": 90888, "created_utc": 1677157717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have little experience myself with DAGs so please correct me if I'm wrong, but I get the feeling that there's a good amount of DE practitioners that doesn't like DAGs.\n\nThe feeling is reinforced by my current experience with Prefect, which seems to make the fact that it doesn't use DAGs one of their selling points (e.g. https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/)\n\nWhat's the deal? What am I missing?", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's wrong with DAGs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119wyu1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677156735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have little experience myself with DAGs so please correct me if I&amp;#39;m wrong, but I get the feeling that there&amp;#39;s a good amount of DE practitioners that doesn&amp;#39;t like DAGs.&lt;/p&gt;\n\n&lt;p&gt;The feeling is reinforced by my current experience with Prefect, which seems to make the fact that it doesn&amp;#39;t use DAGs one of their selling points (e.g. &lt;a href=\"https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/\"&gt;https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the deal? What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?auto=webp&amp;v=enabled&amp;s=547594bb7ad1461e2bf376971cdf95a74f1db4d2", "width": 6930, "height": 4220}, "resolutions": [{"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a957033059ab0398722232920683d08838f596a", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84decadd5dfee4b9d747e77e10a678f0bfab0d9d", "width": 216, "height": 131}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad3d118771a2f5516054663ffee041741d83b000", "width": 320, "height": 194}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=285b88fd76ca6cb55d0eaee34d6d8326d1eeba32", "width": 640, "height": 389}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=119a69e1370db6e473c0f3edbbbea4890b69e281", "width": 960, "height": 584}, {"url": "https://external-preview.redd.it/Mhjpb6z8Gun8-vNr0i6aPYtzDViWPofEeLUVIx3EX9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521052318c5d7fd560d892a96b70506c103b8440", "width": 1080, "height": 657}], "variants": {}, "id": "i8z4NJBB0EQA17Rwbx0Uu43Y6kWtvPxTeve2tIChgrI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119wyu1", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119wyu1/whats_wrong_with_dags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119wyu1/whats_wrong_with_dags/", "subreddit_subscribers": 90888, "created_utc": 1677156735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks!   \n\n\nI work on a logistics startup that currently relies on Superset and Power BI as viz toolings. Superset works as self-service BI, while Power BI has an extra layer of validation by the BI team. \n\nI would like to know if you use tools like **Redash**, **Superset** or **Metabase**, what do you do to extract the most from them? Do you have a routine in place to keep the tools clean? Or somehow certify Dashboards official?", "author_fullname": "t2_ijp90vxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How your company uses Superset? Is it on a big scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119wc2e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677154698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!   &lt;/p&gt;\n\n&lt;p&gt;I work on a logistics startup that currently relies on Superset and Power BI as viz toolings. Superset works as self-service BI, while Power BI has an extra layer of validation by the BI team. &lt;/p&gt;\n\n&lt;p&gt;I would like to know if you use tools like &lt;strong&gt;Redash&lt;/strong&gt;, &lt;strong&gt;Superset&lt;/strong&gt; or &lt;strong&gt;Metabase&lt;/strong&gt;, what do you do to extract the most from them? Do you have a routine in place to keep the tools clean? Or somehow certify Dashboards official?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "119wc2e", "is_robot_indexable": true, "report_reasons": null, "author": "CzarSantos98", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119wc2e/how_your_company_uses_superset_is_it_on_a_big/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119wc2e/how_your_company_uses_superset_is_it_on_a_big/", "subreddit_subscribers": 90888, "created_utc": 1677154698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I've been a DBA for about 8 years now (currently an MSSQL DBA), and I'm looking to learn more about data engineering and other, non-RDBMS systems. \n\nFor a first project, I want to do some basic extraction of data from various apis/web scraping, load it into a data warehouse (either directly into a traditional DW or use parquet files for a data lake). I put together the first steps of such on GCS using python scripts, duckdb, cloud storage and BigQuery. However, I'm not sure if given my background on the Microsoft side of things. This makes me think that I might want to focus on learning Azure, but looking over Synapse, I'm not sure how I feel about it, as it doesn't seem as simple as BigQuery did to me at first, as ADLS gen2 seems a bit odd, as my current parquet files aren't split down into many files, which seems to be the way to go for ADLS?\n\nI thought about using Databricks, but I'm hesitant, as I can't find much about how much it'll cost me to use as a small DW for learning, and I don't have a free tier to use anymore (used it a few years back for basic learning).\n\nI'm thinking of maybe trying out a different DW, something like ClickHouse, and running that on an Azure VM, but I'm not sure if doing that is a good idea for learning, since it isn't an \"official\" Azure tool. Also considered SingleStore, but a VM large enough to self host it might be too much (the docs recommend 4 CPUs/4GB RAM.\n\n&amp;#x200B;\n\nI guess to give TLDR, I currently work as a DBA in the Microsoft Stack, and when trying to figure out where to start learning DE, I feel like Azure is a natural starting point, but I'm not sure if I'm a fan of Synapse at first, compared to my first impressions of BigQuery.\n\nThanks!", "author_fullname": "t2_ssh4888", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What stack to focus on for learning for my background?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_119qxna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677134613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;ve been a DBA for about 8 years now (currently an MSSQL DBA), and I&amp;#39;m looking to learn more about data engineering and other, non-RDBMS systems. &lt;/p&gt;\n\n&lt;p&gt;For a first project, I want to do some basic extraction of data from various apis/web scraping, load it into a data warehouse (either directly into a traditional DW or use parquet files for a data lake). I put together the first steps of such on GCS using python scripts, duckdb, cloud storage and BigQuery. However, I&amp;#39;m not sure if given my background on the Microsoft side of things. This makes me think that I might want to focus on learning Azure, but looking over Synapse, I&amp;#39;m not sure how I feel about it, as it doesn&amp;#39;t seem as simple as BigQuery did to me at first, as ADLS gen2 seems a bit odd, as my current parquet files aren&amp;#39;t split down into many files, which seems to be the way to go for ADLS?&lt;/p&gt;\n\n&lt;p&gt;I thought about using Databricks, but I&amp;#39;m hesitant, as I can&amp;#39;t find much about how much it&amp;#39;ll cost me to use as a small DW for learning, and I don&amp;#39;t have a free tier to use anymore (used it a few years back for basic learning).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of maybe trying out a different DW, something like ClickHouse, and running that on an Azure VM, but I&amp;#39;m not sure if doing that is a good idea for learning, since it isn&amp;#39;t an &amp;quot;official&amp;quot; Azure tool. Also considered SingleStore, but a VM large enough to self host it might be too much (the docs recommend 4 CPUs/4GB RAM.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I guess to give TLDR, I currently work as a DBA in the Microsoft Stack, and when trying to figure out where to start learning DE, I feel like Azure is a natural starting point, but I&amp;#39;m not sure if I&amp;#39;m a fan of Synapse at first, compared to my first impressions of BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "119qxna", "is_robot_indexable": true, "report_reasons": null, "author": "dontmakemeplaypool", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119qxna/what_stack_to_focus_on_for_learning_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119qxna/what_stack_to_focus_on_for_learning_for_my/", "subreddit_subscribers": 90888, "created_utc": 1677134613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In VSCode,the colorizer feature,\n\n ([@id](https://github.com/id):editor.bracketPairColorization.enabled [@id](https://github.com/id):editor.guides.bracketPairs) \n\ndoes not work for brackets inside strings for ex: scores in (), greatest (), cast() etc. The example provided below is very basic, but we've run into nested functions that span &gt; 10 lines which makes debugging difficult. This would be very helpful to a lot of engineers who use sql/jinja. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2\n\nI created an issue for this feature on the official VSCode Github , \n\n[https://github.com/microsoft/vscode/issues/169649](https://github.com/microsoft/vscode/issues/169649)\n\n&amp;#x200B;\n\nIt requires 20 votes for it to move it to their backlog. If you find this would be helpful, please upvote it on github  \n(more on upvoting here: [https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request](https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request))", "author_fullname": "t2_5rikt61xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bracket Pair Colorizer for SQL - VSCode issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bdl9glzcyuja1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=414f2f2e2a175ed9f5f488dac842aa05835d0df1"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=888f9acb467f86766ec4bd54d06812bff16cb0da"}, {"y": 221, "x": 320, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9eedc1f55b572f5dec02d33a88172d47fb407a5"}, {"y": 442, "x": 640, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0266046deeea13439e0e6d664f2fa31f9c00bcb0"}], "s": {"y": 507, "x": 733, "u": "https://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2"}, "id": "bdl9glzcyuja1"}}, "name": "t3_119na3a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b32GTmNvzTSoMHbew6yaoKUffq22YJ4yrtm6BLCeMHk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677123247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In VSCode,the colorizer feature,&lt;/p&gt;\n\n&lt;p&gt;(&lt;a href=\"https://github.com/id\"&gt;@id&lt;/a&gt;:editor.bracketPairColorization.enabled &lt;a href=\"https://github.com/id\"&gt;@id&lt;/a&gt;:editor.guides.bracketPairs) &lt;/p&gt;\n\n&lt;p&gt;does not work for brackets inside strings for ex: scores in (), greatest (), cast() etc. The example provided below is very basic, but we&amp;#39;ve run into nested functions that span &amp;gt; 10 lines which makes debugging difficult. This would be very helpful to a lot of engineers who use sql/jinja. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2\"&gt;https://preview.redd.it/bdl9glzcyuja1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0df85a635fa49a48208b518ea7c57fe8657eaef2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I created an issue for this feature on the official VSCode Github , &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/microsoft/vscode/issues/169649\"&gt;https://github.com/microsoft/vscode/issues/169649&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It requires 20 votes for it to move it to their backlog. If you find this would be helpful, please upvote it on github&lt;br/&gt;\n(more on upvoting here: &lt;a href=\"https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request\"&gt;https://github.com/microsoft/vscode/wiki/Issues-Triaging#up-voting-a-feature-request&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?auto=webp&amp;v=enabled&amp;s=6dd45d060d304a95d84693ac89a3463494910530", "width": 336, "height": 336}, "resolutions": [{"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd83866d1749754918daa4b1d42d8da603983899", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0c884a660b38a41899fb3b5e3e3308d51e7e9ec", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ghVpdNpMmwiBDZW_KhU3rfCPHnN-8y_NZ9Elg-wRm04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f19d55525044730593b95c0af01f089482efa199", "width": 320, "height": 320}], "variants": {}, "id": "wWaXLNqFaTps7pGrJM9prEG2Htb0x9UlwSG11gTLPCc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "119na3a", "is_robot_indexable": true, "report_reasons": null, "author": "KangarOOCase", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/119na3a/bracket_pair_colorizer_for_sql_vscode_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/119na3a/bracket_pair_colorizer_for_sql_vscode_issue/", "subreddit_subscribers": 90888, "created_utc": 1677123247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI am new to spark streaming and joined a new team recently. We are facing a performance issue in a spark streaming job which gets the GPS data via Azure event hub and pushes it to spark streaming job in Databricks for further processing. The job has ran fine for years but for the past couple of weeks , the job is lagging more and more and it's almost 4 days behind now. \n\nWe created a parallel streaming job on a different cluster with the same code . Only difference is it's writing to a different location in blob storage and using a different consumer group. This job is catching up just fine. The major difference here is the spark metadata files. The compact file which gets generated after every 10 checkpoint files is 8.8 GB in the slow job. In the new job it's only a couple hundred MBs. We are almost positive that this is causing the issue, with time taken to write each compact file. \n\nAny suggestions on how to keep the compact file small , with only like past week data. I did some research but did not get a satisfying solution.\n\nTLDR: How to make sure that the checkpoint compact file does not get too big and how to get rid of a lot of the historical checkpoints from an already large compact file.", "author_fullname": "t2_qshu8mn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks performance issue in Spark Streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11adc3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677197904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am new to spark streaming and joined a new team recently. We are facing a performance issue in a spark streaming job which gets the GPS data via Azure event hub and pushes it to spark streaming job in Databricks for further processing. The job has ran fine for years but for the past couple of weeks , the job is lagging more and more and it&amp;#39;s almost 4 days behind now. &lt;/p&gt;\n\n&lt;p&gt;We created a parallel streaming job on a different cluster with the same code . Only difference is it&amp;#39;s writing to a different location in blob storage and using a different consumer group. This job is catching up just fine. The major difference here is the spark metadata files. The compact file which gets generated after every 10 checkpoint files is 8.8 GB in the slow job. In the new job it&amp;#39;s only a couple hundred MBs. We are almost positive that this is causing the issue, with time taken to write each compact file. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions on how to keep the compact file small , with only like past week data. I did some research but did not get a satisfying solution.&lt;/p&gt;\n\n&lt;p&gt;TLDR: How to make sure that the checkpoint compact file does not get too big and how to get rid of a lot of the historical checkpoints from an already large compact file.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11adc3y", "is_robot_indexable": true, "report_reasons": null, "author": "Global_Industry_6801", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11adc3y/databricks_performance_issue_in_spark_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11adc3y/databricks_performance_issue_in_spark_streaming/", "subreddit_subscribers": 90888, "created_utc": 1677197904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I'm thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I'm specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?\n\nAlso, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I'm not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.\n\nCurious if anyone has other ideas of how to show off analytics engineering skills, as well.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Healthcare Analytics Engineering Project Ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abfi3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677192976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I&amp;#39;m thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I&amp;#39;m specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?&lt;/p&gt;\n\n&lt;p&gt;Also, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I&amp;#39;m not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has other ideas of how to show off analytics engineering skills, as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abfi3", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "subreddit_subscribers": 90888, "created_utc": 1677192976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I'm learning python and so far have been pretty frustrated with learning it. I'm doing as many projects as I can, as well as thinking of use cases but there's so much to learn I'm not sure how to tie everything together.\n\nI had an interview today with what looked like a simple technical questions involving lists yet I couldn't figure it out for the life of me. \n\nSo how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?", "author_fullname": "t2_pcb7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not all programmers are equal, how much programming is actually needed for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9pep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I&amp;#39;m learning python and so far have been pretty frustrated with learning it. I&amp;#39;m doing as many projects as I can, as well as thinking of use cases but there&amp;#39;s so much to learn I&amp;#39;m not sure how to tie everything together.&lt;/p&gt;\n\n&lt;p&gt;I had an interview today with what looked like a simple technical questions involving lists yet I couldn&amp;#39;t figure it out for the life of me. &lt;/p&gt;\n\n&lt;p&gt;So how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9pep", "is_robot_indexable": true, "report_reasons": null, "author": "IceStallion", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "subreddit_subscribers": 90888, "created_utc": 1677188740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?", "author_fullname": "t2_bigv1te1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way of merging tables in snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4zio", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677177188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4zio", "is_robot_indexable": true, "report_reasons": null, "author": "SD_strange", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "subreddit_subscribers": 90888, "created_utc": 1677177188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&gt;The **Bronze layer** is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n\n[Databricks Guide on Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n\n[View Poll](https://www.reddit.com/poll/11a0cmf)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you follow Medallion Architecture and utilize Bronze layer storage for ingested data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a0cmf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677166004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The &lt;strong&gt;Bronze layer&lt;/strong&gt; is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures &amp;quot;as-is,&amp;quot; along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/glossary/medallion-architecture\"&gt;Databricks Guide on Medallion Architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/11a0cmf\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a0cmf", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1677425204031, "options": [{"text": "Yes, but stored without schemas (JSON blobs of each row, CSVs, etc)", "id": "21744110"}, {"text": "Yes, but typed (Parquet/Avro/Table/etc)", "id": "21744111"}, {"text": "No", "id": "21744112"}, {"text": "Other (Add comment)", "id": "21744113"}, {"text": "See Results", "id": "21744114"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 47, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/11a0cmf/do_you_follow_medallion_architecture_and_utilize/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/11a0cmf/do_you_follow_medallion_architecture_and_utilize/", "subreddit_subscribers": 90888, "created_utc": 1677166004.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}