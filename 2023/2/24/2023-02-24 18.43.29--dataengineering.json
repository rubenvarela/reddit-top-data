{"kind": "Listing", "data": {"after": "t3_11arseq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently a rockstar senior business systems analyst at fortune 500 with SQL and big query stack. Either I go up into something like data engineering, or I find a new branch career. We have lots of other analyst jobs and many data engineer jobs.\n\n\nWhat's your typical day? What tools do you use? Do I need to know hardcore python programming or something? Don't want to jump into something and then realize I'm drowning cause it's so complex I can't keep up.", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does a data engineer do every day? How do I know if I'm good enough to do that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11amg63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677227305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently a rockstar senior business systems analyst at fortune 500 with SQL and big query stack. Either I go up into something like data engineering, or I find a new branch career. We have lots of other analyst jobs and many data engineer jobs.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your typical day? What tools do you use? Do I need to know hardcore python programming or something? Don&amp;#39;t want to jump into something and then realize I&amp;#39;m drowning cause it&amp;#39;s so complex I can&amp;#39;t keep up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11amg63", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11amg63/what_does_a_data_engineer_do_every_day_how_do_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11amg63/what_does_a_data_engineer_do_every_day_how_do_i/", "subreddit_subscribers": 90967, "created_utc": 1677227305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code autoformatter for SQL in VSCode that plays nicely with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7f4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677183120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7f4f", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "subreddit_subscribers": 90967, "created_utc": 1677183120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building language model powered pipelines with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11ackof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uuZwMK_nbNKEYKYujUACRrINOjep3qB_WH-jNYNuY3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677195889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.fal.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?auto=webp&amp;v=enabled&amp;s=5543fed33667c60dfa9b68fec40bc0db4ca4dbe6", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b544d928e82249086452b75125a9619262e2d70c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c397544223dbf986eea3575d64ba752fbabf3e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c03754374081e4865cd6aac9483acc511dc8c1d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=099525a73aeb4bbf03a161566969d9cc19566ba2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5a9a18b59f17938935585e8006ba219e7ec479b", "width": 960, "height": 960}], "variants": {}, "id": "LD_PEcrSvnsrXVCk_-8W_Qkjy50rlUh4TVFaOcEKjHU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ackof", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ackof/building_language_model_powered_pipelines_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "subreddit_subscribers": 90967, "created_utc": 1677195889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?\n\nWould it work and make sense?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone already used dbt with polars?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9mkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?&lt;/p&gt;\n\n&lt;p&gt;Would it work and make sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9mkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "subreddit_subscribers": 90967, "created_utc": 1677188542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is more a hypothetical. I don\u2019t think I\u2019d do it. \n\nWe spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.\n\nOur pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.\n\nImagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB &amp; Parquet \u2014&gt; Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is more a hypothetical. I don\u2019t think I\u2019d do it. &lt;/p&gt;\n\n&lt;p&gt;We spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.&lt;/p&gt;\n\n&lt;p&gt;Our pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.&lt;/p&gt;\n\n&lt;p&gt;Imagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mzl", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "subreddit_subscribers": 90967, "created_utc": 1677176357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2rku02rf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Techniques You Should Know as a Kafka Streams Developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11al0xa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BFrngw6jOK9dADX8e-L5TlbuBGJlcGfbzA1Ili3TGIg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677221851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kestra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kestra.io/blogs/2023-02-23-techniques-kafka-streams-developer.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?auto=webp&amp;v=enabled&amp;s=ddf81e77be4d6b79c49db7779725f5235ec9771d", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfc0cab571adc9ba69d078c698cd351f8631c9ba", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c27156262e4d20ef542d9d7bd7e0b10c3e58dd03", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7287209d8ee2c4db4e3fbb60ab42031dd788f63c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bef29cd382b91822c871aa448b00d3c2215efc22", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff52541263d0b94d4959bce5922b72f6e7f12bab", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7f391870b5d7d874b3ab0d2ddcc1c69a2437e2c", "width": 1080, "height": 607}], "variants": {}, "id": "jfTPXGQiNOKhMii9w6fkPVeGpyxL1ZT1lqC_p0_TZTY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11al0xa", "is_robot_indexable": true, "report_reasons": null, "author": "tchiotludo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11al0xa/techniques_you_should_know_as_a_kafka_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kestra.io/blogs/2023-02-23-techniques-kafka-streams-developer.html", "subreddit_subscribers": 90967, "created_utc": 1677221851.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I'm learning python and so far have been pretty frustrated with learning it. I'm doing as many projects as I can, as well as thinking of use cases but there's so much to learn I'm not sure how to tie everything together.\n\nI had an interview today with what looked like a simple technical questions involving lists yet I couldn't figure it out for the life of me. \n\nSo how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?", "author_fullname": "t2_pcb7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not all programmers are equal, how much programming is actually needed for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9pep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I&amp;#39;m learning python and so far have been pretty frustrated with learning it. I&amp;#39;m doing as many projects as I can, as well as thinking of use cases but there&amp;#39;s so much to learn I&amp;#39;m not sure how to tie everything together.&lt;/p&gt;\n\n&lt;p&gt;I had an interview today with what looked like a simple technical questions involving lists yet I couldn&amp;#39;t figure it out for the life of me. &lt;/p&gt;\n\n&lt;p&gt;So how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9pep", "is_robot_indexable": true, "report_reasons": null, "author": "IceStallion", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "subreddit_subscribers": 90967, "created_utc": 1677188740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)\n\nThey run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &lt;&gt; \u2014profile-dir &lt;&gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?\n\n(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)\n\nThanks", "author_fullname": "t2_6csnaw5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core from Windows Task Scheduler or SQL Server Agent?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11adgji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677198235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)&lt;/p&gt;\n\n&lt;p&gt;They run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &amp;lt;&amp;gt; \u2014profile-dir &amp;lt;&amp;gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11adgji", "is_robot_indexable": true, "report_reasons": null, "author": "Material-Resource-19", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "subreddit_subscribers": 90967, "created_utc": 1677198235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,I hope this question is suitable around here. I am in the early stages of my career and I have finished in January a 1.5 year contract as a Data Analyst for a company. Right now, I have had interviews with another company and I have successfully received a Data Engineer position with starting date on the 1st of April.My question is, given my background (Python, R, SQL, Postgres, all upper intermediate level), Bachelors in Data Science and ongoing Masters in Data Science, what would be the best way to gain a lot of knowledge in the Data Engineering field? I have just finished the O'Reilly \"Fundamentals of Data Engineering\", but my open to any suggestions (books, courses, YT channels, etc.). To be more specific, I am oriented towards Google Cloud Platform exclusively. Many thanks in advance!", "author_fullname": "t2_2g94ru12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Analyst to Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11argsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677245720.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677245507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,I hope this question is suitable around here. I am in the early stages of my career and I have finished in January a 1.5 year contract as a Data Analyst for a company. Right now, I have had interviews with another company and I have successfully received a Data Engineer position with starting date on the 1st of April.My question is, given my background (Python, R, SQL, Postgres, all upper intermediate level), Bachelors in Data Science and ongoing Masters in Data Science, what would be the best way to gain a lot of knowledge in the Data Engineering field? I have just finished the O&amp;#39;Reilly &amp;quot;Fundamentals of Data Engineering&amp;quot;, but my open to any suggestions (books, courses, YT channels, etc.). To be more specific, I am oriented towards Google Cloud Platform exclusively. Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11argsz", "is_robot_indexable": true, "report_reasons": null, "author": "BubuGly18", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11argsz/from_analyst_to_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11argsz/from_analyst_to_engineer/", "subreddit_subscribers": 90967, "created_utc": 1677245507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, we are using Databricks Repos with Azure DevOps. For some reason, whenever someone makes changes on their branch, these changes are populated to all branches; kind of defeating the point of a branching strategy to begin with.\n\nHas anybody ever experienced this? I could not figure out, what would be wrong in our setup and haven't found people online with a similar problem.\n\nDoes anybody have some insights on this that they can share with me?", "author_fullname": "t2_6kh3i5kp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confusion in Databricks Repos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11apkwt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677239450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, we are using Databricks Repos with Azure DevOps. For some reason, whenever someone makes changes on their branch, these changes are populated to all branches; kind of defeating the point of a branching strategy to begin with.&lt;/p&gt;\n\n&lt;p&gt;Has anybody ever experienced this? I could not figure out, what would be wrong in our setup and haven&amp;#39;t found people online with a similar problem.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have some insights on this that they can share with me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11apkwt", "is_robot_indexable": true, "report_reasons": null, "author": "SolvingGames", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11apkwt/confusion_in_databricks_repos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11apkwt/confusion_in_databricks_repos/", "subreddit_subscribers": 90967, "created_utc": 1677239450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is loosely related to dataengineering, but as this is a problem for my data engineering team, I figured I'd ask here.\n\nWe need to generate lots of statements and dump them as files in S3. But we also need to serve / expose them via a RESTful microservice. \n\nQuestions: \n\n\\- What are the best practices for that? \n\n\\- Should there be a limit of the file size?\n\n\\- Could the contents be read and then supplied in the response?\n\n\\- Or, is there a way to offer the whole file as a file?\n\nI would love you advice on this one team. Please and thank you.\n\nNOTE: for this scenario, we can assume that security is solved (as it already would be in reality).", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serve S3 files from REST API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abg1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677193012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is loosely related to dataengineering, but as this is a problem for my data engineering team, I figured I&amp;#39;d ask here.&lt;/p&gt;\n\n&lt;p&gt;We need to generate lots of statements and dump them as files in S3. But we also need to serve / expose them via a RESTful microservice. &lt;/p&gt;\n\n&lt;p&gt;Questions: &lt;/p&gt;\n\n&lt;p&gt;- What are the best practices for that? &lt;/p&gt;\n\n&lt;p&gt;- Should there be a limit of the file size?&lt;/p&gt;\n\n&lt;p&gt;- Could the contents be read and then supplied in the response?&lt;/p&gt;\n\n&lt;p&gt;- Or, is there a way to offer the whole file as a file?&lt;/p&gt;\n\n&lt;p&gt;I would love you advice on this one team. Please and thank you.&lt;/p&gt;\n\n&lt;p&gt;NOTE: for this scenario, we can assume that security is solved (as it already would be in reality).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abg1k", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abg1k/serve_s3_files_from_rest_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abg1k/serve_s3_files_from_rest_api/", "subreddit_subscribers": 90967, "created_utc": 1677193012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I'm thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I'm specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?\n\nAlso, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I'm not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.\n\nCurious if anyone has other ideas of how to show off analytics engineering skills, as well.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Healthcare Analytics Engineering Project Ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abfi3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677192976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I&amp;#39;m thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I&amp;#39;m specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?&lt;/p&gt;\n\n&lt;p&gt;Also, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I&amp;#39;m not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has other ideas of how to show off analytics engineering skills, as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abfi3", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "subreddit_subscribers": 90967, "created_utc": 1677192976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm about to be 2 years at my current company as a senior day to analyst, and I'm honestly really not enjoying it because of how bad leaders in the industry are at understanding data, or what goes into it. Boss thinks better data sources like BigQuery and Oracle are unreliable even though half the company uses it, So we use Excel exclusively with 500k rows and just send the same workbook back and forth. I spend most days working 7:00 a.m. to 7:00 p.m., incompetent leaders at my current job and my previous jobs. I'm not sure if I'm extremely unlucky, or if this is just how data analytics is but I'm pretty much just exhausted from it....\n\n\nSo my three options are Data engineering, project management, and cybersecurity. My company is a great company and has all three, it's just a matter of which one to go into. \n\n\n\nMy concerns for data engineering specifically are that while I do possess a lot of technical knowledge, and I can get things done if needed to, I suffer sometimes from being a non-technical person. In the past I've just consulted with the engineering team in order to get databases created in SQL or to have a query created or optimized. I feel like data engineering could be a good career, but a huge leap, and the possibility of drowning in how much there is to learn and being overwhelmed by it. Plus I don't know how bad bosses are or how stakeholder management is. \n\n\n\nCybersecurity seems extremely fun, and there are several cyber security one or two analysts, and then I could even become a cybersecurity project manager later on. I have a degree in information systems and I did take a Linux ethical hacking course, as well as a cybersecurity course. I thought it was awesome. But I don't know how far I could get into it because again, non-technical person so I don't know if I would even be able to advance to an engineer position in security. I also have some skill in Python, but I'm not really a fan of programming at all\n\n\nThen there's project management. I started my career as a project analyst, and I thought it was a blast. It was at an organization using waterfall instead of agile, which kind of sucked, But just getting to be involved in so many projects, change requests, the whole project management life cycle was awesome. The only thing I dislike is the work life balance can be extreme sometimes, and whenever there's extra work to be done, they say screw it, just throw it onto the project analyst. So I'm not sure how a senior project analyst would do, but I've seen senior project analyst positions, and project analyst lead positions lately. \n\n\nAny advice you might have about any of these industries that you have or have not worked on would be great", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "for anyone who has experience in data engineering, project management, or cybersecurity, which one have you enjoyed the most? I need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11arg8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677245463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to be 2 years at my current company as a senior day to analyst, and I&amp;#39;m honestly really not enjoying it because of how bad leaders in the industry are at understanding data, or what goes into it. Boss thinks better data sources like BigQuery and Oracle are unreliable even though half the company uses it, So we use Excel exclusively with 500k rows and just send the same workbook back and forth. I spend most days working 7:00 a.m. to 7:00 p.m., incompetent leaders at my current job and my previous jobs. I&amp;#39;m not sure if I&amp;#39;m extremely unlucky, or if this is just how data analytics is but I&amp;#39;m pretty much just exhausted from it....&lt;/p&gt;\n\n&lt;p&gt;So my three options are Data engineering, project management, and cybersecurity. My company is a great company and has all three, it&amp;#39;s just a matter of which one to go into. &lt;/p&gt;\n\n&lt;p&gt;My concerns for data engineering specifically are that while I do possess a lot of technical knowledge, and I can get things done if needed to, I suffer sometimes from being a non-technical person. In the past I&amp;#39;ve just consulted with the engineering team in order to get databases created in SQL or to have a query created or optimized. I feel like data engineering could be a good career, but a huge leap, and the possibility of drowning in how much there is to learn and being overwhelmed by it. Plus I don&amp;#39;t know how bad bosses are or how stakeholder management is. &lt;/p&gt;\n\n&lt;p&gt;Cybersecurity seems extremely fun, and there are several cyber security one or two analysts, and then I could even become a cybersecurity project manager later on. I have a degree in information systems and I did take a Linux ethical hacking course, as well as a cybersecurity course. I thought it was awesome. But I don&amp;#39;t know how far I could get into it because again, non-technical person so I don&amp;#39;t know if I would even be able to advance to an engineer position in security. I also have some skill in Python, but I&amp;#39;m not really a fan of programming at all&lt;/p&gt;\n\n&lt;p&gt;Then there&amp;#39;s project management. I started my career as a project analyst, and I thought it was a blast. It was at an organization using waterfall instead of agile, which kind of sucked, But just getting to be involved in so many projects, change requests, the whole project management life cycle was awesome. The only thing I dislike is the work life balance can be extreme sometimes, and whenever there&amp;#39;s extra work to be done, they say screw it, just throw it onto the project analyst. So I&amp;#39;m not sure how a senior project analyst would do, but I&amp;#39;ve seen senior project analyst positions, and project analyst lead positions lately. &lt;/p&gt;\n\n&lt;p&gt;Any advice you might have about any of these industries that you have or have not worked on would be great&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11arg8c", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11arg8c/for_anyone_who_has_experience_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11arg8c/for_anyone_who_has_experience_in_data_engineering/", "subreddit_subscribers": 90967, "created_utc": 1677245463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I followed some resources in other to build a docker image of my app and the push it to Azure Container Registry. I followed a tutorial in order to deploy it with a Web App, but I am not sure how to set up and magage the \"runs\" of my app.\n\nWhat I want to do is to be able to set up timed tasks that use the hosted dockerized image and calls docker run with a customized startup command. For instance I would like that:\n\n* Every Y minutes, the docker image is runned with command CMD1\n* Every X minutes, the same docker image is runned with a different command CMD2\n\nHow can I accomplish that?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to set and manage timers to run a dockerized app hosted in ACR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11aodbm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677234938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I followed some resources in other to build a docker image of my app and the push it to Azure Container Registry. I followed a tutorial in order to deploy it with a Web App, but I am not sure how to set up and magage the &amp;quot;runs&amp;quot; of my app.&lt;/p&gt;\n\n&lt;p&gt;What I want to do is to be able to set up timed tasks that use the hosted dockerized image and calls docker run with a customized startup command. For instance I would like that:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Every Y minutes, the docker image is runned with command CMD1&lt;/li&gt;\n&lt;li&gt;Every X minutes, the same docker image is runned with a different command CMD2&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How can I accomplish that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11aodbm", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11aodbm/how_to_set_and_manage_timers_to_run_a_dockerized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11aodbm/how_to_set_and_manage_timers_to_run_a_dockerized/", "subreddit_subscribers": 90967, "created_utc": 1677234938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To Improve Data Availability, Think 'Right-Time'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "name": "t3_11agvwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Aatb8m8r8sWEdd3FJNZkDzbgLZjtuViJSUAkJe6laN0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677208096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanami.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datanami.com/2023/02/13/to-improve-data-availability-think-right-time-not-real-time/?blaid=4188838", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?auto=webp&amp;v=enabled&amp;s=d2e030605948155e6c89a2165ddb11fbc337394a", "width": 1389, "height": 531}, "resolutions": [{"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=056b77a7baf445b1ece21ffaf1f9dab33ee2e773", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=deae2bc83e4a0e90fee97da2162d58701b23ad29", "width": 216, "height": 82}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb6904a096734a504bb0928368e153250964a89b", "width": 320, "height": 122}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f24b0323e479cca28b1fe73c408bbdaeaaef9fc", "width": 640, "height": 244}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b978c9cfb64ecd553b5f69046b5f870e20c665d", "width": 960, "height": 366}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8db7b40710eb0d916a52d49d89f677b17a6c3a1", "width": 1080, "height": 412}], "variants": {}, "id": "MTsFQhBimbyfpw5k3sB_a4-MwBM2wdWShDOgTrqsMVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11agvwv", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11agvwv/to_improve_data_availability_think_righttime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datanami.com/2023/02/13/to-improve-data-availability-think-right-time-not-real-time/?blaid=4188838", "subreddit_subscribers": 90967, "created_utc": 1677208096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:\n\nDuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?\n\nHope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.\n\nCan it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7418", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677182400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:&lt;/p&gt;\n\n&lt;p&gt;DuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?&lt;/p&gt;\n\n&lt;p&gt;Hope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.&lt;/p&gt;\n\n&lt;p&gt;Can it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7418", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7418/duckdb_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7418/duckdb_question/", "subreddit_subscribers": 90967, "created_utc": 1677182400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any repos you all would recommend to see how DAGs are put together?  \n\nReason I ask is we're migrating our jobs to airflow and right now I have several custom python scripts to execute. Reading up on best practices and saw you use the @task decorator and functions to execute which is fine, but I also read you don't want lots of processing logic in your dags so figured I could use the bash operator to call the python scripts with lots of logic but how does that work with @task?   \n\nI'm reading a bunch but not sure which is the best way. Can you mix and match @task with other operators (ie bash?) in the same DAG? Should I just store everything in functions under the task decorator? Is there anything wrong about a single large DAG to run my stuff?  \n\nThanks!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow 2.0 examples and tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11arrno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677246394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any repos you all would recommend to see how DAGs are put together?  &lt;/p&gt;\n\n&lt;p&gt;Reason I ask is we&amp;#39;re migrating our jobs to airflow and right now I have several custom python scripts to execute. Reading up on best practices and saw you use the @task decorator and functions to execute which is fine, but I also read you don&amp;#39;t want lots of processing logic in your dags so figured I could use the bash operator to call the python scripts with lots of logic but how does that work with @task?   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reading a bunch but not sure which is the best way. Can you mix and match @task with other operators (ie bash?) in the same DAG? Should I just store everything in functions under the task decorator? Is there anything wrong about a single large DAG to run my stuff?  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11arrno", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11arrno/airflow_20_examples_and_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11arrno/airflow_20_examples_and_tips/", "subreddit_subscribers": 90967, "created_utc": 1677246394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As an analyst.. I have deep knowledge on writing SQL queries.. my queries could range to thousand lines of code.. I am also good with python for data analysis and running machine learning.. what do I need to know to transition to data engineering. \nThis year I started understanding how dbt and fivetran works. I was able to develop a python script that loads data from an RDS instance to S3 bucket then to snowflake. I also executed with fivetran in moving the data\u2026. Even with all these I feel I know nothing", "author_fullname": "t2_c1lb6wq0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for beginner in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11arcj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677245148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As an analyst.. I have deep knowledge on writing SQL queries.. my queries could range to thousand lines of code.. I am also good with python for data analysis and running machine learning.. what do I need to know to transition to data engineering. \nThis year I started understanding how dbt and fivetran works. I was able to develop a python script that loads data from an RDS instance to S3 bucket then to snowflake. I also executed with fivetran in moving the data\u2026. Even with all these I feel I know nothing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11arcj4", "is_robot_indexable": true, "report_reasons": null, "author": "ejiod2021", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11arcj4/advice_for_beginner_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11arcj4/advice_for_beginner_in_de/", "subreddit_subscribers": 90967, "created_utc": 1677245148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?", "author_fullname": "t2_bigv1te1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way of merging tables in snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4zio", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677177188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4zio", "is_robot_indexable": true, "report_reasons": null, "author": "SD_strange", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "subreddit_subscribers": 90967, "created_utc": 1677177188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My new favorite website -https://status.snowflake.com :(", "author_fullname": "t2_6l3ghhxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new favorite website -&lt;a href=\"https://status.snowflake.com\"&gt;https://status.snowflake.com&lt;/a&gt; :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mhv", "is_robot_indexable": true, "report_reasons": null, "author": "MRWH35", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mhv/snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mhv/snowflake/", "subreddit_subscribers": 90967, "created_utc": 1677176324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am wondering what exact steps do you take in the \"Load\" stage when doing ELT data architecture?\n\nBy Extract I am thinking of simply downloading data from sources and dumping them as is to some object storage. Then the load stage? You take this raw data and without any transformations/validations/cleaning just load it into some RDBMS? Well, I guess at least e.g. schema flattening and data type casting has to be done in this step - but nothing else? No aggregations, derived columns, joins etc? Then aggregations, joins, derived columns in the Transform phase? Also, what if this table loaded in the \"Load\" stage does not require any more transformations? You just copy it 1:1 to different database schema?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What steps do you apply in the \"L\" when doing ELT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_11awvyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677259741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering what exact steps do you take in the &amp;quot;Load&amp;quot; stage when doing ELT data architecture?&lt;/p&gt;\n\n&lt;p&gt;By Extract I am thinking of simply downloading data from sources and dumping them as is to some object storage. Then the load stage? You take this raw data and without any transformations/validations/cleaning just load it into some RDBMS? Well, I guess at least e.g. schema flattening and data type casting has to be done in this step - but nothing else? No aggregations, derived columns, joins etc? Then aggregations, joins, derived columns in the Transform phase? Also, what if this table loaded in the &amp;quot;Load&amp;quot; stage does not require any more transformations? You just copy it 1:1 to different database schema?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11awvyp", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11awvyp/what_steps_do_you_apply_in_the_l_when_doing_elt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11awvyp/what_steps_do_you_apply_in_the_l_when_doing_elt/", "subreddit_subscribers": 90967, "created_utc": 1677259741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4 ways to build dbt Python models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_11awcne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ynjDHdVsg0X3Oo_lCwH45EGss1uZvmBFeWuW9r1rE-0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677258417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafold.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafold.com/blog/dbt-python", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?auto=webp&amp;v=enabled&amp;s=03aee5abbd9db5e52d344c939e5442500746c6e3", "width": 1920, "height": 1081}, "resolutions": [{"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f2c21159b63ab2c364a83dfb7ac6c113f2d48e6", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e04bdcb313a79198461f83c2cb4f80d99477dccd", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7883781904e302a1cee8ed081e1f091eab59a805", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ff0953ccd82b2db9a4cc9d22aaac670b576e790", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38ec2abb4df9311394968c3c4ea93cad5f3b7886", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/dYCGaXKKaGsH5P092NQZ3pIC4XIJu9nrayvmEJrZnfI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1179db66a2f8768ca94ac079dc8d053a21a1b9aa", "width": 1080, "height": 608}], "variants": {}, "id": "nPlCs849bn22ZnA2XvOziYJ0AZmmtYWsIi_Zsj6c2xA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11awcne", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11awcne/4_ways_to_build_dbt_python_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafold.com/blog/dbt-python", "subreddit_subscribers": 90967, "created_utc": 1677258417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video: Demonstration of Trying out Apache Iceberg in Spark Locally using a single Docker Image", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11avnyf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_96.mp4", "dash_url": "https://v.redd.it/6dtfvo7nz5ka1/DASHPlaylist.mpd?a=1679856208%2CZjEwOTk4MzE1OTRkYzc3NDVjNDBiY2JmNjY5YTBhYjRhMjc1ODFlMTczZGIyY2NkYWMwNTk1YTZhMDc1Y2ExMQ%3D%3D&amp;v=1&amp;f=sd", "duration": 899, "hls_url": "https://v.redd.it/6dtfvo7nz5ka1/HLSPlaylist.m3u8?a=1679856208%2CZjY4YWEwYzNhYzM5MWU2ZWUyMjc3MTQ5NmMwOWM5Yzg0YmVmNzdjODNjNDJiMzNjNGI5ZmZmMjI5MDMyODc1Nw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AEGCdZLAciaiQCae5diXVPdR-B8dEeupVzeyhWXFP9A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677256697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/6dtfvo7nz5ka1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=00479251a89a675a148d7df12bd18c9267a64c46", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e2cabe9766586ee7d5461e7bb23ea03c8ce949ff", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=933cbc9ead92bf8d80dda203569ef3b3c92478a0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=625bffe483eda9f054adcbd81b53e347561c88dc", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b1f951764f8edfc44e5ec4f2010f4bfb71a6f564", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=08b67eca962347dcd2546a1597c5d18173cb6aca", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Ccw2QiF9jcm6qSun6XwA5N7Bqh3TQ232byLReeYnzOg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7db6b4ff69fbd5d29532cf817cbe3c8b7cd74ac3", "width": 1080, "height": 607}], "variants": {}, "id": "d8muqGO0greRfC0uhiqc91xBpkQLi7AkX5uw8j29uRE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11avnyf", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11avnyf/video_demonstration_of_trying_out_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/6dtfvo7nz5ka1", "subreddit_subscribers": 90967, "created_utc": 1677256697.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/6dtfvo7nz5ka1/DASH_96.mp4", "dash_url": "https://v.redd.it/6dtfvo7nz5ka1/DASHPlaylist.mpd?a=1679856208%2CZjEwOTk4MzE1OTRkYzc3NDVjNDBiY2JmNjY5YTBhYjRhMjc1ODFlMTczZGIyY2NkYWMwNTk1YTZhMDc1Y2ExMQ%3D%3D&amp;v=1&amp;f=sd", "duration": 899, "hls_url": "https://v.redd.it/6dtfvo7nz5ka1/HLSPlaylist.m3u8?a=1679856208%2CZjY4YWEwYzNhYzM5MWU2ZWUyMjc3MTQ5NmMwOWM5Yzg0YmVmNzdjODNjNDJiMzNjNGI5ZmZmMjI5MDMyODc1Nw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nWe use debezium connector as a source with confluent kafka in my org. We use debezium for both MySQL and SQL Server. In MySQL, debezium seems to run fine for the most part, but in SQL Server, there are some limitations which greatly increase the manual work.\n\n1) In MySQL, if CDC is disabled for some reason &amp; re-enabled, debezium automatically detects it and triggers a full snapshot. The same is not true for SQL Server where it's left for the users to figure out and snapshot manually.\n\n2) Unlike MySQL, DDL operations are not straightforward. There are a series of steps to follow, both offline and online methods are available. My org doesn't prefer taking DB down for DDL operations, that only leaves the online method. [That] (https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#online-schema-updates) has a limitation that values of the new column will be missed after new column is added &amp; before new CDC instance is created.\n\nAll of this adds lot of manual work. For reasons beyond our control, we cannot absolutely guarentee that such steps will always be followed by the DB team nor that CDC is never taken down. (If CDC is enabled, some operations like TRUNCATE on the main table are blocked, not sure if other operations are too.)\n\nThus, we are looking for alternatives to get near real time data (i.e. gap of 15 mins atmost). Does anyone know of an alternative that doesn't have all the above limitations and has automations built in to mostly take care of itself?", "author_fullname": "t2_14bzgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to debezium for SQL Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11auhii", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677253696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;We use debezium connector as a source with confluent kafka in my org. We use debezium for both MySQL and SQL Server. In MySQL, debezium seems to run fine for the most part, but in SQL Server, there are some limitations which greatly increase the manual work.&lt;/p&gt;\n\n&lt;p&gt;1) In MySQL, if CDC is disabled for some reason &amp;amp; re-enabled, debezium automatically detects it and triggers a full snapshot. The same is not true for SQL Server where it&amp;#39;s left for the users to figure out and snapshot manually.&lt;/p&gt;\n\n&lt;p&gt;2) Unlike MySQL, DDL operations are not straightforward. There are a series of steps to follow, both offline and online methods are available. My org doesn&amp;#39;t prefer taking DB down for DDL operations, that only leaves the online method. &lt;a href=\"https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#online-schema-updates\"&gt;That&lt;/a&gt; has a limitation that values of the new column will be missed after new column is added &amp;amp; before new CDC instance is created.&lt;/p&gt;\n\n&lt;p&gt;All of this adds lot of manual work. For reasons beyond our control, we cannot absolutely guarentee that such steps will always be followed by the DB team nor that CDC is never taken down. (If CDC is enabled, some operations like TRUNCATE on the main table are blocked, not sure if other operations are too.)&lt;/p&gt;\n\n&lt;p&gt;Thus, we are looking for alternatives to get near real time data (i.e. gap of 15 mins atmost). Does anyone know of an alternative that doesn&amp;#39;t have all the above limitations and has automations built in to mostly take care of itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11auhii", "is_robot_indexable": true, "report_reasons": null, "author": "downloaderfan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11auhii/alternative_to_debezium_for_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11auhii/alternative_to_debezium_for_sql_server/", "subreddit_subscribers": 90967, "created_utc": 1677253696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! \n\nDoes anyone used Benthos in their orgs? What experiences did you have with it?\n\n - Link:  https://github.com/benthosdev/benthos\n\nI'm currently developing a PoC with it, and for really simple tasks is quite good, but while trying more advanced stuff like time window processing, I'm not really liking it. \n\nWhat are your thoughts about it?", "author_fullname": "t2_16enpq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experiences with Benthos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11arseq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677246456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt;\n\n&lt;p&gt;Does anyone used Benthos in their orgs? What experiences did you have with it?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Link:  &lt;a href=\"https://github.com/benthosdev/benthos\"&gt;https://github.com/benthosdev/benthos&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m currently developing a PoC with it, and for really simple tasks is quite good, but while trying more advanced stuff like time window processing, I&amp;#39;m not really liking it. &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?auto=webp&amp;v=enabled&amp;s=63b3dd26530f54ead3722e992dcc0da0e5220c40", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0f640bbccaa5585b3daf9c7385eb2392635d237", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0babdf7ba6068e6ba5eb5fd5e0978f9306931e4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fcbd668d6eaec7d90f2bed19cbd66eb8e0e71378", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f94bedfbc834c51cfe0ab99e959bd55a6d9533c4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=01ddc43fc25b448df758eaeecc3a3d8967dd85a5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/dsopK-ZJQ3pol4Z4JCg43-ShlNbAtwyU0Bp5mHQD_fE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8575e4e6076161dbea7257e8351136bf128adc88", "width": 1080, "height": 540}], "variants": {}, "id": "okpWdrrKp9xiusC64h2Oe4WuQNjwswZrA8pgQOUQE1o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11arseq", "is_robot_indexable": true, "report_reasons": null, "author": "naxmtz91", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11arseq/experiences_with_benthos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11arseq/experiences_with_benthos/", "subreddit_subscribers": 90967, "created_utc": 1677246456.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}