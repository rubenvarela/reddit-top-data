{"kind": "Listing", "data": {"after": "t3_11aef80", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vlp8q84d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A day in the life of centralized IT...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11a3ecv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PGEguyXn6VqV0TOrBaPj1_BRDZvB_Ym2V_GhwQ2t8CQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677173420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1puqfv734zja1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1puqfv734zja1.jpg?auto=webp&amp;v=enabled&amp;s=de267645d8420ae9b14da400b8d3bd42fc98c44d", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8163c8aeb9edc556e46b2977b7e05a4f8053618", "width": 108, "height": 108}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93991da2ca07bfdcd3ced3ec2e13f072a4e1214a", "width": 216, "height": 216}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d8390e6ca8bd81cee8512c97f5479f6692d185", "width": 320, "height": 320}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e45703f97977f05a0ddf779a9785e13fc440726", "width": 640, "height": 640}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=568f2a2dcd0cf1d4df6215cab06a129ea7c3aa1f", "width": 960, "height": 960}, {"url": "https://preview.redd.it/1puqfv734zja1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ab649d8539e8ffda47f1ff8dfd94b3468b3f657", "width": 1080, "height": 1080}], "variants": {}, "id": "p1GSJSfAihfxQGzlZqxswANtTs13sdu2gw0ZCmq9rDg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "11a3ecv", "is_robot_indexable": true, "report_reasons": null, "author": "SheldonMackay", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3ecv/a_day_in_the_life_of_centralized_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1puqfv734zja1.jpg", "subreddit_subscribers": 90955, "created_utc": 1677173420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code autoformatter for SQL in VSCode that plays nicely with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7f4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677183120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im looking for an autoformatting plugin I can use with VSCode that will work nicely with dbt. The options I were able to find tend to mess up the jinja, which makes me have to manually check the work of my code formatter. does anyone have a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7f4f", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7f4f/code_autoformatter_for_sql_in_vscode_that_plays/", "subreddit_subscribers": 90955, "created_utc": 1677183120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently a rockstar senior business systems analyst at fortune 500 with SQL and big query stack. Either I go up into something like data engineering, or I find a new branch career. We have lots of other analyst jobs and many data engineer jobs.\n\n\nWhat's your typical day? What tools do you use? Do I need to know hardcore python programming or something? Don't want to jump into something and then realize I'm drowning cause it's so complex I can't keep up.", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does a data engineer do every day? How do I know if I'm good enough to do that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11amg63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677227305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently a rockstar senior business systems analyst at fortune 500 with SQL and big query stack. Either I go up into something like data engineering, or I find a new branch career. We have lots of other analyst jobs and many data engineer jobs.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your typical day? What tools do you use? Do I need to know hardcore python programming or something? Don&amp;#39;t want to jump into something and then realize I&amp;#39;m drowning cause it&amp;#39;s so complex I can&amp;#39;t keep up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11amg63", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11amg63/what_does_a_data_engineer_do_every_day_how_do_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11amg63/what_does_a_data_engineer_do_every_day_how_do_i/", "subreddit_subscribers": 90955, "created_utc": 1677227305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y15lw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building language model powered pipelines with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_11ackof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uuZwMK_nbNKEYKYujUACRrINOjep3qB_WH-jNYNuY3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677195889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.fal.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?auto=webp&amp;v=enabled&amp;s=5543fed33667c60dfa9b68fec40bc0db4ca4dbe6", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b544d928e82249086452b75125a9619262e2d70c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44c397544223dbf986eea3575d64ba752fbabf3e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c03754374081e4865cd6aac9483acc511dc8c1d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=099525a73aeb4bbf03a161566969d9cc19566ba2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/NsfQIlhv9V4lIshtpjhAEuRN0mCWIAqybJLvoJBm4t4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5a9a18b59f17938935585e8006ba219e7ec479b", "width": 960, "height": 960}], "variants": {}, "id": "LD_PEcrSvnsrXVCk_-8W_Qkjy50rlUh4TVFaOcEKjHU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11ackof", "is_robot_indexable": true, "report_reasons": null, "author": "gorkemyurt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ackof/building_language_model_powered_pipelines_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.fal.ai/building-language-model-powered-pipelines-with-dbt/", "subreddit_subscribers": 90955, "created_utc": 1677195889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create \\*actual\\* streaming pipelines. \n\nBut no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)\n\nAs such, we\u2019d love to have both your **warm** and **snarky** feedback on our baby (beta platform).\n\nThe most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)\n\n**The Pitch:**  \nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. \n\nA free account up to 25gb/mo in data movement can be had here: [www.estuary.dev](https://www.estuary.dev/?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=reddit_feedback&amp;utm_id=18681982783)\n\n**The Details:**\n\nEstuary Flow is built on top of an open-source streaming framework ([Gazette](http://gazette.dev/)) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.\n\nBeyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:\n\n**\\*Collections instead of Buffers.** When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history *and* ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.\n\n**\\*Continuous Views instead of Sinks.** Materialized views update *in-place.* Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make *any* database a \u201creal time\u201d database.  \n\n\n**\\*Completely Incremental, Exactly-Once.** Flow uses a continuous processing model, which propagates transactional data *changes* through your processing graph. This helps keep costs low while maintaining exact copies across different systems.\n\n\\***Turnkey batch and streaming connectors.** Both real-time data as well as historical data supported through one tool and access to pre-built connectors to \\~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.\n\n**\\*Transformations.** We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.\n\n**Managed CDC.**  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  \n\nWe have thick skin and welcome all feedback on our newborn.\n\nSo thick a phlebotomist uses a hammer and nail to take our blood :)  \nBut we also love hugs if that is what you have for us!  \n\n\n[a quick video of our baby, Fivka \\(Estuary Flow\\)](https://reddit.com/link/11a3vga/video/t566u66qlyja1/player)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If Fivetran and Kafka had a baby...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t566u66qlyja1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/DASHPlaylist.mpd?a=1679849328%2CNDEwYjA0ODQ5ZmMwOThhOGI1M2M5MDg4N2IzNTgxYzA3NjkyODYzNGMyYTg5Zjg5ODU1ZTQwNDk1NmZkZTcxNQ%3D%3D&amp;v=1&amp;f=sd", "x": 1080, "y": 720, "hlsUrl": "https://v.redd.it/link/11a3vga/asset/t566u66qlyja1/HLSPlaylist.m3u8?a=1679849328%2CNDBkODIwYzExZDM4MDExYWUwOWZiOWRiNmZkZWY1MDRlZGQxYjVjM2I2YWIxYjExNTVmNjU1N2JmMjVjNTc1YQ%3D%3D&amp;v=1&amp;f=sd", "id": "t566u66qlyja1", "isGif": false}}, "name": "t3_11a3vga", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hcqEDMEJ6eCMTtHlsTHEcAGVk3iLOY4l6Pt3zg-23AQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1677174561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u2026.it\u2019d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create *actual* streaming pipelines. &lt;/p&gt;\n\n&lt;p&gt;But no romantic table for 2 in the back corner of Ruth\u2019s Chris is scheduled for them\u2026 so we got to babymaking :)&lt;/p&gt;\n\n&lt;p&gt;As such, we\u2019d love to have both your &lt;strong&gt;warm&lt;/strong&gt; and &lt;strong&gt;snarky&lt;/strong&gt; feedback on our baby (beta platform).&lt;/p&gt;\n\n&lt;p&gt;The most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Pitch:&lt;/strong&gt;&lt;br/&gt;\nOur goal with the Estuary Flow platform is to enable building no-code reliable pipes that don\u2019t require scheduling, and support batch/streaming and materialized views in milliseconds. &lt;/p&gt;\n\n&lt;p&gt;A free account up to 25gb/mo in data movement can be had here: &lt;a href=\"https://www.estuary.dev/?utm_source=social&amp;amp;utm_medium=reddit&amp;amp;utm_campaign=reddit_feedback&amp;amp;utm_id=18681982783\"&gt;www.estuary.dev&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Estuary Flow is built on top of an open-source streaming framework (&lt;a href=\"http://gazette.dev/\"&gt;Gazette&lt;/a&gt;) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it\u2019s a real-time data lake.&lt;/p&gt;\n\n&lt;p&gt;Beyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Collections instead of Buffers.&lt;/strong&gt; When a data source is captured \u2013 like Postgres CDC, or Kinesis, or streaming Salesforce \u2013 the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history &lt;em&gt;and&lt;/em&gt; ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Continuous Views instead of Sinks.&lt;/strong&gt; Materialized views update &lt;em&gt;in-place.&lt;/em&gt; Go beyond append-only sinks to build real-time fact tables that update with your captured data \u2013 even in systems not designed for it, like PostgreSQL or Google Sheets. Make &lt;em&gt;any&lt;/em&gt; database a \u201creal time\u201d database.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Completely Incremental, Exactly-Once.&lt;/strong&gt; Flow uses a continuous processing model, which propagates transactional data &lt;em&gt;changes&lt;/em&gt; through your processing graph. This helps keep costs low while maintaining exact copies across different systems.&lt;/p&gt;\n\n&lt;p&gt;*&lt;strong&gt;Turnkey batch and streaming connectors.&lt;/strong&gt; Both real-time data as well as historical data supported through one tool and access to pre-built connectors to ~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets \u2013 all without building a custom integration. Or if you want, plug in your own connector through Flow\u2019s open protocol.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Transformations.&lt;/strong&gt; We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed CDC.&lt;/strong&gt;  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills \u2013 even over your very large tables that Debezium tends to choke on \u2013 and real-time streaming out of the box.  &lt;/p&gt;\n\n&lt;p&gt;We have thick skin and welcome all feedback on our newborn.&lt;/p&gt;\n\n&lt;p&gt;So thick a phlebotomist uses a hammer and nail to take our blood :)&lt;br/&gt;\nBut we also love hugs if that is what you have for us!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/11a3vga/video/t566u66qlyja1/player\"&gt;a quick video of our baby, Fivka (Estuary Flow)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?auto=webp&amp;v=enabled&amp;s=70ac2549cbcee50babf14c4348696590af422bb9", "width": 1024, "height": 542}, "resolutions": [{"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d4aa52afef2755d5d67cba98872d25b5fa321a6", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22d039df909086dd8040909466aec6b242932759", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67d17a7aa8da8d0faa193b11b00fce364340cf6d", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7effc9be0656e11886240c621f136698e2b36fda", "width": 640, "height": 338}, {"url": "https://external-preview.redd.it/rCdPCGoGgNA02F9wGm_KeoLQqgK6LUW4yt29gKknzJk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=237b5d6191866d30915c9b83efa0184390c18c6f", "width": 960, "height": 508}], "variants": {}, "id": "gYr1fOXcV-SPELTio4np6ONxmyr0lk2IGzDEuzijt1A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "11a3vga", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a3vga/if_fivetran_and_kafka_had_a_baby/", "subreddit_subscribers": 90955, "created_utc": 1677174561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?\n\nWould it work and make sense?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone already used dbt with polars?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9mkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since dbt supports python and pandas dataframes I guess one can do the transformation logic using polars and then convert the result to pandas dataframe so dbt can understand it?&lt;/p&gt;\n\n&lt;p&gt;Would it work and make sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9mkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9mkz/has_anyone_already_used_dbt_with_polars/", "subreddit_subscribers": 90955, "created_utc": 1677188542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is more a hypothetical. I don\u2019t think I\u2019d do it. \n\nWe spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.\n\nOur pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.\n\nImagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB &amp; Parquet \u2014&gt; Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is more a hypothetical. I don\u2019t think I\u2019d do it. &lt;/p&gt;\n\n&lt;p&gt;We spend a lot on Snowflake. Arguably too much. But then we optimised for speed to business, not cost. At some point there will need to be a cost optimisation, but I digress.&lt;/p&gt;\n\n&lt;p&gt;Our pipelines run over night. They take about 90 minutes to do a full load using dbt with a small WH. Our biggest table is a little over 100m records.&lt;/p&gt;\n\n&lt;p&gt;Imagine for a minute we were to offload the compute to DuckDB on our K8s cluster, persist interim tables in parquet files, and only load the final fact and dimension tables to Snowflake. Where would we be likely to get hurt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mzl", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mzl/duckdb_parquet_snowflake/", "subreddit_subscribers": 90955, "created_utc": 1677176357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2rku02rf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Techniques You Should Know as a Kafka Streams Developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_11al0xa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BFrngw6jOK9dADX8e-L5TlbuBGJlcGfbzA1Ili3TGIg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677221851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kestra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kestra.io/blogs/2023-02-23-techniques-kafka-streams-developer.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?auto=webp&amp;v=enabled&amp;s=ddf81e77be4d6b79c49db7779725f5235ec9771d", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfc0cab571adc9ba69d078c698cd351f8631c9ba", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c27156262e4d20ef542d9d7bd7e0b10c3e58dd03", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7287209d8ee2c4db4e3fbb60ab42031dd788f63c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bef29cd382b91822c871aa448b00d3c2215efc22", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff52541263d0b94d4959bce5922b72f6e7f12bab", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LVv1k2CZxucbMP1zL6tyQf6Hm_YTJah_aV9lKX2hUps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7f391870b5d7d874b3ab0d2ddcc1c69a2437e2c", "width": 1080, "height": 607}], "variants": {}, "id": "jfTPXGQiNOKhMii9w6fkPVeGpyxL1ZT1lqC_p0_TZTY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11al0xa", "is_robot_indexable": true, "report_reasons": null, "author": "tchiotludo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11al0xa/techniques_you_should_know_as_a_kafka_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kestra.io/blogs/2023-02-23-techniques-kafka-streams-developer.html", "subreddit_subscribers": 90955, "created_utc": 1677221851.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I'm learning python and so far have been pretty frustrated with learning it. I'm doing as many projects as I can, as well as thinking of use cases but there's so much to learn I'm not sure how to tie everything together.\n\nI had an interview today with what looked like a simple technical questions involving lists yet I couldn't figure it out for the life of me. \n\nSo how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?", "author_fullname": "t2_pcb7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not all programmers are equal, how much programming is actually needed for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a9pep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677188740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cloud and Devops engineers often do use scripting in python but their programming level (AFAIK) doesnt compare too well with software engineers. I&amp;#39;m learning python and so far have been pretty frustrated with learning it. I&amp;#39;m doing as many projects as I can, as well as thinking of use cases but there&amp;#39;s so much to learn I&amp;#39;m not sure how to tie everything together.&lt;/p&gt;\n\n&lt;p&gt;I had an interview today with what looked like a simple technical questions involving lists yet I couldn&amp;#39;t figure it out for the life of me. &lt;/p&gt;\n\n&lt;p&gt;So how much programming do I need to know? how did self-taught DEs learning to program, what projects did you do and how do you learn the right things?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a9pep", "is_robot_indexable": true, "report_reasons": null, "author": "IceStallion", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a9pep/not_all_programmers_are_equal_how_much/", "subreddit_subscribers": 90955, "created_utc": 1677188740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)\n\nThey run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &lt;&gt; \u2014profile-dir &lt;&gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?\n\n(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)\n\nThanks", "author_fullname": "t2_6csnaw5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core from Windows Task Scheduler or SQL Server Agent?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11adgji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677198235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question - I have a very short side gig and I\u2019ve got to help my client get from replicated tables to a Kimball model they can use with Power BI Report Server (the on prem Power BI)&lt;/p&gt;\n\n&lt;p&gt;They run SQL Server on prem. Can I develop the transforms in DBT Core and just call them with \u201cdbt build \u2014project-dir &amp;lt;&amp;gt; \u2014profile-dir &amp;lt;&amp;gt;\u201d from Task Scheduler or SQL Server Agent? Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;(Yeah, yeah, I know, why aren\u2019t you using Docker or Airflow? Ewww, SQL Server, gross, why aren\u2019t you using Databricks, or Synapse, or SnowFlake? Because they don\u2019t and don\u2019t want to to - they have a mix of SSIS jobs that no one on their staff knows how to support and insanely long and convoluted scripts their one guy wrote. If I can help them modularize their SQL transforms and get them off SSIS, it\u2019s a win)&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11adgji", "is_robot_indexable": true, "report_reasons": null, "author": "Material-Resource-19", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11adgji/dbt_core_from_windows_task_scheduler_or_sql/", "subreddit_subscribers": 90955, "created_utc": 1677198235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is loosely related to dataengineering, but as this is a problem for my data engineering team, I figured I'd ask here.\n\nWe need to generate lots of statements and dump them as files in S3. But we also need to serve / expose them via a RESTful microservice. \n\nQuestions: \n\n\\- What are the best practices for that? \n\n\\- Should there be a limit of the file size?\n\n\\- Could the contents be read and then supplied in the response?\n\n\\- Or, is there a way to offer the whole file as a file?\n\nI would love you advice on this one team. Please and thank you.\n\nNOTE: for this scenario, we can assume that security is solved (as it already would be in reality).", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serve S3 files from REST API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abg1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677193012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is loosely related to dataengineering, but as this is a problem for my data engineering team, I figured I&amp;#39;d ask here.&lt;/p&gt;\n\n&lt;p&gt;We need to generate lots of statements and dump them as files in S3. But we also need to serve / expose them via a RESTful microservice. &lt;/p&gt;\n\n&lt;p&gt;Questions: &lt;/p&gt;\n\n&lt;p&gt;- What are the best practices for that? &lt;/p&gt;\n\n&lt;p&gt;- Should there be a limit of the file size?&lt;/p&gt;\n\n&lt;p&gt;- Could the contents be read and then supplied in the response?&lt;/p&gt;\n\n&lt;p&gt;- Or, is there a way to offer the whole file as a file?&lt;/p&gt;\n\n&lt;p&gt;I would love you advice on this one team. Please and thank you.&lt;/p&gt;\n\n&lt;p&gt;NOTE: for this scenario, we can assume that security is solved (as it already would be in reality).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abg1k", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abg1k/serve_s3_files_from_rest_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abg1k/serve_s3_files_from_rest_api/", "subreddit_subscribers": 90955, "created_utc": 1677193012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I'm thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I'm specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?\n\nAlso, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I'm not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.\n\nCurious if anyone has other ideas of how to show off analytics engineering skills, as well.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Healthcare Analytics Engineering Project Ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11abfi3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677192976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently graduated from an MS Data Science program and have 4 years experience as a data analyst and have found the market tough to get back into, so I&amp;#39;m thinking of doing a project related to analytics engineering that I can post on my resume to make me a candidate for Analytics Engineer roles.  I&amp;#39;m specifically interested in the healthcare sector.  Does anyone know of healthcare datasets that might be worth checking out?  Ideally complex datasets that I could create a complex data model for?&lt;/p&gt;\n\n&lt;p&gt;Also, does anyone have any ideas for how to go about such a project?  I was thinking of hosting data in Snowflake or Redshift, creating a data model in dbt and then displaying something to end users as Tableau dashboards.  I&amp;#39;m not sure how hard this would be, or how expensive - it occurs to me that hosting all this data would cost money.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has other ideas of how to show off analytics engineering skills, as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11abfi3", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11abfi3/healthcare_analytics_engineering_project_ideas/", "subreddit_subscribers": 90955, "created_utc": 1677192976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To Improve Data Availability, Think 'Right-Time'", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "name": "t3_11agvwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Aatb8m8r8sWEdd3FJNZkDzbgLZjtuViJSUAkJe6laN0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1677208096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanami.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datanami.com/2023/02/13/to-improve-data-availability-think-right-time-not-real-time/?blaid=4188838", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?auto=webp&amp;v=enabled&amp;s=d2e030605948155e6c89a2165ddb11fbc337394a", "width": 1389, "height": 531}, "resolutions": [{"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=056b77a7baf445b1ece21ffaf1f9dab33ee2e773", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=deae2bc83e4a0e90fee97da2162d58701b23ad29", "width": 216, "height": 82}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb6904a096734a504bb0928368e153250964a89b", "width": 320, "height": 122}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f24b0323e479cca28b1fe73c408bbdaeaaef9fc", "width": 640, "height": 244}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b978c9cfb64ecd553b5f69046b5f870e20c665d", "width": 960, "height": 366}, {"url": "https://external-preview.redd.it/Jn50EJl9Q1hYYyjpvlGZ4KgHEAvrkW5ABV9M9X42HrI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8db7b40710eb0d916a52d49d89f677b17a6c3a1", "width": 1080, "height": 412}], "variants": {}, "id": "MTsFQhBimbyfpw5k3sB_a4-MwBM2wdWShDOgTrqsMVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11agvwv", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11agvwv/to_improve_data_availability_think_righttime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datanami.com/2023/02/13/to-improve-data-availability-think-right-time-not-real-time/?blaid=4188838", "subreddit_subscribers": 90955, "created_utc": 1677208096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:\n\nDuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?\n\nHope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.\n\nCan it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a7418", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677182400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve not been able to find the answer to this in the docs, so the answer is probably \u2019no\u2019, but thought best to check:&lt;/p&gt;\n\n&lt;p&gt;DuckDB can read/write to parquet. You can even have a view of a parquet - CREATE view duck AS SELECT * FROM \u2018duck.parquet\u2019 - but can you define a table to be stored as parquet (like Hudi or Iceberg)?&lt;/p&gt;\n\n&lt;p&gt;Hope that makes sense? I want to store the contents of the database as a series of parquet files - one or more for each table.&lt;/p&gt;\n\n&lt;p&gt;Can it be done? And not by saving down the parquet files and then dropping and recreating the view over the top. Ideally ACID compliant. I\u2019m might be asking too much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11a7418", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a7418/duckdb_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a7418/duckdb_question/", "subreddit_subscribers": 90955, "created_utc": 1677182400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data Engineers,\n\nYou might have gotten many cold emails from many different vendors. And as a new guy in this field, I don't want to send you another spam message. \n\nWhat problems do you commonly see in cold messaging? \n\nHow would you write a cold email/message if you were in their shoe?  \n\n\nAny feedback and/or example will be super helpful--thanks!", "author_fullname": "t2_udfajs4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What problems do you commonly see in cold messaging to data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a2wvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677172238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;You might have gotten many cold emails from many different vendors. And as a new guy in this field, I don&amp;#39;t want to send you another spam message. &lt;/p&gt;\n\n&lt;p&gt;What problems do you commonly see in cold messaging? &lt;/p&gt;\n\n&lt;p&gt;How would you write a cold email/message if you were in their shoe?  &lt;/p&gt;\n\n&lt;p&gt;Any feedback and/or example will be super helpful--thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a2wvf", "is_robot_indexable": true, "report_reasons": null, "author": "jun_dagster", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a2wvf/what_problems_do_you_commonly_see_in_cold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a2wvf/what_problems_do_you_commonly_see_in_cold/", "subreddit_subscribers": 90955, "created_utc": 1677172238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any repos you all would recommend to see how DAGs are put together?  \n\nReason I ask is we're migrating our jobs to airflow and right now I have several custom python scripts to execute. Reading up on best practices and saw you use the @task decorator and functions to execute which is fine, but I also read you don't want lots of processing logic in your dags so figured I could use the bash operator to call the python scripts with lots of logic but how does that work with @task?   \n\nI'm reading a bunch but not sure which is the best way. Can you mix and match @task with other operators (ie bash?) in the same DAG? Should I just store everything in functions under the task decorator? Is there anything wrong about a single large DAG to run my stuff?  \n\nThanks!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow 2.0 examples and tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11arrno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677246394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any repos you all would recommend to see how DAGs are put together?  &lt;/p&gt;\n\n&lt;p&gt;Reason I ask is we&amp;#39;re migrating our jobs to airflow and right now I have several custom python scripts to execute. Reading up on best practices and saw you use the @task decorator and functions to execute which is fine, but I also read you don&amp;#39;t want lots of processing logic in your dags so figured I could use the bash operator to call the python scripts with lots of logic but how does that work with @task?   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reading a bunch but not sure which is the best way. Can you mix and match @task with other operators (ie bash?) in the same DAG? Should I just store everything in functions under the task decorator? Is there anything wrong about a single large DAG to run my stuff?  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11arrno", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11arrno/airflow_20_examples_and_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11arrno/airflow_20_examples_and_tips/", "subreddit_subscribers": 90955, "created_utc": 1677246394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,I hope this question is suitable around here. I am in the early stages of my career and I have finished in January a 1.5 year contract as a Data Analyst for a company. Right now, I have had interviews with another company and I have successfully received a Data Engineer position with starting date on the 1st of April.My question is, given my background (Python, R, SQL, Postgres, all upper intermediate level), Bachelors in Data Science and ongoing Masters in Data Science, what would be the best way to gain a lot of knowledge in the Data Engineering field? I have just finished the O'Reilly \"Fundamentals of Data Engineering\", but my open to any suggestions (books, courses, YT channels, etc.). To be more specific, I am oriented towards Google Cloud Platform exclusively. Many thanks in advance!", "author_fullname": "t2_2g94ru12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Analyst to Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11argsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1677245720.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677245507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,I hope this question is suitable around here. I am in the early stages of my career and I have finished in January a 1.5 year contract as a Data Analyst for a company. Right now, I have had interviews with another company and I have successfully received a Data Engineer position with starting date on the 1st of April.My question is, given my background (Python, R, SQL, Postgres, all upper intermediate level), Bachelors in Data Science and ongoing Masters in Data Science, what would be the best way to gain a lot of knowledge in the Data Engineering field? I have just finished the O&amp;#39;Reilly &amp;quot;Fundamentals of Data Engineering&amp;quot;, but my open to any suggestions (books, courses, YT channels, etc.). To be more specific, I am oriented towards Google Cloud Platform exclusively. Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11argsz", "is_robot_indexable": true, "report_reasons": null, "author": "BubuGly18", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11argsz/from_analyst_to_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11argsz/from_analyst_to_engineer/", "subreddit_subscribers": 90955, "created_utc": 1677245507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm about to be 2 years at my current company as a senior day to analyst, and I'm honestly really not enjoying it because of how bad leaders in the industry are at understanding data, or what goes into it. Boss thinks better data sources like BigQuery and Oracle are unreliable even though half the company uses it, So we use Excel exclusively with 500k rows and just send the same workbook back and forth. I spend most days working 7:00 a.m. to 7:00 p.m., incompetent leaders at my current job and my previous jobs. I'm not sure if I'm extremely unlucky, or if this is just how data analytics is but I'm pretty much just exhausted from it....\n\n\nSo my three options are Data engineering, project management, and cybersecurity. My company is a great company and has all three, it's just a matter of which one to go into. \n\n\n\nMy concerns for data engineering specifically are that while I do possess a lot of technical knowledge, and I can get things done if needed to, I suffer sometimes from being a non-technical person. In the past I've just consulted with the engineering team in order to get databases created in SQL or to have a query created or optimized. I feel like data engineering could be a good career, but a huge leap, and the possibility of drowning in how much there is to learn and being overwhelmed by it. Plus I don't know how bad bosses are or how stakeholder management is. \n\n\n\nCybersecurity seems extremely fun, and there are several cyber security one or two analysts, and then I could even become a cybersecurity project manager later on. I have a degree in information systems and I did take a Linux ethical hacking course, as well as a cybersecurity course. I thought it was awesome. But I don't know how far I could get into it because again, non-technical person so I don't know if I would even be able to advance to an engineer position in security. I also have some skill in Python, but I'm not really a fan of programming at all\n\n\nThen there's project management. I started my career as a project analyst, and I thought it was a blast. It was at an organization using waterfall instead of agile, which kind of sucked, But just getting to be involved in so many projects, change requests, the whole project management life cycle was awesome. The only thing I dislike is the work life balance can be extreme sometimes, and whenever there's extra work to be done, they say screw it, just throw it onto the project analyst. So I'm not sure how a senior project analyst would do, but I've seen senior project analyst positions, and project analyst lead positions lately. \n\n\nAny advice you might have about any of these industries that you have or have not worked on would be great", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "for anyone who has experience in data engineering, project management, or cybersecurity, which one have you enjoyed the most? I need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11arg8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677245463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to be 2 years at my current company as a senior day to analyst, and I&amp;#39;m honestly really not enjoying it because of how bad leaders in the industry are at understanding data, or what goes into it. Boss thinks better data sources like BigQuery and Oracle are unreliable even though half the company uses it, So we use Excel exclusively with 500k rows and just send the same workbook back and forth. I spend most days working 7:00 a.m. to 7:00 p.m., incompetent leaders at my current job and my previous jobs. I&amp;#39;m not sure if I&amp;#39;m extremely unlucky, or if this is just how data analytics is but I&amp;#39;m pretty much just exhausted from it....&lt;/p&gt;\n\n&lt;p&gt;So my three options are Data engineering, project management, and cybersecurity. My company is a great company and has all three, it&amp;#39;s just a matter of which one to go into. &lt;/p&gt;\n\n&lt;p&gt;My concerns for data engineering specifically are that while I do possess a lot of technical knowledge, and I can get things done if needed to, I suffer sometimes from being a non-technical person. In the past I&amp;#39;ve just consulted with the engineering team in order to get databases created in SQL or to have a query created or optimized. I feel like data engineering could be a good career, but a huge leap, and the possibility of drowning in how much there is to learn and being overwhelmed by it. Plus I don&amp;#39;t know how bad bosses are or how stakeholder management is. &lt;/p&gt;\n\n&lt;p&gt;Cybersecurity seems extremely fun, and there are several cyber security one or two analysts, and then I could even become a cybersecurity project manager later on. I have a degree in information systems and I did take a Linux ethical hacking course, as well as a cybersecurity course. I thought it was awesome. But I don&amp;#39;t know how far I could get into it because again, non-technical person so I don&amp;#39;t know if I would even be able to advance to an engineer position in security. I also have some skill in Python, but I&amp;#39;m not really a fan of programming at all&lt;/p&gt;\n\n&lt;p&gt;Then there&amp;#39;s project management. I started my career as a project analyst, and I thought it was a blast. It was at an organization using waterfall instead of agile, which kind of sucked, But just getting to be involved in so many projects, change requests, the whole project management life cycle was awesome. The only thing I dislike is the work life balance can be extreme sometimes, and whenever there&amp;#39;s extra work to be done, they say screw it, just throw it onto the project analyst. So I&amp;#39;m not sure how a senior project analyst would do, but I&amp;#39;ve seen senior project analyst positions, and project analyst lead positions lately. &lt;/p&gt;\n\n&lt;p&gt;Any advice you might have about any of these industries that you have or have not worked on would be great&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "11arg8c", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11arg8c/for_anyone_who_has_experience_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11arg8c/for_anyone_who_has_experience_in_data_engineering/", "subreddit_subscribers": 90955, "created_utc": 1677245463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I followed some resources in other to build a docker image of my app and the push it to Azure Container Registry. I followed a tutorial in order to deploy it with a Web App, but I am not sure how to set up and magage the \"runs\" of my app.\n\nWhat I want to do is to be able to set up timed tasks that use the hosted dockerized image and calls docker run with a customized startup command. For instance I would like that:\n\n* Every Y minutes, the docker image is runned with command CMD1\n* Every X minutes, the same docker image is runned with a different command CMD2\n\nHow can I accomplish that?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to set and manage timers to run a dockerized app hosted in ACR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11aodbm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677234938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I followed some resources in other to build a docker image of my app and the push it to Azure Container Registry. I followed a tutorial in order to deploy it with a Web App, but I am not sure how to set up and magage the &amp;quot;runs&amp;quot; of my app.&lt;/p&gt;\n\n&lt;p&gt;What I want to do is to be able to set up timed tasks that use the hosted dockerized image and calls docker run with a customized startup command. For instance I would like that:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Every Y minutes, the docker image is runned with command CMD1&lt;/li&gt;\n&lt;li&gt;Every X minutes, the same docker image is runned with a different command CMD2&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How can I accomplish that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11aodbm", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11aodbm/how_to_set_and_manage_timers_to_run_a_dockerized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11aodbm/how_to_set_and_manage_timers_to_run_a_dockerized/", "subreddit_subscribers": 90955, "created_utc": 1677234938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?", "author_fullname": "t2_bigv1te1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way of merging tables in snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4zio", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677177188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have some data in an S3 bucket and want to load it into snowflake tables. I am using pyspark to read and write the data. To merge the incremental data I can create some temp tables in snowflake and write the merge query. But I want to avoid that, is there a way to do the merge operation without using the storage of snowflake for temp tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4zio", "is_robot_indexable": true, "report_reasons": null, "author": "SD_strange", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4zio/what_is_the_best_way_of_merging_tables_in/", "subreddit_subscribers": 90955, "created_utc": 1677177188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My new favorite website -https://status.snowflake.com :(", "author_fullname": "t2_6l3ghhxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11a4mhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677176324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My new favorite website -&lt;a href=\"https://status.snowflake.com\"&gt;https://status.snowflake.com&lt;/a&gt; :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "11a4mhv", "is_robot_indexable": true, "report_reasons": null, "author": "MRWH35", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11a4mhv/snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11a4mhv/snowflake/", "subreddit_subscribers": 90955, "created_utc": 1677176324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hopefully this doesn't breake any rules (let me know if that's the case). \n\n Join us on **March 7, 2023 at 11AM (EST)**, for our online event to learn what remarkable results these companies achieved with Keboola and how they did it. Plus, find out how *you* can use Keboola too.  \n \n\nIn-depth talks and interactive workshops for 500 everyday data heroes. Hosted by Keboola. Workshops for dbt beginners and experts, customer stories and a list of technical walkthroughs.   \n\n\n[https://lp.keboola.com/empower-online-by-keboola?utm\\_source=signature&amp;utm\\_medium=email&amp;utm\\_campaign=empower&amp;utm\\_content=lundberg](https://lp.keboola.com/empower-online-by-keboola?utm_source=signature&amp;utm_medium=email&amp;utm_campaign=empower&amp;utm_content=lundberg)", "author_fullname": "t2_opyjpm1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Online Event: Keboola, dbt and success stories .", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11apnqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1677239731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully this doesn&amp;#39;t breake any rules (let me know if that&amp;#39;s the case). &lt;/p&gt;\n\n&lt;p&gt;Join us on &lt;strong&gt;March 7, 2023 at 11AM (EST)&lt;/strong&gt;, for our online event to learn what remarkable results these companies achieved with Keboola and how they did it. Plus, find out how &lt;em&gt;you&lt;/em&gt; can use Keboola too.  &lt;/p&gt;\n\n&lt;p&gt;In-depth talks and interactive workshops for 500 everyday data heroes. Hosted by Keboola. Workshops for dbt beginners and experts, customer stories and a list of technical walkthroughs.   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lp.keboola.com/empower-online-by-keboola?utm_source=signature&amp;amp;utm_medium=email&amp;amp;utm_campaign=empower&amp;amp;utm_content=lundberg\"&gt;https://lp.keboola.com/empower-online-by-keboola?utm_source=signature&amp;amp;utm_medium=email&amp;amp;utm_campaign=empower&amp;amp;utm_content=lundberg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?auto=webp&amp;v=enabled&amp;s=c91a5fe1ed907329ca23d6a20adb77e577179861", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=502c7bc44df3463ca12c95beb0f4a081a9f86a2d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32b038df2825aba42506cf875cf9ac92d1937cdb", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d1811d17684fb93cd372ebc176b28c7b6c97856", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d9896bb7daf3cc8ff02872b55bb4f62a138f02b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad37f074251353993113d53278948b47635ed536", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fbRPC2dKGy1MUv1uqEinXygCSKdMz7mK1DF7qaDGYeQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbe064ac89ca6965ac32d4728fbbc62ee003bdc7", "width": 1080, "height": 567}], "variants": {}, "id": "piUl_p41XfGk4PLiT4H02atNTQF75mjNyXzv8y6izPY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "11apnqn", "is_robot_indexable": true, "report_reasons": null, "author": "CalleKeboola", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11apnqn/online_event_keboola_dbt_and_success_stories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11apnqn/online_event_keboola_dbt_and_success_stories/", "subreddit_subscribers": 90955, "created_utc": 1677239731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, we are using Databricks Repos with Azure DevOps. For some reason, whenever someone makes changes on their branch, these changes are populated to all branches; kind of defeating the point of a branching strategy to begin with.\n\nHas anybody ever experienced this? I could not figure out, what would be wrong in our setup and haven't found people online with a similar problem.\n\nDoes anybody have some insights on this that they can share with me?", "author_fullname": "t2_6kh3i5kp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confusion in Databricks Repos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11apkwt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677239450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, we are using Databricks Repos with Azure DevOps. For some reason, whenever someone makes changes on their branch, these changes are populated to all branches; kind of defeating the point of a branching strategy to begin with.&lt;/p&gt;\n\n&lt;p&gt;Has anybody ever experienced this? I could not figure out, what would be wrong in our setup and haven&amp;#39;t found people online with a similar problem.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have some insights on this that they can share with me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11apkwt", "is_robot_indexable": true, "report_reasons": null, "author": "SolvingGames", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11apkwt/confusion_in_databricks_repos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11apkwt/confusion_in_databricks_repos/", "subreddit_subscribers": 90955, "created_utc": 1677239450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m starting on a personal project where I need to work with streaming data, but I\u2019ve only ever dealt with batch pipelines. Can anyone give advice on what tools or best practices I should look into?\n\nProject workflow is as follows: \nGet data from a webhook then check if it matches some user defined criteria. If yes, send alert to a Telegram bot. I also want to store everything for historical analysis whether it meets the criteria or not.\n\nDo I check against my criteria in real time or after storing in a database? What are some good cloud platforms I should look into using for this?\n\nIt\u2019s a personal project for now, but I\u2019m willing to pay for a cloud platform as long as it\u2019s reasonably priced. My data throughput should be quite low (&lt;1-2 GB/month?)", "author_fullname": "t2_72tiq3y1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Planning Streaming Pipeline Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11ahtto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677210979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m starting on a personal project where I need to work with streaming data, but I\u2019ve only ever dealt with batch pipelines. Can anyone give advice on what tools or best practices I should look into?&lt;/p&gt;\n\n&lt;p&gt;Project workflow is as follows: \nGet data from a webhook then check if it matches some user defined criteria. If yes, send alert to a Telegram bot. I also want to store everything for historical analysis whether it meets the criteria or not.&lt;/p&gt;\n\n&lt;p&gt;Do I check against my criteria in real time or after storing in a database? What are some good cloud platforms I should look into using for this?&lt;/p&gt;\n\n&lt;p&gt;It\u2019s a personal project for now, but I\u2019m willing to pay for a cloud platform as long as it\u2019s reasonably priced. My data throughput should be quite low (&amp;lt;1-2 GB/month?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11ahtto", "is_robot_indexable": true, "report_reasons": null, "author": "ROCKITZ15", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11ahtto/help_planning_streaming_pipeline_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11ahtto/help_planning_streaming_pipeline_architecture/", "subreddit_subscribers": 90955, "created_utc": 1677210979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone has insights on how to move data from Hyperion to Snowflake?  Fivetran, ADF, Matillion Data Loader, Stitch etc. don't have connector for Hyperion. There has to be a better way than extracting data from Hyperion as files and load files into Snowflake.", "author_fullname": "t2_2y4jk308", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move data from Hyperion to Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_11aef80", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1677200948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone has insights on how to move data from Hyperion to Snowflake?  Fivetran, ADF, Matillion Data Loader, Stitch etc. don&amp;#39;t have connector for Hyperion. There has to be a better way than extracting data from Hyperion as files and load files into Snowflake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "11aef80", "is_robot_indexable": true, "report_reasons": null, "author": "mr2711", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/11aef80/how_to_move_data_from_hyperion_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/11aef80/how_to_move_data_from_hyperion_to_snowflake/", "subreddit_subscribers": 90955, "created_utc": 1677200948.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}