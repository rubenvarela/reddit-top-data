{"kind": "Listing", "data": {"after": "t3_10svsyd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hg3enfgk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do all my BI initiatives end up like this? \ud83d\ude29", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10sxj6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 119, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/3miuaya2e3ga1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/3miuaya2e3ga1/DASH_96.mp4", "dash_url": "https://v.redd.it/3miuaya2e3ga1/DASHPlaylist.mpd?a=1678121052%2CMzlkMGYwYWRjMjBiYmY1MjJlMWFlNTk0YTYxNzcxZmI1MTE0OGY2ZmRlOWZmYmVlZWQ3MTQ4Y2U0ZjMwZTlkNw%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/3miuaya2e3ga1/HLSPlaylist.m3u8?a=1678121052%2CODU0NzdhYTIxMzA1NDhlNzQyMDNhNjlhNWE2NGEyZTk2NzM5NTQ2YjU3ZDdlNDExY2QyZDIxNDRhNjI4NjcyMQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 119, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/X1v7U7_zEvDx-aFcjo5va5mgNFbsSKtx-4ClmvAgrwM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675463793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/3miuaya2e3ga1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3878d1f9f9e60902172ce5ac1af0fdc7d8e3fe1d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d76a62f25a6d867b419e4d789a62a597c61fc596", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ce99fd83df40cab3bc295fd80ca47320ce881332", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3cb7de2b96e12b511f22737cc4bb1fec7c4ddb6d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=49e8406356277c63fb35f0160fcaa2a2060244ff", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7515bf341310dfe023e72bfcd43c892ff7481984", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=14868f6bb0b064f393bb66bc67971a6b876766e4", "width": 1080, "height": 607}], "variants": {}, "id": "WklSmgWvyM2nescx70iL76I-U0tvpKJi8lxo2zVV6Jw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10sxj6r", "is_robot_indexable": true, "report_reasons": null, "author": "Salmon-Advantage", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sxj6r/why_do_all_my_bi_initiatives_end_up_like_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/3miuaya2e3ga1", "subreddit_subscribers": 88450, "created_utc": 1675463793.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/3miuaya2e3ga1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/3miuaya2e3ga1/DASH_96.mp4", "dash_url": "https://v.redd.it/3miuaya2e3ga1/DASHPlaylist.mpd?a=1678121052%2CMzlkMGYwYWRjMjBiYmY1MjJlMWFlNTk0YTYxNzcxZmI1MTE0OGY2ZmRlOWZmYmVlZWQ3MTQ4Y2U0ZjMwZTlkNw%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/3miuaya2e3ga1/HLSPlaylist.m3u8?a=1678121052%2CODU0NzdhYTIxMzA1NDhlNzQyMDNhNjlhNWE2NGEyZTk2NzM5NTQ2YjU3ZDdlNDExY2QyZDIxNDRhNjI4NjcyMQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based \"solution\" to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the \"solution\" has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can't they just let people write Python?\n\nSure the reason might be -- oh BI developers don't want to learn Python or don't have best practices. Well the first reason doesn't hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?\n\nI'm not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...\n\nSincerely, I'd like to know why. I don't really see any benefit. I mean whatever your \"syntax\" can do, Airflow can do that too. I don't see man, I don't really see.", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do people re-invent wrappers for Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10svt5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675459675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based &amp;quot;solution&amp;quot; to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the &amp;quot;solution&amp;quot; has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can&amp;#39;t they just let people write Python?&lt;/p&gt;\n\n&lt;p&gt;Sure the reason might be -- oh BI developers don&amp;#39;t want to learn Python or don&amp;#39;t have best practices. Well the first reason doesn&amp;#39;t hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...&lt;/p&gt;\n\n&lt;p&gt;Sincerely, I&amp;#39;d like to know why. I don&amp;#39;t really see any benefit. I mean whatever your &amp;quot;syntax&amp;quot; can do, Airflow can do that too. I don&amp;#39;t see man, I don&amp;#39;t really see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10svt5y", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/", "subreddit_subscribers": 88450, "created_utc": 1675459675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just set up the company's first automated pipeline using  Python scripts in Google Cloud Functions triggered by daily Cloud Scheduler and store it to BigQuery. It collects data (few hundred rows) from APIs and process them within the script using Pandas . When researching I came across Airflow, but realised that I didnt need it. So I was just wondering how do you use Airflow?", "author_fullname": "t2_2knag8t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you use Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10syqih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675466833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just set up the company&amp;#39;s first automated pipeline using  Python scripts in Google Cloud Functions triggered by daily Cloud Scheduler and store it to BigQuery. It collects data (few hundred rows) from APIs and process them within the script using Pandas . When researching I came across Airflow, but realised that I didnt need it. So I was just wondering how do you use Airflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10syqih", "is_robot_indexable": true, "report_reasons": null, "author": "lordgriefter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10syqih/how_do_you_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10syqih/how_do_you_use_airflow/", "subreddit_subscribers": 88450, "created_utc": 1675466833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any (open-source) alternative to the \"old-school\" JupyterHub? I am aware of e.g. Deepnote but that's not really open source. Also found not exactly alternative but pretty similar thing - Querybook.\n\nIt would be nice to have \"nicer\" JupyterHub ideally with better options to work with Database (some SQL editor/browser) , to share the notebooks with others and also to be able to spawn notebook servers with parametrizable resources.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better JupyterHub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10soo5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675442193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any (open-source) alternative to the &amp;quot;old-school&amp;quot; JupyterHub? I am aware of e.g. Deepnote but that&amp;#39;s not really open source. Also found not exactly alternative but pretty similar thing - Querybook.&lt;/p&gt;\n\n&lt;p&gt;It would be nice to have &amp;quot;nicer&amp;quot; JupyterHub ideally with better options to work with Database (some SQL editor/browser) , to share the notebooks with others and also to be able to spawn notebook servers with parametrizable resources.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10soo5g", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10soo5g/better_jupyterhub/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10soo5g/better_jupyterhub/", "subreddit_subscribers": 88450, "created_utc": 1675442193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started working on a distributed task queue library a few months back. The library is available as a python package to install a start using : [daskqueue - pypi package](https://pypi.org/project/daskqueue/)\n\nFor all its greatness, Dask implements a central scheduler (basically a simple tornado event loop) involved in every decision, which can sometimes create a central bottleneck. **This is a pretty serious limitation when trying to use Dask in high-throughput situations**.\n\nDaskqueue is a small python library built on top of Dask and Dask Distributed that implements a very lightweight **Distributed Task Queue.** Daskqueue also implements persistent queues for holding tasks on disk and surviving Dask cluster restart.\n\nI also wrote an article about implementation details: [https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea](https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea)\n\n&amp;#x200B;\n\nHope you enjoy it, can't wait to hear about your feedback :) !", "author_fullname": "t2_zga5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daskqueue: Dask-based distributed task queue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sqxhh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675447731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started working on a distributed task queue library a few months back. The library is available as a python package to install a start using : &lt;a href=\"https://pypi.org/project/daskqueue/\"&gt;daskqueue - pypi package&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For all its greatness, Dask implements a central scheduler (basically a simple tornado event loop) involved in every decision, which can sometimes create a central bottleneck. &lt;strong&gt;This is a pretty serious limitation when trying to use Dask in high-throughput situations&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Daskqueue is a small python library built on top of Dask and Dask Distributed that implements a very lightweight &lt;strong&gt;Distributed Task Queue.&lt;/strong&gt; Daskqueue also implements persistent queues for holding tasks on disk and surviving Dask cluster restart.&lt;/p&gt;\n\n&lt;p&gt;I also wrote an article about implementation details: &lt;a href=\"https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea\"&gt;https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hope you enjoy it, can&amp;#39;t wait to hear about your feedback :) !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?auto=webp&amp;v=enabled&amp;s=f0cc8dce4c4d114433073f7ec64bf299623fcef9", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b8865fd719f17e774b2178948603d0c4bfb2673", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7f78455787f3622b85aa8394a3ee4b6f14e35c1", "width": 216, "height": 216}], "variants": {}, "id": "IUHM4ctLZQorzkPuYJ4IkGSag8BtaIqZoyqL1L53KuM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10sqxhh", "is_robot_indexable": true, "report_reasons": null, "author": "amindiro", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sqxhh/daskqueue_daskbased_distributed_task_queue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10sqxhh/daskqueue_daskbased_distributed_task_queue/", "subreddit_subscribers": 88450, "created_utc": 1675447731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is **prohibited**. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. \n\nTo mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.", "author_fullname": "t2_eckrjl60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bundle million small json files and save to s3, later easy to process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t5mvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675487162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is &lt;strong&gt;prohibited&lt;/strong&gt;. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. &lt;/p&gt;\n\n&lt;p&gt;To mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t5mvd", "is_robot_indexable": true, "report_reasons": null, "author": "b-y-f", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "subreddit_subscribers": 88450, "created_utc": 1675487162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. \n\nI'm now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn't require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.\n\nI've been at my company for over a year now and mostly due to the outdated stack that I'm limited to in my work environment, I'm looking to transition to a different data engineering position. That way, I'm hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.\n\nThanks for any guidance in advance!", "author_fullname": "t2_x9ryi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Good Free Resources for Learning Data Warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t745z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675492344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn&amp;#39;t require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been at my company for over a year now and mostly due to the outdated stack that I&amp;#39;m limited to in my work environment, I&amp;#39;m looking to transition to a different data engineering position. That way, I&amp;#39;m hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any guidance in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10t745z", "is_robot_indexable": true, "report_reasons": null, "author": "papes_tv", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "subreddit_subscribers": 88450, "created_utc": 1675492344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning more about Trino/Presto and am currently playing around with it on local Docker. Very early in the process but I have connected to my S3 bucket via the Glue catalog, a MySQL database, and Google Sheets.  I have a question about materializing data in terms of functionality as well as best practice architecture. I may be looking at this the wrong way so please correct my assumptions.\n\nIf I have a query that joins tables from all three data sources, I believe I have the capability to create a table in certain catalogs (like Glue/Hive). If executed, is this table written to the storage location? Trying to understand the difference between a Trino table vs view vs materialized view. The comparison I am making is to two architectures I use today: Snowflake and Dremio. Snowflake is like a typical structured database but Dremio processes it's datasets as views in memory with the option to materialize datasets using its Reflections feature. Dremio can also CTAS to locations on object storage as parquet (and iceberg).\n\nHow does Trino relate to this? For my use case, doing everything like a view is satisfactory in terms of performance. But if I am working in an enterprise trying to use Trino for ETL processes, I am assuming that there is functionality to materialize data at some point? Or is that not a best practice/the right use case for Trino? I am also relating it to some of Starburst's (enterprise Trino) marketing, which has ETL data processing as a use case.", "author_fullname": "t2_ywrol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino/Presto materialization and storing query results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10suc08", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675456471.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675456121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning more about Trino/Presto and am currently playing around with it on local Docker. Very early in the process but I have connected to my S3 bucket via the Glue catalog, a MySQL database, and Google Sheets.  I have a question about materializing data in terms of functionality as well as best practice architecture. I may be looking at this the wrong way so please correct my assumptions.&lt;/p&gt;\n\n&lt;p&gt;If I have a query that joins tables from all three data sources, I believe I have the capability to create a table in certain catalogs (like Glue/Hive). If executed, is this table written to the storage location? Trying to understand the difference between a Trino table vs view vs materialized view. The comparison I am making is to two architectures I use today: Snowflake and Dremio. Snowflake is like a typical structured database but Dremio processes it&amp;#39;s datasets as views in memory with the option to materialize datasets using its Reflections feature. Dremio can also CTAS to locations on object storage as parquet (and iceberg).&lt;/p&gt;\n\n&lt;p&gt;How does Trino relate to this? For my use case, doing everything like a view is satisfactory in terms of performance. But if I am working in an enterprise trying to use Trino for ETL processes, I am assuming that there is functionality to materialize data at some point? Or is that not a best practice/the right use case for Trino? I am also relating it to some of Starburst&amp;#39;s (enterprise Trino) marketing, which has ETL data processing as a use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10suc08", "is_robot_indexable": true, "report_reasons": null, "author": "DarkmoonDingo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10suc08/trinopresto_materialization_and_storing_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10suc08/trinopresto_materialization_and_storing_query/", "subreddit_subscribers": 88450, "created_utc": 1675456121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose", "author_fullname": "t2_7gkixkov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you're new to databases should you start with the book Database Design for Mere Mortals or SQL Queries for Mere Mortals or Head first with sql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tag8g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tag8g", "is_robot_indexable": true, "report_reasons": null, "author": "One_Valuable7049", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "subreddit_subscribers": 88450, "created_utc": 1675500898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi experts,\n\nWe are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I'm curious is there recommendation?\n\nFeature needed:\n- Can connect and query cloud database, especially BigQuery;\n\n- Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it's a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;\n\n- Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);\n\n- Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its \"AI\" but full table scan is really expensive;\n\n- Can setup threshold for the metrics, and send alerts to emails and slack channels;\n\nThat's all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don't know if it's good. It's definitely mature though, heard about it for years.\n\nThank you!", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask DE: What nice data quality monitoring tool are you using?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t4n5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675483912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts,&lt;/p&gt;\n\n&lt;p&gt;We are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I&amp;#39;m curious is there recommendation?&lt;/p&gt;\n\n&lt;p&gt;Feature needed:\n- Can connect and query cloud database, especially BigQuery;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it&amp;#39;s a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its &amp;quot;AI&amp;quot; but full table scan is really expensive;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can setup threshold for the metrics, and send alerts to emails and slack channels;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don&amp;#39;t know if it&amp;#39;s good. It&amp;#39;s definitely mature though, heard about it for years.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t4n5x", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "subreddit_subscribers": 88450, "created_utc": 1675483912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am looking to break into data engineering and I am laying out a roadmap of tools to learn and be marketable for an entry level job.  All my learning is through courses on Udemy, after which I will build my own projects!\n\nThat being said, I feel I am going overboard and overlapping / not being consistent with my tech stack choices.  Really looking forward to your input on this learning roadmap (in order of learning first to last) below.\n\nPlease critique my order of learning (edit: they're all 1s after Reddit formatting - assume order of importance is descending from top to bottom of my list) or critique my tool/skill selections!\n\n1. The Basics\n- SQL, Python\n\n2. Visualization\n- PowerBI\n\n3. Data Warehouse\n- Snowflake\n\n4. Cloud Provider\n- Azure\n\n5. Version Control &amp; CI/CD\n- GitHub\n\n6. [Question] \n- Assuming I learn Snowflake and Azure Cloud, would I benefit more from simply learning a tool like Talend, OR should I try to learn Apache Kafka or Spark?\n\n 7. Containerization\n- Docker &amp; Kubernetes\n\n8. Scheduling\n- Airflow\n\n9. Transformation\n- dbt\n\nNOTE: I know next to nothing about any of these - I only know the foundations of data warehousing.\n\nIf anyone is interested in being a short-term mentor (compensated) on data engineering and this stack, please hmu in my chat!\n\nThanks in advance", "author_fullname": "t2_iohcck8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which of these tools compliment each other? + Looking for a mentor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ssko0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675451794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am looking to break into data engineering and I am laying out a roadmap of tools to learn and be marketable for an entry level job.  All my learning is through courses on Udemy, after which I will build my own projects!&lt;/p&gt;\n\n&lt;p&gt;That being said, I feel I am going overboard and overlapping / not being consistent with my tech stack choices.  Really looking forward to your input on this learning roadmap (in order of learning first to last) below.&lt;/p&gt;\n\n&lt;p&gt;Please critique my order of learning (edit: they&amp;#39;re all 1s after Reddit formatting - assume order of importance is descending from top to bottom of my list) or critique my tool/skill selections!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Basics&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;SQL, Python&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Visualization&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;PowerBI&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Warehouse&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Snowflake&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cloud Provider&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Version Control &amp;amp; CI/CD&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GitHub&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;[Question] &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Assuming I learn Snowflake and Azure Cloud, would I benefit more from simply learning a tool like Talend, OR should I try to learn Apache Kafka or Spark?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Containerization&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Docker &amp;amp; Kubernetes&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scheduling&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Airflow&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Transformation&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;dbt&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;NOTE: I know next to nothing about any of these - I only know the foundations of data warehousing.&lt;/p&gt;\n\n&lt;p&gt;If anyone is interested in being a short-term mentor (compensated) on data engineering and this stack, please hmu in my chat!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ssko0", "is_robot_indexable": true, "report_reasons": null, "author": "LieutenantDaredevil", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ssko0/which_of_these_tools_compliment_each_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ssko0/which_of_these_tools_compliment_each_other/", "subreddit_subscribers": 88450, "created_utc": 1675451794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any SQL editor/database browser (something like pgAdmin/MySQL workbench/Adminer) that could be hosted as a \"server\" using e.g. docker container and can connect to Trino? I found only SQLPad which is apparently deprecated.\n\nEdit: not BI tool please", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hostable SQL editor with Trino support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10soq7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675445669.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675442341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any SQL editor/database browser (something like pgAdmin/MySQL workbench/Adminer) that could be hosted as a &amp;quot;server&amp;quot; using e.g. docker container and can connect to Trino? I found only SQLPad which is apparently deprecated.&lt;/p&gt;\n\n&lt;p&gt;Edit: not BI tool please&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10soq7b", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10soq7b/hostable_sql_editor_with_trino_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10soq7b/hostable_sql_editor_with_trino_support/", "subreddit_subscribers": 88450, "created_utc": 1675442341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello ,  \n\n\nWhat is the best way to read data from kafka and write to ES with higher throughput at around 200-300k messages per second. I want to explore real time dashboarding , hence asking", "author_fullname": "t2_sr3rc27q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reading from kafka and pushing to elastic search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10squa4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675447514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello ,  &lt;/p&gt;\n\n&lt;p&gt;What is the best way to read data from kafka and write to ES with higher throughput at around 200-300k messages per second. I want to explore real time dashboarding , hence asking&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10squa4", "is_robot_indexable": true, "report_reasons": null, "author": "honey12123", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10squa4/reading_from_kafka_and_pushing_to_elastic_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10squa4/reading_from_kafka_and_pushing_to_elastic_search/", "subreddit_subscribers": 88450, "created_utc": 1675447514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...\n\nBut I'm curious to know, what did ETL look like before this?\n\nWhat did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?\n\nSometimes I find tutorials like \"Build an ETL pipeline with Airflow and Pandas\" and I think \"Pandas, Really?\".\n\nIn other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?\n\nPlease do share your experience!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What did ETL look like before the \"modern data stack\" was a thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10tjhve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675525724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m curious to know, what did ETL look like before this?&lt;/p&gt;\n\n&lt;p&gt;What did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?&lt;/p&gt;\n\n&lt;p&gt;Sometimes I find tutorials like &amp;quot;Build an ETL pipeline with Airflow and Pandas&amp;quot; and I think &amp;quot;Pandas, Really?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;In other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?&lt;/p&gt;\n\n&lt;p&gt;Please do share your experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tjhve", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/", "subreddit_subscribers": 88450, "created_utc": 1675525724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have stringent performance expectations from such queries? Or is it mostly just being able to access data across sources through a common SQL gateway? \n\nDo you use a separate, smaller cluster for such workloads?\n\n[View Poll](https://www.reddit.com/poll/10t3qdj)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How commonly do you use SQL engines like Trino / Presto, Spark to run queries on data that is NOT in an object store (eg. across data in one or more OLTP DBs, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t3qdj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675481084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have stringent performance expectations from such queries? Or is it mostly just being able to access data across sources through a common SQL gateway? &lt;/p&gt;\n\n&lt;p&gt;Do you use a separate, smaller cluster for such workloads?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10t3qdj\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t3qdj", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1675913084937, "options": [{"text": "&lt;10% of the time", "id": "21414867"}, {"text": "10-20% of the time", "id": "21414868"}, {"text": "20-30% of the time", "id": "21414869"}, {"text": "30-40% of the time", "id": "21414870"}, {"text": "&gt;40% of the time", "id": "21414871"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 39, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t3qdj/how_commonly_do_you_use_sql_engines_like_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10t3qdj/how_commonly_do_you_use_sql_engines_like_trino/", "subreddit_subscribers": 88450, "created_utc": 1675481084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is in the early stages of moving from a monolithic customer platform to rearchitecting around a microservices architecture. I have been asked to write up a whitepaper on master data management/data governance within the new context. Given that a principle of microservices is shared nothing data storage between domains, I'm trying to find out what best practices are for MDM when you have multiple representations of the same data in various microservice back ends. \n\nWhat I've landed on is that you need a message broker that every service that uses a given piece of data, say client_name, subscribes to and if client_name is updated in any of them it propagates to all the other services. In that sense there is no true golden record per se, but you can ascertain the state of any record at any point in time via the event stream. My engineering team would have to ingest this event stream and (because we're still using batch processing into the warehouse) periodically process the events to find the state of a record and that gets loaded into the DB. My thinking is that we'd have to construct a data model that combined microservices, e.g. if five services each own pieces of client data we'd model a client object and then pull the relevant events from each services' stream to construct the state of the client at any given point in time.\n\nThoughts? Has anyone had to do this in the past?", "author_fullname": "t2_wez5v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master Data Management in a Microservices Context", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10swtuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675462104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is in the early stages of moving from a monolithic customer platform to rearchitecting around a microservices architecture. I have been asked to write up a whitepaper on master data management/data governance within the new context. Given that a principle of microservices is shared nothing data storage between domains, I&amp;#39;m trying to find out what best practices are for MDM when you have multiple representations of the same data in various microservice back ends. &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve landed on is that you need a message broker that every service that uses a given piece of data, say client_name, subscribes to and if client_name is updated in any of them it propagates to all the other services. In that sense there is no true golden record per se, but you can ascertain the state of any record at any point in time via the event stream. My engineering team would have to ingest this event stream and (because we&amp;#39;re still using batch processing into the warehouse) periodically process the events to find the state of a record and that gets loaded into the DB. My thinking is that we&amp;#39;d have to construct a data model that combined microservices, e.g. if five services each own pieces of client data we&amp;#39;d model a client object and then pull the relevant events from each services&amp;#39; stream to construct the state of the client at any given point in time.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Has anyone had to do this in the past?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10swtuf", "is_robot_indexable": true, "report_reasons": null, "author": "uchi__mata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10swtuf/master_data_management_in_a_microservices_context/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10swtuf/master_data_management_in_a_microservices_context/", "subreddit_subscribers": 88450, "created_utc": 1675462104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy use case is I will have oracle dump file in S3 and I will need to select some tables from the dump file and store it back to s3 and then transform and back to s3 then read using Athena\n\nI will get the dump file weekly and each time it will have full data and not incremental load.\n\nI am planning to use Oracle RDS instance to load data selectively - only need few tables and not all of the tables, then store data back to s3 as csv/parquet for furter processing using Athena.\n\nSince, I do not need the RDS instance for any other activity until the next load and also because every load is a full load, I also plan to delete this instance / drop data and stop the instance (basically not be billed for it).\n\nSince I am only starting out with AWS and Data engineering as a whole, I know there are knowledge gaps here that need to be filled. Also I would appreciate if you could guide with an overall approach for the same.", "author_fullname": "t2_9rp533dt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pull data from Oracle dump file to S3 bucket", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ssdt8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675451325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My use case is I will have oracle dump file in S3 and I will need to select some tables from the dump file and store it back to s3 and then transform and back to s3 then read using Athena&lt;/p&gt;\n\n&lt;p&gt;I will get the dump file weekly and each time it will have full data and not incremental load.&lt;/p&gt;\n\n&lt;p&gt;I am planning to use Oracle RDS instance to load data selectively - only need few tables and not all of the tables, then store data back to s3 as csv/parquet for furter processing using Athena.&lt;/p&gt;\n\n&lt;p&gt;Since, I do not need the RDS instance for any other activity until the next load and also because every load is a full load, I also plan to delete this instance / drop data and stop the instance (basically not be billed for it).&lt;/p&gt;\n\n&lt;p&gt;Since I am only starting out with AWS and Data engineering as a whole, I know there are knowledge gaps here that need to be filled. Also I would appreciate if you could guide with an overall approach for the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ssdt8", "is_robot_indexable": true, "report_reasons": null, "author": "prasanna_aatma", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ssdt8/pull_data_from_oracle_dump_file_to_s3_bucket/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ssdt8/pull_data_from_oracle_dump_file_to_s3_bucket/", "subreddit_subscribers": 88450, "created_utc": 1675451325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone give me good resources to learn data engineering with Python ? I am already fluent with python and also a backend  engineer so what could be the best way to learn data engineering. I just started reading the book called 'Data Engineering with Python' and It only covers the basic stuff.", "author_fullname": "t2_nwphe35r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course recommendations and Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10thtnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675521382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone give me good resources to learn data engineering with Python ? I am already fluent with python and also a backend  engineer so what could be the best way to learn data engineering. I just started reading the book called &amp;#39;Data Engineering with Python&amp;#39; and It only covers the basic stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10thtnv", "is_robot_indexable": true, "report_reasons": null, "author": "golesu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10thtnv/course_recommendations_and_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10thtnv/course_recommendations_and_resources/", "subreddit_subscribers": 88450, "created_utc": 1675521382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Reading the  [Why do people re-invent wrappers for Airflow? : dataengineering (reddit.com)](https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/)  post I was reflecting about my DE experience and I have to admit I've introduced myself extra layers or patterns that, with all due honesty, were more a \"Let's just try Bridge pattern for the sake of it\" than a well designed and necessary solution.\n\nUnnecessary classes on top of PySpark, a complicated hierarchy of abstract Pipeline classes or YAML-based configurations parsed by another self-made solution.", "author_fullname": "t2_5a2nw4lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confiteor (or how to overcomplicate data engineering for fun)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tg72q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675516640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading the  &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/\"&gt;Why do people re-invent wrappers for Airflow? : dataengineering (reddit.com)&lt;/a&gt;  post I was reflecting about my DE experience and I have to admit I&amp;#39;ve introduced myself extra layers or patterns that, with all due honesty, were more a &amp;quot;Let&amp;#39;s just try Bridge pattern for the sake of it&amp;quot; than a well designed and necessary solution.&lt;/p&gt;\n\n&lt;p&gt;Unnecessary classes on top of PySpark, a complicated hierarchy of abstract Pipeline classes or YAML-based configurations parsed by another self-made solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tg72q", "is_robot_indexable": true, "report_reasons": null, "author": "_raskol_nikov_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tg72q/confiteor_or_how_to_overcomplicate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tg72q/confiteor_or_how_to_overcomplicate_data/", "subreddit_subscribers": 88450, "created_utc": 1675516640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data model which was created in Power BI some years back where they kept on adding new things into it.\nIt has become a mess now, everything loads super slow, refresh gets stuck and what not.\n\nNow.. have a requirement to migrate it into our MySQL DW.\nWhat would be the ideal plan for this scenario?\n\nWhat wIve done is.. create some documentation, metadata about the tables, relations between them and started working on creating the model, initially the important fact tables then the dimension ones and working on expanding it from there on.\n\nIs there anything that I'm missing here?", "author_fullname": "t2_mutfi9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power BI data model to MySQL.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10teucn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675512115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data model which was created in Power BI some years back where they kept on adding new things into it.\nIt has become a mess now, everything loads super slow, refresh gets stuck and what not.&lt;/p&gt;\n\n&lt;p&gt;Now.. have a requirement to migrate it into our MySQL DW.\nWhat would be the ideal plan for this scenario?&lt;/p&gt;\n\n&lt;p&gt;What wIve done is.. create some documentation, metadata about the tables, relations between them and started working on creating the model, initially the important fact tables then the dimension ones and working on expanding it from there on.&lt;/p&gt;\n\n&lt;p&gt;Is there anything that I&amp;#39;m missing here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10teucn", "is_robot_indexable": true, "report_reasons": null, "author": "prokid1911", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10teucn/power_bi_data_model_to_mysql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10teucn/power_bi_data_model_to_mysql/", "subreddit_subscribers": 88450, "created_utc": 1675512115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. \n\nIs there any alternative if we use Hive metastore as catalog instead of Athena/Glue?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data wrangler with Hive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tafdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. &lt;/p&gt;\n\n&lt;p&gt;Is there any alternative if we use Hive metastore as catalog instead of Athena/Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tafdg", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "subreddit_subscribers": 88450, "created_utc": 1675500852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am a complete noob on ETL and could really use some help.\n\nI am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.\n\nHow do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?\n\nVery greatful for any help, thanks!", "author_fullname": "t2_qiw67sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with cluster startup time in azure databricks when having event driven data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t7xs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675495387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am a complete noob on ETL and could really use some help.&lt;/p&gt;\n\n&lt;p&gt;I am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;How do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?&lt;/p&gt;\n\n&lt;p&gt;Very greatful for any help, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10t7xs0", "is_robot_indexable": true, "report_reasons": null, "author": "aLyapunov", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "subreddit_subscribers": 88450, "created_utc": 1675495387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a BI Engineer for the past 2 years, spanning across two different companies. However my experience at the 2nd company has been vastly different, so lately its had me wondering which company better embodies the work of a BI engineer.   \n\n\nHere's a breakdown of various activities:\n\n**Company 1**  \n\\- Bring in data from brand new data sources and design supporting tables. A lot of emphasis was put on this, as we had to make sure our tables were performant and would work flawlessly downstream in Tableau.\n\n\\- Build KPIs   \n\\- Design dashboards\n\n\\- Meet with functional business areas to come up with new reporting requirements\n\n&amp;#x200B;\n\n**Company 2**\n\n\\- Design dashboards \n\n\\- report data issues up to the data engineers\n\n\\- design KPIs into existing tables that the DE's build\n\n\\- Meet with functional business areas to come up with new reporting requirements  \n\n\n  \nBased on your experience, would either of these fit the mold of a traditional BI Engineer?", "author_fullname": "t2_30pyiyf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer vs. BI Engineer roles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t13dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675473265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a BI Engineer for the past 2 years, spanning across two different companies. However my experience at the 2nd company has been vastly different, so lately its had me wondering which company better embodies the work of a BI engineer.   &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a breakdown of various activities:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company 1&lt;/strong&gt;&lt;br/&gt;\n- Bring in data from brand new data sources and design supporting tables. A lot of emphasis was put on this, as we had to make sure our tables were performant and would work flawlessly downstream in Tableau.&lt;/p&gt;\n\n&lt;p&gt;- Build KPIs&lt;br/&gt;\n- Design dashboards&lt;/p&gt;\n\n&lt;p&gt;- Meet with functional business areas to come up with new reporting requirements&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company 2&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Design dashboards &lt;/p&gt;\n\n&lt;p&gt;- report data issues up to the data engineers&lt;/p&gt;\n\n&lt;p&gt;- design KPIs into existing tables that the DE&amp;#39;s build&lt;/p&gt;\n\n&lt;p&gt;- Meet with functional business areas to come up with new reporting requirements  &lt;/p&gt;\n\n&lt;p&gt;Based on your experience, would either of these fit the mold of a traditional BI Engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t13dz", "is_robot_indexable": true, "report_reasons": null, "author": "Schley_them_all", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t13dz/data_engineer_vs_bi_engineer_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t13dz/data_engineer_vs_bi_engineer_roles/", "subreddit_subscribers": 88450, "created_utc": 1675473265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team develops reporting tools for all the projects in our company using c#.Net, python (workers), and SQL. I develop a lot of SQL stored procedures, maintain stored proc packages, inner merge queries for ETL processes and data tables, and views, etc. These stored procedures go through a lot of editing with new CRs in application developments.\n\nI have some doubts if anyone can share wisdom or knowledge in these points:\n\n1) How do I maintain or track the changes if many developers alter the SQL statements in the stored procedure and packages inside the oracle sql developer? Are there any good practices like comments to track dev1 made these changes with dates or any other method? Suggest any guidance\n2) Is sub versioning possible in SQL or backup of sql codes? like other programming languages. (Since we write codes in live server) If yes then how? Suggest any method\n3) How do I know which package and procedure are fired in the backend from the UI click? Since a lot of procedures/packages are used and i have not track\n4) Some base queries are rewritten many times and differently by other developers. So any strict policy to maintain for consistency? Recommendations on that\n5) How to document the backend database process logic. Which table does that, which stored proc does that etc\n6) Suggest me a book for a beginner entering the data engineering field \n\nThanks in advance \ud83d\ude07", "author_fullname": "t2_5svl00at", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Advice in Oracle SQL Developer. Tips &amp; Tricks!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10swyl1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675462416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team develops reporting tools for all the projects in our company using c#.Net, python (workers), and SQL. I develop a lot of SQL stored procedures, maintain stored proc packages, inner merge queries for ETL processes and data tables, and views, etc. These stored procedures go through a lot of editing with new CRs in application developments.&lt;/p&gt;\n\n&lt;p&gt;I have some doubts if anyone can share wisdom or knowledge in these points:&lt;/p&gt;\n\n&lt;p&gt;1) How do I maintain or track the changes if many developers alter the SQL statements in the stored procedure and packages inside the oracle sql developer? Are there any good practices like comments to track dev1 made these changes with dates or any other method? Suggest any guidance\n2) Is sub versioning possible in SQL or backup of sql codes? like other programming languages. (Since we write codes in live server) If yes then how? Suggest any method\n3) How do I know which package and procedure are fired in the backend from the UI click? Since a lot of procedures/packages are used and i have not track\n4) Some base queries are rewritten many times and differently by other developers. So any strict policy to maintain for consistency? Recommendations on that\n5) How to document the backend database process logic. Which table does that, which stored proc does that etc\n6) Suggest me a book for a beginner entering the data engineering field &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance \ud83d\ude07&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10swyl1", "is_robot_indexable": true, "report_reasons": null, "author": "Few_War_6750", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10swyl1/need_advice_in_oracle_sql_developer_tips_tricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10swyl1/need_advice_in_oracle_sql_developer_tips_tricks/", "subreddit_subscribers": 88450, "created_utc": 1675462416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my first post at reddit. Hello everyone!\n\nI am a senior software engineer and have 12+ years of experience. I came to US before 3 years and working as a contractor. Initially I started as some development work in Data team at client side and eventually I got moved to data engineering work. \n\nIn this project I am using matillion ETL tool which is more like UI drag and drop. Initially I liked that work as I love data and have more WLB but now I miss my old programming days. I want to switch job now but since last 3 years I worked in data team it is hard to crack interview for software engineer role and no one will take me for senior data engineer. I see there are many tools in data engineering which is more on programming side. I am happy to learn that as well but I am directionless.\n\nI work for a contracting company right now so I am not earning much like product company. For better financial growth and career perspective I am not sure if I should go back to software engineering or I should learn more in data engineering and proceed in data only. One friend told me that salary for data engineer will be very low than software engineer.  Data science engineer will have more programming and better salary but they only hire phd person for that.\n\nLooks like I am stuck at this situation and need some guidance from fellow engineers.\n\nKindly share your opinion in this matter. Your any feedback will be greatly helpful.", "author_fullname": "t2_99nnwpys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advise - SSE or Data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10svsyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675459660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my first post at reddit. Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I am a senior software engineer and have 12+ years of experience. I came to US before 3 years and working as a contractor. Initially I started as some development work in Data team at client side and eventually I got moved to data engineering work. &lt;/p&gt;\n\n&lt;p&gt;In this project I am using matillion ETL tool which is more like UI drag and drop. Initially I liked that work as I love data and have more WLB but now I miss my old programming days. I want to switch job now but since last 3 years I worked in data team it is hard to crack interview for software engineer role and no one will take me for senior data engineer. I see there are many tools in data engineering which is more on programming side. I am happy to learn that as well but I am directionless.&lt;/p&gt;\n\n&lt;p&gt;I work for a contracting company right now so I am not earning much like product company. For better financial growth and career perspective I am not sure if I should go back to software engineering or I should learn more in data engineering and proceed in data only. One friend told me that salary for data engineer will be very low than software engineer.  Data science engineer will have more programming and better salary but they only hire phd person for that.&lt;/p&gt;\n\n&lt;p&gt;Looks like I am stuck at this situation and need some guidance from fellow engineers.&lt;/p&gt;\n\n&lt;p&gt;Kindly share your opinion in this matter. Your any feedback will be greatly helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10svsyd", "is_robot_indexable": true, "report_reasons": null, "author": "seattleshawk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10svsyd/need_advise_sse_or_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10svsyd/need_advise_sse_or_data_engineering/", "subreddit_subscribers": 88450, "created_utc": 1675459660.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}