{"kind": "Listing", "data": {"after": "t3_10t7xs0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hg3enfgk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do all my BI initiatives end up like this? \ud83d\ude29", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10sxj6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/3miuaya2e3ga1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/3miuaya2e3ga1/DASH_96.mp4", "dash_url": "https://v.redd.it/3miuaya2e3ga1/DASHPlaylist.mpd?a=1678107256%2CNzM4YWEzMjJhOTE4ZDVkYWNjZmZmODk5MGU2MmVkNTNlMDYwZmVhMjdkMjAyYjI2OTkxYWNmNjkzOTBkNWJlZQ%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/3miuaya2e3ga1/HLSPlaylist.m3u8?a=1678107256%2CMzdmMGMxNjI0ZjQzYmIyM2QzN2FiMWQwMWU5ZDQzZTdiYzliY2IxOGEzMThiMDYzZTYxOTcyMzBkZTY0OTkxYQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/X1v7U7_zEvDx-aFcjo5va5mgNFbsSKtx-4ClmvAgrwM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675463793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/3miuaya2e3ga1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3878d1f9f9e60902172ce5ac1af0fdc7d8e3fe1d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d76a62f25a6d867b419e4d789a62a597c61fc596", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ce99fd83df40cab3bc295fd80ca47320ce881332", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3cb7de2b96e12b511f22737cc4bb1fec7c4ddb6d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=49e8406356277c63fb35f0160fcaa2a2060244ff", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7515bf341310dfe023e72bfcd43c892ff7481984", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=14868f6bb0b064f393bb66bc67971a6b876766e4", "width": 1080, "height": 607}], "variants": {}, "id": "WklSmgWvyM2nescx70iL76I-U0tvpKJi8lxo2zVV6Jw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10sxj6r", "is_robot_indexable": true, "report_reasons": null, "author": "Salmon-Advantage", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sxj6r/why_do_all_my_bi_initiatives_end_up_like_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/3miuaya2e3ga1", "subreddit_subscribers": 88424, "created_utc": 1675463793.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/3miuaya2e3ga1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/3miuaya2e3ga1/DASH_96.mp4", "dash_url": "https://v.redd.it/3miuaya2e3ga1/DASHPlaylist.mpd?a=1678107256%2CNzM4YWEzMjJhOTE4ZDVkYWNjZmZmODk5MGU2MmVkNTNlMDYwZmVhMjdkMjAyYjI2OTkxYWNmNjkzOTBkNWJlZQ%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/3miuaya2e3ga1/HLSPlaylist.m3u8?a=1678107256%2CMzdmMGMxNjI0ZjQzYmIyM2QzN2FiMWQwMWU5ZDQzZTdiYzliY2IxOGEzMThiMDYzZTYxOTcyMzBkZTY0OTkxYQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based \"solution\" to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the \"solution\" has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can't they just let people write Python?\n\nSure the reason might be -- oh BI developers don't want to learn Python or don't have best practices. Well the first reason doesn't hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?\n\nI'm not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...\n\nSincerely, I'd like to know why. I don't really see any benefit. I mean whatever your \"syntax\" can do, Airflow can do that too. I don't see man, I don't really see.", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do people re-invent wrappers for Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10svt5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675459675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based &amp;quot;solution&amp;quot; to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the &amp;quot;solution&amp;quot; has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can&amp;#39;t they just let people write Python?&lt;/p&gt;\n\n&lt;p&gt;Sure the reason might be -- oh BI developers don&amp;#39;t want to learn Python or don&amp;#39;t have best practices. Well the first reason doesn&amp;#39;t hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...&lt;/p&gt;\n\n&lt;p&gt;Sincerely, I&amp;#39;d like to know why. I don&amp;#39;t really see any benefit. I mean whatever your &amp;quot;syntax&amp;quot; can do, Airflow can do that too. I don&amp;#39;t see man, I don&amp;#39;t really see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10svt5y", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/", "subreddit_subscribers": 88424, "created_utc": 1675459675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone work for a hedge fund/Quant firm or has interviewed for DE positions?\n\nI've had a look at some job specs and I haven't seen anything out of the ordinary: Python, SQL, some cloud, Kafka, Spark, Kubernetes, Docker. Some of them mention C++/Rust/Java as additional languages - fair enough.\n\nDo these roles usually require financial markets knowledge? Do they care much about specific degrees and target schools for experienced hires (5 yoe so it's too late anyway).\n\nWhat does the interview process look like? Is it a lot of DSA and Leetcode or focused on SQL and systems design?\n\nIs there anything in particular I could do to make my CV stand out? Any additional technologies I should pick up that you wouldn't use in other DE roles?", "author_fullname": "t2_vsf3ige5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer at hedge funds/Quant firms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sivgs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675426368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone work for a hedge fund/Quant firm or has interviewed for DE positions?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had a look at some job specs and I haven&amp;#39;t seen anything out of the ordinary: Python, SQL, some cloud, Kafka, Spark, Kubernetes, Docker. Some of them mention C++/Rust/Java as additional languages - fair enough.&lt;/p&gt;\n\n&lt;p&gt;Do these roles usually require financial markets knowledge? Do they care much about specific degrees and target schools for experienced hires (5 yoe so it&amp;#39;s too late anyway).&lt;/p&gt;\n\n&lt;p&gt;What does the interview process look like? Is it a lot of DSA and Leetcode or focused on SQL and systems design?&lt;/p&gt;\n\n&lt;p&gt;Is there anything in particular I could do to make my CV stand out? Any additional technologies I should pick up that you wouldn&amp;#39;t use in other DE roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10sivgs", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated_Ad_1108", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sivgs/data_engineer_at_hedge_fundsquant_firms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10sivgs/data_engineer_at_hedge_fundsquant_firms/", "subreddit_subscribers": 88424, "created_utc": 1675426368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started working on a distributed task queue library a few months back. The library is available as a python package to install a start using : [daskqueue - pypi package](https://pypi.org/project/daskqueue/)\n\nFor all its greatness, Dask implements a central scheduler (basically a simple tornado event loop) involved in every decision, which can sometimes create a central bottleneck. **This is a pretty serious limitation when trying to use Dask in high-throughput situations**.\n\nDaskqueue is a small python library built on top of Dask and Dask Distributed that implements a very lightweight **Distributed Task Queue.** Daskqueue also implements persistent queues for holding tasks on disk and surviving Dask cluster restart.\n\nI also wrote an article about implementation details: [https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea](https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea)\n\n&amp;#x200B;\n\nHope you enjoy it, can't wait to hear about your feedback :) !", "author_fullname": "t2_zga5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daskqueue: Dask-based distributed task queue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sqxhh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675447731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started working on a distributed task queue library a few months back. The library is available as a python package to install a start using : &lt;a href=\"https://pypi.org/project/daskqueue/\"&gt;daskqueue - pypi package&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For all its greatness, Dask implements a central scheduler (basically a simple tornado event loop) involved in every decision, which can sometimes create a central bottleneck. &lt;strong&gt;This is a pretty serious limitation when trying to use Dask in high-throughput situations&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Daskqueue is a small python library built on top of Dask and Dask Distributed that implements a very lightweight &lt;strong&gt;Distributed Task Queue.&lt;/strong&gt; Daskqueue also implements persistent queues for holding tasks on disk and surviving Dask cluster restart.&lt;/p&gt;\n\n&lt;p&gt;I also wrote an article about implementation details: &lt;a href=\"https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea\"&gt;https://medium.com/@aminedirhoussi1/daskqueue-dask-based-distributed-task-queue-6fb95517dfea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hope you enjoy it, can&amp;#39;t wait to hear about your feedback :) !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?auto=webp&amp;v=enabled&amp;s=f0cc8dce4c4d114433073f7ec64bf299623fcef9", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b8865fd719f17e774b2178948603d0c4bfb2673", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7f78455787f3622b85aa8394a3ee4b6f14e35c1", "width": 216, "height": 216}], "variants": {}, "id": "IUHM4ctLZQorzkPuYJ4IkGSag8BtaIqZoyqL1L53KuM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "10sqxhh", "is_robot_indexable": true, "report_reasons": null, "author": "amindiro", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sqxhh/daskqueue_daskbased_distributed_task_queue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10sqxhh/daskqueue_daskbased_distributed_task_queue/", "subreddit_subscribers": 88424, "created_utc": 1675447731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any (open-source) alternative to the \"old-school\" JupyterHub? I am aware of e.g. Deepnote but that's not really open source. Also found not exactly alternative but pretty similar thing - Querybook.\n\nIt would be nice to have \"nicer\" JupyterHub ideally with better options to work with Database (some SQL editor/browser) , to share the notebooks with others and also to be able to spawn notebook servers with parametrizable resources.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better JupyterHub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10soo5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675442193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any (open-source) alternative to the &amp;quot;old-school&amp;quot; JupyterHub? I am aware of e.g. Deepnote but that&amp;#39;s not really open source. Also found not exactly alternative but pretty similar thing - Querybook.&lt;/p&gt;\n\n&lt;p&gt;It would be nice to have &amp;quot;nicer&amp;quot; JupyterHub ideally with better options to work with Database (some SQL editor/browser) , to share the notebooks with others and also to be able to spawn notebook servers with parametrizable resources.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10soo5g", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10soo5g/better_jupyterhub/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10soo5g/better_jupyterhub/", "subreddit_subscribers": 88424, "created_utc": 1675442193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just set up the company's first automated pipeline using  Python scripts in Google Cloud Functions triggered by daily Cloud Scheduler and store it to BigQuery. It collects data (few hundred rows) from APIs and process them within the script using Pandas . When researching I came across Airflow, but realised that I didnt need it. So I was just wondering how do you use Airflow?", "author_fullname": "t2_2knag8t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you use Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10syqih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675466833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just set up the company&amp;#39;s first automated pipeline using  Python scripts in Google Cloud Functions triggered by daily Cloud Scheduler and store it to BigQuery. It collects data (few hundred rows) from APIs and process them within the script using Pandas . When researching I came across Airflow, but realised that I didnt need it. So I was just wondering how do you use Airflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10syqih", "is_robot_indexable": true, "report_reasons": null, "author": "lordgriefter", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10syqih/how_do_you_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10syqih/how_do_you_use_airflow/", "subreddit_subscribers": 88424, "created_utc": 1675466833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nWe are looking for a solution or tool that allows a data exploration in a real time scenario. \n\nWe have some streaming processes (Kafka ecosystem) that generate final data. This data must be consumed by BI staff in real time. Monitoring tasks but with some freedom to choose what data should be presented.\n\nFor now, we are surviving with NiFi + postgresql + PowerBI.\n\n&amp;#x200B;\n\nBut what new or better options are there for this? \n\nI throw some options: Superset? Elastic+Kibana? others?\n\nThanks in advance.", "author_fullname": "t2_83c4agcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming data exploration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10skpl2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675432067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;We are looking for a solution or tool that allows a data exploration in a real time scenario. &lt;/p&gt;\n\n&lt;p&gt;We have some streaming processes (Kafka ecosystem) that generate final data. This data must be consumed by BI staff in real time. Monitoring tasks but with some freedom to choose what data should be presented.&lt;/p&gt;\n\n&lt;p&gt;For now, we are surviving with NiFi + postgresql + PowerBI.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But what new or better options are there for this? &lt;/p&gt;\n\n&lt;p&gt;I throw some options: Superset? Elastic+Kibana? others?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10skpl2", "is_robot_indexable": true, "report_reasons": null, "author": "jscrls", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10skpl2/streaming_data_exploration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10skpl2/streaming_data_exploration/", "subreddit_subscribers": 88424, "created_utc": 1675432067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is **prohibited**. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. \n\nTo mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.", "author_fullname": "t2_eckrjl60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bundle million small json files and save to s3, later easy to process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t5mvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675487162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is &lt;strong&gt;prohibited&lt;/strong&gt;. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. &lt;/p&gt;\n\n&lt;p&gt;To mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t5mvd", "is_robot_indexable": true, "report_reasons": null, "author": "b-y-f", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "subreddit_subscribers": 88424, "created_utc": 1675487162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m going to do masters in US. I have 4 years experience with 3 years as a data engineer. \n\n\nCould you guys share what type of questions are asked in interviews in US companies for DE roles?\nAlso what are the skills which are mainly helping you in your current job?\nYour experiences and any tips for me to clear some crucial topics. I wanna do some kickass work there, so want to be prepared my best.\n\nHave a great day guys!", "author_fullname": "t2_ajjg3i2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Insights about US tech market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sn3jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675438307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m going to do masters in US. I have 4 years experience with 3 years as a data engineer. &lt;/p&gt;\n\n&lt;p&gt;Could you guys share what type of questions are asked in interviews in US companies for DE roles?\nAlso what are the skills which are mainly helping you in your current job?\nYour experiences and any tips for me to clear some crucial topics. I wanna do some kickass work there, so want to be prepared my best.&lt;/p&gt;\n\n&lt;p&gt;Have a great day guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "10sn3jc", "is_robot_indexable": true, "report_reasons": null, "author": "MediumZealousideal29", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sn3jc/insights_about_us_tech_market/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10sn3jc/insights_about_us_tech_market/", "subreddit_subscribers": 88424, "created_utc": 1675438307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning more about Trino/Presto and am currently playing around with it on local Docker. Very early in the process but I have connected to my S3 bucket via the Glue catalog, a MySQL database, and Google Sheets.  I have a question about materializing data in terms of functionality as well as best practice architecture. I may be looking at this the wrong way so please correct my assumptions.\n\nIf I have a query that joins tables from all three data sources, I believe I have the capability to create a table in certain catalogs (like Glue/Hive). If executed, is this table written to the storage location? Trying to understand the difference between a Trino table vs view vs materialized view. The comparison I am making is to two architectures I use today: Snowflake and Dremio. Snowflake is like a typical structured database but Dremio processes it's datasets as views in memory with the option to materialize datasets using its Reflections feature. Dremio can also CTAS to locations on object storage as parquet (and iceberg).\n\nHow does Trino relate to this? For my use case, doing everything like a view is satisfactory in terms of performance. But if I am working in an enterprise trying to use Trino for ETL processes, I am assuming that there is functionality to materialize data at some point? Or is that not a best practice/the right use case for Trino? I am also relating it to some of Starburst's (enterprise Trino) marketing, which has ETL data processing as a use case.", "author_fullname": "t2_ywrol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino/Presto materialization and storing query results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10suc08", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675456471.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675456121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning more about Trino/Presto and am currently playing around with it on local Docker. Very early in the process but I have connected to my S3 bucket via the Glue catalog, a MySQL database, and Google Sheets.  I have a question about materializing data in terms of functionality as well as best practice architecture. I may be looking at this the wrong way so please correct my assumptions.&lt;/p&gt;\n\n&lt;p&gt;If I have a query that joins tables from all three data sources, I believe I have the capability to create a table in certain catalogs (like Glue/Hive). If executed, is this table written to the storage location? Trying to understand the difference between a Trino table vs view vs materialized view. The comparison I am making is to two architectures I use today: Snowflake and Dremio. Snowflake is like a typical structured database but Dremio processes it&amp;#39;s datasets as views in memory with the option to materialize datasets using its Reflections feature. Dremio can also CTAS to locations on object storage as parquet (and iceberg).&lt;/p&gt;\n\n&lt;p&gt;How does Trino relate to this? For my use case, doing everything like a view is satisfactory in terms of performance. But if I am working in an enterprise trying to use Trino for ETL processes, I am assuming that there is functionality to materialize data at some point? Or is that not a best practice/the right use case for Trino? I am also relating it to some of Starburst&amp;#39;s (enterprise Trino) marketing, which has ETL data processing as a use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10suc08", "is_robot_indexable": true, "report_reasons": null, "author": "DarkmoonDingo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10suc08/trinopresto_materialization_and_storing_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10suc08/trinopresto_materialization_and_storing_query/", "subreddit_subscribers": 88424, "created_utc": 1675456121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\nWorking in a company that deploys data science pipelines for several customers I often find myself working on quite heterogenous stacks to do what in the end Is a conceptually simple thing. This most often happens for smaller projects having simple tasks.\n\nAre you used to visual data processing/pipelining tools? I am assuming the UI may facilitate the High level understanding of the projects, while a managed infra tends to do the job well in simple scenario. I am also scared of the learning curve, rigidity and lock ins of such solutions, so reality want to here your thoughts. I have heard of AWS data wrangler, KNIME and data haiku so far.\n\nA sample scenario: get a batch of data from a database, another from from a datalake, join them, fitpredict, save predictions and save to a third db. The typical volume of data is small (megabytes) and the scheduling frequencies quite low.\n\n\n\nP.s. Hope It Is the right community - the topic Is a bit crossdisciplinary", "author_fullname": "t2_5w9jnbsn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visual data pipelining tools - KNIME datahaiku and competitors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10slgyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675434068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,\nWorking in a company that deploys data science pipelines for several customers I often find myself working on quite heterogenous stacks to do what in the end Is a conceptually simple thing. This most often happens for smaller projects having simple tasks.&lt;/p&gt;\n\n&lt;p&gt;Are you used to visual data processing/pipelining tools? I am assuming the UI may facilitate the High level understanding of the projects, while a managed infra tends to do the job well in simple scenario. I am also scared of the learning curve, rigidity and lock ins of such solutions, so reality want to here your thoughts. I have heard of AWS data wrangler, KNIME and data haiku so far.&lt;/p&gt;\n\n&lt;p&gt;A sample scenario: get a batch of data from a database, another from from a datalake, join them, fitpredict, save predictions and save to a third db. The typical volume of data is small (megabytes) and the scheduling frequencies quite low.&lt;/p&gt;\n\n&lt;p&gt;P.s. Hope It Is the right community - the topic Is a bit crossdisciplinary&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10slgyi", "is_robot_indexable": true, "report_reasons": null, "author": "BenXavier", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10slgyi/visual_data_pipelining_tools_knime_datahaiku_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10slgyi/visual_data_pipelining_tools_knime_datahaiku_and/", "subreddit_subscribers": 88424, "created_utc": 1675434068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. \n\nI'm now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn't require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.\n\nI've been at my company for over a year now and mostly due to the outdated stack that I'm limited to in my work environment, I'm looking to transition to a different data engineering position. That way, I'm hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.\n\nThanks for any guidance in advance!", "author_fullname": "t2_x9ryi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Good Free Resources for Learning Data Warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t745z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675492344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn&amp;#39;t require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been at my company for over a year now and mostly due to the outdated stack that I&amp;#39;m limited to in my work environment, I&amp;#39;m looking to transition to a different data engineering position. That way, I&amp;#39;m hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any guidance in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10t745z", "is_robot_indexable": true, "report_reasons": null, "author": "papes_tv", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "subreddit_subscribers": 88424, "created_utc": 1675492344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi experts,\n\nWe are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I'm curious is there recommendation?\n\nFeature needed:\n- Can connect and query cloud database, especially BigQuery;\n\n- Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it's a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;\n\n- Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);\n\n- Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its \"AI\" but full table scan is really expensive;\n\n- Can setup threshold for the metrics, and send alerts to emails and slack channels;\n\nThat's all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don't know if it's good. It's definitely mature though, heard about it for years.\n\nThank you!", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask DE: What nice data quality monitoring tool are you using?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t4n5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675483912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts,&lt;/p&gt;\n\n&lt;p&gt;We are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I&amp;#39;m curious is there recommendation?&lt;/p&gt;\n\n&lt;p&gt;Feature needed:\n- Can connect and query cloud database, especially BigQuery;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it&amp;#39;s a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its &amp;quot;AI&amp;quot; but full table scan is really expensive;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can setup threshold for the metrics, and send alerts to emails and slack channels;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don&amp;#39;t know if it&amp;#39;s good. It&amp;#39;s definitely mature though, heard about it for years.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t4n5x", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "subreddit_subscribers": 88424, "created_utc": 1675483912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am looking to break into data engineering and I am laying out a roadmap of tools to learn and be marketable for an entry level job.  All my learning is through courses on Udemy, after which I will build my own projects!\n\nThat being said, I feel I am going overboard and overlapping / not being consistent with my tech stack choices.  Really looking forward to your input on this learning roadmap (in order of learning first to last) below.\n\nPlease critique my order of learning (edit: they're all 1s after Reddit formatting - assume order of importance is descending from top to bottom of my list) or critique my tool/skill selections!\n\n1. The Basics\n- SQL, Python\n\n2. Visualization\n- PowerBI\n\n3. Data Warehouse\n- Snowflake\n\n4. Cloud Provider\n- Azure\n\n5. Version Control &amp; CI/CD\n- GitHub\n\n6. [Question] \n- Assuming I learn Snowflake and Azure Cloud, would I benefit more from simply learning a tool like Talend, OR should I try to learn Apache Kafka or Spark?\n\n 7. Containerization\n- Docker &amp; Kubernetes\n\n8. Scheduling\n- Airflow\n\n9. Transformation\n- dbt\n\nNOTE: I know next to nothing about any of these - I only know the foundations of data warehousing.\n\nIf anyone is interested in being a short-term mentor (compensated) on data engineering and this stack, please hmu in my chat!\n\nThanks in advance", "author_fullname": "t2_iohcck8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which of these tools compliment each other? + Looking for a mentor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ssko0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675451794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am looking to break into data engineering and I am laying out a roadmap of tools to learn and be marketable for an entry level job.  All my learning is through courses on Udemy, after which I will build my own projects!&lt;/p&gt;\n\n&lt;p&gt;That being said, I feel I am going overboard and overlapping / not being consistent with my tech stack choices.  Really looking forward to your input on this learning roadmap (in order of learning first to last) below.&lt;/p&gt;\n\n&lt;p&gt;Please critique my order of learning (edit: they&amp;#39;re all 1s after Reddit formatting - assume order of importance is descending from top to bottom of my list) or critique my tool/skill selections!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Basics&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;SQL, Python&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Visualization&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;PowerBI&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Warehouse&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Snowflake&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cloud Provider&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Azure&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Version Control &amp;amp; CI/CD&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GitHub&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;[Question] &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Assuming I learn Snowflake and Azure Cloud, would I benefit more from simply learning a tool like Talend, OR should I try to learn Apache Kafka or Spark?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Containerization&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Docker &amp;amp; Kubernetes&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scheduling&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Airflow&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Transformation&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;dbt&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;NOTE: I know next to nothing about any of these - I only know the foundations of data warehousing.&lt;/p&gt;\n\n&lt;p&gt;If anyone is interested in being a short-term mentor (compensated) on data engineering and this stack, please hmu in my chat!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10ssko0", "is_robot_indexable": true, "report_reasons": null, "author": "LieutenantDaredevil", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ssko0/which_of_these_tools_compliment_each_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ssko0/which_of_these_tools_compliment_each_other/", "subreddit_subscribers": 88424, "created_utc": 1675451794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any SQL editor/database browser (something like pgAdmin/MySQL workbench/Adminer) that could be hosted as a \"server\" using e.g. docker container and can connect to Trino? I found only SQLPad which is apparently deprecated.\n\nEdit: not BI tool please", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hostable SQL editor with Trino support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10soq7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675445669.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675442341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any SQL editor/database browser (something like pgAdmin/MySQL workbench/Adminer) that could be hosted as a &amp;quot;server&amp;quot; using e.g. docker container and can connect to Trino? I found only SQLPad which is apparently deprecated.&lt;/p&gt;\n\n&lt;p&gt;Edit: not BI tool please&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10soq7b", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10soq7b/hostable_sql_editor_with_trino_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10soq7b/hostable_sql_editor_with_trino_support/", "subreddit_subscribers": 88424, "created_utc": 1675442341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello ,  \n\n\nWhat is the best way to read data from kafka and write to ES with higher throughput at around 200-300k messages per second. I want to explore real time dashboarding , hence asking", "author_fullname": "t2_sr3rc27q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reading from kafka and pushing to elastic search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10squa4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675447514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello ,  &lt;/p&gt;\n\n&lt;p&gt;What is the best way to read data from kafka and write to ES with higher throughput at around 200-300k messages per second. I want to explore real time dashboarding , hence asking&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10squa4", "is_robot_indexable": true, "report_reasons": null, "author": "honey12123", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10squa4/reading_from_kafka_and_pushing_to_elastic_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10squa4/reading_from_kafka_and_pushing_to_elastic_search/", "subreddit_subscribers": 88424, "created_utc": 1675447514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, I've created my resume in HTML and CSS. I know only the basics of front-end, but I think it looks pretty decent, but I am probably biased. Please review it and inform me if I should change something.\n\nThe pink color is for hyperlinks.\n\nhttps://preview.redd.it/kq7ci8h7yzfa1.jpg?width=1653&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8cb76df3c5f376dc0df51e2168bb3237b9a2457d", "author_fullname": "t2_99xm7vrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HTML + CSS resume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"kq7ci8h7yzfa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=600125b76ed8d4fc203d6a32eaf66273e9484756"}, {"y": 305, "x": 216, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ddbe97f3f420a1a68fa0a8d8d67bd681615aa74"}, {"y": 452, "x": 320, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2c16c1940f4ce1e131de3dc63d27153fa629385"}, {"y": 905, "x": 640, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a59e41cb4c6baeed4b7fee3e73a2ca0634a26d89"}, {"y": 1358, "x": 960, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8f4f778052ebf12aa73838687d74a453701bd05"}, {"y": 1528, "x": 1080, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a31423e096dfea17b55c01c3fde9129028c7a31a"}], "s": {"y": 2339, "x": 1653, "u": "https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=1653&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8cb76df3c5f376dc0df51e2168bb3237b9a2457d"}, "id": "kq7ci8h7yzfa1"}}, "name": "t3_10snua0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/tNH4FQZXD2bJgkWeqmzCV0eqCSqhYrOzl5W0S5Dbesc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675440174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I&amp;#39;ve created my resume in HTML and CSS. I know only the basics of front-end, but I think it looks pretty decent, but I am probably biased. Please review it and inform me if I should change something.&lt;/p&gt;\n\n&lt;p&gt;The pink color is for hyperlinks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=1653&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8cb76df3c5f376dc0df51e2168bb3237b9a2457d\"&gt;https://preview.redd.it/kq7ci8h7yzfa1.jpg?width=1653&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8cb76df3c5f376dc0df51e2168bb3237b9a2457d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "10snua0", "is_robot_indexable": true, "report_reasons": null, "author": "Brief_Priority_2193", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10snua0/html_css_resume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10snua0/html_css_resume/", "subreddit_subscribers": 88424, "created_utc": 1675440174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nI'm currently a data analyst (\\~1.5y) looking to transition into a data engineering role. I've had my current role since I have graduated (with a business degree) so I do not have an experience creating a resume for a tech based role. \n\nMy main questions are:  \n1) I understand that stating your tech stack is the most important in a resume. How exactly do I display that I have experience with a specific tech stack, but am not the best in it? E.g. I learned about docker/spark/airflow through a bootcamp but I'm obviously not the most well-versed at it. Do I state something along the lines of airflow(basic competency) ?  \n2) How important is it to have an online resume? I've been seeing people create their own 'resume/portfolio' on websites and having it linked on their actual resume/linkedin.\n\n(Please feel free to pm me as well. I am looking for people who can provide me additional advise on my transition, thank you!)", "author_fullname": "t2_cpluh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to creating tech-based resume. Need some help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10slb85", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675433656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently a data analyst (~1.5y) looking to transition into a data engineering role. I&amp;#39;ve had my current role since I have graduated (with a business degree) so I do not have an experience creating a resume for a tech based role. &lt;/p&gt;\n\n&lt;p&gt;My main questions are:&lt;br/&gt;\n1) I understand that stating your tech stack is the most important in a resume. How exactly do I display that I have experience with a specific tech stack, but am not the best in it? E.g. I learned about docker/spark/airflow through a bootcamp but I&amp;#39;m obviously not the most well-versed at it. Do I state something along the lines of airflow(basic competency) ?&lt;br/&gt;\n2) How important is it to have an online resume? I&amp;#39;ve been seeing people create their own &amp;#39;resume/portfolio&amp;#39; on websites and having it linked on their actual resume/linkedin.&lt;/p&gt;\n\n&lt;p&gt;(Please feel free to pm me as well. I am looking for people who can provide me additional advise on my transition, thank you!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10slb85", "is_robot_indexable": true, "report_reasons": null, "author": "Slideout", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10slb85/new_to_creating_techbased_resume_need_some_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10slb85/new_to_creating_techbased_resume_need_some_help/", "subreddit_subscribers": 88424, "created_utc": 1675433656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is in the early stages of moving from a monolithic customer platform to rearchitecting around a microservices architecture. I have been asked to write up a whitepaper on master data management/data governance within the new context. Given that a principle of microservices is shared nothing data storage between domains, I'm trying to find out what best practices are for MDM when you have multiple representations of the same data in various microservice back ends. \n\nWhat I've landed on is that you need a message broker that every service that uses a given piece of data, say client_name, subscribes to and if client_name is updated in any of them it propagates to all the other services. In that sense there is no true golden record per se, but you can ascertain the state of any record at any point in time via the event stream. My engineering team would have to ingest this event stream and (because we're still using batch processing into the warehouse) periodically process the events to find the state of a record and that gets loaded into the DB. My thinking is that we'd have to construct a data model that combined microservices, e.g. if five services each own pieces of client data we'd model a client object and then pull the relevant events from each services' stream to construct the state of the client at any given point in time.\n\nThoughts? Has anyone had to do this in the past?", "author_fullname": "t2_wez5v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master Data Management in a Microservices Context", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10swtuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675462104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is in the early stages of moving from a monolithic customer platform to rearchitecting around a microservices architecture. I have been asked to write up a whitepaper on master data management/data governance within the new context. Given that a principle of microservices is shared nothing data storage between domains, I&amp;#39;m trying to find out what best practices are for MDM when you have multiple representations of the same data in various microservice back ends. &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve landed on is that you need a message broker that every service that uses a given piece of data, say client_name, subscribes to and if client_name is updated in any of them it propagates to all the other services. In that sense there is no true golden record per se, but you can ascertain the state of any record at any point in time via the event stream. My engineering team would have to ingest this event stream and (because we&amp;#39;re still using batch processing into the warehouse) periodically process the events to find the state of a record and that gets loaded into the DB. My thinking is that we&amp;#39;d have to construct a data model that combined microservices, e.g. if five services each own pieces of client data we&amp;#39;d model a client object and then pull the relevant events from each services&amp;#39; stream to construct the state of the client at any given point in time.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Has anyone had to do this in the past?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10swtuf", "is_robot_indexable": true, "report_reasons": null, "author": "uchi__mata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10swtuf/master_data_management_in_a_microservices_context/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10swtuf/master_data_management_in_a_microservices_context/", "subreddit_subscribers": 88424, "created_utc": 1675462104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy use case is I will have oracle dump file in S3 and I will need to select some tables from the dump file and store it back to s3 and then transform and back to s3 then read using Athena\n\nI will get the dump file weekly and each time it will have full data and not incremental load.\n\nI am planning to use Oracle RDS instance to load data selectively - only need few tables and not all of the tables, then store data back to s3 as csv/parquet for furter processing using Athena.\n\nSince, I do not need the RDS instance for any other activity until the next load and also because every load is a full load, I also plan to delete this instance / drop data and stop the instance (basically not be billed for it).\n\nSince I am only starting out with AWS and Data engineering as a whole, I know there are knowledge gaps here that need to be filled. Also I would appreciate if you could guide with an overall approach for the same.", "author_fullname": "t2_9rp533dt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pull data from Oracle dump file to S3 bucket", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10ssdt8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675451325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My use case is I will have oracle dump file in S3 and I will need to select some tables from the dump file and store it back to s3 and then transform and back to s3 then read using Athena&lt;/p&gt;\n\n&lt;p&gt;I will get the dump file weekly and each time it will have full data and not incremental load.&lt;/p&gt;\n\n&lt;p&gt;I am planning to use Oracle RDS instance to load data selectively - only need few tables and not all of the tables, then store data back to s3 as csv/parquet for furter processing using Athena.&lt;/p&gt;\n\n&lt;p&gt;Since, I do not need the RDS instance for any other activity until the next load and also because every load is a full load, I also plan to delete this instance / drop data and stop the instance (basically not be billed for it).&lt;/p&gt;\n\n&lt;p&gt;Since I am only starting out with AWS and Data engineering as a whole, I know there are knowledge gaps here that need to be filled. Also I would appreciate if you could guide with an overall approach for the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10ssdt8", "is_robot_indexable": true, "report_reasons": null, "author": "prasanna_aatma", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10ssdt8/pull_data_from_oracle_dump_file_to_s3_bucket/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10ssdt8/pull_data_from_oracle_dump_file_to_s3_bucket/", "subreddit_subscribers": 88424, "created_utc": 1675451325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data model which was created in Power BI some years back where they kept on adding new things into it.\nIt has become a mess now, everything loads super slow, refresh gets stuck and what not.\n\nNow.. have a requirement to migrate it into our MySQL DW.\nWhat would be the ideal plan for this scenario?\n\nWhat wIve done is.. create some documentation, metadata about the tables, relations between them and started working on creating the model, initially the important fact tables then the dimension ones and working on expanding it from there on.\n\nIs there anything that I'm missing here?", "author_fullname": "t2_mutfi9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power BI data model to MySQL.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10teucn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675512115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data model which was created in Power BI some years back where they kept on adding new things into it.\nIt has become a mess now, everything loads super slow, refresh gets stuck and what not.&lt;/p&gt;\n\n&lt;p&gt;Now.. have a requirement to migrate it into our MySQL DW.\nWhat would be the ideal plan for this scenario?&lt;/p&gt;\n\n&lt;p&gt;What wIve done is.. create some documentation, metadata about the tables, relations between them and started working on creating the model, initially the important fact tables then the dimension ones and working on expanding it from there on.&lt;/p&gt;\n\n&lt;p&gt;Is there anything that I&amp;#39;m missing here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10teucn", "is_robot_indexable": true, "report_reasons": null, "author": "prokid1911", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10teucn/power_bi_data_model_to_mysql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10teucn/power_bi_data_model_to_mysql/", "subreddit_subscribers": 88424, "created_utc": 1675512115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose", "author_fullname": "t2_7gkixkov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you're new to databases should you start with the book Database Design for Mere Mortals or SQL Queries for Mere Mortals or Head first with sql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tag8g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "10tag8g", "is_robot_indexable": true, "report_reasons": null, "author": "One_Valuable7049", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "subreddit_subscribers": 88424, "created_utc": 1675500898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. \n\nIs there any alternative if we use Hive metastore as catalog instead of Athena/Glue?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data wrangler with Hive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tafdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. &lt;/p&gt;\n\n&lt;p&gt;Is there any alternative if we use Hive metastore as catalog instead of Athena/Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tafdg", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "subreddit_subscribers": 88424, "created_utc": 1675500852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Everybody keeps saying that one of the advantages of Parquet is that you can query only selected columns. Well, but this is also true for traditional relational database, isnt it? I mean you can specify exact column which you want to return, or what is the point? Is it meant that row based data are PHYSICALLY stored together so one still needs to firstly load ALL columns and then filter them?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet - selecting only columns advantage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tadkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everybody keeps saying that one of the advantages of Parquet is that you can query only selected columns. Well, but this is also true for traditional relational database, isnt it? I mean you can specify exact column which you want to return, or what is the point? Is it meant that row based data are PHYSICALLY stored together so one still needs to firstly load ALL columns and then filter them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tadkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tadkz/parquet_selecting_only_columns_advantage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tadkz/parquet_selecting_only_columns_advantage/", "subreddit_subscribers": 88424, "created_utc": 1675500757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am a complete noob on ETL and could really use some help.\n\nI am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.\n\nHow do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?\n\nVery greatful for any help, thanks!", "author_fullname": "t2_qiw67sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with cluster startup time in azure databricks when having event driven data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t7xs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675495387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am a complete noob on ETL and could really use some help.&lt;/p&gt;\n\n&lt;p&gt;I am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;How do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?&lt;/p&gt;\n\n&lt;p&gt;Very greatful for any help, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10t7xs0", "is_robot_indexable": true, "report_reasons": null, "author": "aLyapunov", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "subreddit_subscribers": 88424, "created_utc": 1675495387.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}