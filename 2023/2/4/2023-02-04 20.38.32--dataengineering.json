{"kind": "Listing", "data": {"after": "t3_10tlk3j", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hg3enfgk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do all my BI initiatives end up like this? \ud83d\ude29", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_10sxj6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 140, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/3miuaya2e3ga1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/3miuaya2e3ga1/DASH_96.mp4", "dash_url": "https://v.redd.it/3miuaya2e3ga1/DASHPlaylist.mpd?a=1678135112%2CMGE1YzNlOTIyYTNlOGFhYTZhZmMyNGE2YWJiYTY5ZDhkZDQxNTFiNzM1MDM3MThlOTY4NTViOWFhNDI4YTdjMQ%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/3miuaya2e3ga1/HLSPlaylist.m3u8?a=1678135112%2CMTg2MmM4ZDYxMmZlMDQyNWUyYTczMGQ2NjdkNjU5OWZkZTc5OWEzOWMyYWI0MmY1MDAzMWUyZWQxNjE2MDEyMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 140, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/X1v7U7_zEvDx-aFcjo5va5mgNFbsSKtx-4ClmvAgrwM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675463793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/3miuaya2e3ga1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3878d1f9f9e60902172ce5ac1af0fdc7d8e3fe1d", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d76a62f25a6d867b419e4d789a62a597c61fc596", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ce99fd83df40cab3bc295fd80ca47320ce881332", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3cb7de2b96e12b511f22737cc4bb1fec7c4ddb6d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=49e8406356277c63fb35f0160fcaa2a2060244ff", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7515bf341310dfe023e72bfcd43c892ff7481984", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LMiBhPfutNaCXzm6DbW2yQ66ZMglaxiwUJKCEmctMZg.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=14868f6bb0b064f393bb66bc67971a6b876766e4", "width": 1080, "height": 607}], "variants": {}, "id": "WklSmgWvyM2nescx70iL76I-U0tvpKJi8lxo2zVV6Jw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "10sxj6r", "is_robot_indexable": true, "report_reasons": null, "author": "Salmon-Advantage", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10sxj6r/why_do_all_my_bi_initiatives_end_up_like_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/3miuaya2e3ga1", "subreddit_subscribers": 88471, "created_utc": 1675463793.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/3miuaya2e3ga1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/3miuaya2e3ga1/DASH_96.mp4", "dash_url": "https://v.redd.it/3miuaya2e3ga1/DASHPlaylist.mpd?a=1678135112%2CMGE1YzNlOTIyYTNlOGFhYTZhZmMyNGE2YWJiYTY5ZDhkZDQxNTFiNzM1MDM3MThlOTY4NTViOWFhNDI4YTdjMQ%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/3miuaya2e3ga1/HLSPlaylist.m3u8?a=1678135112%2CMTg2MmM4ZDYxMmZlMDQyNWUyYTczMGQ2NjdkNjU5OWZkZTc5OWEzOWMyYWI0MmY1MDAzMWUyZWQxNjE2MDEyMA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based \"solution\" to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the \"solution\" has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can't they just let people write Python?\n\nSure the reason might be -- oh BI developers don't want to learn Python or don't have best practices. Well the first reason doesn't hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?\n\nI'm not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...\n\nSincerely, I'd like to know why. I don't really see any benefit. I mean whatever your \"syntax\" can do, Airflow can do that too. I don't see man, I don't really see.", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do people re-invent wrappers for Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10svt5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675459675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based &amp;quot;solution&amp;quot; to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the &amp;quot;solution&amp;quot; has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can&amp;#39;t they just let people write Python?&lt;/p&gt;\n\n&lt;p&gt;Sure the reason might be -- oh BI developers don&amp;#39;t want to learn Python or don&amp;#39;t have best practices. Well the first reason doesn&amp;#39;t hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...&lt;/p&gt;\n\n&lt;p&gt;Sincerely, I&amp;#39;d like to know why. I don&amp;#39;t really see any benefit. I mean whatever your &amp;quot;syntax&amp;quot; can do, Airflow can do that too. I don&amp;#39;t see man, I don&amp;#39;t really see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10svt5y", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 65, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/", "subreddit_subscribers": 88471, "created_utc": 1675459675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just set up the company's first automated pipeline using  Python scripts in Google Cloud Functions triggered by daily Cloud Scheduler and store it to BigQuery. It collects data (few hundred rows) from APIs and process them within the script using Pandas . When researching I came across Airflow, but realised that I didnt need it. So I was just wondering how do you use Airflow?", "author_fullname": "t2_2knag8t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you use Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10syqih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675466833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just set up the company&amp;#39;s first automated pipeline using  Python scripts in Google Cloud Functions triggered by daily Cloud Scheduler and store it to BigQuery. It collects data (few hundred rows) from APIs and process them within the script using Pandas . When researching I came across Airflow, but realised that I didnt need it. So I was just wondering how do you use Airflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10syqih", "is_robot_indexable": true, "report_reasons": null, "author": "lordgriefter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10syqih/how_do_you_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10syqih/how_do_you_use_airflow/", "subreddit_subscribers": 88471, "created_utc": 1675466833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...\n\nBut I'm curious to know, what did ETL look like before this?\n\nWhat did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?\n\nSometimes I find tutorials like \"Build an ETL pipeline with Airflow and Pandas\" and I think \"Pandas, Really?\".\n\nIn other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?\n\nPlease do share your experience!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What did ETL look like before the \"modern data stack\" was a thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tjhve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675525724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m curious to know, what did ETL look like before this?&lt;/p&gt;\n\n&lt;p&gt;What did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?&lt;/p&gt;\n\n&lt;p&gt;Sometimes I find tutorials like &amp;quot;Build an ETL pipeline with Airflow and Pandas&amp;quot; and I think &amp;quot;Pandas, Really?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;In other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?&lt;/p&gt;\n\n&lt;p&gt;Please do share your experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tjhve", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 92, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/", "subreddit_subscribers": 88471, "created_utc": 1675525724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is **prohibited**. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. \n\nTo mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.", "author_fullname": "t2_eckrjl60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bundle million small json files and save to s3, later easy to process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t5mvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675487162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We receive billions of small JSON files on a daily basis, which are raw data sourced from smart meters. Each JSON file is 1-30KB in size and manipulation of these files, such as merging them using scripts, is &lt;strong&gt;prohibited&lt;/strong&gt;. However, we are required to store the data in S3, which is incurring a high number of API requests due to the vast number of small files. &lt;/p&gt;\n\n&lt;p&gt;To mitigate this issue, we are considering compressing the data into fewer files on a daily basis using TAR or a similar compression method. This will allow us to process the data in the future using tools such as AWS Glue or Apache Spark. If anyone has information on how to best tackle this situation, please provide it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t5mvd", "is_robot_indexable": true, "report_reasons": null, "author": "b-y-f", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t5mvd/how_to_bundle_million_small_json_files_and_save/", "subreddit_subscribers": 88471, "created_utc": 1675487162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi experts,\n\nWe are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I'm curious is there recommendation?\n\nFeature needed:\n- Can connect and query cloud database, especially BigQuery;\n\n- Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it's a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;\n\n- Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);\n\n- Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its \"AI\" but full table scan is really expensive;\n\n- Can setup threshold for the metrics, and send alerts to emails and slack channels;\n\nThat's all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don't know if it's good. It's definitely mature though, heard about it for years.\n\nThank you!", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask DE: What nice data quality monitoring tool are you using?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t4n5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675483912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts,&lt;/p&gt;\n\n&lt;p&gt;We are using a data quality monitoring tool but it is still on early stage so missing a lot of features. Apparently no one was consulted when the tool was imported. I&amp;#39;m curious is there recommendation?&lt;/p&gt;\n\n&lt;p&gt;Feature needed:\n- Can connect and query cloud database, especially BigQuery;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data quality has two parts: 1) technical metrics (any duplications? did it arrive on time? etc.) and 2) business metrics (does column A * column B + column F looks good comparing to same value in the last 10 days? yeah it&amp;#39;s a bit vague but business is always like that). Business metrics are very flexible so probably have to consume SQL queries or something similar;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Support custom schedules (cron is OK but better if it supports custom ones such as the next 2 days + every Monday for the next 10 weeks);&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Should be cost smart. Our tool sometimes do full table scan in BQ which costs a few hundred to a couple of thousand bucks. I get it might need some historical data for its &amp;quot;AI&amp;quot; but full table scan is really expensive;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can setup threshold for the metrics, and send alerts to emails and slack channels;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s all I can think of right now. My previous company uses Prometheus but I was not on the DE team so I don&amp;#39;t know if it&amp;#39;s good. It&amp;#39;s definitely mature though, heard about it for years.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t4n5x", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t4n5x/ask_de_what_nice_data_quality_monitoring_tool_are/", "subreddit_subscribers": 88471, "created_utc": 1675483912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose", "author_fullname": "t2_7gkixkov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you're new to databases should you start with the book Database Design for Mere Mortals or SQL Queries for Mere Mortals or Head first with sql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tag8g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tag8g", "is_robot_indexable": true, "report_reasons": null, "author": "One_Valuable7049", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tag8g/if_youre_new_to_databases_should_you_start_with/", "subreddit_subscribers": 88471, "created_utc": 1675500898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. \n\nI'm now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn't require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.\n\nI've been at my company for over a year now and mostly due to the outdated stack that I'm limited to in my work environment, I'm looking to transition to a different data engineering position. That way, I'm hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.\n\nThanks for any guidance in advance!", "author_fullname": "t2_x9ryi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Good Free Resources for Learning Data Warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t745z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675492344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work a data science position by title, but tbh my daily stack includes Tableau, R for data manipulation/cleaning/visualization, an ungodly amount of Excel, but not SQL whatsoever. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now looking to get more exposure to SQL and data warehousing and was wondering if anyone might have any good free learning resources to get started learning so I can land a data engineering position. I did a little reading and it sounds like BigQuery may be a good one to start with since it doesn&amp;#39;t require a credit card to start playing around, so I plan on looking up some YouTube videos guides over the weekend.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been at my company for over a year now and mostly due to the outdated stack that I&amp;#39;m limited to in my work environment, I&amp;#39;m looking to transition to a different data engineering position. That way, I&amp;#39;m hoping to get more exposure to more up to date platforms/tools, but I need to get somewhat familiar with these tools in order to land the job.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any guidance in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10t745z", "is_robot_indexable": true, "report_reasons": null, "author": "papes_tv", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t745z/any_good_free_resources_for_learning_data/", "subreddit_subscribers": 88471, "created_utc": 1675492344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning more about Trino/Presto and am currently playing around with it on local Docker. Very early in the process but I have connected to my S3 bucket via the Glue catalog, a MySQL database, and Google Sheets.  I have a question about materializing data in terms of functionality as well as best practice architecture. I may be looking at this the wrong way so please correct my assumptions.\n\nIf I have a query that joins tables from all three data sources, I believe I have the capability to create a table in certain catalogs (like Glue/Hive). If executed, is this table written to the storage location? Trying to understand the difference between a Trino table vs view vs materialized view. The comparison I am making is to two architectures I use today: Snowflake and Dremio. Snowflake is like a typical structured database but Dremio processes it's datasets as views in memory with the option to materialize datasets using its Reflections feature. Dremio can also CTAS to locations on object storage as parquet (and iceberg).\n\nHow does Trino relate to this? For my use case, doing everything like a view is satisfactory in terms of performance. But if I am working in an enterprise trying to use Trino for ETL processes, I am assuming that there is functionality to materialize data at some point? Or is that not a best practice/the right use case for Trino? I am also relating it to some of Starburst's (enterprise Trino) marketing, which has ETL data processing as a use case.", "author_fullname": "t2_ywrol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino/Presto materialization and storing query results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10suc08", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675456471.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675456121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning more about Trino/Presto and am currently playing around with it on local Docker. Very early in the process but I have connected to my S3 bucket via the Glue catalog, a MySQL database, and Google Sheets.  I have a question about materializing data in terms of functionality as well as best practice architecture. I may be looking at this the wrong way so please correct my assumptions.&lt;/p&gt;\n\n&lt;p&gt;If I have a query that joins tables from all three data sources, I believe I have the capability to create a table in certain catalogs (like Glue/Hive). If executed, is this table written to the storage location? Trying to understand the difference between a Trino table vs view vs materialized view. The comparison I am making is to two architectures I use today: Snowflake and Dremio. Snowflake is like a typical structured database but Dremio processes it&amp;#39;s datasets as views in memory with the option to materialize datasets using its Reflections feature. Dremio can also CTAS to locations on object storage as parquet (and iceberg).&lt;/p&gt;\n\n&lt;p&gt;How does Trino relate to this? For my use case, doing everything like a view is satisfactory in terms of performance. But if I am working in an enterprise trying to use Trino for ETL processes, I am assuming that there is functionality to materialize data at some point? Or is that not a best practice/the right use case for Trino? I am also relating it to some of Starburst&amp;#39;s (enterprise Trino) marketing, which has ETL data processing as a use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10suc08", "is_robot_indexable": true, "report_reasons": null, "author": "DarkmoonDingo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10suc08/trinopresto_materialization_and_storing_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10suc08/trinopresto_materialization_and_storing_query/", "subreddit_subscribers": 88471, "created_utc": 1675456121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined a huge company in November of last year. I switched careers from Data Science to engineering, so I do not know much about the typical DE environment. Previously, I worked together with one Data Engineer and I am now one of 9 in my team. However, it certainly is different from how I expected it to be. My expectations were that everything would be more structured. You get a task, there is a straightforward way to do it, and then you spend time implementing it. My current style of working is instead 90% searching where to put my 5 lines of code and then testing if this is really how it should be done.\n\nThe personal red flags are:\n\n1. Everything is exceptionally complex, from Airflow pipelines with 1000 lines config files to a vast Terraform multi repo (around 20 individual repos). They use a lot of internal libraries, most of them wrapping well-known libraries and adding some functionality. For example, the Airflow S3 hook was wrapped in a company internal one giving it like 30 more options to read data for specific use cases. It took me hours to figure this out because it's only documented inline and the file is around 5000 lines of code.\n2. There is one colleague who is the single source of truth. He has been in the company for 10 years and knows why things are implemented like they are. Others have been around 5 years but basically only refer to him. He is quite good at what he does, but I still find this hierarchy concerning. I understand that more experienced colleagues are asked for their opinion more often, but this goes as far as at least one colleague approving his pull requests without actually looking at them.\n3. Many colleagues know much about the existing infrastructure but fail to understand what's behind it or what they are doing. For example, a colleague from another department asked what kubectl port-forward is doing and a colleague responsible for our Kubernetes cluster replied, \"It gives your local user access rights to the cluster.\" This was the most extreme example, but I found many smaller ones confusing. For instance, I talked about writing a bash script for a simple renaming task and a colleague said: \"Why not implement it with Python? We shouldn't use languages that need additional installations on our servers\".\n\nWould you consider that red flags? What would you do to improve the situation, expecially the complexity of the existing code?", "author_fullname": "t2_4htedsojh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New job - red flags or just the usual nightmare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tju13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675526548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a huge company in November of last year. I switched careers from Data Science to engineering, so I do not know much about the typical DE environment. Previously, I worked together with one Data Engineer and I am now one of 9 in my team. However, it certainly is different from how I expected it to be. My expectations were that everything would be more structured. You get a task, there is a straightforward way to do it, and then you spend time implementing it. My current style of working is instead 90% searching where to put my 5 lines of code and then testing if this is really how it should be done.&lt;/p&gt;\n\n&lt;p&gt;The personal red flags are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Everything is exceptionally complex, from Airflow pipelines with 1000 lines config files to a vast Terraform multi repo (around 20 individual repos). They use a lot of internal libraries, most of them wrapping well-known libraries and adding some functionality. For example, the Airflow S3 hook was wrapped in a company internal one giving it like 30 more options to read data for specific use cases. It took me hours to figure this out because it&amp;#39;s only documented inline and the file is around 5000 lines of code.&lt;/li&gt;\n&lt;li&gt;There is one colleague who is the single source of truth. He has been in the company for 10 years and knows why things are implemented like they are. Others have been around 5 years but basically only refer to him. He is quite good at what he does, but I still find this hierarchy concerning. I understand that more experienced colleagues are asked for their opinion more often, but this goes as far as at least one colleague approving his pull requests without actually looking at them.&lt;/li&gt;\n&lt;li&gt;Many colleagues know much about the existing infrastructure but fail to understand what&amp;#39;s behind it or what they are doing. For example, a colleague from another department asked what kubectl port-forward is doing and a colleague responsible for our Kubernetes cluster replied, &amp;quot;It gives your local user access rights to the cluster.&amp;quot; This was the most extreme example, but I found many smaller ones confusing. For instance, I talked about writing a bash script for a simple renaming task and a colleague said: &amp;quot;Why not implement it with Python? We shouldn&amp;#39;t use languages that need additional installations on our servers&amp;quot;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would you consider that red flags? What would you do to improve the situation, expecially the complexity of the existing code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10tju13", "is_robot_indexable": true, "report_reasons": null, "author": "Front_Sun_8213", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tju13/new_job_red_flags_or_just_the_usual_nightmare/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tju13/new_job_red_flags_or_just_the_usual_nightmare/", "subreddit_subscribers": 88471, "created_utc": 1675526548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have stringent performance expectations from such queries? Or is it mostly just being able to access data across sources through a common SQL gateway? \n\nDo you use a separate, smaller cluster for such workloads?\n\n[View Poll](https://www.reddit.com/poll/10t3qdj)", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How commonly do you use SQL engines like Trino / Presto, Spark to run queries on data that is NOT in an object store (eg. across data in one or more OLTP DBs, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t3qdj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675481084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have stringent performance expectations from such queries? Or is it mostly just being able to access data across sources through a common SQL gateway? &lt;/p&gt;\n\n&lt;p&gt;Do you use a separate, smaller cluster for such workloads?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10t3qdj\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t3qdj", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1675913084937, "options": [{"text": "&lt;10% of the time", "id": "21414867"}, {"text": "10-20% of the time", "id": "21414868"}, {"text": "20-30% of the time", "id": "21414869"}, {"text": "30-40% of the time", "id": "21414870"}, {"text": "&gt;40% of the time", "id": "21414871"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 46, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t3qdj/how_commonly_do_you_use_sql_engines_like_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10t3qdj/how_commonly_do_you_use_sql_engines_like_trino/", "subreddit_subscribers": 88471, "created_utc": 1675481084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a start-up where we will have 0-10 million of rows stored in our database daily, and about 500k users. I am very new to data engineering but have some solid background. The thing is, the database need to have high read and write capacity. Users will use search functions in our mobile app to get rows of data to their phone. At the same time it needs to take in many new rows of data.\n\nWhat are efficient ways to build a data warehouse with theese needs? I was looking at azure data lake storage and google bigTable as possible solutions.can someone give me some insight and opinion on how i should tackle this problem?", "author_fullname": "t2_6441kcer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help on best practice for building a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tmn9e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675533391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a start-up where we will have 0-10 million of rows stored in our database daily, and about 500k users. I am very new to data engineering but have some solid background. The thing is, the database need to have high read and write capacity. Users will use search functions in our mobile app to get rows of data to their phone. At the same time it needs to take in many new rows of data.&lt;/p&gt;\n\n&lt;p&gt;What are efficient ways to build a data warehouse with theese needs? I was looking at azure data lake storage and google bigTable as possible solutions.can someone give me some insight and opinion on how i should tackle this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10tmn9e", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible_Hawk8015", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tmn9e/need_help_on_best_practice_for_building_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tmn9e/need_help_on_best_practice_for_building_a_data/", "subreddit_subscribers": 88471, "created_utc": 1675533391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that this must vary by pipeline, but in my example I have an Excel file users can manually update in a SharePoint site, and I want to make sure that I want to make sure that the data passes a number of validity checks (i.e. type checking, regex checking, cross-column dependency checking).\n\nThe actual pipeline is basic, and triggered via Airflow:\n\n1. **Extract:** Read file into memory via SharePoint API &amp; Write to S3 as CSV\n2. **Load:** Read CSV into memory from S3 &amp; Write to RedShift\n\nNow I know that I have the flexibility to do this in countless ways, but for those with experience, what's the best way to validate? In memory during the Extract task? A new task between Extract &amp; Load? In memory during the Load task? Or somewhere after load like before or during Transformation?\n\nAlso unrelated, is it write for me to throw an error on invalid data, or should I attempt to automatically transform it to be valid?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When exactly should I be validating data in my ingestion pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tkcac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675527812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that this must vary by pipeline, but in my example I have an Excel file users can manually update in a SharePoint site, and I want to make sure that I want to make sure that the data passes a number of validity checks (i.e. type checking, regex checking, cross-column dependency checking).&lt;/p&gt;\n\n&lt;p&gt;The actual pipeline is basic, and triggered via Airflow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Extract:&lt;/strong&gt; Read file into memory via SharePoint API &amp;amp; Write to S3 as CSV&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Load:&lt;/strong&gt; Read CSV into memory from S3 &amp;amp; Write to RedShift&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now I know that I have the flexibility to do this in countless ways, but for those with experience, what&amp;#39;s the best way to validate? In memory during the Extract task? A new task between Extract &amp;amp; Load? In memory during the Load task? Or somewhere after load like before or during Transformation?&lt;/p&gt;\n\n&lt;p&gt;Also unrelated, is it write for me to throw an error on invalid data, or should I attempt to automatically transform it to be valid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10tkcac", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tkcac/when_exactly_should_i_be_validating_data_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tkcac/when_exactly_should_i_be_validating_data_in_my/", "subreddit_subscribers": 88471, "created_utc": 1675527812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a BI Engineer for the past 2 years, spanning across two different companies. However my experience at the 2nd company has been vastly different, so lately its had me wondering which company better embodies the work of a BI engineer.   \n\n\nHere's a breakdown of various activities:\n\n**Company 1**  \n\\- Bring in data from brand new data sources and design supporting tables. A lot of emphasis was put on this, as we had to make sure our tables were performant and would work flawlessly downstream in Tableau.\n\n\\- Build KPIs   \n\\- Design dashboards\n\n\\- Meet with functional business areas to come up with new reporting requirements\n\n&amp;#x200B;\n\n**Company 2**\n\n\\- Design dashboards \n\n\\- report data issues up to the data engineers\n\n\\- design KPIs into existing tables that the DE's build\n\n\\- Meet with functional business areas to come up with new reporting requirements  \n\n\n  \nBased on your experience, would either of these fit the mold of a traditional BI Engineer?", "author_fullname": "t2_30pyiyf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer vs. BI Engineer roles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t13dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675473265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a BI Engineer for the past 2 years, spanning across two different companies. However my experience at the 2nd company has been vastly different, so lately its had me wondering which company better embodies the work of a BI engineer.   &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a breakdown of various activities:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company 1&lt;/strong&gt;&lt;br/&gt;\n- Bring in data from brand new data sources and design supporting tables. A lot of emphasis was put on this, as we had to make sure our tables were performant and would work flawlessly downstream in Tableau.&lt;/p&gt;\n\n&lt;p&gt;- Build KPIs&lt;br/&gt;\n- Design dashboards&lt;/p&gt;\n\n&lt;p&gt;- Meet with functional business areas to come up with new reporting requirements&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company 2&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Design dashboards &lt;/p&gt;\n\n&lt;p&gt;- report data issues up to the data engineers&lt;/p&gt;\n\n&lt;p&gt;- design KPIs into existing tables that the DE&amp;#39;s build&lt;/p&gt;\n\n&lt;p&gt;- Meet with functional business areas to come up with new reporting requirements  &lt;/p&gt;\n\n&lt;p&gt;Based on your experience, would either of these fit the mold of a traditional BI Engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10t13dz", "is_robot_indexable": true, "report_reasons": null, "author": "Schley_them_all", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t13dz/data_engineer_vs_bi_engineer_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t13dz/data_engineer_vs_bi_engineer_roles/", "subreddit_subscribers": 88471, "created_utc": 1675473265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is in the early stages of moving from a monolithic customer platform to rearchitecting around a microservices architecture. I have been asked to write up a whitepaper on master data management/data governance within the new context. Given that a principle of microservices is shared nothing data storage between domains, I'm trying to find out what best practices are for MDM when you have multiple representations of the same data in various microservice back ends. \n\nWhat I've landed on is that you need a message broker that every service that uses a given piece of data, say client_name, subscribes to and if client_name is updated in any of them it propagates to all the other services. In that sense there is no true golden record per se, but you can ascertain the state of any record at any point in time via the event stream. My engineering team would have to ingest this event stream and (because we're still using batch processing into the warehouse) periodically process the events to find the state of a record and that gets loaded into the DB. My thinking is that we'd have to construct a data model that combined microservices, e.g. if five services each own pieces of client data we'd model a client object and then pull the relevant events from each services' stream to construct the state of the client at any given point in time.\n\nThoughts? Has anyone had to do this in the past?", "author_fullname": "t2_wez5v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master Data Management in a Microservices Context", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10swtuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675462104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is in the early stages of moving from a monolithic customer platform to rearchitecting around a microservices architecture. I have been asked to write up a whitepaper on master data management/data governance within the new context. Given that a principle of microservices is shared nothing data storage between domains, I&amp;#39;m trying to find out what best practices are for MDM when you have multiple representations of the same data in various microservice back ends. &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve landed on is that you need a message broker that every service that uses a given piece of data, say client_name, subscribes to and if client_name is updated in any of them it propagates to all the other services. In that sense there is no true golden record per se, but you can ascertain the state of any record at any point in time via the event stream. My engineering team would have to ingest this event stream and (because we&amp;#39;re still using batch processing into the warehouse) periodically process the events to find the state of a record and that gets loaded into the DB. My thinking is that we&amp;#39;d have to construct a data model that combined microservices, e.g. if five services each own pieces of client data we&amp;#39;d model a client object and then pull the relevant events from each services&amp;#39; stream to construct the state of the client at any given point in time.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Has anyone had to do this in the past?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10swtuf", "is_robot_indexable": true, "report_reasons": null, "author": "uchi__mata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10swtuf/master_data_management_in_a_microservices_context/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10swtuf/master_data_management_in_a_microservices_context/", "subreddit_subscribers": 88471, "created_utc": 1675462104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The AWS Certified Big Data or Databricks certs both look interesting and would fill in an experience gap for me. But I would be really concerned about running up a huge bill of hundreds of dollars with those types of services while learning. In my experience, it is really easy to forget to tear everything down within a cloud platform and miss something, and I'd probably be stressed about it constantly. \n\nFrom what I know, Databricks has a community edition and then of course there's AWS free tier. But free tier is a pittance for most \"big data\" services. Anyone have some experience with either or both for certification study? I lean towards Databricks just because I'm more interested in the Spark ecosystem. I would just like to avoid hundreds of dollars of charges in either case.", "author_fullname": "t2_4i8pl31g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS DE and Databricks Certs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10tpkeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675540514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The AWS Certified Big Data or Databricks certs both look interesting and would fill in an experience gap for me. But I would be really concerned about running up a huge bill of hundreds of dollars with those types of services while learning. In my experience, it is really easy to forget to tear everything down within a cloud platform and miss something, and I&amp;#39;d probably be stressed about it constantly. &lt;/p&gt;\n\n&lt;p&gt;From what I know, Databricks has a community edition and then of course there&amp;#39;s AWS free tier. But free tier is a pittance for most &amp;quot;big data&amp;quot; services. Anyone have some experience with either or both for certification study? I lean towards Databricks just because I&amp;#39;m more interested in the Spark ecosystem. I would just like to avoid hundreds of dollars of charges in either case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tpkeq", "is_robot_indexable": true, "report_reasons": null, "author": "knowledgebass", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tpkeq/aws_de_and_databricks_certs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tpkeq/aws_de_and_databricks_certs/", "subreddit_subscribers": 88471, "created_utc": 1675540514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which of the \u201clatest\u201d pandas alternatives (excl. Spark) do you use the most? Also, if you could share the reason I would appreciate it.\n\n[View Poll](https://www.reddit.com/poll/10tpigy)", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What pandas alternative you use the most?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10tpigy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675540384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which of the \u201clatest\u201d pandas alternatives (excl. Spark) do you use the most? Also, if you could share the reason I would appreciate it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/10tpigy\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tpigy", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1675972384570, "options": [{"text": "Modin", "id": "21426577"}, {"text": "Polars", "id": "21426578"}, {"text": "Dask", "id": "21426579"}, {"text": "PyArrow", "id": "21426580"}, {"text": "Ray", "id": "21426581"}, {"text": "Other", "id": "21426582"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 11, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tpigy/what_pandas_alternative_you_use_the_most/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/10tpigy/what_pandas_alternative_you_use_the_most/", "subreddit_subscribers": 88471, "created_utc": 1675540384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone give me good resources to learn data engineering with Python ? I am already fluent with python and also a backend  engineer so what could be the best way to learn data engineering. I just started reading the book called 'Data Engineering with Python' and It only covers the basic stuff.", "author_fullname": "t2_nwphe35r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course recommendations and Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10thtnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675521382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone give me good resources to learn data engineering with Python ? I am already fluent with python and also a backend  engineer so what could be the best way to learn data engineering. I just started reading the book called &amp;#39;Data Engineering with Python&amp;#39; and It only covers the basic stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10thtnv", "is_robot_indexable": true, "report_reasons": null, "author": "golesu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10thtnv/course_recommendations_and_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10thtnv/course_recommendations_and_resources/", "subreddit_subscribers": 88471, "created_utc": 1675521382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Reading the  [Why do people re-invent wrappers for Airflow? : dataengineering (reddit.com)](https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/)  post I was reflecting about my DE experience and I have to admit I've introduced myself extra layers or patterns that, with all due honesty, were more a \"Let's just try Bridge pattern for the sake of it\" than a well designed and necessary solution.\n\nUnnecessary classes on top of PySpark, a complicated hierarchy of abstract Pipeline classes or YAML-based configurations parsed by another self-made solution.", "author_fullname": "t2_5a2nw4lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confiteor (or how to overcomplicate data engineering for fun)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tg72q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675516640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading the  &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/\"&gt;Why do people re-invent wrappers for Airflow? : dataengineering (reddit.com)&lt;/a&gt;  post I was reflecting about my DE experience and I have to admit I&amp;#39;ve introduced myself extra layers or patterns that, with all due honesty, were more a &amp;quot;Let&amp;#39;s just try Bridge pattern for the sake of it&amp;quot; than a well designed and necessary solution.&lt;/p&gt;\n\n&lt;p&gt;Unnecessary classes on top of PySpark, a complicated hierarchy of abstract Pipeline classes or YAML-based configurations parsed by another self-made solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tg72q", "is_robot_indexable": true, "report_reasons": null, "author": "_raskol_nikov_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tg72q/confiteor_or_how_to_overcomplicate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tg72q/confiteor_or_how_to_overcomplicate_data/", "subreddit_subscribers": 88471, "created_utc": 1675516640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. \n\nIs there any alternative if we use Hive metastore as catalog instead of Athena/Glue?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data wrangler with Hive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tafdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found AWS SDK for pandas (previously Data wrangler) that is able to write parquet table and also directly register it in the Athena or Glue. &lt;/p&gt;\n\n&lt;p&gt;Is there any alternative if we use Hive metastore as catalog instead of Athena/Glue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tafdg", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tafdg/aws_data_wrangler_with_hive/", "subreddit_subscribers": 88471, "created_utc": 1675500852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Everybody keeps saying that one of the advantages of Parquet is that you can query only selected columns. Well, but this is also true for traditional relational database, isnt it? I mean you can specify exact column which you want to return, or what is the point? Is it meant that row based data are PHYSICALLY stored together so one still needs to firstly load ALL columns and then filter them?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet - selecting only columns advantage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tadkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everybody keeps saying that one of the advantages of Parquet is that you can query only selected columns. Well, but this is also true for traditional relational database, isnt it? I mean you can specify exact column which you want to return, or what is the point? Is it meant that row based data are PHYSICALLY stored together so one still needs to firstly load ALL columns and then filter them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10tadkz", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tadkz/parquet_selecting_only_columns_advantage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tadkz/parquet_selecting_only_columns_advantage/", "subreddit_subscribers": 88471, "created_utc": 1675500757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am a complete noob on ETL and could really use some help.\n\nI am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.\n\nHow do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?\n\nVery greatful for any help, thanks!", "author_fullname": "t2_qiw67sj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with cluster startup time in azure databricks when having event driven data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t7xs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675495387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am a complete noob on ETL and could really use some help.&lt;/p&gt;\n\n&lt;p&gt;I am dealing with event driven data. Randomly in time there will be json-files generated that will go through an ETL-process where the last step is through a notebook in databricks which implements scd1 and scd2 before it is inserted to Azure Sql database and then visualized in a power bi report. I want the report to be as real time as possible but the cluster start up time takes everything between 1 to 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;How do you usually deal with clusters in such case? It feels a bit weird to let the cluster startup everytime there is a new event. Should it instead be continiously running?&lt;/p&gt;\n\n&lt;p&gt;Very greatful for any help, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10t7xs0", "is_robot_indexable": true, "report_reasons": null, "author": "aLyapunov", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10t7xs0/how_to_deal_with_cluster_startup_time_in_azure/", "subreddit_subscribers": 88471, "created_utc": 1675495387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team develops reporting tools for all the projects in our company using c#.Net, python (workers), and SQL. I develop a lot of SQL stored procedures, maintain stored proc packages, inner merge queries for ETL processes and data tables, and views, etc. These stored procedures go through a lot of editing with new CRs in application developments.\n\nI have some doubts if anyone can share wisdom or knowledge in these points:\n\n1) How do I maintain or track the changes if many developers alter the SQL statements in the stored procedure and packages inside the oracle sql developer? Are there any good practices like comments to track dev1 made these changes with dates or any other method? Suggest any guidance\n2) Is sub versioning possible in SQL or backup of sql codes? like other programming languages. (Since we write codes in live server) If yes then how? Suggest any method\n3) How do I know which package and procedure are fired in the backend from the UI click? Since a lot of procedures/packages are used and i have not track\n4) Some base queries are rewritten many times and differently by other developers. So any strict policy to maintain for consistency? Recommendations on that\n5) How to document the backend database process logic. Which table does that, which stored proc does that etc\n6) Suggest me a book for a beginner entering the data engineering field \n\nThanks in advance \ud83d\ude07", "author_fullname": "t2_5svl00at", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Advice in Oracle SQL Developer. Tips &amp; Tricks!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10swyl1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675462416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team develops reporting tools for all the projects in our company using c#.Net, python (workers), and SQL. I develop a lot of SQL stored procedures, maintain stored proc packages, inner merge queries for ETL processes and data tables, and views, etc. These stored procedures go through a lot of editing with new CRs in application developments.&lt;/p&gt;\n\n&lt;p&gt;I have some doubts if anyone can share wisdom or knowledge in these points:&lt;/p&gt;\n\n&lt;p&gt;1) How do I maintain or track the changes if many developers alter the SQL statements in the stored procedure and packages inside the oracle sql developer? Are there any good practices like comments to track dev1 made these changes with dates or any other method? Suggest any guidance\n2) Is sub versioning possible in SQL or backup of sql codes? like other programming languages. (Since we write codes in live server) If yes then how? Suggest any method\n3) How do I know which package and procedure are fired in the backend from the UI click? Since a lot of procedures/packages are used and i have not track\n4) Some base queries are rewritten many times and differently by other developers. So any strict policy to maintain for consistency? Recommendations on that\n5) How to document the backend database process logic. Which table does that, which stored proc does that etc\n6) Suggest me a book for a beginner entering the data engineering field &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance \ud83d\ude07&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "10swyl1", "is_robot_indexable": true, "report_reasons": null, "author": "Few_War_6750", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10swyl1/need_advice_in_oracle_sql_developer_tips_tricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10swyl1/need_advice_in_oracle_sql_developer_tips_tricks/", "subreddit_subscribers": 88471, "created_utc": 1675462416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my first post at reddit. Hello everyone!\n\nI am a senior software engineer and have 12+ years of experience. I came to US before 3 years and working as a contractor. Initially I started as some development work in Data team at client side and eventually I got moved to data engineering work. \n\nIn this project I am using matillion ETL tool which is more like UI drag and drop. Initially I liked that work as I love data and have more WLB but now I miss my old programming days. I want to switch job now but since last 3 years I worked in data team it is hard to crack interview for software engineer role and no one will take me for senior data engineer. I see there are many tools in data engineering which is more on programming side. I am happy to learn that as well but I am directionless.\n\nI work for a contracting company right now so I am not earning much like product company. For better financial growth and career perspective I am not sure if I should go back to software engineering or I should learn more in data engineering and proceed in data only. One friend told me that salary for data engineer will be very low than software engineer.  Data science engineer will have more programming and better salary but they only hire phd person for that.\n\nLooks like I am stuck at this situation and need some guidance from fellow engineers.\n\nKindly share your opinion in this matter. Your any feedback will be greatly helpful.", "author_fullname": "t2_99nnwpys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advise - SSE or Data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10svsyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675459660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my first post at reddit. Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I am a senior software engineer and have 12+ years of experience. I came to US before 3 years and working as a contractor. Initially I started as some development work in Data team at client side and eventually I got moved to data engineering work. &lt;/p&gt;\n\n&lt;p&gt;In this project I am using matillion ETL tool which is more like UI drag and drop. Initially I liked that work as I love data and have more WLB but now I miss my old programming days. I want to switch job now but since last 3 years I worked in data team it is hard to crack interview for software engineer role and no one will take me for senior data engineer. I see there are many tools in data engineering which is more on programming side. I am happy to learn that as well but I am directionless.&lt;/p&gt;\n\n&lt;p&gt;I work for a contracting company right now so I am not earning much like product company. For better financial growth and career perspective I am not sure if I should go back to software engineering or I should learn more in data engineering and proceed in data only. One friend told me that salary for data engineer will be very low than software engineer.  Data science engineer will have more programming and better salary but they only hire phd person for that.&lt;/p&gt;\n\n&lt;p&gt;Looks like I am stuck at this situation and need some guidance from fellow engineers.&lt;/p&gt;\n\n&lt;p&gt;Kindly share your opinion in this matter. Your any feedback will be greatly helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10svsyd", "is_robot_indexable": true, "report_reasons": null, "author": "seattleshawk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10svsyd/need_advise_sse_or_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10svsyd/need_advise_sse_or_data_engineering/", "subreddit_subscribers": 88471, "created_utc": 1675459660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nPlease give your insights by the metrics bellow:\n\nStatistical programmer vs Data engineer. Who will win?\n\n1. Salary\n2. Supply-demand curve. Which one has more jobs (i.e. better burgaining power)?\n3. Work-life balance\n4. Future career growth (i.e. more recession-proof)", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Statistical programmer vs Data engineer. Which one is a better job(salary/demand/work-life balance/career growth)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tlk3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675530805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please give your insights by the metrics bellow:&lt;/p&gt;\n\n&lt;p&gt;Statistical programmer vs Data engineer. Who will win?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Salary&lt;/li&gt;\n&lt;li&gt;Supply-demand curve. Which one has more jobs (i.e. better burgaining power)?&lt;/li&gt;\n&lt;li&gt;Work-life balance&lt;/li&gt;\n&lt;li&gt;Future career growth (i.e. more recession-proof)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "10tlk3j", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10tlk3j/statistical_programmer_vs_data_engineer_which_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10tlk3j/statistical_programmer_vs_data_engineer_which_one/", "subreddit_subscribers": 88471, "created_utc": 1675530805.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}