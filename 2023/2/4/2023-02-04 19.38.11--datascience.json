{"kind": "Listing", "data": {"after": "t3_10t3ejl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Has anyone experienced this? I'm 2 weeks into a new job and had a meeting with my manager where he said he was concerned I was working slowly.  However I've finished the two tasks he's assigned me each a day early..", "author_fullname": "t2_2d6rhe2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Completing Tasks before the finish date but manager says I'm slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tjqwi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675526359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone experienced this? I&amp;#39;m 2 weeks into a new job and had a meeting with my manager where he said he was concerned I was working slowly.  However I&amp;#39;ve finished the two tasks he&amp;#39;s assigned me each a day early..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tjqwi", "is_robot_indexable": true, "report_reasons": null, "author": "HappyKoalaCub", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tjqwi/completing_tasks_before_the_finish_date_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tjqwi/completing_tasks_before_the_finish_date_but/", "subreddit_subscribers": 844576, "created_utc": 1675526359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve completed 2 recent company take home assignments for data science/analyst and found the suggested time completely off. Both claimed 2-4 hours, and I spent 7 on the first set without fully completing it and 6 on the second. That was to answer the questions, not overachieve for perfect responses.\nIs that typical? \n\nFor context I\u2019ve been an analyst for 4 years and a DS for 2.", "author_fullname": "t2_cyc47ptx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Actual Time Spent on Interview Take Home Assignments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10su30j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675455519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve completed 2 recent company take home assignments for data science/analyst and found the suggested time completely off. Both claimed 2-4 hours, and I spent 7 on the first set without fully completing it and 6 on the second. That was to answer the questions, not overachieve for perfect responses.\nIs that typical? &lt;/p&gt;\n\n&lt;p&gt;For context I\u2019ve been an analyst for 4 years and a DS for 2.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10su30j", "is_robot_indexable": true, "report_reasons": null, "author": "Useful-Comfortable57", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10su30j/actual_time_spent_on_interview_take_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10su30j/actual_time_spent_on_interview_take_home/", "subreddit_subscribers": 844576, "created_utc": 1675455519.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_vsksryrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Data Science work flows follow agile? How do we handle EDA, when directions change on a weekly basis based on findings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sviok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675458972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10sviok", "is_robot_indexable": true, "report_reasons": null, "author": "datanerd_naive", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10sviok/can_data_science_work_flows_follow_agile_how_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10sviok/can_data_science_work_flows_follow_agile_how_do/", "subreddit_subscribers": 844576, "created_utc": 1675458972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am teaching a course on Big Data starting next week and had planned a project with the twitter API for exercising spark structured streaming. Of course, that might be rather difficult now. So I am wondering if there is a similar alternative to get a stream of text using an api which is either saved in a directory or kafka. I have mostly found examples where you would have to poll continuously like facebook. \n\nAny ideas? Thanks!", "author_fullname": "t2_dnnob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Twitter API alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t6ana", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675489456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am teaching a course on Big Data starting next week and had planned a project with the twitter API for exercising spark structured streaming. Of course, that might be rather difficult now. So I am wondering if there is a similar alternative to get a stream of text using an api which is either saved in a directory or kafka. I have mostly found examples where you would have to poll continuously like facebook. &lt;/p&gt;\n\n&lt;p&gt;Any ideas? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t6ana", "is_robot_indexable": true, "report_reasons": null, "author": "aziriel", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t6ana/twitter_api_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t6ana/twitter_api_alternative/", "subreddit_subscribers": 844576, "created_utc": 1675489456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, \n\nI believe that some data scientists/ML engineers with experience in this field could help me and other future graduates in this situation. \n\nI will be graduating from a Data Science BS in May and am currently searching for a job.\n\n&amp;#x200B;\n\n1. I currently see most entry-level offers requiring a year of experience. Should I still apply for these positions, or is there actually no chance of getting them?\n2. For interview preparation, LeetCode is pretty useful for Python and SQL. Do you have any advice on how to get ready for ML or statistics questions?\n\n&amp;#x200B;\n\nFeel free to DM me or comment any advise you have. \n\nThanks for your help.", "author_fullname": "t2_tf604sc2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Search strategy Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sw2g9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675460292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I believe that some data scientists/ML engineers with experience in this field could help me and other future graduates in this situation. &lt;/p&gt;\n\n&lt;p&gt;I will be graduating from a Data Science BS in May and am currently searching for a job.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I currently see most entry-level offers requiring a year of experience. Should I still apply for these positions, or is there actually no chance of getting them?&lt;/li&gt;\n&lt;li&gt;For interview preparation, LeetCode is pretty useful for Python and SQL. Do you have any advice on how to get ready for ML or statistics questions?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Feel free to DM me or comment any advise you have. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10sw2g9", "is_robot_indexable": true, "report_reasons": null, "author": "Motor-Ad8645", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10sw2g9/job_search_strategy_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10sw2g9/job_search_strategy_question/", "subreddit_subscribers": 844576, "created_utc": 1675460292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious about the UPenn MSE in Data Science program's statistics, and timeline. I was also wondering if anyone here has any experience with it. If you have, I'd love to hear about:\n\n1. Did you or someone you know enroll in the program?\n2. What was your admission stats like?\n3. How long did it take for the decision to arrive?\n4. What kind of job opportunities came up after finishing the program?\n5. Would you recommend the program to others who are already data scientists?\n\nAny info or thoughts you have would be awesome. Thanks!", "author_fullname": "t2_6gpihliq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MSE DS (Online) U Penn Admission Statistics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t2zba", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675478786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious about the UPenn MSE in Data Science program&amp;#39;s statistics, and timeline. I was also wondering if anyone here has any experience with it. If you have, I&amp;#39;d love to hear about:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Did you or someone you know enroll in the program?&lt;/li&gt;\n&lt;li&gt;What was your admission stats like?&lt;/li&gt;\n&lt;li&gt;How long did it take for the decision to arrive?&lt;/li&gt;\n&lt;li&gt;What kind of job opportunities came up after finishing the program?&lt;/li&gt;\n&lt;li&gt;Would you recommend the program to others who are already data scientists?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any info or thoughts you have would be awesome. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t2zba", "is_robot_indexable": true, "report_reasons": null, "author": "wardrobe_creator", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t2zba/mse_ds_online_u_penn_admission_statistics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t2zba/mse_ds_online_u_penn_admission_statistics/", "subreddit_subscribers": 844576, "created_utc": 1675478786.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Here is the link to the article: https://towardsdatascience.com/today-i-quit-data-sciences-here-are-7-reasons-why-15c29e51d032", "author_fullname": "t2_mv20pew7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectations for Data Scientists, a very interesting perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_10tnpsw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QkJnpthDPFBmmUR3V94UIbp1ZfZ2xZ1wSW30oz5-9dw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675535970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the link to the article: &lt;a href=\"https://towardsdatascience.com/today-i-quit-data-sciences-here-are-7-reasons-why-15c29e51d032\"&gt;https://towardsdatascience.com/today-i-quit-data-sciences-here-are-7-reasons-why-15c29e51d032&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/j6jjlhvoc9ga1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?auto=webp&amp;v=enabled&amp;s=723856c1f3a56e4f57869e84c989d5046ccfe54d", "width": 1179, "height": 1637}, "resolutions": [{"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a2fdfcd36388505ebc77874a2573ed827abc7c3", "width": 108, "height": 149}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f02a59fab63b11a67f9fa8e9f651f2eb95c82bf", "width": 216, "height": 299}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b89ae0e07f114d4e669dc55221599cf6b40d600", "width": 320, "height": 444}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ea54189da0b48e9084736b2e2bd7308d53b4c4e", "width": 640, "height": 888}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e44e0dfdf6d476c7a066a556df5d689dca96f693", "width": 960, "height": 1332}, {"url": "https://preview.redd.it/j6jjlhvoc9ga1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=914eed27bbee9e3dcf13d3587831c89d21f2b566", "width": 1080, "height": 1499}], "variants": {}, "id": "ALeTGh-u0ujUsoTVmsd6yjnZOItqRA0QfPgcJq3IeKw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tnpsw", "is_robot_indexable": true, "report_reasons": null, "author": "Calm_Inky", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tnpsw/expectations_for_data_scientists_a_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/j6jjlhvoc9ga1.jpg", "subreddit_subscribers": 844576, "created_utc": 1675535970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nI'm currently working on my masters and I'm having a hard time choosing which electives to choose from. I want to make sure I come out of the program as well-equipped as possible. I am able to choose four out of the following five classes as my remaining electives. I've also provided a synopsis of what each course will cover:  \n  \n\n* **Data Science for Business**: This course explores the various ways data and science can be applied to business contexts. Particular emphasis will be placed on using data to make informed business decisions.\n* **Principles of Python Programming**: Programming, problem solving and algorithmic thinking in Python. Topics include variables, input/output, conditional, statements/logic, Boolean expressions, flow control, loops and functions \n* **Data Manipulation**: This course focuses on the loading, manipulating, processing, cleaning, aggregating, and grouping of data. Students will practice on real world data sets, learning how to manipulate data using Python and continue their study of intermediate and advanced topics from the NumPy and Pandas libraries\n* **Information Visualization**: each students the best practices in Data Visualization, the key trends in the industry, and how to become great storytellers with data. Students taking this class will learn the importance of using actionable dashboards that enable their organizations to make data-driven decisions. For this class students will be exposed to Qlik and Tableau\n* **Applied Machine Learning**: This course will further explore modern machine learning applications such as decision trees and various ensemble learning methods including random forests. Special attention will be given to performing hyperparameter tuning to improve our models. Students will also focus on different dimensionality reduction techniques with emphasis on using principal component analysis. Students will explore the K-Nearest Neighbors algorithm and use it to build a machine learning project from scratch. Finally, students will study an introduction to using artificial neural networks to solve classification and regression problems. This course is entirely taught in Python.\n\nAny advice is much appreciated!", "author_fullname": "t2_1x7s010", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Masters Elective Selection Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t4rdc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675516985.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675484275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on my masters and I&amp;#39;m having a hard time choosing which electives to choose from. I want to make sure I come out of the program as well-equipped as possible. I am able to choose four out of the following five classes as my remaining electives. I&amp;#39;ve also provided a synopsis of what each course will cover:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Data Science for Business&lt;/strong&gt;: This course explores the various ways data and science can be applied to business contexts. Particular emphasis will be placed on using data to make informed business decisions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Principles of Python Programming&lt;/strong&gt;: Programming, problem solving and algorithmic thinking in Python. Topics include variables, input/output, conditional, statements/logic, Boolean expressions, flow control, loops and functions &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Manipulation&lt;/strong&gt;: This course focuses on the loading, manipulating, processing, cleaning, aggregating, and grouping of data. Students will practice on real world data sets, learning how to manipulate data using Python and continue their study of intermediate and advanced topics from the NumPy and Pandas libraries&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Information Visualization&lt;/strong&gt;: each students the best practices in Data Visualization, the key trends in the industry, and how to become great storytellers with data. Students taking this class will learn the importance of using actionable dashboards that enable their organizations to make data-driven decisions. For this class students will be exposed to Qlik and Tableau&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Applied Machine Learning&lt;/strong&gt;: This course will further explore modern machine learning applications such as decision trees and various ensemble learning methods including random forests. Special attention will be given to performing hyperparameter tuning to improve our models. Students will also focus on different dimensionality reduction techniques with emphasis on using principal component analysis. Students will explore the K-Nearest Neighbors algorithm and use it to build a machine learning project from scratch. Finally, students will study an introduction to using artificial neural networks to solve classification and regression problems. This course is entirely taught in Python.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice is much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t4rdc", "is_robot_indexable": true, "report_reasons": null, "author": "HercHuntsdirty", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t4rdc/masters_elective_selection_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t4rdc/masters_elective_selection_advice/", "subreddit_subscribers": 844576, "created_utc": 1675484275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My company has a $1k education stipend and I am not sure how best to spend it. I was given advice to not take a technical class but rather executive coaching/communication/non-technical class. I thought about toastmasters to work on presentations but I have been told it may not align well with technical presentations. Does anyone have any recommendations?", "author_fullname": "t2_tvatl21k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How best to spend an education stipend at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t0oxk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675472135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has a $1k education stipend and I am not sure how best to spend it. I was given advice to not take a technical class but rather executive coaching/communication/non-technical class. I thought about toastmasters to work on presentations but I have been told it may not align well with technical presentations. Does anyone have any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t0oxk", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate-Gur403", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t0oxk/how_best_to_spend_an_education_stipend_at_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t0oxk/how_best_to_spend_an_education_stipend_at_work/", "subreddit_subscribers": 844576, "created_utc": 1675472135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, at uni they told us that if the measured quantities are uncorrelatedwe need to use the quadratic formula to sum errors (sqrt(E1\\^2+...)). however they didn't exactly told us what correlated quantities are. For example, measuring current and voltage through a resistance in a circuit, are those 2 quantities correlated meassure? On internet I couldn't find a definitive answer as everything is so vague. I read that you need to calculate the covariance, but I didn't find what values determines if they are uncorrelated or not, for examples, a covariance of 0.07 means that the 2 values are correlated or uncorrelated from one another? I know this is very chaotic but it's 1 am and am very sleepy.", "author_fullname": "t2_1ow72dg7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to use the quadratic formula to evaluate errors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10szuli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675469776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, at uni they told us that if the measured quantities are uncorrelatedwe need to use the quadratic formula to sum errors (sqrt(E1^2+...)). however they didn&amp;#39;t exactly told us what correlated quantities are. For example, measuring current and voltage through a resistance in a circuit, are those 2 quantities correlated meassure? On internet I couldn&amp;#39;t find a definitive answer as everything is so vague. I read that you need to calculate the covariance, but I didn&amp;#39;t find what values determines if they are uncorrelated or not, for examples, a covariance of 0.07 means that the 2 values are correlated or uncorrelated from one another? I know this is very chaotic but it&amp;#39;s 1 am and am very sleepy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10szuli", "is_robot_indexable": true, "report_reasons": null, "author": "TurbulentDragon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10szuli/when_to_use_the_quadratic_formula_to_evaluate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10szuli/when_to_use_the_quadratic_formula_to_evaluate/", "subreddit_subscribers": 844576, "created_utc": 1675469776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I am familiar with machine learning, but mostly for making predictions like who to sell to or how many service tickets well get. I'm wondering if someone can link me an article or repo that shows how to do entity resolution by tokenizing things life first name and last name. Thanks.", "author_fullname": "t2_zivdx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Entity Resolution With Skleabr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10synhl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675466613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am familiar with machine learning, but mostly for making predictions like who to sell to or how many service tickets well get. I&amp;#39;m wondering if someone can link me an article or repo that shows how to do entity resolution by tokenizing things life first name and last name. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10synhl", "is_robot_indexable": true, "report_reasons": null, "author": "Meclimax", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10synhl/entity_resolution_with_skleabr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10synhl/entity_resolution_with_skleabr/", "subreddit_subscribers": 844576, "created_utc": 1675466613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi y'all,\n\nSo I'm currently a PM, and I'm looking into transitioning into Data Science. \n\nI have just have 2.5 years of industry experience, and I did my undergrad in Data Science, but I got a PM job right out of college.\n\nI'm currently doing an online masters in CS, to strengthen my technical skills further. \n\nAny suggestions on how i can make this transition, or if it's even worth it?", "author_fullname": "t2_6as9km3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Product Management to Data Science. Anyone done that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10sud7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675456204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y&amp;#39;all,&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m currently a PM, and I&amp;#39;m looking into transitioning into Data Science. &lt;/p&gt;\n\n&lt;p&gt;I have just have 2.5 years of industry experience, and I did my undergrad in Data Science, but I got a PM job right out of college.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently doing an online masters in CS, to strengthen my technical skills further. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions on how i can make this transition, or if it&amp;#39;s even worth it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10sud7a", "is_robot_indexable": true, "report_reasons": null, "author": "tvshowaddictbb", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10sud7a/transitioning_from_product_management_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10sud7a/transitioning_from_product_management_to_data/", "subreddit_subscribers": 844576, "created_utc": 1675456204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey folks - I wanted to put this live course from Rob Donnelly (Arena, Instacart and Facebook) on your radar!\n\nThe course looks at how to improve product and business decisions with causal inference. It draws on his experience at Instacart and Meta and features real-world examples from Amazon Prime and Facebook.\n\nIt kicks off on Feb 27 and you can find more info here:\n\n[https://www.getsphere.com/cohorts/applied-causal-inference?source=Sphere-Comm-r-ds](https://www.getsphere.com/cohorts/applied-causal-inference?source=Sphere-Comm-)", "author_fullname": "t2_2rh4tgqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rob Donnelly (Arena, Instacart, Facebook) Teaches Applied Causal Inference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10stpu2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1675456163.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1675454636.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks - I wanted to put this live course from Rob Donnelly (Arena, Instacart and Facebook) on your radar!&lt;/p&gt;\n\n&lt;p&gt;The course looks at how to improve product and business decisions with causal inference. It draws on his experience at Instacart and Meta and features real-world examples from Amazon Prime and Facebook.&lt;/p&gt;\n\n&lt;p&gt;It kicks off on Feb 27 and you can find more info here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.getsphere.com/cohorts/applied-causal-inference?source=Sphere-Comm-\"&gt;https://www.getsphere.com/cohorts/applied-causal-inference?source=Sphere-Comm-r-ds&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?auto=webp&amp;v=enabled&amp;s=21cef0048d2534eb1b314e3385ae3eff16233a92", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db4c139ea56f0bccff763152f27640f9d7833f17", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36355f77338af7794e7ee017d103d981fe1b78ba", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=212559afb252189781ff2b98d52d4d8d34ce5709", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=014e1cda0a7b65275583176409d7343edf309d6a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=edd7cf9106a82bdcbbe6f650f83cf3d3d1601281", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zqwx2grseG29Tx3GDyUNYEXNQnXpfCbX6rNdZHCQcts.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db24834fba38658c8b79edc4c31cb130c359d8b9", "width": 1080, "height": 607}], "variants": {}, "id": "u-STCoW1FioYTcMpyVF5v4Fk66tQpQ_mY2YnYKZrQDE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10stpu2", "is_robot_indexable": true, "report_reasons": null, "author": "lorenzo_1999", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10stpu2/rob_donnelly_arena_instacart_facebook_teaches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10stpu2/rob_donnelly_arena_instacart_facebook_teaches/", "subreddit_subscribers": 844576, "created_utc": 1675454636.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a problem where I'm looking at API calls and downstream impact. Not really a specific question to answer, but predicting performance of the calls based on path through the audit trail would be interesting to do. Or, anomaly detection (i.e, this API is defective in 10% of transactions)  \n\n\nI have thought of two approaches. One of them is regarding the data contained within the request/response payloads. The issue is, it's all text based, or changes depending on the specific API, so the data transformation aspect is very tough. Encoding one API's response to compare to a different API's response would be challenging. Perhaps there is an opportunity for deep learning here.  \n\n\nThe other approach I'm thinking about, is treating this as a graph problem. The majority of these transactions will form a tree-like structure, but some of them don't have a true root, so its more of a directed, acyclic graph (think it's called a polytree?). My approach would be to use graph theory principals such as degrees, closeness, vertex distance, etc. as features instead of parsing the very dynamic payloads. Since these are rules-based systems, I would love to be able to model the underlying relationships between these calls, that way when one transaction chain doesn't match the model, it's likely an anomaly.   \n\n\nDoes this make sense to do? Has anyone here done something similar? Any recommended literature? I've tried googling it, but when you search \"machine learning approach to tree-based systems\", you get a lot of papers or articles referencing tree-based ML models, which is not exactly what I'm looking for yet. Thoughts?", "author_fullname": "t2_1wcsnihu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applying Graph Theory in Machine Learning problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10to95b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675537246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a problem where I&amp;#39;m looking at API calls and downstream impact. Not really a specific question to answer, but predicting performance of the calls based on path through the audit trail would be interesting to do. Or, anomaly detection (i.e, this API is defective in 10% of transactions)  &lt;/p&gt;\n\n&lt;p&gt;I have thought of two approaches. One of them is regarding the data contained within the request/response payloads. The issue is, it&amp;#39;s all text based, or changes depending on the specific API, so the data transformation aspect is very tough. Encoding one API&amp;#39;s response to compare to a different API&amp;#39;s response would be challenging. Perhaps there is an opportunity for deep learning here.  &lt;/p&gt;\n\n&lt;p&gt;The other approach I&amp;#39;m thinking about, is treating this as a graph problem. The majority of these transactions will form a tree-like structure, but some of them don&amp;#39;t have a true root, so its more of a directed, acyclic graph (think it&amp;#39;s called a polytree?). My approach would be to use graph theory principals such as degrees, closeness, vertex distance, etc. as features instead of parsing the very dynamic payloads. Since these are rules-based systems, I would love to be able to model the underlying relationships between these calls, that way when one transaction chain doesn&amp;#39;t match the model, it&amp;#39;s likely an anomaly.   &lt;/p&gt;\n\n&lt;p&gt;Does this make sense to do? Has anyone here done something similar? Any recommended literature? I&amp;#39;ve tried googling it, but when you search &amp;quot;machine learning approach to tree-based systems&amp;quot;, you get a lot of papers or articles referencing tree-based ML models, which is not exactly what I&amp;#39;m looking for yet. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10to95b", "is_robot_indexable": true, "report_reasons": null, "author": "tigerclaw468", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10to95b/applying_graph_theory_in_machine_learning_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10to95b/applying_graph_theory_in_machine_learning_problem/", "subreddit_subscribers": 844576, "created_utc": 1675537246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose", "author_fullname": "t2_7gkixkov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you're new to databases should you start with the book Database Design for Mere Mortals or SQL Queries for Mere Mortals or Head first with sql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tadz8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675500777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tadz8", "is_robot_indexable": true, "report_reasons": null, "author": "One_Valuable7049", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tadz8/if_youre_new_to_databases_should_you_start_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tadz8/if_youre_new_to_databases_should_you_start_with/", "subreddit_subscribers": 844576, "created_utc": 1675500777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " What is a calculation to identify times when one set of time series data (dataset 1) has peaks/spikes and another set of simultaneous data (dataset 2) is stable and has more of a plateau pattern and no spikes or peaks at that same time? I tried graphing the difference between the two datasets, but this didn't work because there would be a high difference when both datasets are peaking if one dataset had a higher peak than the other. I only want to see times where one dataset has a peak and the other dataset does not have a peak at all.", "author_fullname": "t2_sa00umho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to identify time periods when one set of time series data has a peak at the same time as another set of simultanerous data is stable and has no peaks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t03b0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675470429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is a calculation to identify times when one set of time series data (dataset 1) has peaks/spikes and another set of simultaneous data (dataset 2) is stable and has more of a plateau pattern and no spikes or peaks at that same time? I tried graphing the difference between the two datasets, but this didn&amp;#39;t work because there would be a high difference when both datasets are peaking if one dataset had a higher peak than the other. I only want to see times where one dataset has a peak and the other dataset does not have a peak at all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t03b0", "is_robot_indexable": true, "report_reasons": null, "author": "gogogadgad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t03b0/how_to_identify_time_periods_when_one_set_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t03b0/how_to_identify_time_periods_when_one_set_of_time/", "subreddit_subscribers": 844576, "created_utc": 1675470429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How hard/easy will it be for me to get a data science job having this under my belt:\n\n\\- 2 years working as a Data Analyst\n\n\\- 2 years as Computational biologist in undergrad\n\n\\- Masters in Data Science", "author_fullname": "t2_6xabempy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some perspective (related to the DS job market):", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_10tnx8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675536470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How hard/easy will it be for me to get a data science job having this under my belt:&lt;/p&gt;\n\n&lt;p&gt;- 2 years working as a Data Analyst&lt;/p&gt;\n\n&lt;p&gt;- 2 years as Computational biologist in undergrad&lt;/p&gt;\n\n&lt;p&gt;- Masters in Data Science&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tnx8m", "is_robot_indexable": true, "report_reasons": null, "author": "marjose2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tnx8m/i_need_some_perspective_related_to_the_ds_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tnx8m/i_need_some_perspective_related_to_the_ds_job/", "subreddit_subscribers": 844576, "created_utc": 1675536470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm given a school assignment and one of the requirements is to visualize the important aspects and give meaningful realization of a real world dataset.\nSo I was wondering I someone could recommend me a dataset that is simple, real world, has some real use", "author_fullname": "t2_i3uokfwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please give data set recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tj322", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675524660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m given a school assignment and one of the requirements is to visualize the important aspects and give meaningful realization of a real world dataset.\nSo I was wondering I someone could recommend me a dataset that is simple, real world, has some real use&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tj322", "is_robot_indexable": true, "report_reasons": null, "author": "A_B_1_2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tj322/please_give_data_set_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tj322/please_give_data_set_recommendation/", "subreddit_subscribers": 844576, "created_utc": 1675524660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So as title says i have advanced to the next round of interviews. I am not really sure what to do next. For first round I got project to do at home (company really liked the results). Not really sure what to prepare next or even how many rounds there will be. Any suggestions on what to shift my focus to for next interview? It will be held by current data scientists of the firm.    \n    \nThis is my first interview for position of data scientist so i don't really know how it goes, any tips or guidance would be appriciated. Thanks.", "author_fullname": "t2_tv2fyu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advancing to the next round of interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tc11j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675503997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as title says i have advanced to the next round of interviews. I am not really sure what to do next. For first round I got project to do at home (company really liked the results). Not really sure what to prepare next or even how many rounds there will be. Any suggestions on what to shift my focus to for next interview? It will be held by current data scientists of the firm.    &lt;/p&gt;\n\n&lt;p&gt;This is my first interview for position of data scientist so i don&amp;#39;t really know how it goes, any tips or guidance would be appriciated. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tc11j", "is_robot_indexable": true, "report_reasons": null, "author": "CherryGG2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tc11j/advancing_to_the_next_round_of_interviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tc11j/advancing_to_the_next_round_of_interviews/", "subreddit_subscribers": 844576, "created_utc": 1675503997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work in telecom. These are fake numbers for illustrating purposes.\n\nI have customer A who has 20% market share in US overall. Let's say they have 1000 calls per month and they send 10% to NYC (100 calls).\n\nI am trying a win another customer so I have to estimate their call volume per city. I know they have 10% market share overall (500 calls). I need to estimate how many calls they send to NYC. I could use customer's A ratio (10%) but this ratio could be wrong. They could send 5% to NYC or 15% or something else.\n\nAny ideas how to estimate this?", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do market sizing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10svnyk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675459325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in telecom. These are fake numbers for illustrating purposes.&lt;/p&gt;\n\n&lt;p&gt;I have customer A who has 20% market share in US overall. Let&amp;#39;s say they have 1000 calls per month and they send 10% to NYC (100 calls).&lt;/p&gt;\n\n&lt;p&gt;I am trying a win another customer so I have to estimate their call volume per city. I know they have 10% market share overall (500 calls). I need to estimate how many calls they send to NYC. I could use customer&amp;#39;s A ratio (10%) but this ratio could be wrong. They could send 5% to NYC or 15% or something else.&lt;/p&gt;\n\n&lt;p&gt;Any ideas how to estimate this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10svnyk", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10svnyk/how_to_do_market_sizing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10svnyk/how_to_do_market_sizing/", "subreddit_subscribers": 844576, "created_utc": 1675459325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TL; DR:  I explore a hypothetical scenario where widget makers of varying experience are compared according to defect rate over a 5 year window.  To make things comparable (accounting for the experience problem), I inquire whether using a weighted moving average is a good strategy for balancing the problem of experience out or whether there is a better technique/solution to employ.  \n\n&amp;#x200B;\n\nThe Scenario:\n\n* Assume you were to look at the historical performance of  several widget makers who manually produce widgets.  These widget makers could be active employees at present  or left at some point during the window.\n* You want to compare the historical performance of the widget makers in terms of defect rate over the last 5 years (2018-2022).  My definition of defect rate is simply the  total number of defects/number of widgets produced (inclusive of defects)\n* Some makers have decades of experience while other makers only have 1 year of experience (minimum required for this analysis)\n* The original data is by year (2018,2019,...).  Let's assume Maker A, who started their career over a decade ago, made widgets over the entire window. Maker B has 3 years of experience but was let go sometime before 2022.  Maker C was recently hired and worked all of 2022.\n* To make things more intuitive (hopefully) and comparable, I converted the table from calendar years to years of experience within the time window. So Year 1 doesn't necessarily translate to 2018, it's simply represents the first year worked  in that time window. See table below.\n\n&amp;#x200B;\n\n|Maker|Year 1|2|3|4|5|\n|:-|:-|:-|:-|:-|:-|\n|A|2/17|2/15|3/11|1/8|2/10|\n|B|5/15|3/12|3/13|||\n|C|2/7|||||\n\n&amp;#x200B;\n\n*  Their defect rate is recorded over the last 5 years. For example Maker A produced 17 widgets of which 2 were defective in the first year of the time window, then produced 15 widgets in which 2 were defective Year 2, and so on...  Note that demand could be variable for widget production.\n* Simply looking at the defect rate above, my calculations  show Maker A would have an overall defect rate of .25, B would have .35 and C would have .43.  The problem I see  is less experience penalizes more heavily than more experience; so if this were a bar graph the 'worst performers' would likely always be the least experienced and that may not be very fair or  informative and at worst, hiding poor performers with a lot of experience.\n\nSo, this implies that some sort of smoothing or weighting is needed in order to make things more comparable. I was wondering if it made sense to use a weighted moving average.  My weighted average would look like the following:\n\n&amp;#x200B;\n\n* Each year of the window has an arbitrary weight associated to it.  For this example we want to downweight that first year  and gradually increase weights later on:\n\n&amp;#x200B;\n\n|Year |1|2|3|4|5|\n|:-|:-|:-|:-|:-|:-|\n|Weight|.6|.7|.8|.9|1|\n\n&amp;#x200B;\n\n* I obtain the sum of the product of each year's weight by  the maker's  defect rate  for each year they worked divided by the sum of the weights for the length of time that the maker worked.  Clarifying this last part - Maker A's denominator would be the sum of all 5 weights, B would be the sum of 3 weights and C would be only the Year 1 weight\n* Using the weight and formulas mentioned, I show that A has a weighted average defect rate of .17, B has .43 and C has .29, suggesting that B is producing more defects relative to their experience compared to the other two.\n\nThis results make some intuitive sense to me as B continued their career with a higher rate of defects relative to A.  My concern is that 1) weighting is arbitrary  and that 2) this method might cause an inverse problem from the one I was originally attempting to solve - giving too much leniency to the inexperienced and overly penalizing experience. I understand there is no perfect answer here but wondering if this is a sound approach or there are better, more intuitive ways to approach this. \n\nTIA", "author_fullname": "t2_2nzhcx9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is This A Good Method for Weighting Varying Levels of Experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tl1jk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675529596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL; DR:  I explore a hypothetical scenario where widget makers of varying experience are compared according to defect rate over a 5 year window.  To make things comparable (accounting for the experience problem), I inquire whether using a weighted moving average is a good strategy for balancing the problem of experience out or whether there is a better technique/solution to employ.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The Scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Assume you were to look at the historical performance of  several widget makers who manually produce widgets.  These widget makers could be active employees at present  or left at some point during the window.&lt;/li&gt;\n&lt;li&gt;You want to compare the historical performance of the widget makers in terms of defect rate over the last 5 years (2018-2022).  My definition of defect rate is simply the  total number of defects/number of widgets produced (inclusive of defects)&lt;/li&gt;\n&lt;li&gt;Some makers have decades of experience while other makers only have 1 year of experience (minimum required for this analysis)&lt;/li&gt;\n&lt;li&gt;The original data is by year (2018,2019,...).  Let&amp;#39;s assume Maker A, who started their career over a decade ago, made widgets over the entire window. Maker B has 3 years of experience but was let go sometime before 2022.  Maker C was recently hired and worked all of 2022.&lt;/li&gt;\n&lt;li&gt;To make things more intuitive (hopefully) and comparable, I converted the table from calendar years to years of experience within the time window. So Year 1 doesn&amp;#39;t necessarily translate to 2018, it&amp;#39;s simply represents the first year worked  in that time window. See table below.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Maker&lt;/th&gt;\n&lt;th align=\"left\"&gt;Year 1&lt;/th&gt;\n&lt;th align=\"left\"&gt;2&lt;/th&gt;\n&lt;th align=\"left\"&gt;3&lt;/th&gt;\n&lt;th align=\"left\"&gt;4&lt;/th&gt;\n&lt;th align=\"left\"&gt;5&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/17&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/15&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/11&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;5/15&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/12&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/13&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;C&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/7&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Their defect rate is recorded over the last 5 years. For example Maker A produced 17 widgets of which 2 were defective in the first year of the time window, then produced 15 widgets in which 2 were defective Year 2, and so on...  Note that demand could be variable for widget production.&lt;/li&gt;\n&lt;li&gt;Simply looking at the defect rate above, my calculations  show Maker A would have an overall defect rate of .25, B would have .35 and C would have .43.  The problem I see  is less experience penalizes more heavily than more experience; so if this were a bar graph the &amp;#39;worst performers&amp;#39; would likely always be the least experienced and that may not be very fair or  informative and at worst, hiding poor performers with a lot of experience.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So, this implies that some sort of smoothing or weighting is needed in order to make things more comparable. I was wondering if it made sense to use a weighted moving average.  My weighted average would look like the following:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Each year of the window has an arbitrary weight associated to it.  For this example we want to downweight that first year  and gradually increase weights later on:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Year&lt;/th&gt;\n&lt;th align=\"left\"&gt;1&lt;/th&gt;\n&lt;th align=\"left\"&gt;2&lt;/th&gt;\n&lt;th align=\"left\"&gt;3&lt;/th&gt;\n&lt;th align=\"left\"&gt;4&lt;/th&gt;\n&lt;th align=\"left\"&gt;5&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Weight&lt;/td&gt;\n&lt;td align=\"left\"&gt;.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I obtain the sum of the product of each year&amp;#39;s weight by  the maker&amp;#39;s  defect rate  for each year they worked divided by the sum of the weights for the length of time that the maker worked.  Clarifying this last part - Maker A&amp;#39;s denominator would be the sum of all 5 weights, B would be the sum of 3 weights and C would be only the Year 1 weight&lt;/li&gt;\n&lt;li&gt;Using the weight and formulas mentioned, I show that A has a weighted average defect rate of .17, B has .43 and C has .29, suggesting that B is producing more defects relative to their experience compared to the other two.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This results make some intuitive sense to me as B continued their career with a higher rate of defects relative to A.  My concern is that 1) weighting is arbitrary  and that 2) this method might cause an inverse problem from the one I was originally attempting to solve - giving too much leniency to the inexperienced and overly penalizing experience. I understand there is no perfect answer here but wondering if this is a sound approach or there are better, more intuitive ways to approach this. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tl1jk", "is_robot_indexable": true, "report_reasons": null, "author": "outskirtsofparadise", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tl1jk/is_this_a_good_method_for_weighting_varying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tl1jk/is_this_a_good_method_for_weighting_varying/", "subreddit_subscribers": 844576, "created_utc": 1675529596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_b71e9j7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happiness and Meaning in What We Do", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_10tkcts", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QUsQw0AQYHzN9jfM5MniTiDOMTzeUYYRu9QeubFh41U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1675527852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "flowingdata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://flowingdata.com/2023/01/11/happiness-and-meaning/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?auto=webp&amp;v=enabled&amp;s=021a7d0111ed3a520890d19700459aaf9ef6ba71", "width": 2002, "height": 1384}, "resolutions": [{"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9077316aa3da2500d96d9faac26bdcb2b03e0d11", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=285f586d50982f85347f71360137e14269ffbc6f", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c49429ab851f7ba714eb6e292e2735ff87296bc3", "width": 320, "height": 221}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d319403f4d62209f1d22d7f9b2773269d5444e4", "width": 640, "height": 442}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cc6865ef5ff462da12de89fa39069dbd6f3af55", "width": 960, "height": 663}, {"url": "https://external-preview.redd.it/sjEJtXRbyOYwUGQtZxyBSS4rKcwR1LfOXkHuawZLQNs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8a0770c97242367e386083afaf4e3eefd3176a8", "width": 1080, "height": 746}], "variants": {}, "id": "44qKXOapwlucIy6lsY36iDrLifWLEcJhv2rdvR-x3x8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tkcts", "is_robot_indexable": true, "report_reasons": null, "author": "fchung", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tkcts/happiness_and_meaning_in_what_we_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://flowingdata.com/2023/01/11/happiness-and-meaning/", "subreddit_subscribers": 844576, "created_utc": 1675527852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nHi, Anyone has an idea how can i perform the regression between the convolution result and the vecto r of non black voxels? I just tried so many times\n\n%procedure in order to eliminate the black voxel (the background)\n\nthreshold = 250;\n\ndata(data &lt; threshold) = 0;\n\n% Get linear indices of non-zero voxels\n\nnonZeroIndices = find(data);\n\n% Convert linear indices to (x, y, z) coordinates\n\n\\[x, y, z\\] = ind2sub(size(data), nonZeroIndices);\n\nvector=\\[x,y,z\\];\n\n% Create new histogram with non-zero voxels\n\nhist(data(nonZeroIndices));\n\nxlabel('gray level')\n\nylabel('Frequency')\n\ntitle('Histogram without black voxels')\n\n%defining the signals during the stimuli and during the resting phase\n\nt = 0:1:160; % time vector\n\nf = 1/20; % frequency of the square signal\n\nsquare\\_signal = zeros(size(t)); % initialize square\\_signal as a vector of zeros\n\nsquare\\_signal(mod(t, 2\\*1/f) &lt; 1/f) = 1; % set values within one period to 1 using ones function\n\nsquare\\_signal(end-floor(1/f):end) = 0; % set the last period to zero\n\n%convolution between the signal and the bold equation\n\na1=6;\n\na2=12;\n\nb1=0.9;\n\nb2=0.9;\n\nc=0.35;\n\nd1=a1\\*b1;\n\nd2=a2\\*b2;\n\nbold\\_eq= @(t) ((t/d1).\\^(a1)).\\*exp(-(t-d1)/b1)-c.\\*((t/d2).\\^(a2)).\\*exp(-(t-d2)/b2);\n\n% zero-pad the bold\\_eq to the same length as t\n\nbold\\_eq\\_pad = zeros(1, length(t) + length(square\\_signal) - 1);\n\nbold\\_eq\\_pad(1:length(t)) = bold\\_eq(t);\n\n% perform the convolution\n\nresult = conv(square\\_signal, bold\\_eq\\_pad);", "author_fullname": "t2_uj49bh3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regression Matlab", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10tdkup", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675507493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, Anyone has an idea how can i perform the regression between the convolution result and the vecto r of non black voxels? I just tried so many times&lt;/p&gt;\n\n&lt;p&gt;%procedure in order to eliminate the black voxel (the background)&lt;/p&gt;\n\n&lt;p&gt;threshold = 250;&lt;/p&gt;\n\n&lt;p&gt;data(data &amp;lt; threshold) = 0;&lt;/p&gt;\n\n&lt;p&gt;% Get linear indices of non-zero voxels&lt;/p&gt;\n\n&lt;p&gt;nonZeroIndices = find(data);&lt;/p&gt;\n\n&lt;p&gt;% Convert linear indices to (x, y, z) coordinates&lt;/p&gt;\n\n&lt;p&gt;[x, y, z] = ind2sub(size(data), nonZeroIndices);&lt;/p&gt;\n\n&lt;p&gt;vector=[x,y,z];&lt;/p&gt;\n\n&lt;p&gt;% Create new histogram with non-zero voxels&lt;/p&gt;\n\n&lt;p&gt;hist(data(nonZeroIndices));&lt;/p&gt;\n\n&lt;p&gt;xlabel(&amp;#39;gray level&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;ylabel(&amp;#39;Frequency&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;title(&amp;#39;Histogram without black voxels&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;%defining the signals during the stimuli and during the resting phase&lt;/p&gt;\n\n&lt;p&gt;t = 0:1:160; % time vector&lt;/p&gt;\n\n&lt;p&gt;f = 1/20; % frequency of the square signal&lt;/p&gt;\n\n&lt;p&gt;square_signal = zeros(size(t)); % initialize square_signal as a vector of zeros&lt;/p&gt;\n\n&lt;p&gt;square_signal(mod(t, 2*1/f) &amp;lt; 1/f) = 1; % set values within one period to 1 using ones function&lt;/p&gt;\n\n&lt;p&gt;square_signal(end-floor(1/f):end) = 0; % set the last period to zero&lt;/p&gt;\n\n&lt;p&gt;%convolution between the signal and the bold equation&lt;/p&gt;\n\n&lt;p&gt;a1=6;&lt;/p&gt;\n\n&lt;p&gt;a2=12;&lt;/p&gt;\n\n&lt;p&gt;b1=0.9;&lt;/p&gt;\n\n&lt;p&gt;b2=0.9;&lt;/p&gt;\n\n&lt;p&gt;c=0.35;&lt;/p&gt;\n\n&lt;p&gt;d1=a1*b1;&lt;/p&gt;\n\n&lt;p&gt;d2=a2*b2;&lt;/p&gt;\n\n&lt;p&gt;bold_eq= @(t) ((t/d1).^(a1)).*exp(-(t-d1)/b1)-c.*((t/d2).^(a2)).*exp(-(t-d2)/b2);&lt;/p&gt;\n\n&lt;p&gt;% zero-pad the bold_eq to the same length as t&lt;/p&gt;\n\n&lt;p&gt;bold_eq_pad = zeros(1, length(t) + length(square_signal) - 1);&lt;/p&gt;\n\n&lt;p&gt;bold_eq_pad(1:length(t)) = bold_eq(t);&lt;/p&gt;\n\n&lt;p&gt;% perform the convolution&lt;/p&gt;\n\n&lt;p&gt;result = conv(square_signal, bold_eq_pad);&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10tdkup", "is_robot_indexable": true, "report_reasons": null, "author": "MrBrain_99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10tdkup/regression_matlab/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10tdkup/regression_matlab/", "subreddit_subscribers": 844576, "created_utc": 1675507493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, so I am a fresher of Data Science, I am trying to apply to the Data Scientist position at one company. Here are the question they ask me, I just want to ask the structure to find the answer. Thank you. \n\n ***At 6 AM today, you purchased 1 MW of electricity contract for 12 PM at a price of 100 pounds/MWh. Two hours later, the forecast for solar generation for 12 PM has changed from 4 GW to 4.5 GW. The market is currently bid at 95 pounds/MWh and offered at 105 pounds/MWh. What would you do, and why? Please answer logically, stating all assumptions.*** \n\n**\\*Note that no additional research is needed.\u00a0\\***", "author_fullname": "t2_e0hr0ptp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Question Asked Before Interview]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t3kj3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675480591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, so I am a fresher of Data Science, I am trying to apply to the Data Scientist position at one company. Here are the question they ask me, I just want to ask the structure to find the answer. Thank you. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;At 6 AM today, you purchased 1 MW of electricity contract for 12 PM at a price of 100 pounds/MWh. Two hours later, the forecast for solar generation for 12 PM has changed from 4 GW to 4.5 GW. The market is currently bid at 95 pounds/MWh and offered at 105 pounds/MWh. What would you do, and why? Please answer logically, stating all assumptions.&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;*Note that no additional research is needed.\u00a0\\&lt;/strong&gt;*&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t3kj3", "is_robot_indexable": true, "report_reasons": null, "author": "elgrabielnino", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t3kj3/question_asked_before_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t3kj3/question_asked_before_interview/", "subreddit_subscribers": 844576, "created_utc": 1675480591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I have multivariate dataset which i need to predict 4 variables off of it. I know that these 4 variables decline over time and have a relationship with the other dependent variables. So I want to predict these 4 variables and their trend for the next year. Obviously I do not have the data for the dependent variables over next year. Any ideas on how to start? I built an LSTM model to predict the 4 variables already but it needs of course the dependent variables as input for next year.", "author_fullname": "t2_966j5hjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Series Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10t3ejl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1675480082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have multivariate dataset which i need to predict 4 variables off of it. I know that these 4 variables decline over time and have a relationship with the other dependent variables. So I want to predict these 4 variables and their trend for the next year. Obviously I do not have the data for the dependent variables over next year. Any ideas on how to start? I built an LSTM model to predict the 4 variables already but it needs of course the dependent variables as input for next year.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "10t3ejl", "is_robot_indexable": true, "report_reasons": null, "author": "Klutzy_Court1591", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/10t3ejl/time_series_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/10t3ejl/time_series_analysis/", "subreddit_subscribers": 844576, "created_utc": 1675480082.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}