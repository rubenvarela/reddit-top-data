{"kind": "Listing", "data": {"after": "t3_14mg6hm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Now in Snowflake: GROUP BY ALL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_14midyu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 251, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 251, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-SGlAIBI2vHhnDi7bOpijE2FAWxPT60Usg34asS3kZo.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1688077232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/Kh5unKB.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YcIvg7ZeBxPbAdje28vrFzMVrY-6WyeSEg2ikd7IVXE.jpg?auto=webp&amp;v=enabled&amp;s=00a32ce57bb0813b42bbf29fbf3ba37ed187eee3", "width": 900, "height": 614}, "resolutions": [{"url": "https://external-preview.redd.it/YcIvg7ZeBxPbAdje28vrFzMVrY-6WyeSEg2ikd7IVXE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09c0f49321beeb6497fef61cc4652ae827aa78e6", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/YcIvg7ZeBxPbAdje28vrFzMVrY-6WyeSEg2ikd7IVXE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8876c08cf042dbb48c2acbb733a4924d1975281", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/YcIvg7ZeBxPbAdje28vrFzMVrY-6WyeSEg2ikd7IVXE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e105d5380611472551c8da6e064f35b70887a3f", "width": 320, "height": 218}, {"url": "https://external-preview.redd.it/YcIvg7ZeBxPbAdje28vrFzMVrY-6WyeSEg2ikd7IVXE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=840680c7d96dec40e33e33e03342cb7148e7630b", "width": 640, "height": 436}], "variants": {}, "id": "Y6vV9sD9BGWBgVPu64-LAUmOCPZzG3Xk7KXvgmxYXNs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14midyu", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/14midyu/now_in_snowflake_group_by_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/Kh5unKB.jpg", "subreddit_subscribers": 113242, "created_utc": 1688077232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you guys think ?\n\nhttps://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark\n\nhttps://github.com/databrickslabs/pyspark-ai", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Our Days Are Numbered ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mkrvv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688083312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys think ?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark\"&gt;https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/databrickslabs/pyspark-ai\"&gt;https://github.com/databrickslabs/pyspark-ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?auto=webp&amp;v=enabled&amp;s=07702af3ccbcfb45ca9ca0246ff11956a407d034", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94745d07ed74df5060f4a5ec13f5d4c4772b6799", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf88a51c7a8a1026f7039142233eacac3b090be2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee1d2efc25fec8f150d98bd439b41b3544464ec9", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2f8f4a08c330da70ff1921abe6c7c9dcff8ae28", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd149b2f8b4ab02f257c861959ec52860e8646dc", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/80FpuMKaP-uF7j-NvgbUI5Z7r1khj2YfJVE3-fz-GI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=048373236a1eca4e14922ba32189012cdab346c2", "width": 1080, "height": 565}], "variants": {}, "id": "-GPKg0u5dq3hDp5gpC5cw1mkM8UDz08oLmsytjlET5M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14mkrvv", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 73, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mkrvv/our_days_are_numbered/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mkrvv/our_days_are_numbered/", "subreddit_subscribers": 113242, "created_utc": 1688083312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of day to day is working with python scripts (with a smattering of spark, pandas, etc) with pipelines moving data to/from Postgres/SQL server/Snowflake. The team I'm on is very comfortable with python, but we're exploring using duckdb or glaredb in some spots for data transformation, both for performance and how well sql maps to these transformations.\n\nWe're still hammering out what exactly this would look like, but I thought we could get some outside opinions on using either of these projects in our pipelines. Concretely, has anyone introduced either of these projects into their pipelines, and how did that go? Any pitfalls?\n\nFor reference:\n\nDuckdb: [https://github.com/duckdb/duckdb](https://github.com/duckdb/duckdb) \\- seems pretty popular, been keeping an eye on this for close to a year now.\n\nGlaredb: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb) \\- just heard about this last week. We played around with hooking directly into snowflake, so that was cool, but I haven't heard of anyone else using it.\n\nAny other projects like this that I'm missing?", "author_fullname": "t2_tlibil3a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using SQL inside Python pipelines with Duckdb, Glaredb (and others?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14n1xjw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688135234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of day to day is working with python scripts (with a smattering of spark, pandas, etc) with pipelines moving data to/from Postgres/SQL server/Snowflake. The team I&amp;#39;m on is very comfortable with python, but we&amp;#39;re exploring using duckdb or glaredb in some spots for data transformation, both for performance and how well sql maps to these transformations.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re still hammering out what exactly this would look like, but I thought we could get some outside opinions on using either of these projects in our pipelines. Concretely, has anyone introduced either of these projects into their pipelines, and how did that go? Any pitfalls?&lt;/p&gt;\n\n&lt;p&gt;For reference:&lt;/p&gt;\n\n&lt;p&gt;Duckdb: &lt;a href=\"https://github.com/duckdb/duckdb\"&gt;https://github.com/duckdb/duckdb&lt;/a&gt; - seems pretty popular, been keeping an eye on this for close to a year now.&lt;/p&gt;\n\n&lt;p&gt;Glaredb: &lt;a href=\"https://github.com/GlareDB/glaredb\"&gt;https://github.com/GlareDB/glaredb&lt;/a&gt; - just heard about this last week. We played around with hooking directly into snowflake, so that was cool, but I haven&amp;#39;t heard of anyone else using it.&lt;/p&gt;\n\n&lt;p&gt;Any other projects like this that I&amp;#39;m missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CMNIDTw44_ZkcOXcAsz9eSr0xJJk21Rp4sDzSmpPEqg.jpg?auto=webp&amp;v=enabled&amp;s=bd710c3ad880e33fa51723bcf550c6b3dbc30b58", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/CMNIDTw44_ZkcOXcAsz9eSr0xJJk21Rp4sDzSmpPEqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d599b1eb06cee45320e07eb1dd0713dc5549d11", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/CMNIDTw44_ZkcOXcAsz9eSr0xJJk21Rp4sDzSmpPEqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=496239147ca765153ea290ae11411ba76b498cfb", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/CMNIDTw44_ZkcOXcAsz9eSr0xJJk21Rp4sDzSmpPEqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=651acaced3eca5a85485b6977a917726b31cf098", "width": 320, "height": 320}], "variants": {}, "id": "tWw9eIdR80nmgZaRh-6bxdRnIteNj4-dUri_8Fs9NGg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14n1xjw", "is_robot_indexable": true, "report_reasons": null, "author": "lackbookpro", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n1xjw/using_sql_inside_python_pipelines_with_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14n1xjw/using_sql_inside_python_pipelines_with_duckdb/", "subreddit_subscribers": 113242, "created_utc": 1688135234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys,  \nAs i didn't find any resources describing the whole chain allowing to test deploying Airflow and Spark within a Kubernetes cluster.  \nI have wrote a description of the whole process to get it work in local.  \n[https://medium.com/p/869c6b48a026](https://medium.com/p/869c6b48a026)\n\nI hope this could help any begginer stucking on running Airflow and Apache Spark in a local machine.", "author_fullname": "t2_ae4tw1pnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Airflow, Spark, and Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mc2m9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688062286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;br/&gt;\nAs i didn&amp;#39;t find any resources describing the whole chain allowing to test deploying Airflow and Spark within a Kubernetes cluster.&lt;br/&gt;\nI have wrote a description of the whole process to get it work in local.&lt;br/&gt;\n&lt;a href=\"https://medium.com/p/869c6b48a026\"&gt;https://medium.com/p/869c6b48a026&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I hope this could help any begginer stucking on running Airflow and Apache Spark in a local machine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QYi-iUbo0f9piut-pcH5kjBpqw6QMfeuLZ6kf61bJ7U.jpg?auto=webp&amp;v=enabled&amp;s=d2f8396770daa068b1b5e9330e7001f5c1af2bb6", "width": 574, "height": 312}, "resolutions": [{"url": "https://external-preview.redd.it/QYi-iUbo0f9piut-pcH5kjBpqw6QMfeuLZ6kf61bJ7U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06655eb8c95104e0e0453fb7670a60a63f0dd2ee", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/QYi-iUbo0f9piut-pcH5kjBpqw6QMfeuLZ6kf61bJ7U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63c69afe02189654b1523c7d194fa29ea686771d", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/QYi-iUbo0f9piut-pcH5kjBpqw6QMfeuLZ6kf61bJ7U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=190aee51e5a8747ac2b7cc50859079731f4aa660", "width": 320, "height": 173}], "variants": {}, "id": "lPMLX4UOLiG31L-cTEzW4wWugTTDUOqJo0ISp0dvBM8"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_b4ff447e-05a5-42dc-9002-63568807cfe6", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A glowing commendation for all to see", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "All-Seeing Upvote", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=25880d00e4283ff6a3851b64c83fea465c3fac48", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=67fae0c2af26488b9d70cb7afe877086707cf1d1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=3033c6583809a27b998883b7c56c158102cb0420", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=cccb48d7518eaba72894a7ac4214bb70c7120793", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=4101e0eff424bbd818195853387db358bec74ed0", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14mc2m9", "is_robot_indexable": true, "report_reasons": null, "author": "PhysicalTomorrow2098", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mc2m9/apache_airflow_spark_and_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mc2m9/apache_airflow_spark_and_kubernetes/", "subreddit_subscribers": 113242, "created_utc": 1688062286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my org, there's a concern about vendor lock and how they could ramp up the costs once we are dependent on them.  My viewpoint is that that's unlikely given the competition between vendors trying to win market share.  More likely is that rising costs will be due to increasing adoption (it's working so well we move more functions here) rather than unit cost increases (your compute unit was X and now it's X\\*1.2)\n\nWondering if anyone has any actual data, either anecdotal or hard data about pricing trends?", "author_fullname": "t2_4y8hfgky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost trend for tools like Snowflake, BigQuery, RedShift, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mmqew", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688088835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my org, there&amp;#39;s a concern about vendor lock and how they could ramp up the costs once we are dependent on them.  My viewpoint is that that&amp;#39;s unlikely given the competition between vendors trying to win market share.  More likely is that rising costs will be due to increasing adoption (it&amp;#39;s working so well we move more functions here) rather than unit cost increases (your compute unit was X and now it&amp;#39;s X*1.2)&lt;/p&gt;\n\n&lt;p&gt;Wondering if anyone has any actual data, either anecdotal or hard data about pricing trends?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14mmqew", "is_robot_indexable": true, "report_reasons": null, "author": "DarkSolarLamp", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mmqew/cost_trend_for_tools_like_snowflake_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mmqew/cost_trend_for_tools_like_snowflake_bigquery/", "subreddit_subscribers": 113242, "created_utc": 1688088835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I completed an interview loop with a position that was titled DE, but it's really a backend engineer who might do some light pipeline work. I am familiar with the typical DE tech stack including Python, SQL, AWS services, Airflow, Spark, etc. They are moving data from source to Snowflake using Kafka, protobuf/gRPC while using languages like Go, some Python, and sometimes have to do some front end work using Typescript and React. It seemed like a lot of stuff I don't have familiarity with, but also sounded exciting.\n\nI've kind of always wanted to make the transition to SWE. Has anyone done this? How difficult was it? I don't want to join a place and then fail. During my technical rounds, I was expecting to have to do SQL and light python, but ended up having to do leetcode and another python round. I asked based on my different background if the hiring manager saw any issues, and he said my background seemed great.", "author_fullname": "t2_5pjz5m35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE to Backend SWE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14metqr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688068833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed an interview loop with a position that was titled DE, but it&amp;#39;s really a backend engineer who might do some light pipeline work. I am familiar with the typical DE tech stack including Python, SQL, AWS services, Airflow, Spark, etc. They are moving data from source to Snowflake using Kafka, protobuf/gRPC while using languages like Go, some Python, and sometimes have to do some front end work using Typescript and React. It seemed like a lot of stuff I don&amp;#39;t have familiarity with, but also sounded exciting.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve kind of always wanted to make the transition to SWE. Has anyone done this? How difficult was it? I don&amp;#39;t want to join a place and then fail. During my technical rounds, I was expecting to have to do SQL and light python, but ended up having to do leetcode and another python round. I asked based on my different background if the hiring manager saw any issues, and he said my background seemed great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14metqr", "is_robot_indexable": true, "report_reasons": null, "author": "maraskooknah", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14metqr/de_to_backend_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14metqr/de_to_backend_swe/", "subreddit_subscribers": 113242, "created_utc": 1688068833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm here seeking your valuable insights and opinions on data strategies, particularly in the context of my organization's current situation. We have made some progress so far, but we're eager to explore what more we can do to enhance our data practices.\n\nHere's a summary of our current data setup: We have sourced data from multiple channels, applied cleansing techniques (such as duplicate removal), and implemented SCD2 (Slowly Changing Dimensions) in our data layer. Our tech stack includes AWS, Glue, DBT, and Redshift, where the final cleansed data resides.\n\n&amp;#x200B;\n\n1. What additional steps can my organization take to improve our data strategy?\n\n&amp;#x200B;\n\n2. How can we maximize the value of our cleansed data in Redshift?\n\n&amp;#x200B;\n\n3. What are open sources available to add more value to the data, infra , DevOps, etc..\n\n&amp;#x200B;\n\n4. Are there any cost-effective solutions or techniques that we should consider implementing to improve data quality, data governance, or data management? I.e Open Source\n\n&amp;#x200B;\n\n5. Data observability, Precise Alerts, and notifications.  \n\n\n6. AI on Data and analytics. \n\nI'm good to hear about your experiences, suggestions, and success stories related to data strategies.\n\n&amp;#x200B;", "author_fullname": "t2_3sqs3uub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's Your Data Strategy? Help us take it to the next level! :)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14n38qz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688138256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m here seeking your valuable insights and opinions on data strategies, particularly in the context of my organization&amp;#39;s current situation. We have made some progress so far, but we&amp;#39;re eager to explore what more we can do to enhance our data practices.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a summary of our current data setup: We have sourced data from multiple channels, applied cleansing techniques (such as duplicate removal), and implemented SCD2 (Slowly Changing Dimensions) in our data layer. Our tech stack includes AWS, Glue, DBT, and Redshift, where the final cleansed data resides.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What additional steps can my organization take to improve our data strategy?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How can we maximize the value of our cleansed data in Redshift?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are open sources available to add more value to the data, infra , DevOps, etc..&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are there any cost-effective solutions or techniques that we should consider implementing to improve data quality, data governance, or data management? I.e Open Source&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Data observability, Precise Alerts, and notifications.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AI on Data and analytics. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m good to hear about your experiences, suggestions, and success stories related to data strategies.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14n38qz", "is_robot_indexable": true, "report_reasons": null, "author": "priyasweety1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n38qz/whats_your_data_strategy_help_us_take_it_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14n38qz/whats_your_data_strategy_help_us_take_it_to_the/", "subreddit_subscribers": 113242, "created_utc": 1688138256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "7 Data Engineering Projects To Put On Your Resume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14mtvvj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PLx3HovBpEko74J2ku_LaazyWwWTedoLC72Ra6FYJis.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688111327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coriers/7-data-engineering-projects-to-put-on-your-resume-aeccacc704c3", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L7IkC9-25FV0mRaMfjE0CZQWu4ZVhjOL_1gscN8U5U0.jpg?auto=webp&amp;v=enabled&amp;s=d1e358deae8b286714077c80963b8301a08a938b", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/L7IkC9-25FV0mRaMfjE0CZQWu4ZVhjOL_1gscN8U5U0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02d7ef55208dc44eaaf10a108bb89e9d9f941e74", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/L7IkC9-25FV0mRaMfjE0CZQWu4ZVhjOL_1gscN8U5U0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36fb28a1288b3000063527032718c509daf4a2f7", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/L7IkC9-25FV0mRaMfjE0CZQWu4ZVhjOL_1gscN8U5U0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbd83f900d93ce2f09e4ac9624463fab10d06743", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/L7IkC9-25FV0mRaMfjE0CZQWu4ZVhjOL_1gscN8U5U0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d882e302c4cca1221777e95d5c623a505692df7c", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/L7IkC9-25FV0mRaMfjE0CZQWu4ZVhjOL_1gscN8U5U0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a34bc6a729b204168556fa6d573806f74847c13", "width": 960, "height": 540}], "variants": {}, "id": "aACovfd828J_nyZJmEXw1CV2teUDkcpXRa1nzgDjqW0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14mtvvj", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mtvvj/7_data_engineering_projects_to_put_on_your_resume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coriers/7-data-engineering-projects-to-put-on-your-resume-aeccacc704c3", "subreddit_subscribers": 113242, "created_utc": 1688111327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just scored a 2nd interview at a company for the role of Data Engineer, half of the interview will be behavioral half will be technical python/ SQL coding. While my python/ SQL skills are good, they're a little bit rusty atm. I have DS Masters from GA Tech\n\nIs there any website out there where I can just drill python related questions back to back for the next week to work out any of the rust?\n\nThanks!", "author_fullname": "t2_hy6y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Prepare for 2nd Round Technical Interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mgc3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688072349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just scored a 2nd interview at a company for the role of Data Engineer, half of the interview will be behavioral half will be technical python/ SQL coding. While my python/ SQL skills are good, they&amp;#39;re a little bit rusty atm. I have DS Masters from GA Tech&lt;/p&gt;\n\n&lt;p&gt;Is there any website out there where I can just drill python related questions back to back for the next week to work out any of the rust?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14mgc3s", "is_robot_indexable": true, "report_reasons": null, "author": "SonOfaSaracen", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mgc3s/how_to_prepare_for_2nd_round_technical_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mgc3s/how_to_prepare_for_2nd_round_technical_interview/", "subreddit_subscribers": 113242, "created_utc": 1688072349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Leaving my more established company to be the sole DE for a small start-up. My current workflow in GCP is to get data into GCS -&gt; Orchestrate in Composer (Airflow) -&gt; Create and delete staging tables in BQ using the DAG operations + Biz Logic -&gt; Finalized table. \n\nI'm pretty comfortable with this workflow and I was wondering how best to replicate it using AWS. I know they have S3 and Airflow, but I'm not sure if Redshift is just as flexible. I saw a thread where someone said they used Athena to mimic this workflow. Don't think the data is big enough for Glue or even Redshift but I want to use some sort of DW.\n\nAlso as I'm building from the ground up, feedback on this architecture is encouraged, and I would to know if what I'm trying to replicate can be improved on. ", "author_fullname": "t2_geamh3dd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Cloud Question GCP -&gt; AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mqvsc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688101362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Leaving my more established company to be the sole DE for a small start-up. My current workflow in GCP is to get data into GCS -&amp;gt; Orchestrate in Composer (Airflow) -&amp;gt; Create and delete staging tables in BQ using the DAG operations + Biz Logic -&amp;gt; Finalized table. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pretty comfortable with this workflow and I was wondering how best to replicate it using AWS. I know they have S3 and Airflow, but I&amp;#39;m not sure if Redshift is just as flexible. I saw a thread where someone said they used Athena to mimic this workflow. Don&amp;#39;t think the data is big enough for Glue or even Redshift but I want to use some sort of DW.&lt;/p&gt;\n\n&lt;p&gt;Also as I&amp;#39;m building from the ground up, feedback on this architecture is encouraged, and I would to know if what I&amp;#39;m trying to replicate can be improved on. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14mqvsc", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency_Estate_866", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mqvsc/new_cloud_question_gcp_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mqvsc/new_cloud_question_gcp_aws/", "subreddit_subscribers": 113242, "created_utc": 1688101362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd like to make a business case to my bosses that we can save a ton of money on human labor by beefing up some of our cloud spending. Anything from something as trivial as giving each engineer on my team a dedicated VM (as opposed to sharing them and running out of RAM), to spending more on our cloud compute for auto scaling so we don't need to worry about spending months designing our systems to be more fault tolerant of the days when users decide once per month to hit our system all at once.\n\nMy issue is other than my boss the CTO, who also favors automation, none of the rest of the C-Suite understands what a typical cloud bill in our industry is and may look at my ask and say \"we could hire 3 FTEs to do xyz in other departments with the money your asking for\", which I can counter with the hypothetical that hiring 2 software engineers to do this stuff is more costly. \n\n Compared to past companies I've worked for which support a similar number of customers, our current cloud bill is fractions of what we spent in the past for similar problems. Obviously I can't go sharing that out loud with specific numbers since it's non public information, but I'm curious if there are publicly available well respected benchmarks I can reference?", "author_fullname": "t2_decct72a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good resources for average cloud bill benchmarking for different company sizes/ industries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mc7wg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688062634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to make a business case to my bosses that we can save a ton of money on human labor by beefing up some of our cloud spending. Anything from something as trivial as giving each engineer on my team a dedicated VM (as opposed to sharing them and running out of RAM), to spending more on our cloud compute for auto scaling so we don&amp;#39;t need to worry about spending months designing our systems to be more fault tolerant of the days when users decide once per month to hit our system all at once.&lt;/p&gt;\n\n&lt;p&gt;My issue is other than my boss the CTO, who also favors automation, none of the rest of the C-Suite understands what a typical cloud bill in our industry is and may look at my ask and say &amp;quot;we could hire 3 FTEs to do xyz in other departments with the money your asking for&amp;quot;, which I can counter with the hypothetical that hiring 2 software engineers to do this stuff is more costly. &lt;/p&gt;\n\n&lt;p&gt;Compared to past companies I&amp;#39;ve worked for which support a similar number of customers, our current cloud bill is fractions of what we spent in the past for similar problems. Obviously I can&amp;#39;t go sharing that out loud with specific numbers since it&amp;#39;s non public information, but I&amp;#39;m curious if there are publicly available well respected benchmarks I can reference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14mc7wg", "is_robot_indexable": true, "report_reasons": null, "author": "Dull_Lettuce_4622", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mc7wg/any_good_resources_for_average_cloud_bill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mc7wg/any_good_resources_for_average_cloud_bill/", "subreddit_subscribers": 113242, "created_utc": 1688062634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data pipeline to runs daily, transporting data from Area A to Area B. \n\nOnce the data arrives in Area B, it is immediately consumed to deliver a notification.\n\nI have created business &amp; logic checks to ensure that the data produced is of quality but I'd also like to test the actual numbers that are produced.\n\nA typical week looks like this:\n\n* Monday: 350 \n* Tuesday: 57,000\n* Wednesday: 24,000\n* Thursday: 20,000\n* Friday: 48,000\n* Saturday: 33,000\n* Sunday: 1,100\n\nThese numbers will fluctuate but their proportion relative to each other will stay the same. \n\nUsing only SQL, how would you go about testing this output? \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_ppxqmnml", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to go about testing data pipeline output?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mpveu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688098141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data pipeline to runs daily, transporting data from Area A to Area B. &lt;/p&gt;\n\n&lt;p&gt;Once the data arrives in Area B, it is immediately consumed to deliver a notification.&lt;/p&gt;\n\n&lt;p&gt;I have created business &amp;amp; logic checks to ensure that the data produced is of quality but I&amp;#39;d also like to test the actual numbers that are produced.&lt;/p&gt;\n\n&lt;p&gt;A typical week looks like this:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Monday: 350 &lt;/li&gt;\n&lt;li&gt;Tuesday: 57,000&lt;/li&gt;\n&lt;li&gt;Wednesday: 24,000&lt;/li&gt;\n&lt;li&gt;Thursday: 20,000&lt;/li&gt;\n&lt;li&gt;Friday: 48,000&lt;/li&gt;\n&lt;li&gt;Saturday: 33,000&lt;/li&gt;\n&lt;li&gt;Sunday: 1,100&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These numbers will fluctuate but their proportion relative to each other will stay the same. &lt;/p&gt;\n\n&lt;p&gt;Using only SQL, how would you go about testing this output? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14mpveu", "is_robot_indexable": true, "report_reasons": null, "author": "DataEngineerA", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mpveu/how_to_go_about_testing_data_pipeline_output/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mpveu/how_to_go_about_testing_data_pipeline_output/", "subreddit_subscribers": 113242, "created_utc": 1688098141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just got my first data engineering position. During the interview I asked about how the company contributes to employees wanting to further their education. They said that they cover all certification exams.\n\nThe certs I am thinking of getting:\n\n* Azure DP-203\n* An industry recognized Docker/Kubernetes certification\n* Maybe a Spark Cert?\n\nWhat would some of you more experienced DEs do if you were just starting out your career like I am?", "author_fullname": "t2_71x5v4dp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My New Job Pays for All My Wanted Certifications. Recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14n67sd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688145129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just got my first data engineering position. During the interview I asked about how the company contributes to employees wanting to further their education. They said that they cover all certification exams.&lt;/p&gt;\n\n&lt;p&gt;The certs I am thinking of getting:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Azure DP-203&lt;/li&gt;\n&lt;li&gt;An industry recognized Docker/Kubernetes certification&lt;/li&gt;\n&lt;li&gt;Maybe a Spark Cert?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What would some of you more experienced DEs do if you were just starting out your career like I am?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14n67sd", "is_robot_indexable": true, "report_reasons": null, "author": "Emosk8rboi42969", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n67sd/my_new_job_pays_for_all_my_wanted_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14n67sd/my_new_job_pays_for_all_my_wanted_certifications/", "subreddit_subscribers": 113242, "created_utc": 1688145129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI'm a 27/M data engineer based in Europe, and I could really use some guidance in my career journey. As an immigrant and the only one in my family in the industry, I often find myself unsure of where to turn for advice.\n\nI'm on the lookout for an experienced professional who has successfully navigated the data engineering industry and can lend me some insights to make better decisions.\n\nIf any of you have been in a similar position, I'd love to hear your stories. How did you manage to find that mentor figure who provided you with the guidance you needed?", "author_fullname": "t2_73mpr6fx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I find a mentor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mxlvn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688123976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a 27/M data engineer based in Europe, and I could really use some guidance in my career journey. As an immigrant and the only one in my family in the industry, I often find myself unsure of where to turn for advice.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on the lookout for an experienced professional who has successfully navigated the data engineering industry and can lend me some insights to make better decisions.&lt;/p&gt;\n\n&lt;p&gt;If any of you have been in a similar position, I&amp;#39;d love to hear your stories. How did you manage to find that mentor figure who provided you with the guidance you needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14mxlvn", "is_robot_indexable": true, "report_reasons": null, "author": "Maxtasis", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mxlvn/how_can_i_find_a_mentor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mxlvn/how_can_i_find_a_mentor/", "subreddit_subscribers": 113242, "created_utc": 1688123976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I believe that open-source contributions will significantly impact candidate selection processes. Although I'm relatively new to contributing to ML models and data science or even something related to data engineering, I'm eager to get involved. It would greatly benefit me if individuals could share their bookmarked projects that I could contribute to. Thank you!! ", "author_fullname": "t2_8i81bbqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "List secret ML repositories to contribute?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14msy8b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688108260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe that open-source contributions will significantly impact candidate selection processes. Although I&amp;#39;m relatively new to contributing to ML models and data science or even something related to data engineering, I&amp;#39;m eager to get involved. It would greatly benefit me if individuals could share their bookmarked projects that I could contribute to. Thank you!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14msy8b", "is_robot_indexable": true, "report_reasons": null, "author": "trafalgar28", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14msy8b/list_secret_ml_repositories_to_contribute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14msy8b/list_secret_ml_repositories_to_contribute/", "subreddit_subscribers": 113242, "created_utc": 1688108260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been looking at Iceberg and Hudi more lately, and learnt that they both require Hive metastore, but I don't understand why we have this dependency.  \n\nHive metastore is a pain to setup.  It has a dependency on Hadoop and is configured with XML.  It also has a dependency on a relational database.  So to query my Iceberg table I need Hive, Hadoop and Postgres running.  \n\nI thought Iceberg and Hudi were self describing?  It seems in the case of Iceberg that the only thing the Metastore is used for is to point the table name at the Metadata file.  \n\nIs there not something much more lightweight?  It's just a matter of recording schemas.  Why does something as legacy as Metastore persist for so long?  ", "author_fullname": "t2_aik5l2tsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Hive Metastore everywhere? (Especially Iceberg)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mrhhw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688103317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been looking at Iceberg and Hudi more lately, and learnt that they both require Hive metastore, but I don&amp;#39;t understand why we have this dependency.  &lt;/p&gt;\n\n&lt;p&gt;Hive metastore is a pain to setup.  It has a dependency on Hadoop and is configured with XML.  It also has a dependency on a relational database.  So to query my Iceberg table I need Hive, Hadoop and Postgres running.  &lt;/p&gt;\n\n&lt;p&gt;I thought Iceberg and Hudi were self describing?  It seems in the case of Iceberg that the only thing the Metastore is used for is to point the table name at the Metadata file.  &lt;/p&gt;\n\n&lt;p&gt;Is there not something much more lightweight?  It&amp;#39;s just a matter of recording schemas.  Why does something as legacy as Metastore persist for so long?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14mrhhw", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Act_3652", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mrhhw/why_is_hive_metastore_everywhere_especially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mrhhw/why_is_hive_metastore_everywhere_especially/", "subreddit_subscribers": 113242, "created_utc": 1688103317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently confused on this and I'd appreciate some advice/suggestion.\n\nFew ways I've identified\n\nMaking their pipeline incremental\n\nMoving the modelling away from the Viz tool\n\nDocumentation \n    1) the current dashboard, the tables powering it \n     2) having a place where a search can be done on all the tables (have the feature of adding comments/business meaning to columns/tables)\n\nAdopting an orchestrator and adding proper alert\n     currently using the schedule feature of the tool which requires one to go and check for status.\n\nI was talking to a higher up and I was asked what next after all these.", "author_fullname": "t2_msnzcdmf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bringing value as a DE in a startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mp34n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688095761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently confused on this and I&amp;#39;d appreciate some advice/suggestion.&lt;/p&gt;\n\n&lt;p&gt;Few ways I&amp;#39;ve identified&lt;/p&gt;\n\n&lt;p&gt;Making their pipeline incremental&lt;/p&gt;\n\n&lt;p&gt;Moving the modelling away from the Viz tool&lt;/p&gt;\n\n&lt;p&gt;Documentation \n    1) the current dashboard, the tables powering it \n     2) having a place where a search can be done on all the tables (have the feature of adding comments/business meaning to columns/tables)&lt;/p&gt;\n\n&lt;p&gt;Adopting an orchestrator and adding proper alert\n     currently using the schedule feature of the tool which requires one to go and check for status.&lt;/p&gt;\n\n&lt;p&gt;I was talking to a higher up and I was asked what next after all these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14mp34n", "is_robot_indexable": true, "report_reasons": null, "author": "it_wasnt_me_who_said", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mp34n/bringing_value_as_a_de_in_a_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mp34n/bringing_value_as_a_de_in_a_startup/", "subreddit_subscribers": 113242, "created_utc": 1688095761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the best design for providing access to users to the current state of data in a source in a Data Lake architecture, given a source with keyed /entity data that might change.\n\n**Use Case:**\n\nYou are replicating table A from a source (maybe it's an API or table or whatever). Table A has an insert/update date. Rows are occasionally deleted. Table A grows over time.\n\nYou do an extract into your lake's object storage each day of the prior day's inserted updated data. So you have an entry for each day's worth of data.\n\nSo now different versions of a given entity are in different files. **An end user would have to know this and would have to setup their queries in such a way to make sure they got the latest / correct version.**\n\n**Comments**:\n\nMy guess would be that you'd have another tier in your lake, where you'd have a process that gets the latest and organizes that in a different way (e.g., insert date, latest version)\n\nWith infinite resources, you could do a full extract every day, but that seems overkill, and the source might not support that.", "author_fullname": "t2_ahf8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replicating Changing Data for Accurate End User Access in a Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14n210w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688135465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best design for providing access to users to the current state of data in a source in a Data Lake architecture, given a source with keyed /entity data that might change.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Use Case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You are replicating table A from a source (maybe it&amp;#39;s an API or table or whatever). Table A has an insert/update date. Rows are occasionally deleted. Table A grows over time.&lt;/p&gt;\n\n&lt;p&gt;You do an extract into your lake&amp;#39;s object storage each day of the prior day&amp;#39;s inserted updated data. So you have an entry for each day&amp;#39;s worth of data.&lt;/p&gt;\n\n&lt;p&gt;So now different versions of a given entity are in different files. &lt;strong&gt;An end user would have to know this and would have to setup their queries in such a way to make sure they got the latest / correct version.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Comments&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;My guess would be that you&amp;#39;d have another tier in your lake, where you&amp;#39;d have a process that gets the latest and organizes that in a different way (e.g., insert date, latest version)&lt;/p&gt;\n\n&lt;p&gt;With infinite resources, you could do a full extract every day, but that seems overkill, and the source might not support that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14n210w", "is_robot_indexable": true, "report_reasons": null, "author": "PencilBoy99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n210w/replicating_changing_data_for_accurate_end_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14n210w/replicating_changing_data_for_accurate_end_user/", "subreddit_subscribers": 113242, "created_utc": 1688135465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As stated in the title, I'm learning how to download a parquet file from Azure Blob Storage with the Python Client Library. Yesterday I was able to implement the code but I was wondering if I could filter only the desidered columns before actualing downloading the file from Azure in order to limite the resources and the time spent on the I/O networking. Is there a solution?\n\nMy code so far:\n\n    class BlobStorageAsync:\n        def __init__(self, connection_string, container_name, logging_enable):\n            self.connection_string = connection_string\n            self.container_name = container_name\n            container_client = ContainerClient.from_connection_string(\n                conn_str=connection_string,\n                container_name=container_name,\n                # This client will log detailed information about its HTTP sessions, at DEBUG level\n                logging_enable=logging_enable\n            )\n            self.container_client = container_client\n    \n        async def list_blobs_in_container_async(self, name_starts_with):\n            blobs_list = []\n            async for blob in self.container_client.list_blobs(name_starts_with=name_starts_with):\n                blobs_list.append(blob)\n            return blobs_list\n    \n        async def download_blob_async(self, blob_name):\n            try:\n                blob_client = self.container_client.get_blob_client(blob=blob_name)\n                async with blob_client:\n                    stream = await blob_client.download_blob()\n                    data = await stream.readall() # data returned as bytes-like object\n                # return data as bytes (in-memory binary stream)\n                return BytesIO(data)\n            except ResourceNotFoundError:\n                logging.warning(f'The file {blob_name} was not found')\n                return None\n    \n        async def download_blobs_async(self, blobs_list):\n            tasks = []\n            async with asyncio.TaskGroup() as tg:\n                for blob_name in blobs_list:\n                    task = tg.create_task(self.download_blob_async(blob_name))\n                    tasks.append(task)\n            return tasks\n\n&amp;#x200B;", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do column projection (filtering) server-side with Azure Blob Storage (Python Client Library)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mt29y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688108602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As stated in the title, I&amp;#39;m learning how to download a parquet file from Azure Blob Storage with the Python Client Library. Yesterday I was able to implement the code but I was wondering if I could filter only the desidered columns before actualing downloading the file from Azure in order to limite the resources and the time spent on the I/O networking. Is there a solution?&lt;/p&gt;\n\n&lt;p&gt;My code so far:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class BlobStorageAsync:\n    def __init__(self, connection_string, container_name, logging_enable):\n        self.connection_string = connection_string\n        self.container_name = container_name\n        container_client = ContainerClient.from_connection_string(\n            conn_str=connection_string,\n            container_name=container_name,\n            # This client will log detailed information about its HTTP sessions, at DEBUG level\n            logging_enable=logging_enable\n        )\n        self.container_client = container_client\n\n    async def list_blobs_in_container_async(self, name_starts_with):\n        blobs_list = []\n        async for blob in self.container_client.list_blobs(name_starts_with=name_starts_with):\n            blobs_list.append(blob)\n        return blobs_list\n\n    async def download_blob_async(self, blob_name):\n        try:\n            blob_client = self.container_client.get_blob_client(blob=blob_name)\n            async with blob_client:\n                stream = await blob_client.download_blob()\n                data = await stream.readall() # data returned as bytes-like object\n            # return data as bytes (in-memory binary stream)\n            return BytesIO(data)\n        except ResourceNotFoundError:\n            logging.warning(f&amp;#39;The file {blob_name} was not found&amp;#39;)\n            return None\n\n    async def download_blobs_async(self, blobs_list):\n        tasks = []\n        async with asyncio.TaskGroup() as tg:\n            for blob_name in blobs_list:\n                task = tg.create_task(self.download_blob_async(blob_name))\n                tasks.append(task)\n        return tasks\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14mt29y", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mt29y/how_to_do_column_projection_filtering_serverside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mt29y/how_to_do_column_projection_filtering_serverside/", "subreddit_subscribers": 113242, "created_utc": 1688108602.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "First time seeing a company making hardware for running SQL", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing the SQL Processing Unit (SPU) | NeuroBlade", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14n4uz4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/l-tAaPFSwXKao_WV51axWfT_dpNDlEKRBMrtd81Kyq8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688141947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "neuroblade.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time seeing a company making hardware for running SQL&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.neuroblade.com/product/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?auto=webp&amp;v=enabled&amp;s=c6ede938bf4256458aef5f361495ed2cc3287ac8", "width": 1208, "height": 636}, "resolutions": [{"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdaf5ce9f2a24f1f48f8dd2949e6b99b7ba0f1fe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c60c89894ee83055ffbc2bd9044b256ae6907710", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=994dae05c210046ab1948fef03a8faade747d7c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e6723cd3db6adc2c6781dd4cd256cb506490f52", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9b4e0fbbdba486af9b7800a201e182a8c7b4b52", "width": 960, "height": 505}, {"url": "https://external-preview.redd.it/0j7MuGpTKSnWA0xTqOEiAHLiMeRGQdrb5tLUXDzx_lU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7214a1ee1185b5b968d55d5ec954c1a47f4bba1", "width": 1080, "height": 568}], "variants": {}, "id": "1VzBNVIlk8V6-JjT3FYbe12fh_-VAVIp6fBUo8kE4o8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14n4uz4", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n4uz4/introducing_the_sql_processing_unit_spu_neuroblade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.neuroblade.com/product/", "subreddit_subscribers": 113242, "created_utc": 1688141947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI am using awswrangler to write two pandas DataFrames to an iceberg table. For some reason both tables have \\~900k rows, most of the rows are empty. The dataframes only contains 200k rows.\n\nI also write this to Impala and there it is fine? What is going on? Is this a bug in awswrangler?", "author_fullname": "t2_5o9ebpsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AwsWrangler Athena Iceberg writes empty rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14n1jrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688134353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I am using awswrangler to write two pandas DataFrames to an iceberg table. For some reason both tables have ~900k rows, most of the rows are empty. The dataframes only contains 200k rows.&lt;/p&gt;\n\n&lt;p&gt;I also write this to Impala and there it is fine? What is going on? Is this a bug in awswrangler?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14n1jrr", "is_robot_indexable": true, "report_reasons": null, "author": "mosquitsch", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n1jrr/awswrangler_athena_iceberg_writes_empty_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14n1jrr/awswrangler_athena_iceberg_writes_empty_rows/", "subreddit_subscribers": 113242, "created_utc": 1688134353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I developed an open source OLAP multidimensional database, its name is EuclidOLAP, and its most unique feature is the automatic real-time aggregation capability, if you use it for data analysis then you can focus more on the semantic model that is close to the actual business, and do not need to think about how the detailed data is aggregated upward.\n\nIt also has the following features:\n\n* The deployment is very simple to run;\n* It can run stand-alone or in a distributed architecture;\n* Using SQL-like language MDX, it has more powerful multi-dimensional query capabilities.\n\nThis project can be accessed on [Github](https://github.com/EuclidOLAP/EuclidOLAP).\n\n&amp;#x200B;\n\nNext, you can learn about the logical model of a multidimensional database through the following.\n\nMultidimensional databases organize and present data in the structure of hyper-cubes, and you can imagine that these cubes exist in a multidimensional space, and each cube can be associated with several dimensions. Dimensions are similar to axes, while a dimension represents a specific business perspective, such as the date dimension and the region dimension. The interior of a cube is filled with some quantifiable data, which is called measure.\n\n# Dimension\n\nDimensions are similar to axes, and a dimension also represents a business perspective. For example, an airline can analyze its turnover data through three business perspectives: classes of service dimension, aircraft models dimension and date dimension.\n\n[dimension](https://preview.redd.it/i0eu7owrs59b1.png?width=793&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97237e2b26963ddc1a05e5c2763fad16e2261ef4)\n\n# Member\n\nA member represents a definite value under the business perspective that a dimension represents. For example, there are members such as economy plus, business suite and first class suite under the classes of service dimension, A380 and Boeing 787 members under the aircraft models dimension, and members such as 2022, the first quarter of 2022, and January 2022 under the date dimension. Similar to thinking of dimensions as axes, you can think of members as scales on axes. Members under a dimension have parent-child relationships with each other, and they form a tree-like structure, the tree has a default root member.\n\n[member](https://preview.redd.it/o1thzn3vs59b1.png?width=767&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c552e779f8fc9ce7614efc4afdda72b6d668e5b5)\n\n# Cube\n\nCubes model represent data marts oriented to business topics, which are associated with several dimensions that describe some business perspectives. You can think of an 1D cube and a 2D cube as a line segment and a plane, and a 3D cube as a Rubik's cube in a Cartesian coordinate system. For cubes associated with more dimensions, they are multidimensional structures that exist in higher dimensional space, but you don't need to imagine what they look like, they are just described from more business perspectives, they have exactly the same characters as 3D cubes, as long as you understand 3D cubes you can understand higher dimensional cubes.\n\nSome data called measure is populated inside cubes, such as revenue and cost.\n\nHere's a cube about an airline, it has two measures revenue and cost, and associates three dimensions classes of service, aircraft models, and date.\n\n[cube](https://preview.redd.it/p17p2glys59b1.png?width=532&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ec605e1a94ff32f6589c9ccb61b391b34f0aafac)\n\n# Measure\n\nFor a cube, dimensions representing business perspectives are descriptive information, and measures represent values that can be quantified.\n\nJust as selecting a scale on each axis of a coordinate system determines a point, selecting a member on all dimensions of a cube determines a measure position that represents one or more measures.\n\nIn the cube shown in the figure below, the marked measure represents the revenue and cost of business suite service on the A380 aircraft in 2022.\n\n[measure](https://preview.redd.it/s64hogh1t59b1.png?width=810&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0e77a6796243147e5e85e51f7e4798426860cf8b)\n\n# Level\n\nLevels represent that members of the same level in the dimension tree structure, for example, the date dimension has four levels, one default root level represents the root member, and the other three levels represent the year, quarter, and month.\n\n[level](https://preview.redd.it/9pmuz3l4t59b1.png?width=953&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=25a7d0dcb421234bd6f86d6013a336bf4abe262b)\n\n# Hierarchy\n\nA hierarchy is a tree structure composed of dimension members, and a dimension will have a default hierarchy, and its name is generally the same as the dimension name.\n\nA dimension may have multiple hierarchies, such as the aircraft models dimension, which has two hierarchies, one that categorizes aircraft models by aircraft manufacturer and one that categorizes aircraft models by aircraft size.\n\n[hierarchy](https://preview.redd.it/x559iv88t59b1.png?width=1057&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8bd5df4e7cabe96e671706ee60fad606a73b33d3)\n\nIt should be noted that members with the same name under different hierarchies are not the same member, such as \\[Airbus\\]. \\[A320\\] and \\[Medium\\]. \\[A320\\] is two different members, and a non-default hierarchy member is generally associated with a default hierarchy member.\n\nDate dimension also have two hierarchies, one is the calendar hierarchy and the other is the financial hierarchy. Because some companies' fiscal years do not coincide with calendar years, for example, fiscal year 2022 starts in April 2022 and ends in March 2023, a separate date dimension hierarchy suitable for financial analysis needs to be designed.\n\nThe date dimension financial hierarchy is three months behind the calendar hierarchy.\n\n[hierarchy of date dimension](https://preview.redd.it/6le8slzbt59b1.png?width=846&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=15e9a0c242e0c5ca33c09bd01f836f16da44126f)\n\n# Dimension Role\n\nAn airline builds a cube for analyzing turnover data, which not only correlates the three dimensions of classes of service, aircraft models, and date, but also needs to analyze it from the perspective of flight starting point and flight ending point. Flight starting point and ending point are describing the region, they can be represented by the region dimension, this cube needs to be associated with the region dimension twice, so the concept of dimension role is introduced in EuclidOLAP, and the region dimension plays two dimension roles on this cube, flight starting point and flight ending point.\n\n[dimension role](https://preview.redd.it/2fvvh39ht59b1.png?width=863&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2fcfd959226f8fc6845c0428a11b9d7bb851bf7d)\n\nFor a dimension itself, it does not have a role attribute, and only when the dimension is associated by a cube, the association relationship is a dimensional role.\n\nIf a dimension is associated with a cube, it plays at least one dimension role, and if it plays multiple dimension roles, we need to indicate the specific dimension role when analyzing.\n\nSince the hierarchy, level and member models all belong to a specific dimension, when this dimension is associated with a cube to form a dimensional role, these models under the dimensions will also form corresponding role information, which are hierarchy role, level role and member role.\n\n[dimension role 2](https://preview.redd.it/j6x09c0kt59b1.png?width=1316&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fbd2179adba025314451460b1770d03431f1bbf3)\n\n# Tuple\n\nA tuple consists of several dimension members associated with a cube, and the following are two tuples:\n\n* (\\[Date\\].\\[2022\\].\\[Q1\\].\\[M2\\], \\[Measures\\].\\[Revenue\\])\n* (\\[Classes of Service\\].\\[Economy Plus\\], \\[Aircraft Models\\].\\[Airbus\\].\\[A380\\])\n\nA dimension member can also be treated as a tuple, for example: \\[Measures\\].\\[Cost\\] can be equated with (\\[Measures\\].\\[Cost\\]).\n\nWhen a tuple contains members of all dimensions to which the cube is associated, it is a complete tuple, otherwise it is a tuple fragment.\n\n(\\[Classes of Service\\].\\[Economy Plus\\], \\[Aircraft Models\\].\\[Boeing\\], \\[Date\\].\\[2022\\], \\[Measures\\].\\[Cost\\]) is a complete tuple.\n\n(\\[Aircraft Models\\].\\[Boeing\\], \\[Date\\].\\[2022\\]) is a tuple fragment.\n\nWhen performing multidimensional queries, a tuple fragment can be equated to a complete tuple in combination with context information.\n\nIf a complete tuple is composed of all leaf members, then it represents a specific detail measure of a cube, and if a complete tuple contains aggregate members, it corresponds to a aggregate measure, which is automatically summarized from some detail measures.\n\n[tuple](https://preview.redd.it/auia6wiot59b1.png?width=1019&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7daf15c01f645805e9c85652fb08e912f8b8be2c)\n\n# Set\n\nA Set consists of several tuples, and here are two sets:\n\n* { Tuple\\_1, Tuple\\_2 }\n* { Tuple\\_1, Tuple\\_2, Tuple\\_3, Tuple\\_4 }\n\nA single tuple can also be thought of as a set, for example: Tuple\\_1 equivalent to { Tuple\\_1 }.\n\nSimilarly, since a single dimension member can be thought of as a tuple, a single member can also be thought of as a set, \\[Date\\].\\[2022\\].\\[Q1\\] is equivalent to { ( \\[Date\\].\\[2022\\].\\[Q1\\] ) }.\n\nSet is generally used to define the display of a multidimensional query result, such as the following multidimensional query expression:\n\n    select\n    {\n        [Date]. [2020],\n        [Date]. [2021]\n    } on rows,\n    {\n        [Measures]. [Revenue], [Measures]. [Cost]\n    } on columns\n    from [Airline A];\n\nIt defines two sets, { \\[Date\\].\\[ 2020\\], \\[Date\\]. \\[2021\\] } will be displayed on row position, { \\[Measures\\].\\[ Revenue\\], \\[Measures\\]. \\[Cost\\] } will be displayed on column position.\n\nExecute this MDX(Multidimensional Expressions), and you will see the query result similar to the following table:\n\n&amp;#x200B;\n\n|| Revenue |Cost|\n|:-|:-|:-|\n|2020| 494849380 | 415181710 |\n|2021| 590103250 | 497682690 |\n\n&amp;#x200B;", "author_fullname": "t2_ecwgpeq01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Logical model of OLAP multidimensional database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j6x09c0kt59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=920fa2fb98205417f50925e36819050b1ff1f48f"}, {"y": 72, "x": 216, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=238aee7c0866f74f7f9cf2f4c95192c308d8b0be"}, {"y": 107, "x": 320, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c5ee370d4fdae1e437dc6b760d60d5db52ae940"}, {"y": 214, "x": 640, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7413350ccb6f95082bea141b1bbe31abacd7a119"}, {"y": 321, "x": 960, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd55b0dd7b6a08099221ebbe8cfc4e951ff0a26a"}, {"y": 361, "x": 1080, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=892c450a2e54b28fbfc154096c8c684cd662b08b"}], "s": {"y": 441, "x": 1316, "u": "https://preview.redd.it/j6x09c0kt59b1.png?width=1316&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fbd2179adba025314451460b1770d03431f1bbf3"}, "id": "j6x09c0kt59b1"}, "x559iv88t59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 29, "x": 108, "u": "https://preview.redd.it/x559iv88t59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4444ca06e1db88e45bc4025a9b23b95cee42afb6"}, {"y": 59, "x": 216, "u": "https://preview.redd.it/x559iv88t59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb0c22625cec89526e630b8728e2fb45ede2bb33"}, {"y": 87, "x": 320, "u": "https://preview.redd.it/x559iv88t59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6af16a8df739dd77a5fff79501061a68ab70628e"}, {"y": 175, "x": 640, "u": "https://preview.redd.it/x559iv88t59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f90ee1e1d9879a3085a45884fa17c8e8e88fc98"}, {"y": 263, "x": 960, "u": "https://preview.redd.it/x559iv88t59b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ce072bf7be6060c9967e1de35c276a86415c793"}], "s": {"y": 290, "x": 1057, "u": "https://preview.redd.it/x559iv88t59b1.png?width=1057&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8bd5df4e7cabe96e671706ee60fad606a73b33d3"}, "id": "x559iv88t59b1"}, "i0eu7owrs59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/i0eu7owrs59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3551c4bd3a836702ae881064c0ba3e8c0e2094f"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/i0eu7owrs59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f10fd80e5caf9877e790786df5a129346674e5a"}, {"y": 114, "x": 320, "u": "https://preview.redd.it/i0eu7owrs59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d36c740a6260f4f71066b5f3161233525c58a052"}, {"y": 228, "x": 640, "u": "https://preview.redd.it/i0eu7owrs59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2a7f6caac89922665054145b30500f1cc7f13be"}], "s": {"y": 283, "x": 793, "u": "https://preview.redd.it/i0eu7owrs59b1.png?width=793&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97237e2b26963ddc1a05e5c2763fad16e2261ef4"}, "id": "i0eu7owrs59b1"}, "6le8slzbt59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/6le8slzbt59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc6df2b3d95b95b650796aec538f314c0fe6a9d0"}, {"y": 166, "x": 216, "u": "https://preview.redd.it/6le8slzbt59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80ca5865a2e55e48e6b6db92b5b6145327a1284e"}, {"y": 246, "x": 320, "u": "https://preview.redd.it/6le8slzbt59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38d009d30c9473195226b2946890277632b7ccf1"}, {"y": 492, "x": 640, "u": "https://preview.redd.it/6le8slzbt59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9fec79f3d982bf4ad14274c7b41d511c50d44f1"}], "s": {"y": 651, "x": 846, "u": "https://preview.redd.it/6le8slzbt59b1.png?width=846&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=15e9a0c242e0c5ca33c09bd01f836f16da44126f"}, "id": "6le8slzbt59b1"}, "p17p2glys59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/p17p2glys59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4afdf0ef900cca9e0f2fb62c28a7a0ec9e175eac"}, {"y": 112, "x": 216, "u": "https://preview.redd.it/p17p2glys59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb6b7340e3db391f5c3f13b5a72ad86eb980b5e4"}, {"y": 166, "x": 320, "u": "https://preview.redd.it/p17p2glys59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b364b38953b4b67000ed1d64ab3fef8bafbc1ae"}], "s": {"y": 277, "x": 532, "u": "https://preview.redd.it/p17p2glys59b1.png?width=532&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ec605e1a94ff32f6589c9ccb61b391b34f0aafac"}, "id": "p17p2glys59b1"}, "s64hogh1t59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/s64hogh1t59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2790d20fec1f99faa5d084f85da361de1de35c38"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/s64hogh1t59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=222a201ebe9298f6905e7fbe55bb758add0bd9c7"}, {"y": 122, "x": 320, "u": "https://preview.redd.it/s64hogh1t59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b4dbe4d44c6ed773978a9c42595d3a7e5b6febb"}, {"y": 244, "x": 640, "u": "https://preview.redd.it/s64hogh1t59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4dc018104afe5df11fd60d48d754478032b4af71"}], "s": {"y": 310, "x": 810, "u": "https://preview.redd.it/s64hogh1t59b1.png?width=810&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0e77a6796243147e5e85e51f7e4798426860cf8b"}, "id": "s64hogh1t59b1"}, "9pmuz3l4t59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 23, "x": 108, "u": "https://preview.redd.it/9pmuz3l4t59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adc8c64ea2a8c1b155d7fbf93429fb9f5968b49d"}, {"y": 46, "x": 216, "u": "https://preview.redd.it/9pmuz3l4t59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b01880aff2ad2593c4d00013391a3f3d776f969"}, {"y": 68, "x": 320, "u": "https://preview.redd.it/9pmuz3l4t59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c975156bbc2c1e28f759fd127fe506e3a787ad72"}, {"y": 137, "x": 640, "u": "https://preview.redd.it/9pmuz3l4t59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cd6c30a68856fda9477f27dc1e0570597129b22"}], "s": {"y": 205, "x": 953, "u": "https://preview.redd.it/9pmuz3l4t59b1.png?width=953&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=25a7d0dcb421234bd6f86d6013a336bf4abe262b"}, "id": "9pmuz3l4t59b1"}, "2fvvh39ht59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/2fvvh39ht59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cf83618e894677e9a7ad2c0a5cd1772a9a870da"}, {"y": 67, "x": 216, "u": "https://preview.redd.it/2fvvh39ht59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d86132f629b1522eec65ce8cdba57ada62f21d2b"}, {"y": 99, "x": 320, "u": "https://preview.redd.it/2fvvh39ht59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b23ef3b867829d84d9e7a461c70016bee2273f17"}, {"y": 199, "x": 640, "u": "https://preview.redd.it/2fvvh39ht59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeff170e69106a5511f53338348005dffe705381"}], "s": {"y": 269, "x": 863, "u": "https://preview.redd.it/2fvvh39ht59b1.png?width=863&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2fcfd959226f8fc6845c0428a11b9d7bb851bf7d"}, "id": "2fvvh39ht59b1"}, "auia6wiot59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/auia6wiot59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d747a9097bf296315214c2e9863e2b5a7c9d8c28"}, {"y": 114, "x": 216, "u": "https://preview.redd.it/auia6wiot59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e910e2aacb40235d9e792229b21360b5e8044d7"}, {"y": 169, "x": 320, "u": "https://preview.redd.it/auia6wiot59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac173d448e2ba7bab19d67bd6c2fb3318e9bfdb8"}, {"y": 339, "x": 640, "u": "https://preview.redd.it/auia6wiot59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9133efcc77a2fb957e43a59b98ceb5348d65bbef"}, {"y": 509, "x": 960, "u": "https://preview.redd.it/auia6wiot59b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40a09e13cb115e74520639ec1daad1fa76284d3a"}], "s": {"y": 541, "x": 1019, "u": "https://preview.redd.it/auia6wiot59b1.png?width=1019&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7daf15c01f645805e9c85652fb08e912f8b8be2c"}, "id": "auia6wiot59b1"}, "o1thzn3vs59b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 29, "x": 108, "u": "https://preview.redd.it/o1thzn3vs59b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0acc9208ef0f456721b61a14919ec79b5ef66645"}, {"y": 58, "x": 216, "u": "https://preview.redd.it/o1thzn3vs59b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f688dd02d883c17d4bbc630d6549d9fc88c431a"}, {"y": 86, "x": 320, "u": "https://preview.redd.it/o1thzn3vs59b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=430bb990a88a382f9d3cd4bada05bfd1ed6122ba"}, {"y": 173, "x": 640, "u": "https://preview.redd.it/o1thzn3vs59b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=063f9c31207548afc775b0d25b1db3a01ab952b7"}], "s": {"y": 208, "x": 767, "u": "https://preview.redd.it/o1thzn3vs59b1.png?width=767&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c552e779f8fc9ce7614efc4afdda72b6d668e5b5"}, "id": "o1thzn3vs59b1"}}, "name": "t3_14n0zo0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_PyZo50T3dowaNmDDDzgTYpsQqX1ZUs_VSxeD0uMdLI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1688133048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I developed an open source OLAP multidimensional database, its name is EuclidOLAP, and its most unique feature is the automatic real-time aggregation capability, if you use it for data analysis then you can focus more on the semantic model that is close to the actual business, and do not need to think about how the detailed data is aggregated upward.&lt;/p&gt;\n\n&lt;p&gt;It also has the following features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The deployment is very simple to run;&lt;/li&gt;\n&lt;li&gt;It can run stand-alone or in a distributed architecture;&lt;/li&gt;\n&lt;li&gt;Using SQL-like language MDX, it has more powerful multi-dimensional query capabilities.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This project can be accessed on &lt;a href=\"https://github.com/EuclidOLAP/EuclidOLAP\"&gt;Github&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Next, you can learn about the logical model of a multidimensional database through the following.&lt;/p&gt;\n\n&lt;p&gt;Multidimensional databases organize and present data in the structure of hyper-cubes, and you can imagine that these cubes exist in a multidimensional space, and each cube can be associated with several dimensions. Dimensions are similar to axes, while a dimension represents a specific business perspective, such as the date dimension and the region dimension. The interior of a cube is filled with some quantifiable data, which is called measure.&lt;/p&gt;\n\n&lt;h1&gt;Dimension&lt;/h1&gt;\n\n&lt;p&gt;Dimensions are similar to axes, and a dimension also represents a business perspective. For example, an airline can analyze its turnover data through three business perspectives: classes of service dimension, aircraft models dimension and date dimension.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i0eu7owrs59b1.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97237e2b26963ddc1a05e5c2763fad16e2261ef4\"&gt;dimension&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Member&lt;/h1&gt;\n\n&lt;p&gt;A member represents a definite value under the business perspective that a dimension represents. For example, there are members such as economy plus, business suite and first class suite under the classes of service dimension, A380 and Boeing 787 members under the aircraft models dimension, and members such as 2022, the first quarter of 2022, and January 2022 under the date dimension. Similar to thinking of dimensions as axes, you can think of members as scales on axes. Members under a dimension have parent-child relationships with each other, and they form a tree-like structure, the tree has a default root member.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/o1thzn3vs59b1.png?width=767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c552e779f8fc9ce7614efc4afdda72b6d668e5b5\"&gt;member&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Cube&lt;/h1&gt;\n\n&lt;p&gt;Cubes model represent data marts oriented to business topics, which are associated with several dimensions that describe some business perspectives. You can think of an 1D cube and a 2D cube as a line segment and a plane, and a 3D cube as a Rubik&amp;#39;s cube in a Cartesian coordinate system. For cubes associated with more dimensions, they are multidimensional structures that exist in higher dimensional space, but you don&amp;#39;t need to imagine what they look like, they are just described from more business perspectives, they have exactly the same characters as 3D cubes, as long as you understand 3D cubes you can understand higher dimensional cubes.&lt;/p&gt;\n\n&lt;p&gt;Some data called measure is populated inside cubes, such as revenue and cost.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a cube about an airline, it has two measures revenue and cost, and associates three dimensions classes of service, aircraft models, and date.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/p17p2glys59b1.png?width=532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ec605e1a94ff32f6589c9ccb61b391b34f0aafac\"&gt;cube&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Measure&lt;/h1&gt;\n\n&lt;p&gt;For a cube, dimensions representing business perspectives are descriptive information, and measures represent values that can be quantified.&lt;/p&gt;\n\n&lt;p&gt;Just as selecting a scale on each axis of a coordinate system determines a point, selecting a member on all dimensions of a cube determines a measure position that represents one or more measures.&lt;/p&gt;\n\n&lt;p&gt;In the cube shown in the figure below, the marked measure represents the revenue and cost of business suite service on the A380 aircraft in 2022.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s64hogh1t59b1.png?width=810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0e77a6796243147e5e85e51f7e4798426860cf8b\"&gt;measure&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Level&lt;/h1&gt;\n\n&lt;p&gt;Levels represent that members of the same level in the dimension tree structure, for example, the date dimension has four levels, one default root level represents the root member, and the other three levels represent the year, quarter, and month.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9pmuz3l4t59b1.png?width=953&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=25a7d0dcb421234bd6f86d6013a336bf4abe262b\"&gt;level&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Hierarchy&lt;/h1&gt;\n\n&lt;p&gt;A hierarchy is a tree structure composed of dimension members, and a dimension will have a default hierarchy, and its name is generally the same as the dimension name.&lt;/p&gt;\n\n&lt;p&gt;A dimension may have multiple hierarchies, such as the aircraft models dimension, which has two hierarchies, one that categorizes aircraft models by aircraft manufacturer and one that categorizes aircraft models by aircraft size.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x559iv88t59b1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=8bd5df4e7cabe96e671706ee60fad606a73b33d3\"&gt;hierarchy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It should be noted that members with the same name under different hierarchies are not the same member, such as [Airbus]. [A320] and [Medium]. [A320] is two different members, and a non-default hierarchy member is generally associated with a default hierarchy member.&lt;/p&gt;\n\n&lt;p&gt;Date dimension also have two hierarchies, one is the calendar hierarchy and the other is the financial hierarchy. Because some companies&amp;#39; fiscal years do not coincide with calendar years, for example, fiscal year 2022 starts in April 2022 and ends in March 2023, a separate date dimension hierarchy suitable for financial analysis needs to be designed.&lt;/p&gt;\n\n&lt;p&gt;The date dimension financial hierarchy is three months behind the calendar hierarchy.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6le8slzbt59b1.png?width=846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=15e9a0c242e0c5ca33c09bd01f836f16da44126f\"&gt;hierarchy of date dimension&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Dimension Role&lt;/h1&gt;\n\n&lt;p&gt;An airline builds a cube for analyzing turnover data, which not only correlates the three dimensions of classes of service, aircraft models, and date, but also needs to analyze it from the perspective of flight starting point and flight ending point. Flight starting point and ending point are describing the region, they can be represented by the region dimension, this cube needs to be associated with the region dimension twice, so the concept of dimension role is introduced in EuclidOLAP, and the region dimension plays two dimension roles on this cube, flight starting point and flight ending point.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2fvvh39ht59b1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2fcfd959226f8fc6845c0428a11b9d7bb851bf7d\"&gt;dimension role&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For a dimension itself, it does not have a role attribute, and only when the dimension is associated by a cube, the association relationship is a dimensional role.&lt;/p&gt;\n\n&lt;p&gt;If a dimension is associated with a cube, it plays at least one dimension role, and if it plays multiple dimension roles, we need to indicate the specific dimension role when analyzing.&lt;/p&gt;\n\n&lt;p&gt;Since the hierarchy, level and member models all belong to a specific dimension, when this dimension is associated with a cube to form a dimensional role, these models under the dimensions will also form corresponding role information, which are hierarchy role, level role and member role.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j6x09c0kt59b1.png?width=1316&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fbd2179adba025314451460b1770d03431f1bbf3\"&gt;dimension role 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Tuple&lt;/h1&gt;\n\n&lt;p&gt;A tuple consists of several dimension members associated with a cube, and the following are two tuples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;([Date].[2022].[Q1].[M2], [Measures].[Revenue])&lt;/li&gt;\n&lt;li&gt;([Classes of Service].[Economy Plus], [Aircraft Models].[Airbus].[A380])&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A dimension member can also be treated as a tuple, for example: [Measures].[Cost] can be equated with ([Measures].[Cost]).&lt;/p&gt;\n\n&lt;p&gt;When a tuple contains members of all dimensions to which the cube is associated, it is a complete tuple, otherwise it is a tuple fragment.&lt;/p&gt;\n\n&lt;p&gt;([Classes of Service].[Economy Plus], [Aircraft Models].[Boeing], [Date].[2022], [Measures].[Cost]) is a complete tuple.&lt;/p&gt;\n\n&lt;p&gt;([Aircraft Models].[Boeing], [Date].[2022]) is a tuple fragment.&lt;/p&gt;\n\n&lt;p&gt;When performing multidimensional queries, a tuple fragment can be equated to a complete tuple in combination with context information.&lt;/p&gt;\n\n&lt;p&gt;If a complete tuple is composed of all leaf members, then it represents a specific detail measure of a cube, and if a complete tuple contains aggregate members, it corresponds to a aggregate measure, which is automatically summarized from some detail measures.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/auia6wiot59b1.png?width=1019&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7daf15c01f645805e9c85652fb08e912f8b8be2c\"&gt;tuple&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Set&lt;/h1&gt;\n\n&lt;p&gt;A Set consists of several tuples, and here are two sets:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;{ Tuple_1, Tuple_2 }&lt;/li&gt;\n&lt;li&gt;{ Tuple_1, Tuple_2, Tuple_3, Tuple_4 }&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A single tuple can also be thought of as a set, for example: Tuple_1 equivalent to { Tuple_1 }.&lt;/p&gt;\n\n&lt;p&gt;Similarly, since a single dimension member can be thought of as a tuple, a single member can also be thought of as a set, [Date].[2022].[Q1] is equivalent to { ( [Date].[2022].[Q1] ) }.&lt;/p&gt;\n\n&lt;p&gt;Set is generally used to define the display of a multidimensional query result, such as the following multidimensional query expression:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select\n{\n    [Date]. [2020],\n    [Date]. [2021]\n} on rows,\n{\n    [Measures]. [Revenue], [Measures]. [Cost]\n} on columns\nfrom [Airline A];\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It defines two sets, { [Date].[ 2020], [Date]. [2021] } will be displayed on row position, { [Measures].[ Revenue], [Measures]. [Cost] } will be displayed on column position.&lt;/p&gt;\n\n&lt;p&gt;Execute this MDX(Multidimensional Expressions), and you will see the query result similar to the following table:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;Revenue&lt;/th&gt;\n&lt;th align=\"left\"&gt;Cost&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2020&lt;/td&gt;\n&lt;td align=\"left\"&gt;494849380&lt;/td&gt;\n&lt;td align=\"left\"&gt;415181710&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2021&lt;/td&gt;\n&lt;td align=\"left\"&gt;590103250&lt;/td&gt;\n&lt;td align=\"left\"&gt;497682690&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?auto=webp&amp;v=enabled&amp;s=7c824e2978cdafc08d226af3c01bf1e18256e773", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29698ad079f1bb9ee5a83c087964fc937cf6daf5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c56f0191c26ae202f07ca2649793ec39edc99213", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d394cb94412e3a47c626e35f9a1c6fdf47ed4cb8", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2009b7c9ec6e903bf3e932ed679fc9b913911ddb", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c9be6fc8e438b34bfa2462a9b0e2bfe39873c53", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fhZ57UzOnoTc-yAbyku-wufw3Ztpn4PMgitew8ZqGuA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a6fb339501b991943af73234394ba22e5b91778", "width": 1080, "height": 540}], "variants": {}, "id": "5TPQbzaqLgN2NReWbhF5x5KeogyPiqzmXTS0CYpGTHE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "14n0zo0", "is_robot_indexable": true, "report_reasons": null, "author": "czg715", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14n0zo0/logical_model_of_olap_multidimensional_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14n0zo0/logical_model_of_olap_multidimensional_database/", "subreddit_subscribers": 113242, "created_utc": 1688133048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI come seeking your valuable insights and suggestions on utilizing Dataform for dependency management and automating table creation in a data pipeline.\n\nMy team and I have recently started exploring Dataform as a solution for managing dependencies between our data tables in BigQuery. We believe that Dataform can help us eliminate redundancies in queries and facilitate smoother data pipeline management.\n\nHere are a few specific things we have trouble understanding:\n\n1. **Dependency Management**: We would like to understand how to effectively use Dataform to manage dependencies between different tables. How can we set up these dependencies so that when one table is created after executing a query, the next layer of dependent tables is automatically generated, is that done automatically or do we have to setup something here(like workflow configurations).\n2. **Partial Updates Handling**: If in some cases, only a few source tables are updated, while others remain unchanged. We are curious to know how Dataform handles partial updates and ensures that our data pipeline stays consistent even when some source tables change.\n3. **Managing Automation**: We want to explore automation possibilities with Dataform. How can we ensure that once a query is executed and a table is created, the subsequent layers of dependent tables are automatically generated without manual intervention?\n\nI would greatly appreciate any advice, best practices, or architectural suggestions from those who have experience with Dataform or a similar dependency management tool. We want to optimize our data pipeline and streamline the process as much as possible.\n\nThank you in advance for your time and expertise. Looking forward to your valuable responses!", "author_fullname": "t2_cu6opso3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions/Advice on Dataform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mwa32", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688119740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I come seeking your valuable insights and suggestions on utilizing Dataform for dependency management and automating table creation in a data pipeline.&lt;/p&gt;\n\n&lt;p&gt;My team and I have recently started exploring Dataform as a solution for managing dependencies between our data tables in BigQuery. We believe that Dataform can help us eliminate redundancies in queries and facilitate smoother data pipeline management.&lt;/p&gt;\n\n&lt;p&gt;Here are a few specific things we have trouble understanding:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Dependency Management&lt;/strong&gt;: We would like to understand how to effectively use Dataform to manage dependencies between different tables. How can we set up these dependencies so that when one table is created after executing a query, the next layer of dependent tables is automatically generated, is that done automatically or do we have to setup something here(like workflow configurations).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Partial Updates Handling&lt;/strong&gt;: If in some cases, only a few source tables are updated, while others remain unchanged. We are curious to know how Dataform handles partial updates and ensures that our data pipeline stays consistent even when some source tables change.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Managing Automation&lt;/strong&gt;: We want to explore automation possibilities with Dataform. How can we ensure that once a query is executed and a table is created, the subsequent layers of dependent tables are automatically generated without manual intervention?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would greatly appreciate any advice, best practices, or architectural suggestions from those who have experience with Dataform or a similar dependency management tool. We want to optimize our data pipeline and streamline the process as much as possible.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your time and expertise. Looking forward to your valuable responses!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14mwa32", "is_robot_indexable": true, "report_reasons": null, "author": "bha159", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mwa32/suggestionsadvice_on_dataform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mwa32/suggestionsadvice_on_dataform/", "subreddit_subscribers": 113242, "created_utc": 1688119740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am running a pipeline from source to staging (output: flat-file, csv), from staging to data lake and lastly to the target database at BQ.\n\nWhat do I check at staging and how do I do it?", "author_fullname": "t2_55thi7w8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Output check at each layer. What metrics should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mvtwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688118202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running a pipeline from source to staging (output: flat-file, csv), from staging to data lake and lastly to the target database at BQ.&lt;/p&gt;\n\n&lt;p&gt;What do I check at staging and how do I do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14mvtwh", "is_robot_indexable": true, "report_reasons": null, "author": "faizfablillah", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mvtwh/output_check_at_each_layer_what_metrics_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mvtwh/output_check_at_each_layer_what_metrics_should_i/", "subreddit_subscribers": 113242, "created_utc": 1688118202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm writing my master's thesis and I would like to conduct 1:1 interviews with data scientists, BI practitioners, and executives to understand their perspective on the topic.\n\n&amp;#x200B;\n\nIn the business context, we often hear statements like \"AI enhances decision-making,\" \"AI can tell executives what to do\" and \"data-driven decision-making is easy\". However, I saw a considerable amount of misinformation, snake oil solutions, references to Gartner, and the same hypothetical use cases over and over.\n\n&amp;#x200B;\n\nEssentially, I am conducting a reality check on the capabilities and limitations of AI technologies (ML, expert systems, fuzzy logic, etc.) in assisting analytical decision-making within a business context. I aim to determine when it is reasonable to adopt these solutions. If AI proves to be unattainable for most companies, I want to provide managers with practical advices on what they should focus instead (e.g. data culture, centralization and availability of data, real problem identification).\n\nLastly, I am also interested in the role of human intution in decision making, because it plays a role in \"managing the things you can't measure\" or those situations that are not practical to model.\n\n&amp;#x200B;\n\nIf you are a data scientist, BI practitioner, or executive, I would greatly appreciate your insights and expertise in this field. \n\nPlease send me a DM to schedule a 30-minute interview at your convenience. Your participation will have a significant impact on my research, and your insights will help shape practical recommendations for businesses navigating the AI landscape. Together, we can uncover the truth behind AI's potential and empower decision-makers with accurate information.\n\n&amp;#x200B;\n\nThank you in advance for your time and contributions.\n\n&amp;#x200B;\n\nNote: All information provided will be anonymized", "author_fullname": "t2_93r5sac5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experts wanted: exploring the gap between AI hype and real-world decision making", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14mg6hm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688071971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m writing my master&amp;#39;s thesis and I would like to conduct 1:1 interviews with data scientists, BI practitioners, and executives to understand their perspective on the topic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In the business context, we often hear statements like &amp;quot;AI enhances decision-making,&amp;quot; &amp;quot;AI can tell executives what to do&amp;quot; and &amp;quot;data-driven decision-making is easy&amp;quot;. However, I saw a considerable amount of misinformation, snake oil solutions, references to Gartner, and the same hypothetical use cases over and over.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Essentially, I am conducting a reality check on the capabilities and limitations of AI technologies (ML, expert systems, fuzzy logic, etc.) in assisting analytical decision-making within a business context. I aim to determine when it is reasonable to adopt these solutions. If AI proves to be unattainable for most companies, I want to provide managers with practical advices on what they should focus instead (e.g. data culture, centralization and availability of data, real problem identification).&lt;/p&gt;\n\n&lt;p&gt;Lastly, I am also interested in the role of human intution in decision making, because it plays a role in &amp;quot;managing the things you can&amp;#39;t measure&amp;quot; or those situations that are not practical to model.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you are a data scientist, BI practitioner, or executive, I would greatly appreciate your insights and expertise in this field. &lt;/p&gt;\n\n&lt;p&gt;Please send me a DM to schedule a 30-minute interview at your convenience. Your participation will have a significant impact on my research, and your insights will help shape practical recommendations for businesses navigating the AI landscape. Together, we can uncover the truth behind AI&amp;#39;s potential and empower decision-makers with accurate information.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your time and contributions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note: All information provided will be anonymized&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14mg6hm", "is_robot_indexable": true, "report_reasons": null, "author": "Mister_Ragusa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14mg6hm/experts_wanted_exploring_the_gap_between_ai_hype/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14mg6hm/experts_wanted_exploring_the_gap_between_ai_hype/", "subreddit_subscribers": 113242, "created_utc": 1688071971.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}