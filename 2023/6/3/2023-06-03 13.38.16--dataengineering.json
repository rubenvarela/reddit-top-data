{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "And if they don't, would it make sense to do so? I feel like it would allow them to increase rate limits and sell their data in greater quantity with less strain on the site itself.", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do websites have separate (duplicate) databases for use with APIs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13yywky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685761918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And if they don&amp;#39;t, would it make sense to do so? I feel like it would allow them to increase rate limits and sell their data in greater quantity with less strain on the site itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13yywky", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13yywky/do_websites_have_separate_duplicate_databases_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13yywky/do_websites_have_separate_duplicate_databases_for/", "subreddit_subscribers": 108672, "created_utc": 1685761918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any courses that detail how to be GDPR compliant on the cloud.", "author_fullname": "t2_t05ji4fs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Courses on implementing GDPR in cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13z6khq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685782684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any courses that detail how to be GDPR compliant on the cloud.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13z6khq", "is_robot_indexable": true, "report_reasons": null, "author": "kkchn001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13z6khq/courses_on_implementing_gdpr_in_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13z6khq/courses_on_implementing_gdpr_in_cloud/", "subreddit_subscribers": 108672, "created_utc": 1685782684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What work did you do as interns/junior-level DEs and how did it change as you progressed?", "author_fullname": "t2_ens6kw85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A question for all data engineers:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13yelms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685716828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What work did you do as interns/junior-level DEs and how did it change as you progressed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13yelms", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent-Tadpole-564", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13yelms/a_question_for_all_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13yelms/a_question_for_all_data_engineers/", "subreddit_subscribers": 108672, "created_utc": 1685716828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the trade off to consider moving SAP BW4HANA views to datalake architecture given 20 years of data with 2000 reports. I am still positive to move the views or recreate the views in DWH \\[ Datalake\\] but this entails huge risk of copying the data. I wonder if i should consider having a compute engine \\[ DREMIO \\] sitting on top of SAP BW4HANA or take the data to Datalake.", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake Vs SAP BW4HANA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13z09ad", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685765336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the trade off to consider moving SAP BW4HANA views to datalake architecture given 20 years of data with 2000 reports. I am still positive to move the views or recreate the views in DWH [ Datalake] but this entails huge risk of copying the data. I wonder if i should consider having a compute engine [ DREMIO ] sitting on top of SAP BW4HANA or take the data to Datalake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13z09ad", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13z09ad/datalake_vs_sap_bw4hana/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13z09ad/datalake_vs_sap_bw4hana/", "subreddit_subscribers": 108672, "created_utc": 1685765336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How is data modelling solved in medallion/lakehouse data architecture? Bronze + silver layers are just plain tables, no relationships and gold is \u201cdata mart-ish\u201d with all the relationships? How about the normalization? Bronze + silver denormalized and gold normalized? Or? \n\nAlso, how do you actually make a normalization considering you are working with e.g. parquet files? In the database it is simple but with files?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Medallion/lakehouse architecture data modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13z9byl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685790484.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685790124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How is data modelling solved in medallion/lakehouse data architecture? Bronze + silver layers are just plain tables, no relationships and gold is \u201cdata mart-ish\u201d with all the relationships? How about the normalization? Bronze + silver denormalized and gold normalized? Or? &lt;/p&gt;\n\n&lt;p&gt;Also, how do you actually make a normalization considering you are working with e.g. parquet files? In the database it is simple but with files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13z9byl", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13z9byl/medallionlakehouse_architecture_data_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13z9byl/medallionlakehouse_architecture_data_modelling/", "subreddit_subscribers": 108672, "created_utc": 1685790124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI was trying my hand at setting up pipeline which reads data from parquet from a remote site via url and writes to a directory, I am using the NY Taxi database, I have a working code but I feel that I am unnecessarily reading data into memory by using  BytesIO  &amp;  response.iter\\_content.\n\nI am using BytesIO to get a file like object which I can pass to ParquetFile, was wondering if there is a way I can directly pass the streaming request to Parquet file and process it in chunks.\n\n&amp;#x200B;\n\nI tried directly passing the ***chunk***  to Parquet file but got the error `Cannot convert bytes to pyarrow.lib.NativeFile`\n\n&amp;#x200B;\n\n    import pandas as pd\n    import requests\n    import os\n    import pyarrow.parquet as pq\n    import io\n    from memory_profiler import profile\n    \n    def fetch_NY_Data(year:int):\n    \n        #url to fetch NY Taxi data from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page, url is from inspecting the paruet file\n        url =  f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-01.parquet\"\n        \n        response = requests.get(url, stream=True,verify = False) #verify = False to get around self signed certificate error, need to see how at self signed certificates to trusted certificates \n        chunks = []\n    \n        # Process the response content in chunks\n        for chunk in response.iter_content(chunk_size=4096):\n            if chunk:\n                chunks.append(chunk)\n    \n        #create a byte file from the chunks\n        parquet_content = b\"\".join(chunks)\n        #converting the byte file to a file like object\n        parquet_buffer = io.BytesIO(parquet_content)\n    \n        #Direct reading of parquet file using read_parquet_file leads to high memory consumption as observed from docker-stats \n        #df = pd.read_parquet(parquet_file)\n    \n        #Set up the file pointer to Parquet object\n        parquet_file = pq.ParquetFile(parquet_buffer)\n        batch_size = 1024 #Experiment for performance \n        batches = parquet_file.iter_batches(batch_size) #batches will be a generator\n        file_name = None\n        \n        parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir,'data'))\n        \n        cnt= 0\n        for batch in batches:\n            #need to check if to_pandas is required\n            df = batch.to_pandas()    \n            #Construct the file name\n            file_name = os.path.join(parent_dir, f\"{year}_{cnt}.parquet\") \n            try:\n                write_file_to_path(df,file_name)\n            except Exception as e:\n                print(f\"Error writing: {e}\")\n                return e\n            cnt = cnt+1\n    \n    def write_file_to_path(df,filename):\n            directory = os.path.dirname(filename)\n            if not os.path.exists(directory):\n                try:\n                    os.makedirs(directory)\n                    print(f\"Directory '{directory}' created.\")\n                except Exception as e:\n                    print(f\"Error occured while creating directory '{directory}'.\")\n            \n            #TO remove any existing files in the parquet directory\n            if os.path.exists(filename):\n                    os.system(f\"rm -r {filename}\")\n            #Write data to the directory\n            df.to_parquet(filename)\n    if __name__ == \"__main__\":\n         fetch_NY_Data(2023)", "author_fullname": "t2_6ys5mu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need feedback from the folks here on efficiency for streaming a parquet file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13z626a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685781241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I was trying my hand at setting up pipeline which reads data from parquet from a remote site via url and writes to a directory, I am using the NY Taxi database, I have a working code but I feel that I am unnecessarily reading data into memory by using  BytesIO  &amp;amp;  response.iter_content.&lt;/p&gt;\n\n&lt;p&gt;I am using BytesIO to get a file like object which I can pass to ParquetFile, was wondering if there is a way I can directly pass the streaming request to Parquet file and process it in chunks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I tried directly passing the &lt;strong&gt;&lt;em&gt;chunk&lt;/em&gt;&lt;/strong&gt;  to Parquet file but got the error &lt;code&gt;Cannot convert bytes to pyarrow.lib.NativeFile&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pandas as pd\nimport requests\nimport os\nimport pyarrow.parquet as pq\nimport io\nfrom memory_profiler import profile\n\ndef fetch_NY_Data(year:int):\n\n    #url to fetch NY Taxi data from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page, url is from inspecting the paruet file\n    url =  f&amp;quot;https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-01.parquet&amp;quot;\n\n    response = requests.get(url, stream=True,verify = False) #verify = False to get around self signed certificate error, need to see how at self signed certificates to trusted certificates \n    chunks = []\n\n    # Process the response content in chunks\n    for chunk in response.iter_content(chunk_size=4096):\n        if chunk:\n            chunks.append(chunk)\n\n    #create a byte file from the chunks\n    parquet_content = b&amp;quot;&amp;quot;.join(chunks)\n    #converting the byte file to a file like object\n    parquet_buffer = io.BytesIO(parquet_content)\n\n    #Direct reading of parquet file using read_parquet_file leads to high memory consumption as observed from docker-stats \n    #df = pd.read_parquet(parquet_file)\n\n    #Set up the file pointer to Parquet object\n    parquet_file = pq.ParquetFile(parquet_buffer)\n    batch_size = 1024 #Experiment for performance \n    batches = parquet_file.iter_batches(batch_size) #batches will be a generator\n    file_name = None\n\n    parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir,&amp;#39;data&amp;#39;))\n\n    cnt= 0\n    for batch in batches:\n        #need to check if to_pandas is required\n        df = batch.to_pandas()    \n        #Construct the file name\n        file_name = os.path.join(parent_dir, f&amp;quot;{year}_{cnt}.parquet&amp;quot;) \n        try:\n            write_file_to_path(df,file_name)\n        except Exception as e:\n            print(f&amp;quot;Error writing: {e}&amp;quot;)\n            return e\n        cnt = cnt+1\n\ndef write_file_to_path(df,filename):\n        directory = os.path.dirname(filename)\n        if not os.path.exists(directory):\n            try:\n                os.makedirs(directory)\n                print(f&amp;quot;Directory &amp;#39;{directory}&amp;#39; created.&amp;quot;)\n            except Exception as e:\n                print(f&amp;quot;Error occured while creating directory &amp;#39;{directory}&amp;#39;.&amp;quot;)\n\n        #TO remove any existing files in the parquet directory\n        if os.path.exists(filename):\n                os.system(f&amp;quot;rm -r {filename}&amp;quot;)\n        #Write data to the directory\n        df.to_parquet(filename)\nif __name__ == &amp;quot;__main__&amp;quot;:\n     fetch_NY_Data(2023)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13z626a", "is_robot_indexable": true, "report_reasons": null, "author": "user19911506", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13z626a/need_feedback_from_the_folks_here_on_efficiency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13z626a/need_feedback_from_the_folks_here_on_efficiency/", "subreddit_subscribers": 108672, "created_utc": 1685781241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey /r/dataengineering,\n\nI've done some research on \"how to store large amounts of blockchain data for analysis and low-latency querying?\" and feel quite stumped - I can't seem to find a solution to; **analysis**, **low-latency querying** and **storing large amounts of data** (the Ethereum + Bitcoin blockchain is already 25\u201330TB).\n\nThe furthest I've gotten is:\n\n* Ethereum ETL into Parquet files (perhaps using Delta Lake too) in AWS S3\n* Spark to query these Parquet files\n\nThis allows for a nice way to store and run analytics on the blockchain data.\n\nThe problem I'm facing now is how do I make it queryable by an API in a reasonable amount of time (say &lt; 500ms).\n\nI'm thinking maybe there's a layer I can build on top of these files? Maybe I'm missing something here? Is this a good use-case for Elasticsearch?", "author_fullname": "t2_11os2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to store large amounts of blockchain data for analysis and low-latency querying?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13z4qzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685777456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done some research on &amp;quot;how to store large amounts of blockchain data for analysis and low-latency querying?&amp;quot; and feel quite stumped - I can&amp;#39;t seem to find a solution to; &lt;strong&gt;analysis&lt;/strong&gt;, &lt;strong&gt;low-latency querying&lt;/strong&gt; and &lt;strong&gt;storing large amounts of data&lt;/strong&gt; (the Ethereum + Bitcoin blockchain is already 25\u201330TB).&lt;/p&gt;\n\n&lt;p&gt;The furthest I&amp;#39;ve gotten is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ethereum ETL into Parquet files (perhaps using Delta Lake too) in AWS S3&lt;/li&gt;\n&lt;li&gt;Spark to query these Parquet files&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This allows for a nice way to store and run analytics on the blockchain data.&lt;/p&gt;\n\n&lt;p&gt;The problem I&amp;#39;m facing now is how do I make it queryable by an API in a reasonable amount of time (say &amp;lt; 500ms).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking maybe there&amp;#39;s a layer I can build on top of these files? Maybe I&amp;#39;m missing something here? Is this a good use-case for Elasticsearch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13z4qzw", "is_robot_indexable": true, "report_reasons": null, "author": "PM_SQL_INJECTION", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13z4qzw/how_to_store_large_amounts_of_blockchain_data_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13z4qzw/how_to_store_large_amounts_of_blockchain_data_for/", "subreddit_subscribers": 108672, "created_utc": 1685777456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's is the community's take on story points in Data engineering?  If you use story points how do account for a lot of the unknowns or hard to estimate complexity in data pipeline work when assessing points to complete a new pipeline?  Any norms you guys have settled on for estimating points during PI planning?  How long do you generally estimate each phase of a project will take from discovery and modeling to development and testing to final production deployment?\n\nI should add I don't like story points for DE work so if you don't use them, what is your approach?", "author_fullname": "t2_puuzgu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Story point norms in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13yr4ph", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685744842.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685743759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s is the community&amp;#39;s take on story points in Data engineering?  If you use story points how do account for a lot of the unknowns or hard to estimate complexity in data pipeline work when assessing points to complete a new pipeline?  Any norms you guys have settled on for estimating points during PI planning?  How long do you generally estimate each phase of a project will take from discovery and modeling to development and testing to final production deployment?&lt;/p&gt;\n\n&lt;p&gt;I should add I don&amp;#39;t like story points for DE work so if you don&amp;#39;t use them, what is your approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13yr4ph", "is_robot_indexable": true, "report_reasons": null, "author": "getafterit123", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13yr4ph/story_point_norms_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13yr4ph/story_point_norms_in_de/", "subreddit_subscribers": 108672, "created_utc": 1685743759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I am migrating code to Databricks/pyspark, which requires reading a bunch of Snowflake tables and views. Then a bunch of joins and such. \n\nIt generally works but some statements are not finishing with larger tables. I tried to troubleshoot one like this: \n\nsome\\_query = \"select \\~50 columns from Snowflake view with 1400 columns and 235m rows\"\n\ndf = spark.read.format(\"snowflake\").options(\\*\\*sfOptions).option(\"query\", some\\_query).load()\n\ndf.explain()\n\nAnd it's been going for 50 minutes now. Is it normal?\n\nSomeone suggested I go to Snowflake and look at Query History. That helps, but sometimes I don't even see the query there, so I don't know what's going on. \n\nI went to the cluster and just clicking around, like the Spark UI tab &gt; Executors, and the \"Active tasks\" is  0. So is it doing anything?? Driver logs don't show errors. \n\nWould love some tips on how to confirm what a query is doing. Thanks.", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking tips to troubleshoot Databricks queries of large Snowflake tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ymkox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685734584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I am migrating code to Databricks/pyspark, which requires reading a bunch of Snowflake tables and views. Then a bunch of joins and such. &lt;/p&gt;\n\n&lt;p&gt;It generally works but some statements are not finishing with larger tables. I tried to troubleshoot one like this: &lt;/p&gt;\n\n&lt;p&gt;some_query = &amp;quot;select ~50 columns from Snowflake view with 1400 columns and 235m rows&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;df = spark.read.format(&amp;quot;snowflake&amp;quot;).options(**sfOptions).option(&amp;quot;query&amp;quot;, some_query).load()&lt;/p&gt;\n\n&lt;p&gt;df.explain()&lt;/p&gt;\n\n&lt;p&gt;And it&amp;#39;s been going for 50 minutes now. Is it normal?&lt;/p&gt;\n\n&lt;p&gt;Someone suggested I go to Snowflake and look at Query History. That helps, but sometimes I don&amp;#39;t even see the query there, so I don&amp;#39;t know what&amp;#39;s going on. &lt;/p&gt;\n\n&lt;p&gt;I went to the cluster and just clicking around, like the Spark UI tab &amp;gt; Executors, and the &amp;quot;Active tasks&amp;quot; is  0. So is it doing anything?? Driver logs don&amp;#39;t show errors. &lt;/p&gt;\n\n&lt;p&gt;Would love some tips on how to confirm what a query is doing. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ymkox", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ymkox/seeking_tips_to_troubleshoot_databricks_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ymkox/seeking_tips_to_troubleshoot_databricks_queries/", "subreddit_subscribers": 108672, "created_utc": 1685734584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys. I currently work in research in the field of crypto/web 3. (Glorified way to say that I write on crypto topics for my employer.)\n\nI'm considering learning Dune (formerly called Dune Analytics) since it helps extract onchain data better and is a valuable skill that pays well. \n\nIs there anyone here who has dabbled with Dune and is sufficiently proficient in it?  Or were you able to land projects/jobs because you know your way around it? Would love to get in touch with you for some help and guidance.", "author_fullname": "t2_qj1xoq0b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Dune wizards here?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ylfgz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685732240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys. I currently work in research in the field of crypto/web 3. (Glorified way to say that I write on crypto topics for my employer.)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering learning Dune (formerly called Dune Analytics) since it helps extract onchain data better and is a valuable skill that pays well. &lt;/p&gt;\n\n&lt;p&gt;Is there anyone here who has dabbled with Dune and is sufficiently proficient in it?  Or were you able to land projects/jobs because you know your way around it? Would love to get in touch with you for some help and guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ylfgz", "is_robot_indexable": true, "report_reasons": null, "author": "heeguunte", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ylfgz/any_dune_wizards_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ylfgz/any_dune_wizards_here/", "subreddit_subscribers": 108672, "created_utc": 1685732240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone please help me find the best authentic physical conference related to Data Engineering or Software Engineer anywhere in Canada?\nIt can be any re:invent also. Anything which can add value. \nThanks in advance", "author_fullname": "t2_7r5xenrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering conference in Canada (anywhere)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ygpe4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685721746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone please help me find the best authentic physical conference related to Data Engineering or Software Engineer anywhere in Canada?\nIt can be any re:invent also. Anything which can add value. \nThanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ygpe4", "is_robot_indexable": true, "report_reasons": null, "author": "Heavy_End_2971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ygpe4/data_engineering_conference_in_canada_anywhere/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ygpe4/data_engineering_conference_in_canada_anywhere/", "subreddit_subscribers": 108672, "created_utc": 1685721746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Types of NoSQL Databases: Deep Dive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13ygmj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tbjolvjcd4pFGYwrUy21pTO8xhj6HfhJnXoILsY07vY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685721582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/types-of-nosql-databases-deep-dive", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?auto=webp&amp;v=enabled&amp;s=30a10ec625bb545da69ce94ff3eaecf423192ffb", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cb781c8024b80019cbb4d8671b9ac82ce55239a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3019c8983a64031de7cc3d528cc1a4082b979ce3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c14465cc9cf34e73a9dfcd9cee8f25a2024ac4f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=855bc3dfe0b174024f1952f236e466bf3704d9aa", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e348a1a0827d26c78592ce17c4f91f62d1984ae2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/80tNcHNbIRQjfde08NBFmc9ZPdlpjJmzmxjr5RwfcDA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21faf68b901e2ba8bc3816beedccedea6c06e79f", "width": 1080, "height": 540}], "variants": {}, "id": "M2IsBpOmOWUv3ZenklBzRFx5hxz60Ew2I2gjTGE5GCg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13ygmj4", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ygmj4/types_of_nosql_databases_deep_dive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/types-of-nosql-databases-deep-dive", "subreddit_subscribers": 108672, "created_utc": 1685721582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently building our pipeline from scratch with plain old Python and began to invest a little bit too much in the \"frameworkisation\" of our stuff. So we are looking for a fashionable framework to handle stream of events from PubSub and doing a little bit of lookup in some PostgreSQL instances.\n\nWhat would have been your primary choice of framework for this kind of project in Python?", "author_fullname": "t2_flv2knd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream processing framework for a new project in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13yftqn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685719716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently building our pipeline from scratch with plain old Python and began to invest a little bit too much in the &amp;quot;frameworkisation&amp;quot; of our stuff. So we are looking for a fashionable framework to handle stream of events from PubSub and doing a little bit of lookup in some PostgreSQL instances.&lt;/p&gt;\n\n&lt;p&gt;What would have been your primary choice of framework for this kind of project in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13yftqn", "is_robot_indexable": true, "report_reasons": null, "author": "Hashrann", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13yftqn/stream_processing_framework_for_a_new_project_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13yftqn/stream_processing_framework_for_a_new_project_in/", "subreddit_subscribers": 108672, "created_utc": 1685719716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have experience with doing test on the ref function? For example that the intermediate layer does not ref the mart layer but only staging?\n\nModel: intermediate_table1\nselect * from {{ ref(\u2018mart_table1\u2019) }}\n\nTest: fail\n\nModel: intermediate_table1\nselect * from {{ ref(\u2018staging_table1\u2019) }}\n\nTest: pass", "author_fullname": "t2_1j8f19jc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt QA on the ref function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13z2hlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685771274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have experience with doing test on the ref function? For example that the intermediate layer does not ref the mart layer but only staging?&lt;/p&gt;\n\n&lt;p&gt;Model: intermediate_table1\nselect * from {{ ref(\u2018mart_table1\u2019) }}&lt;/p&gt;\n\n&lt;p&gt;Test: fail&lt;/p&gt;\n\n&lt;p&gt;Model: intermediate_table1\nselect * from {{ ref(\u2018staging_table1\u2019) }}&lt;/p&gt;\n\n&lt;p&gt;Test: pass&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13z2hlk", "is_robot_indexable": true, "report_reasons": null, "author": "bgarcevic", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13z2hlk/dbt_qa_on_the_ref_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13z2hlk/dbt_qa_on_the_ref_function/", "subreddit_subscribers": 108672, "created_utc": 1685771274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I'm curious to hear your opinions on Cloudera's decision to transition from using the Spark engine with Hive to Tez engine with Hive. \n\nI've been using Hive on Tez recently and it appears to be slower compared to Hive on Spark. Even though big SQL queries have similar run times after optimizing the queue on Hive with Tez (Hive on Spark still outperforms it), smaller queries are significantly slower on Hive with Tez.\n\nI know Tez has some use cases and would be great if someone was using MapReduce and then transitioned to Tez. However, it can be challenging for someone who was previously using Spark engine and now moved to Tez engine.\n\nWould love to hear your thoughts\u2026", "author_fullname": "t2_658ryj26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hive on Tez engine vs Spark engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13yzd1z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685763039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I&amp;#39;m curious to hear your opinions on Cloudera&amp;#39;s decision to transition from using the Spark engine with Hive to Tez engine with Hive. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Hive on Tez recently and it appears to be slower compared to Hive on Spark. Even though big SQL queries have similar run times after optimizing the queue on Hive with Tez (Hive on Spark still outperforms it), smaller queries are significantly slower on Hive with Tez.&lt;/p&gt;\n\n&lt;p&gt;I know Tez has some use cases and would be great if someone was using MapReduce and then transitioned to Tez. However, it can be challenging for someone who was previously using Spark engine and now moved to Tez engine.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13yzd1z", "is_robot_indexable": true, "report_reasons": null, "author": "Different-Ad-2901", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13yzd1z/hive_on_tez_engine_vs_spark_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13yzd1z/hive_on_tez_engine_vs_spark_engine/", "subreddit_subscribers": 108672, "created_utc": 1685763039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey everyone!\n\nWe are a group of design students currently conducting academic research on an intriguing topic: the democratization of data and its potential of data to benefits the public. We believe that data can play a vital role in improving people's lives outside the realm of business, and we would love to hear your thoughts and experiences on this subject.\n\nIf you have a moment, we kindly invite you to answer one or more of the following questions either privately or as a comment:\n\n**- Please share your most recent experience using datasets for self-- worth or public value (non-business purposes). For example, a project that makes data accessible or extracts insights that can help the general public?**\n\n**- Working on the project, what worked and what didn't work? Were there barriers and challenges that you can share?**\n\n**- Are there any insights or tips you would like to share following the project?**\n\n**- Do you have any insights or thoughts regarding the use or accessibility of data for the public good?**\n\nYour contribution can be as brief or as detailed as you like. We greatly appreciate any answers, thoughts, or perspectives you are willing to share.\n\nThank you all!", "author_fullname": "t2_ummt1ubq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Academic research: data for the public good", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13zbgkq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685795509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;We are a group of design students currently conducting academic research on an intriguing topic: the democratization of data and its potential of data to benefits the public. We believe that data can play a vital role in improving people&amp;#39;s lives outside the realm of business, and we would love to hear your thoughts and experiences on this subject.&lt;/p&gt;\n\n&lt;p&gt;If you have a moment, we kindly invite you to answer one or more of the following questions either privately or as a comment:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- Please share your most recent experience using datasets for self-- worth or public value (non-business purposes). For example, a project that makes data accessible or extracts insights that can help the general public?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- Working on the project, what worked and what didn&amp;#39;t work? Were there barriers and challenges that you can share?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- Are there any insights or tips you would like to share following the project?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- Do you have any insights or thoughts regarding the use or accessibility of data for the public good?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Your contribution can be as brief or as detailed as you like. We greatly appreciate any answers, thoughts, or perspectives you are willing to share.&lt;/p&gt;\n\n&lt;p&gt;Thank you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13zbgkq", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Goat-2072", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13zbgkq/academic_research_data_for_the_public_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13zbgkq/academic_research_data_for_the_public_good/", "subreddit_subscribers": 108672, "created_utc": 1685795509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Redditors!\n\nI thought this community might find it very useful that Databricks has partnered with [Cleanlab](https://cleanlab.ai/) to bring automated data correction and ML model improvement for both structured and unstructured datasets to all Databricks users.\n\nA big problem for companies on platforms like Databricks is underutilized data: data and label quality is often too poor to be useful input for reliable business intelligence, training of ML models, or fine-tuning of LLMs. Using the new partner integration for Databricks, users get more value out of their data with automated finding and fixing of outliers, label issues, and other data issues in image, text, and tabular datasets, enabling them to train more reliable models and derive more accurate analytics and insights.\n\nTo highlight what's possible with this new integration, their recent [blog](https://www.databricks.com/blog/better-llms-better-data-using-cleanlab-studio) shows how LLMs (Large Language Models) trained on Databricks data can be **boosted in test accuracy (by over 30%) using Cleanlab Studio** to train ML models on an improved text dataset. \n\nYou only need a couple of lines of code too:\n\n    cleanlab_studio.upload_dataset(dataset)\n    dataset_fixed = cleanlab_studio.apply_corrections(id, dataset)", "author_fullname": "t2_s0qucgfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks users can now automatically correct data and improve ML models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13yhf6e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.48, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685723424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Redditors!&lt;/p&gt;\n\n&lt;p&gt;I thought this community might find it very useful that Databricks has partnered with &lt;a href=\"https://cleanlab.ai/\"&gt;Cleanlab&lt;/a&gt; to bring automated data correction and ML model improvement for both structured and unstructured datasets to all Databricks users.&lt;/p&gt;\n\n&lt;p&gt;A big problem for companies on platforms like Databricks is underutilized data: data and label quality is often too poor to be useful input for reliable business intelligence, training of ML models, or fine-tuning of LLMs. Using the new partner integration for Databricks, users get more value out of their data with automated finding and fixing of outliers, label issues, and other data issues in image, text, and tabular datasets, enabling them to train more reliable models and derive more accurate analytics and insights.&lt;/p&gt;\n\n&lt;p&gt;To highlight what&amp;#39;s possible with this new integration, their recent &lt;a href=\"https://www.databricks.com/blog/better-llms-better-data-using-cleanlab-studio\"&gt;blog&lt;/a&gt; shows how LLMs (Large Language Models) trained on Databricks data can be &lt;strong&gt;boosted in test accuracy (by over 30%) using Cleanlab Studio&lt;/strong&gt; to train ML models on an improved text dataset. &lt;/p&gt;\n\n&lt;p&gt;You only need a couple of lines of code too:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cleanlab_studio.upload_dataset(dataset)\ndataset_fixed = cleanlab_studio.apply_corrections(id, dataset)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?auto=webp&amp;v=enabled&amp;s=4fa344feec1203c5a3c8037ff4dc262b8199993c", "width": 1272, "height": 686}, "resolutions": [{"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=497bb037271a9f730fba0c9762fccfd03bc83854", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e9a92773876a44dff24033bb28bfcac23cc6be9", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d8289065076165108742e1f06e1eba13b2ba27e", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=860eca71c83a330feea1f4e293c3767498138aec", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29a33bb3ca9ee068da332d2221827e16765a2a4a", "width": 960, "height": 517}, {"url": "https://external-preview.redd.it/u93AAPorjY_6onZ2tzlt9podh1NWDKD3d0WUVvRJ3JU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bd1c44daed0be645a75f28344e835d728418327", "width": 1080, "height": 582}], "variants": {}, "id": "ZToDd-YI1Es8iCGrsYQ-wSRxT7XW4KUdV7ki6pqFGdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13yhf6e", "is_robot_indexable": true, "report_reasons": null, "author": "cmauck10", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13yhf6e/databricks_users_can_now_automatically_correct/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13yhf6e/databricks_users_can_now_automatically_correct/", "subreddit_subscribers": 108672, "created_utc": 1685723424.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}