{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nApologies if this has been posted before. \n\nDo you think one should learn AWS or Azure first? - I understand that several companies use multiple providers, but which provider would yield the best ROI in the current climate while considering the context below:\n\n**Some context:** I possess a background in Data Analytics (PowerBI, Python, SQL) and am interested in embarking on my DE journey. Thinking of starting off with Udacity and supplemental textbooks (maybe even exploring the famous youtube/GitHub Zoom course down the road). - Focus will be on understanding foundations and building projects because I think this is a great way to learn.\n\nWould love to know your thoughts. Thank you! xD", "author_fullname": "t2_5qxvthqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Recommendations: Azure vs. AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14h0b8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687531196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Apologies if this has been posted before. &lt;/p&gt;\n\n&lt;p&gt;Do you think one should learn AWS or Azure first? - I understand that several companies use multiple providers, but which provider would yield the best ROI in the current climate while considering the context below:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Some context:&lt;/strong&gt; I possess a background in Data Analytics (PowerBI, Python, SQL) and am interested in embarking on my DE journey. Thinking of starting off with Udacity and supplemental textbooks (maybe even exploring the famous youtube/GitHub Zoom course down the road). - Focus will be on understanding foundations and building projects because I think this is a great way to learn.&lt;/p&gt;\n\n&lt;p&gt;Would love to know your thoughts. Thank you! xD&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14h0b8x", "is_robot_indexable": true, "report_reasons": null, "author": "BullianBear", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14h0b8x/need_recommendations_azure_vs_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14h0b8x/need_recommendations_azure_vs_aws/", "subreddit_subscribers": 111919, "created_utc": 1687531196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At work, we have an ETL pipeline in airflow, via GCP cloud composer. I've just started working with this pipeline, and I've noticed that none of the code (all python) is actually stored in github or has any version control. \n\nThe dev who is more or less in charge of this project told me that there's no need to keep the code tracked with git/github, since it's stored and tracked by cloud composer. I know very little about cloud composer, but is super weird to me.\n\nLike how do you work without commit logs? Or git blame? And of course there's no CI/CD either, which  is kinda rough. \n\nI did see [this guide](https://cloud.google.com/composer/docs/dag-cicd-integration-guide) for setting up cloud composer to read from github, but it raised some questions that I can't find easy answers to:\n\n1. what code should actually be stored in github? Just the DAGs?\n2. how are dependencies managed in airflow? Like if I have a repository with a bunch of utility functions and DAGs, how do you make sure you have the right dependencies for running/testing code? Just pip install everything you see that is imported?\n3. is it normal to track DAGs and other airflow-related python code in github?? Or am I just crazy?", "author_fullname": "t2_8k5ls63w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal to pull airflow DAGs from a remote git repository?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14h00lk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687530459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At work, we have an ETL pipeline in airflow, via GCP cloud composer. I&amp;#39;ve just started working with this pipeline, and I&amp;#39;ve noticed that none of the code (all python) is actually stored in github or has any version control. &lt;/p&gt;\n\n&lt;p&gt;The dev who is more or less in charge of this project told me that there&amp;#39;s no need to keep the code tracked with git/github, since it&amp;#39;s stored and tracked by cloud composer. I know very little about cloud composer, but is super weird to me.&lt;/p&gt;\n\n&lt;p&gt;Like how do you work without commit logs? Or git blame? And of course there&amp;#39;s no CI/CD either, which  is kinda rough. &lt;/p&gt;\n\n&lt;p&gt;I did see &lt;a href=\"https://cloud.google.com/composer/docs/dag-cicd-integration-guide\"&gt;this guide&lt;/a&gt; for setting up cloud composer to read from github, but it raised some questions that I can&amp;#39;t find easy answers to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;what code should actually be stored in github? Just the DAGs?&lt;/li&gt;\n&lt;li&gt;how are dependencies managed in airflow? Like if I have a repository with a bunch of utility functions and DAGs, how do you make sure you have the right dependencies for running/testing code? Just pip install everything you see that is imported?&lt;/li&gt;\n&lt;li&gt;is it normal to track DAGs and other airflow-related python code in github?? Or am I just crazy?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;v=enabled&amp;s=20d96e49b41bb7bd00ff56b8e9ed66dc6ed60231", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cefbc38d0f527d52311150f3e2a5ee0cd4294045", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23a8451c1ff5cb4b86d4b12437a301825fbdeb9f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ac4fd116586a05dcece0cf47055879bc2ac44aa", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=467e1aba873015b34b1a4e2b7b44a79b2ab7343c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f90c3f55c5ad89d8d04390c2907361929ba9d300", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2c9d97288732e216db7689a964c326a90bdbe29", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14h00lk", "is_robot_indexable": true, "report_reasons": null, "author": "chamomile-crumbs", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14h00lk/is_it_normal_to_pull_airflow_dags_from_a_remote/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14h00lk/is_it_normal_to_pull_airflow_dags_from_a_remote/", "subreddit_subscribers": 111919, "created_utc": 1687530459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, so I've been self studying and lately, got a 3 month subscription on datacamp and I decided to take the opportunity to get a deeper look into data engineering. They seem to have a variety of courses, but I'm not exactly sure which to take and in what order. I know that they have a data engineering track thing, but I don't think it's enough and it seems to be missing many concepts that I've seen from some youtube roadmaps. So, I was hoping that some of you, whether you're a data engineer or someone who tried these courses before, would be able to help me learn in the appropriate order without missing some key concepts. Your help would be really appreciated. \n\nThis is the link to the courses\n\n[https://www.datacamp.com/data-courses/data-engineering-courses](https://www.datacamp.com/data-courses/data-engineering-courses)", "author_fullname": "t2_hvy9bzsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datacamp data engineering courses roadmap", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14h2sgr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687537265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so I&amp;#39;ve been self studying and lately, got a 3 month subscription on datacamp and I decided to take the opportunity to get a deeper look into data engineering. They seem to have a variety of courses, but I&amp;#39;m not exactly sure which to take and in what order. I know that they have a data engineering track thing, but I don&amp;#39;t think it&amp;#39;s enough and it seems to be missing many concepts that I&amp;#39;ve seen from some youtube roadmaps. So, I was hoping that some of you, whether you&amp;#39;re a data engineer or someone who tried these courses before, would be able to help me learn in the appropriate order without missing some key concepts. Your help would be really appreciated. &lt;/p&gt;\n\n&lt;p&gt;This is the link to the courses&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.datacamp.com/data-courses/data-engineering-courses\"&gt;https://www.datacamp.com/data-courses/data-engineering-courses&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?auto=webp&amp;v=enabled&amp;s=7b98d550c0803837fe3d4a013e7e5abb715007f7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1dbf4ded87a8d9eb65d6bb061303f2d920d52874", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ab1761e5190e30d94122d78ccc62ab846e9fc8f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c3d569d7a6e0f4fc4668e309315f4c167244de2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96f1e8442b3aef432d6d858d01a816c55f3d2de2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a7f3557b0d56d8c6ba51af4c56a5b1ebcae12a4", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/S2wmkVS2noOwqJFkqAOgzW276TRr-Xz0PKLEA3MuIy4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bec3b8ad604fa4b316ba9ff30f524357eaaa72e", "width": 1080, "height": 567}], "variants": {}, "id": "ljwW312xt1z1t0ZiTa9rJsrX-p4IHiPkBQDDxV5w3Wo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14h2sgr", "is_robot_indexable": true, "report_reasons": null, "author": "IamHoussem", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14h2sgr/datacamp_data_engineering_courses_roadmap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14h2sgr/datacamp_data_engineering_courses_roadmap/", "subreddit_subscribers": 111919, "created_utc": 1687537265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a code based on 10 stored procedurs which fills the same table. If is understand the dtb way correctly, I first should change this into 10 models (views or table). Then I need to make a final model which merge all these models together with union statments. And I choose to materialize these tables as view, the final merge statment will try to merge 10 views together, which has to become very very slow. \n\nWill this not be very slow? My experience with union is not great. If I choose to use vieAnother think is that it will make 10 tables/view that I did not have before, which will pollute our database. \n\nOr is it something I do not understand about DBT", "author_fullname": "t2_7hbs1ihu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gsiah", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687507344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a code based on 10 stored procedurs which fills the same table. If is understand the dtb way correctly, I first should change this into 10 models (views or table). Then I need to make a final model which merge all these models together with union statments. And I choose to materialize these tables as view, the final merge statment will try to merge 10 views together, which has to become very very slow. &lt;/p&gt;\n\n&lt;p&gt;Will this not be very slow? My experience with union is not great. If I choose to use vieAnother think is that it will make 10 tables/view that I did not have before, which will pollute our database. &lt;/p&gt;\n\n&lt;p&gt;Or is it something I do not understand about DBT&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14gsiah", "is_robot_indexable": true, "report_reasons": null, "author": "Wise-Ad-7492", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gsiah/dbt_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gsiah/dbt_speed/", "subreddit_subscribers": 111919, "created_utc": 1687507344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Disclaimer:** My background is in Statistics and ML -- so I admittedly have little experience in creating/architecting pipelines.\n\nI am working on a personal project that scrapes auction results and my goal is to create a live pricing model. Up to this point the work has been \"ad-hoc\" and collection is manually triggered. However, I feel pretty satisfied with the initial scraped outputs (\\~4K auctions) and I want to begin the steps towards standardizing the scraping workflow with an automated and scheduled pipeline. My very open ended question is -- **what are the best routes for scaling up this type of workflow?**\n\nHere are some additional details:\n\n1. Only a select few pages from the entire website are scraped. These pages are independent from one another and as a result, I imagine that scaling horizontally with multiple VMs or Containers doing one page/scrape-job would be efficient.\n2. All past and historical auctions are viewable. This means that *initial* scraping is a bit expensive -- it involves crawling *all* historical auctions and scraping the results from each one. However after this first initial scrape -- ideally any subsequent scrapes would only crawl the most recently closed auctions and **update and add** to the previous scrape (at most I expect 3-5 auction results added).\n3. The scraped data are dumped into individual CSV files at the moment (a file for each page/product that is scraped). I am not certain but I foresee this may be an unsuitable format if I'd like to be able to update the data. In that case -- would I likely need to set up a DB? I guess the problem I am foreseeing is how can the scraper \"know\" which auctions are \"new\" versus which have already been scraped.\n\n**Potential Idea:**\n\n1. Use GitHub Actions to automate scraping on a cron schedule. However does this scale \"horizontally\" if I'd like to have several scrapers running different jobs in parallel?\n\nTIA for any inputs! I am also curious what platforms and tools people think would be most appropriate for this type of project as well (AWS, GCP, Azure)", "author_fullname": "t2_tzg6wdez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Scale and Automate Webscraping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14hdpg4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687570779.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687564323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; My background is in Statistics and ML -- so I admittedly have little experience in creating/architecting pipelines.&lt;/p&gt;\n\n&lt;p&gt;I am working on a personal project that scrapes auction results and my goal is to create a live pricing model. Up to this point the work has been &amp;quot;ad-hoc&amp;quot; and collection is manually triggered. However, I feel pretty satisfied with the initial scraped outputs (~4K auctions) and I want to begin the steps towards standardizing the scraping workflow with an automated and scheduled pipeline. My very open ended question is -- &lt;strong&gt;what are the best routes for scaling up this type of workflow?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are some additional details:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Only a select few pages from the entire website are scraped. These pages are independent from one another and as a result, I imagine that scaling horizontally with multiple VMs or Containers doing one page/scrape-job would be efficient.&lt;/li&gt;\n&lt;li&gt;All past and historical auctions are viewable. This means that &lt;em&gt;initial&lt;/em&gt; scraping is a bit expensive -- it involves crawling &lt;em&gt;all&lt;/em&gt; historical auctions and scraping the results from each one. However after this first initial scrape -- ideally any subsequent scrapes would only crawl the most recently closed auctions and &lt;strong&gt;update and add&lt;/strong&gt; to the previous scrape (at most I expect 3-5 auction results added).&lt;/li&gt;\n&lt;li&gt;The scraped data are dumped into individual CSV files at the moment (a file for each page/product that is scraped). I am not certain but I foresee this may be an unsuitable format if I&amp;#39;d like to be able to update the data. In that case -- would I likely need to set up a DB? I guess the problem I am foreseeing is how can the scraper &amp;quot;know&amp;quot; which auctions are &amp;quot;new&amp;quot; versus which have already been scraped.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Potential Idea:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use GitHub Actions to automate scraping on a cron schedule. However does this scale &amp;quot;horizontally&amp;quot; if I&amp;#39;d like to have several scrapers running different jobs in parallel?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;TIA for any inputs! I am also curious what platforms and tools people think would be most appropriate for this type of project as well (AWS, GCP, Azure)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14hdpg4", "is_robot_indexable": true, "report_reasons": null, "author": "divergingLoss", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14hdpg4/how_to_scale_and_automate_webscraping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14hdpg4/how_to_scale_and_automate_webscraping/", "subreddit_subscribers": 111919, "created_utc": 1687564323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a Data Science company where we had a discussion regarding what should be the best layer for this use case:\n\nWe want to create an analytics dashboard in Power BI. Let's imagine we are digesting data from an on-premise/cloud SQL Server Database. That data will be ingested in batch once a day, and that data may be updated, so we must import past data (not only the new one) since old data may be updated. We are imagining the following layer model:\n\n- Bronze: Raw data in parquet format with a DATE/TABLE folder structure\n- Silver: Delta tables that will be upserted from the bronze data (using the merge statement, so inserting new values and updating old ones that have been modified). These tables are transactional, so are not optimal for being used in Power BI directly.\n- Gold: Star shema model of the transactional tables, optimized for Power BI ingestion.\n\nThe main doubt is:\n- If, in the future, I decide to pre-join tables of the star schema to make digestion on Power BI easier (for example, join tableA and tableB to create tableAB, so Power BI engine don't have to do that job), I saw the Gold Layer as the appropriate layer to store that table, but since the star schema is already there, I don't know if that would be messy and get into a bad practice. \n\nA solution would be to actually store the star schema in the Silver layer, and leave the Gold layer initially empty. When a case as described appear, store the tableAB in that layer.\n\nI don't really know what would be the best practice in this use case scenario.", "author_fullname": "t2_nt1xl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should be the right layer for normalized data in a Data Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14hale0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687556321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a Data Science company where we had a discussion regarding what should be the best layer for this use case:&lt;/p&gt;\n\n&lt;p&gt;We want to create an analytics dashboard in Power BI. Let&amp;#39;s imagine we are digesting data from an on-premise/cloud SQL Server Database. That data will be ingested in batch once a day, and that data may be updated, so we must import past data (not only the new one) since old data may be updated. We are imagining the following layer model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Bronze: Raw data in parquet format with a DATE/TABLE folder structure&lt;/li&gt;\n&lt;li&gt;Silver: Delta tables that will be upserted from the bronze data (using the merge statement, so inserting new values and updating old ones that have been modified). These tables are transactional, so are not optimal for being used in Power BI directly.&lt;/li&gt;\n&lt;li&gt;Gold: Star shema model of the transactional tables, optimized for Power BI ingestion.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The main doubt is:\n- If, in the future, I decide to pre-join tables of the star schema to make digestion on Power BI easier (for example, join tableA and tableB to create tableAB, so Power BI engine don&amp;#39;t have to do that job), I saw the Gold Layer as the appropriate layer to store that table, but since the star schema is already there, I don&amp;#39;t know if that would be messy and get into a bad practice. &lt;/p&gt;\n\n&lt;p&gt;A solution would be to actually store the star schema in the Silver layer, and leave the Gold layer initially empty. When a case as described appear, store the tableAB in that layer.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really know what would be the best practice in this use case scenario.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14hale0", "is_robot_indexable": true, "report_reasons": null, "author": "marcos249", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14hale0/what_should_be_the_right_layer_for_normalized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14hale0/what_should_be_the_right_layer_for_normalized/", "subreddit_subscribers": 111919, "created_utc": 1687556321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "T\u0336r\u0336i\u0336n\u0336o\u0336 \u0336 Large Scale Data Processing: An Origin Story", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_14gzn7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/_VUQ-Jh-M68?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Trino: An Origin Story\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Trino: An Origin Story", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/_VUQ-Jh-M68?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Trino: An Origin Story\"&gt;&lt;/iframe&gt;", "author_name": "bur2chee", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/_VUQ-Jh-M68/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@bur2chee"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/_VUQ-Jh-M68?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Trino: An Origin Story\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/14gzn7a", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/600nsnS7fh8m_Xtm2DpflK2SbL24YbPF6nDpDEq9ng8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687529563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=_VUQ-Jh-M68", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qryQzrpLLecwe6WeHqp1i8zSnn2F_8g1BAJTMnbwMHQ.jpg?auto=webp&amp;v=enabled&amp;s=68fff25d45881ac0179e660c3710896084963bd8", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/qryQzrpLLecwe6WeHqp1i8zSnn2F_8g1BAJTMnbwMHQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0b4b881f6374bba3c23e22a73854449099a06d0", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/qryQzrpLLecwe6WeHqp1i8zSnn2F_8g1BAJTMnbwMHQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=415bca337a704c7cffd033206dde2a85c2615b69", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/qryQzrpLLecwe6WeHqp1i8zSnn2F_8g1BAJTMnbwMHQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10573c774ac8145206e7d7472f5a4cd032f9b9fb", "width": 320, "height": 240}], "variants": {}, "id": "Fk-3zbgpnmC0phFSgmbOFr87DWyWvPnP7n731NP4Me4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14gzn7a", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gzn7a/trino_large_scale_data_processing_an_origin_story/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=_VUQ-Jh-M68", "subreddit_subscribers": 111919, "created_utc": 1687529563.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Trino: An Origin Story", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/_VUQ-Jh-M68?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Trino: An Origin Story\"&gt;&lt;/iframe&gt;", "author_name": "bur2chee", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/_VUQ-Jh-M68/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@bur2chee"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I hope this article could help anyone struggling with Helm charts and Spark operator.\n\nAs i have finished writing a series of tutorials about manipulating Spark on Kubernetes for begginers.I've found only old exemples not working well, added to changes in version, this makes Spark operator tricky to use even for a simple exemple.Here is the tutorial  \n[https://medium.com/@SaphE/deploying-apache-spark-on-kubernetes-using-helm-charts-simplified-cluster-management-and-ee5e4f2264fd](https://medium.com/@SaphE/deploying-apache-spark-on-kubernetes-using-helm-charts-simplified-cluster-management-and-ee5e4f2264fd)  \n\n\nPeace", "author_fullname": "t2_ae4tw1pnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploy Spark on kubernetes using spark operator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14hdshn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687564553.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this article could help anyone struggling with Helm charts and Spark operator.&lt;/p&gt;\n\n&lt;p&gt;As i have finished writing a series of tutorials about manipulating Spark on Kubernetes for begginers.I&amp;#39;ve found only old exemples not working well, added to changes in version, this makes Spark operator tricky to use even for a simple exemple.Here is the tutorial&lt;br/&gt;\n&lt;a href=\"https://medium.com/@SaphE/deploying-apache-spark-on-kubernetes-using-helm-charts-simplified-cluster-management-and-ee5e4f2264fd\"&gt;https://medium.com/@SaphE/deploying-apache-spark-on-kubernetes-using-helm-charts-simplified-cluster-management-and-ee5e4f2264fd&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Peace&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?auto=webp&amp;v=enabled&amp;s=6cb27f16e4b39771d9ec229f7a0d639823f44263", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31f330f50053b5df405ec0bb66809b86a509ad09", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d8800124f03f07ed923620c964b15aa6c75c90", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9a6b614464dcc01f53161cfc18dad2d4981ac65", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9853dc192df755ea1eeb18d316a3f0dd265f63a1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b2076caf0afbe06b9f91b6c02420c42d7fceffb", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/6YNTiRolKDmgoAaO3Rtb6LtXklCdHHJanIXZIFKwevA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc7a32edf4a4feaed51f86f5749bfc018fd2def3", "width": 1080, "height": 607}], "variants": {}, "id": "_JrtXfbRg-zigPtfgE5-pxqxVwXpZzVRPSDGes3EzEo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14hdshn", "is_robot_indexable": true, "report_reasons": null, "author": "PhysicalTomorrow2098", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14hdshn/deploy_spark_on_kubernetes_using_spark_operator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14hdshn/deploy_spark_on_kubernetes_using_spark_operator/", "subreddit_subscribers": 111919, "created_utc": 1687564553.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7kx0y9gzk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg, Delta Lake, and Hudi, oh my!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_14gztjo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PgbxiD-ALxktaGujzuDbgg55-_CcUCdhrY7Kzi-HgyE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687529994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dev.to", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dev.to/starburstengineering/well-well-well-how-the-open-tables-have-turned-58og", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?auto=webp&amp;v=enabled&amp;s=7119a957b34f47d91440f6b59fcac58dd69a6a0b", "width": 1128, "height": 598}, "resolutions": [{"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fe8e66c9435302e522a0c2bb129dfe1fe489cf0", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5559ce659c2293e05d5781a8f9fc77c82af6c0c", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b4670c294df5912e1d8ce12a3bce0459fb904d7", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec92c242e7544fcd627f4f0cad8168ddb06c9839", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dabb34cc18d18a32c01b768836a84ecf4ddd9f8", "width": 960, "height": 508}, {"url": "https://external-preview.redd.it/uC9Za73urUHR-fmDhN2YoA0TosewPMoTB1KYSEApyBk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cacf28132324dcba20afef1b8830396b7a07e430", "width": 1080, "height": 572}], "variants": {}, "id": "PrcLAOTCETOXTwqaw2KpNWqhTV4e_X8W9AVV189PTYo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14gztjo", "is_robot_indexable": true, "report_reasons": null, "author": "Candid-Avocado-5191", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gztjo/iceberg_delta_lake_and_hudi_oh_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dev.to/starburstengineering/well-well-well-how-the-open-tables-have-turned-58og", "subreddit_subscribers": 111919, "created_utc": 1687529994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we built Gradient to optimize Databricks clusters at scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_14gz5us", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1-9vViNDlO9vTOZfPjQWOiRnzSwrCBvdKU2Ckov1KFk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687528370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/sync-computing/developing-gradient-part-i-cd39b462c6a7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?auto=webp&amp;v=enabled&amp;s=a201a897252ec7c911542e628d862c45d57bb876", "width": 1200, "height": 874}, "resolutions": [{"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f78eaa94073b4cd429bc458ec7324e8c8caca88", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe6e73c4209df358b6946ba79e7584e043cd9c44", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b89240dfad23d402eaf0955f785e32cff45b08f", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bee39f6bb1f26c7fe6257705ef1873b1ed2078fa", "width": 640, "height": 466}, {"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad746c4699da43b83ff944ec07122a80ef4268ee", "width": 960, "height": 699}, {"url": "https://external-preview.redd.it/SmvcbbPCp5lW-5fjh32I4DBC5rI0xJkvarcXmyn47ZI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3322130b9e5d9af20699225a65972ee94aeec0ed", "width": 1080, "height": 786}], "variants": {}, "id": "Ptr9d6pNPCgeJtZisEaIWIu_1BOTjDhlu_P-2Jmrbhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14gz5us", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gz5us/how_we_built_gradient_to_optimize_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/sync-computing/developing-gradient-part-i-cd39b462c6a7", "subreddit_subscribers": 111919, "created_utc": 1687528370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have just started experimenting with dbt\\_unit\\_testing and the mock\\_ref and expect calls. I have got tests working with some mock\\_ref csv input formats for models I have created one layer up. But I am stuck trying to test out a metric I have built which is built from a model below and that model from a source. Ideally what I would like to happen, is supply a mock csv as a source that data get passed into my model which then using that mock data in the model populates my metric and runs tests on that. When trying I am getting errors saying it wants me to declare the data in the middle layer but I would like to use the output from the mock source data passed through the model to use as my mock data for the metric.\n\nI think we're probably getting into the realms of integration testing but just thought I'd ask if anyone had a similar issue.\n\nThanks", "author_fullname": "t2_1xgwxgkl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Unit Testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14h0fxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687531529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have just started experimenting with dbt_unit_testing and the mock_ref and expect calls. I have got tests working with some mock_ref csv input formats for models I have created one layer up. But I am stuck trying to test out a metric I have built which is built from a model below and that model from a source. Ideally what I would like to happen, is supply a mock csv as a source that data get passed into my model which then using that mock data in the model populates my metric and runs tests on that. When trying I am getting errors saying it wants me to declare the data in the middle layer but I would like to use the output from the mock source data passed through the model to use as my mock data for the metric.&lt;/p&gt;\n\n&lt;p&gt;I think we&amp;#39;re probably getting into the realms of integration testing but just thought I&amp;#39;d ask if anyone had a similar issue.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14h0fxf", "is_robot_indexable": true, "report_reasons": null, "author": "bensmithy97", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14h0fxf/dbt_unit_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14h0fxf/dbt_unit_testing/", "subreddit_subscribers": 111919, "created_utc": 1687531529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1opcp19k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Difference Between Micro-Partitioning vs. Indexing and a Better Way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_14gzaaf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4a7LvRxMwzD3NKfdytRORxe9FAsvydCdD1T0NiVqb38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687528688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dev.to", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dev.to/romanvainbrand/the-difference-between-micro-partitioning-vs-indexing-and-a-better-way-13n2", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Kxo_pegK4ShZjk9wxaiLUkC854FyLHU9s2JWlSvbtVc.jpg?auto=webp&amp;v=enabled&amp;s=80aa24b811c5eff6c60ac9e13003dfc2f4e1cf3a", "width": 1000, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/Kxo_pegK4ShZjk9wxaiLUkC854FyLHU9s2JWlSvbtVc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f148516aefffa8d4c06d1a5d04909175da911506", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Kxo_pegK4ShZjk9wxaiLUkC854FyLHU9s2JWlSvbtVc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff7993c5484ec9793c4c57e837e5b4c9a2ef0eb3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Kxo_pegK4ShZjk9wxaiLUkC854FyLHU9s2JWlSvbtVc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63a7d82c843f3283d67ec32d18b4340d0389f429", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Kxo_pegK4ShZjk9wxaiLUkC854FyLHU9s2JWlSvbtVc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2b4528a5f1149bc36b334f47f3c8f9a737c82bf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Kxo_pegK4ShZjk9wxaiLUkC854FyLHU9s2JWlSvbtVc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=145f2d41419167e9065519173639092115c8b4a2", "width": 960, "height": 480}], "variants": {}, "id": "uZs3mcLmQBE5Ip9gP0OCr-cgIA11CUEJTqfi2Q1bSVQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14gzaaf", "is_robot_indexable": true, "report_reasons": null, "author": "CorgiLover4lyfe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gzaaf/the_difference_between_micropartitioning_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dev.to/romanvainbrand/the-difference-between-micro-partitioning-vs-indexing-and-a-better-way-13n2", "subreddit_subscribers": 111919, "created_utc": 1687528688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone recommend me a data version control tool that has similar functionality to Git LFS and can be used with files located in Google drive? Any advice is appreciated.", "author_fullname": "t2_tqpb779o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a Google drive compatible solution for versioning large files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gxwut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687525067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone recommend me a data version control tool that has similar functionality to Git LFS and can be used with files located in Google drive? Any advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14gxwut", "is_robot_indexable": true, "report_reasons": null, "author": "bofor6157", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gxwut/is_there_a_google_drive_compatible_solution_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gxwut/is_there_a_google_drive_compatible_solution_for/", "subreddit_subscribers": 111919, "created_utc": 1687525067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I found out today the airflow instance i have set up on an azure vm is not running. I wiped the images, containers,and volumes to launch a fresh copy using docker-compose up -d. But it looks like i am not able to the other services to run(webserver,scehduler). only \n\nCreating prod\\_airflow\\_postgres\\_1 ... done\n\nCreating prod\\_airflow\\_redis\\_1    ... done\n\nCreating prod\\_airflow\\_airflow-init\\_1 ... done\n\nit gets stuck here and sometimes loses connection to VM. \n\nanyone ran into this ?", "author_fullname": "t2_wcnw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14go4i7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687492500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found out today the airflow instance i have set up on an azure vm is not running. I wiped the images, containers,and volumes to launch a fresh copy using docker-compose up -d. But it looks like i am not able to the other services to run(webserver,scehduler). only &lt;/p&gt;\n\n&lt;p&gt;Creating prod_airflow_postgres_1 ... done&lt;/p&gt;\n\n&lt;p&gt;Creating prod_airflow_redis_1    ... done&lt;/p&gt;\n\n&lt;p&gt;Creating prod_airflow_airflow-init_1 ... done&lt;/p&gt;\n\n&lt;p&gt;it gets stuck here and sometimes loses connection to VM. &lt;/p&gt;\n\n&lt;p&gt;anyone ran into this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14go4i7", "is_robot_indexable": true, "report_reasons": null, "author": "obokima", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14go4i7/airflow_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14go4i7/airflow_help/", "subreddit_subscribers": 111919, "created_utc": 1687492500.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}