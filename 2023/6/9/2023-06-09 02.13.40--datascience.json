{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Faster sorting algorithms discovered using deep reinforcement learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 51, "top_awarded_type": null, "hide_score": false, "name": "t3_1445idg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 145, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 145, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/_6CMidQNwnpryTTrkgZsDmQu4g4ZU1lwDpYQLRvTkR8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686220142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "nature.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.nature.com/articles/s41586-023-06004-9", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?auto=webp&amp;v=enabled&amp;s=0dbe22f40422f2d827efa1ddbdcd3bc253dcd72b", "width": 685, "height": 254}, "resolutions": [{"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10b04aa3d0856412bce5e98a15197bc73f8bb4a1", "width": 108, "height": 40}, {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=201b1153b6006c3ed2254e8ef4e6a2742193e750", "width": 216, "height": 80}, {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1394dc9af391fc433d0d2b19dcb458adca9ca4cb", "width": 320, "height": 118}, {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22558066595da63560a5e442dc49ac9fc99f18fa", "width": 640, "height": 237}], "variants": {}, "id": "5XnGLzdtcgD58mlZFry7KmbBERR_Bafyn5GyOj4D83k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1445idg", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1445idg/faster_sorting_algorithms_discovered_using_deep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.nature.com/articles/s41586-023-06004-9", "subreddit_subscribers": 921072, "created_utc": 1686220142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm shitting myself for the last interview: HR -&gt; business case -&gt; manager -&gt; director (tomorrow). It's a junior data scientist consultant position, nothing fancy, but it'd be my first job after graduation, and it's in my city of choice.\n\nWish me luck :)", "author_fullname": "t2_nw83t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Last of 4 interviews tomorrow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144jtzm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 116, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 116, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686255454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m shitting myself for the last interview: HR -&amp;gt; business case -&amp;gt; manager -&amp;gt; director (tomorrow). It&amp;#39;s a junior data scientist consultant position, nothing fancy, but it&amp;#39;d be my first job after graduation, and it&amp;#39;s in my city of choice.&lt;/p&gt;\n\n&lt;p&gt;Wish me luck :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144jtzm", "is_robot_indexable": true, "report_reasons": null, "author": "chacalgamer", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144jtzm/last_of_4_interviews_tomorrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144jtzm/last_of_4_interviews_tomorrow/", "subreddit_subscribers": 921072, "created_utc": 1686255454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, I have an upcoming business case interview, I have one week to prepare and provide the presentation.\n\nThe odd thing is that the 'business case' is two questions that I need to answer, however, the tricky part is, I have **not received** any data sets or anything to analyze.\n\nI suppose I'm expected to retrieve data online to answer the questions and provide a solution that is both strategic and logical.\n\nAnyone else that had experience with a similar situation and that can provide some tips on what to focus?", "author_fullname": "t2_8qgzhwuo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business case without data to analyze", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144f7fy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686244763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I have an upcoming business case interview, I have one week to prepare and provide the presentation.&lt;/p&gt;\n\n&lt;p&gt;The odd thing is that the &amp;#39;business case&amp;#39; is two questions that I need to answer, however, the tricky part is, I have &lt;strong&gt;not received&lt;/strong&gt; any data sets or anything to analyze.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m expected to retrieve data online to answer the questions and provide a solution that is both strategic and logical.&lt;/p&gt;\n\n&lt;p&gt;Anyone else that had experience with a similar situation and that can provide some tips on what to focus?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144f7fy", "is_robot_indexable": true, "report_reasons": null, "author": "PizzaStartup", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144f7fy/business_case_without_data_to_analyze/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144f7fy/business_case_without_data_to_analyze/", "subreddit_subscribers": 921072, "created_utc": 1686244763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Unlike  normal reporting, A/B testing collects data of a different combination  of dimensions every time. It is also a complicated kind of analysis of  immense data. In our case, we have a real-time data volume of millions  of OPS (Operations Per Second), with each operation involving around 20  data tags and over a dozen dimensions.\n\nFor  effective A/B testing, as data engineers, we must ensure quick  computation as well as high data integrity (which means no duplication  and no data loss). I\u2019m sure I\u2019m not the only one to say this: it is  hard!\n\nLet me show you our long-term struggle with our previous Druid-based data platform.\n\n# Platform Architecture 1.0\n\n**Components**: Apache Storm + Apache Druid + MySQL\n\nThis  was our real-time datawarehouse, where Apache Storm was the real-time  data processing engine and Apache Druid pre-aggregated the data.  However, Druid did not support certain paging and join queries, so we  wrote data from Druid to MySQL regularly, making MySQL the \u201cmaterialized  view\u201d of Druid. But that was only a duct tape solution as it couldn\u2019t  support our ever enlarging real-time data size. So data timeliness was  unattainable.\n\nhttps://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2\n\n# Platform Architecture 2.0\n\n**Components**: Apache Flink + Apache Druid + TiDB\n\nThis  time, we replaced Storm with Flink, and MySQL with TiDB. Flink was more  powerful in terms of semantics and features, while TiDB, with its  distributed capability, was more maintainable than MySQL. But  architecture 2.0 was nowhere near our goal of end-to-end data  consistency, either, because when processing huge data, enabling TiDB  transactions largely slowed down data writing. Plus, Druid itself did  not support standard SQL, so there were some learning costs and  frictions in usage.\n\nhttps://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68756da95ae7d2221d85e419759103e068205ad8\n\n# Platform Architecture 3.0\n\n**Components**: Apache Flink + [Apache Doris](https://github.com/apache/doris)\n\nWe  replaced Apache Druid with Apache Doris as the OLAP engine, which could  also serve as a unified data serving gateway. So in Architecture 3.0,  we only need to maintain one set of query logic. And we layered our  real-time datawarehouse to increase reusability of real-time data.\n\nhttps://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409\n\nTurns  out the combination of Flink and Doris was the answer. We can exploit  their features to realize quick computation and data consistency. Keep  reading and see how we make it happen.\n\n# Quick Computation\n\nAs  one piece of operation data can be attached to 20 tags, in A/B testing,  we compare two groups of data centering only one tag each time. At  first, we thought about splitting one piece of operation data (with 20  tags) into 20 pieces of data of only one tag upon data ingestion, and  then importing them into Doris for analysis, but that could cause a data  explosion and thus huge pressure on our clusters.\n\nThen  we tried moving part of such workload to the computation engine. So we  tried and \u201cexploded\u201d the data in Flink, but soon regretted it, because  when we aggregated the data using the global hash windows in Flink jobs,  the network and CPU usage also \u201cexploded\u201d.\n\nOur  third shot was to aggregate data locally in Flink right after we split  it. As is shown below, we create a window in the memory of one operator  for local aggregation; then we further aggregate it using the global  hash windows. Since two operators chained together are in one thread,  transferring data between operators consumes much less network  resources. **The two-step aggregation method, combined with the** [**Aggregate model**](https://doris.apache.org/docs/dev/data-table/data-model) **of Apache Doris, can keep data explosion in a manageable range.**\n\nhttps://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152\n\nFor  convenience in A/B testing, we make the test tag ID the first sorted  field in Apache Doris, so we can quickly locate the target data using  sorted indexes. To further minimize data processing in queries, we  create materialized views with the frequently used dimensions. With  constant modification and updates, the materialized views are applicable  in 80% of our queries.\n\nTo  sum up, with the application of sorted index and materialized views, we  reduce our query response time to merely seconds in A/B testing.\n\n# Data Integrity Guarantee\n\nImagine  that your algorithm designers worked sweat and tears trying to improve  the business, only to find their solution unable to be validated by A/B  testing due to data loss. This is an unbearable situation, and we make  every effort to avoid it.\n\n# Develop a Sink-to-Doris Component\n\nTo  ensure end-to-end data integrity, we developed a Sink-to-Doris  component. It is built on our own Flink Stream API scaffolding and  realized by the idempotent writing of Apache Doris and the two-stage  commit mechanism of Apache Flink. On top of it, we have a data  protection mechanism against anomalies.\n\nIt  is the result of our long-term evolution. We used to ensure data  consistency by implementing \u201cone writing for one tag ID\u201d. Then we  realized we could make good use of the transactions in Apache Doris and  the two-stage commit of Apache Flink.\n\nhttps://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df\n\nAs is shown above, this is how two-stage commit works to guarantee data consistency:\n\n1. Write data into local files;\n2. Stage One: pre-commit data to Apache Doris. Save the Doris transaction ID into status;\n3. If checkpoint fails, manually abandon the transaction; if checkpoint succeeds, commit the transaction in Stage Two;\n4. If  the commit fails after multiple retries, the transaction ID and the  relevant data will be saved in HDFS, and we can restore the data via  Broker Load.\n\nWe  make it possible to split a single checkpoint into multiple  transactions, so that we can prevent one Stream Load from taking more  time than a Flink checkpoint in the event of large data volumes.\n\n# Application Display\n\nThis  is how we implement Sink-to-Doris. The component has blocked API calls  and topology assembly. With simple configuration, we can write data into  Apache Doris via Stream Load.\n\nhttps://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19\n\n# Cluster Monitoring\n\nFor  cluster and host monitoring, we adopted the metrics templates provided  by the Apache Doris community. For data monitoring, in addition to the  template metrics, we added Stream Load request numbers and loading  rates.\n\nhttps://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c64d709b3f894937e97519e94305df903125c5\n\nOther metrics of our concerns include data writing speed and task  processing time. In the case of anomalies, we will receive notifications  in the form of phone calls, messages, and emails.\n\nhttps://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9\n\n# Key Takeaways\n\nThe  recipe for successful A/B testing is quick computation and high data  integrity. For this purpose, we implement a two-step aggregation method  in Apache Flink, utilize the Aggregate model, materialized view, and  short indexes of Apache Doris. Then we develop a Sink-to-Doris  component, which is realized by the idempotent writing of Apache Doris  and the two-stage commit mechanism of Apache Flink.", "author_fullname": "t2_uu592ayo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A/B Testing was a handful", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z58h5wi0nt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c186c8d84ad698b1adffca6c9c64b6301108a92"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16b576ec77a9d852b4df6e1d63cf0ed5b6410b4d"}, {"y": 222, "x": 320, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ebe77c0cbe174b1236494a5ea257cbb442265ed"}, {"y": 444, "x": 640, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14e64d246ced5ab52476c332a91eaae763b0a9e5"}, {"y": 666, "x": 960, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8c12701cebe7f1c9c669bf93b73981ee89db6cc"}, {"y": 749, "x": 1080, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34792d4a2e64abdce938b6891a947f55ecf4253f"}], "s": {"y": 888, "x": 1280, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9"}, "id": "z58h5wi0nt4b1"}, "q9n64kxkmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 73, "x": 108, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0603e3d2c444bc4f87f72b192643ae18b7664b8e"}, {"y": 146, "x": 216, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa932ac3d49037d216936831e1caa3dbfc54c01c"}, {"y": 217, "x": 320, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26474b55a8ecc9c786e038a151026f9a1b096534"}, {"y": 435, "x": 640, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=109702225c81efa8993ba39e8c3aa82d79a427dd"}, {"y": 653, "x": 960, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a246f002a5420725c8c24200c7803d93b6fc3d8"}, {"y": 734, "x": 1080, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee640b626a81a2d48f8c9592235b25d2121d11b0"}], "s": {"y": 1083, "x": 1592, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68756da95ae7d2221d85e419759103e068205ad8"}, "id": "q9n64kxkmt4b1"}, "tgbprtyqmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff36b6a06a4ac8f93ba7a36af9187ab32904d634"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9ca2b68989ff7725cb179e447663fd66315f780"}, {"y": 121, "x": 320, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d6a8f713d87af08fdfc70a02fce3fa0be5d7a6"}, {"y": 243, "x": 640, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13cfdbb1daad2ebc6f7c8553d8e5b66d5aacb0d6"}, {"y": 364, "x": 960, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd8202247a46501cceb7f6685d3965dac6777a1f"}, {"y": 410, "x": 1080, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46882ee3cc966e4484ea670a4da79b92ad103e0"}], "s": {"y": 624, "x": 1642, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152"}, "id": "tgbprtyqmt4b1"}, "ap45rzcimt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1060d30e8e8b426a4679486a310b1c68d9ae8f46"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5239e031369d5865370f0f3c393deaf013939b3"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ad226e08dddd3e079249fd77ad8ec767db7be82"}, {"y": 359, "x": 640, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acdd001188c2eba27245633b48727358b8db19f7"}, {"y": 539, "x": 960, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66e6fede5197c1f0f35ceac6ec16aa7d9172ffc4"}, {"y": 606, "x": 1080, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4bd30e72998308d3144af8bc4beb7b5a9a144183"}], "s": {"y": 960, "x": 1709, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2"}, "id": "ap45rzcimt4b1"}, "959gakxvmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4972a7fc2ee20f78f6382073037faee99a8fb86"}, {"y": 70, "x": 216, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec74e5cda741392c42a239be359afb8caf9f5ecf"}, {"y": 104, "x": 320, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4cc00e39254fd8aa94da00aa618bdea994a1991"}, {"y": 209, "x": 640, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f368c5ab850faaa8b9a83a38b754835890bb632"}, {"y": 314, "x": 960, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b0623e145a6ca61d5e41a17972c1d783c3edae4"}, {"y": 353, "x": 1080, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fba560b79ca348ada09825e52dc5ca642319a6b5"}], "s": {"y": 1077, "x": 3289, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19"}, "id": "959gakxvmt4b1"}, "xpbtl0qtmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861849f31aca428cea518ea44aa7efb8e4c5c86f"}, {"y": 213, "x": 216, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dcaeab2b61a82650a80bba6ea00165dab6085205"}, {"y": 315, "x": 320, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=932f5c0dbaebf378db7be90eb4fa9c50a8551580"}, {"y": 631, "x": 640, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1dcc3f2215da1ccacabd2ca66a57b034b18d42a"}, {"y": 946, "x": 960, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e10bd14283e50ab5013996cf166e2a42c2702cee"}, {"y": 1065, "x": 1080, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d03a7529df0c08b87d1c4bc36969986c66e88d2f"}], "s": {"y": 3334, "x": 3380, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df"}, "id": "xpbtl0qtmt4b1"}, "1nq2spknmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 88, "x": 108, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f956430ac71dfcc10b585283675ef6eb02baa2e7"}, {"y": 177, "x": 216, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0457ffa834e73dc4759aa14b92e446c1838f94a0"}, {"y": 262, "x": 320, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07232fe4cbb19c96079332219db2623f3f994214"}, {"y": 525, "x": 640, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06972e5fc878794c1947ab79ccf48c3ab20b4e2b"}, {"y": 788, "x": 960, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be277ff1b357dc92a0ab2e9fbd00cc154fa389a0"}, {"y": 887, "x": 1080, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e9cc0cf7c63362ca7e732901ffc11a15687e6d"}], "s": {"y": 1101, "x": 1340, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409"}, "id": "1nq2spknmt4b1"}, "e9mu9nsxmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1902eafba7fcf630a6e11a4e3a502987d3f99c1"}, {"y": 89, "x": 216, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=156b22585646291a2ec49dfea975e244b000ac0a"}, {"y": 133, "x": 320, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f989b277baca4a11ea0275296f0224960cbb1894"}, {"y": 266, "x": 640, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35120ab9e0bc9d0ac7352c9ee152530b0aa70c49"}, {"y": 399, "x": 960, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4035886dc47e5f7acf1ec9e107f862850504292"}, {"y": 449, "x": 1080, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9c11f276f65f3d2b5a953f6241558b97cf659ba"}], "s": {"y": 832, "x": 2001, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c64d709b3f894937e97519e94305df903125c5"}, "id": "e9mu9nsxmt4b1"}}, "name": "t3_144e0cp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kS9dMuDFFlEJP-gyR3r-H4sq1T3T440E8jWxQqiwrEk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1686242035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unlike  normal reporting, A/B testing collects data of a different combination  of dimensions every time. It is also a complicated kind of analysis of  immense data. In our case, we have a real-time data volume of millions  of OPS (Operations Per Second), with each operation involving around 20  data tags and over a dozen dimensions.&lt;/p&gt;\n\n&lt;p&gt;For  effective A/B testing, as data engineers, we must ensure quick  computation as well as high data integrity (which means no duplication  and no data loss). I\u2019m sure I\u2019m not the only one to say this: it is  hard!&lt;/p&gt;\n\n&lt;p&gt;Let me show you our long-term struggle with our previous Druid-based data platform.&lt;/p&gt;\n\n&lt;h1&gt;Platform Architecture 1.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;: Apache Storm + Apache Druid + MySQL&lt;/p&gt;\n\n&lt;p&gt;This  was our real-time datawarehouse, where Apache Storm was the real-time  data processing engine and Apache Druid pre-aggregated the data.  However, Druid did not support certain paging and join queries, so we  wrote data from Druid to MySQL regularly, making MySQL the \u201cmaterialized  view\u201d of Druid. But that was only a duct tape solution as it couldn\u2019t  support our ever enlarging real-time data size. So data timeliness was  unattainable.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2\"&gt;https://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Platform Architecture 2.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;: Apache Flink + Apache Druid + TiDB&lt;/p&gt;\n\n&lt;p&gt;This  time, we replaced Storm with Flink, and MySQL with TiDB. Flink was more  powerful in terms of semantics and features, while TiDB, with its  distributed capability, was more maintainable than MySQL. But  architecture 2.0 was nowhere near our goal of end-to-end data  consistency, either, because when processing huge data, enabling TiDB  transactions largely slowed down data writing. Plus, Druid itself did  not support standard SQL, so there were some learning costs and  frictions in usage.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=68756da95ae7d2221d85e419759103e068205ad8\"&gt;https://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=68756da95ae7d2221d85e419759103e068205ad8&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Platform Architecture 3.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;: Apache Flink + &lt;a href=\"https://github.com/apache/doris\"&gt;Apache Doris&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We  replaced Apache Druid with Apache Doris as the OLAP engine, which could  also serve as a unified data serving gateway. So in Architecture 3.0,  we only need to maintain one set of query logic. And we layered our  real-time datawarehouse to increase reusability of real-time data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409\"&gt;https://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Turns  out the combination of Flink and Doris was the answer. We can exploit  their features to realize quick computation and data consistency. Keep  reading and see how we make it happen.&lt;/p&gt;\n\n&lt;h1&gt;Quick Computation&lt;/h1&gt;\n\n&lt;p&gt;As  one piece of operation data can be attached to 20 tags, in A/B testing,  we compare two groups of data centering only one tag each time. At  first, we thought about splitting one piece of operation data (with 20  tags) into 20 pieces of data of only one tag upon data ingestion, and  then importing them into Doris for analysis, but that could cause a data  explosion and thus huge pressure on our clusters.&lt;/p&gt;\n\n&lt;p&gt;Then  we tried moving part of such workload to the computation engine. So we  tried and \u201cexploded\u201d the data in Flink, but soon regretted it, because  when we aggregated the data using the global hash windows in Flink jobs,  the network and CPU usage also \u201cexploded\u201d.&lt;/p&gt;\n\n&lt;p&gt;Our  third shot was to aggregate data locally in Flink right after we split  it. As is shown below, we create a window in the memory of one operator  for local aggregation; then we further aggregate it using the global  hash windows. Since two operators chained together are in one thread,  transferring data between operators consumes much less network  resources. &lt;strong&gt;The two-step aggregation method, combined with the&lt;/strong&gt; &lt;a href=\"https://doris.apache.org/docs/dev/data-table/data-model\"&gt;&lt;strong&gt;Aggregate model&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;of Apache Doris, can keep data explosion in a manageable range.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152\"&gt;https://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For  convenience in A/B testing, we make the test tag ID the first sorted  field in Apache Doris, so we can quickly locate the target data using  sorted indexes. To further minimize data processing in queries, we  create materialized views with the frequently used dimensions. With  constant modification and updates, the materialized views are applicable  in 80% of our queries.&lt;/p&gt;\n\n&lt;p&gt;To  sum up, with the application of sorted index and materialized views, we  reduce our query response time to merely seconds in A/B testing.&lt;/p&gt;\n\n&lt;h1&gt;Data Integrity Guarantee&lt;/h1&gt;\n\n&lt;p&gt;Imagine  that your algorithm designers worked sweat and tears trying to improve  the business, only to find their solution unable to be validated by A/B  testing due to data loss. This is an unbearable situation, and we make  every effort to avoid it.&lt;/p&gt;\n\n&lt;h1&gt;Develop a Sink-to-Doris Component&lt;/h1&gt;\n\n&lt;p&gt;To  ensure end-to-end data integrity, we developed a Sink-to-Doris  component. It is built on our own Flink Stream API scaffolding and  realized by the idempotent writing of Apache Doris and the two-stage  commit mechanism of Apache Flink. On top of it, we have a data  protection mechanism against anomalies.&lt;/p&gt;\n\n&lt;p&gt;It  is the result of our long-term evolution. We used to ensure data  consistency by implementing \u201cone writing for one tag ID\u201d. Then we  realized we could make good use of the transactions in Apache Doris and  the two-stage commit of Apache Flink.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df\"&gt;https://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As is shown above, this is how two-stage commit works to guarantee data consistency:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write data into local files;&lt;/li&gt;\n&lt;li&gt;Stage One: pre-commit data to Apache Doris. Save the Doris transaction ID into status;&lt;/li&gt;\n&lt;li&gt;If checkpoint fails, manually abandon the transaction; if checkpoint succeeds, commit the transaction in Stage Two;&lt;/li&gt;\n&lt;li&gt;If  the commit fails after multiple retries, the transaction ID and the  relevant data will be saved in HDFS, and we can restore the data via  Broker Load.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We  make it possible to split a single checkpoint into multiple  transactions, so that we can prevent one Stream Load from taking more  time than a Flink checkpoint in the event of large data volumes.&lt;/p&gt;\n\n&lt;h1&gt;Application Display&lt;/h1&gt;\n\n&lt;p&gt;This  is how we implement Sink-to-Doris. The component has blocked API calls  and topology assembly. With simple configuration, we can write data into  Apache Doris via Stream Load.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19\"&gt;https://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Cluster Monitoring&lt;/h1&gt;\n\n&lt;p&gt;For  cluster and host monitoring, we adopted the metrics templates provided  by the Apache Doris community. For data monitoring, in addition to the  template metrics, we added Stream Load request numbers and loading  rates.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c64d709b3f894937e97519e94305df903125c5\"&gt;https://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c64d709b3f894937e97519e94305df903125c5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Other metrics of our concerns include data writing speed and task  processing time. In the case of anomalies, we will receive notifications  in the form of phone calls, messages, and emails.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9\"&gt;https://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Key Takeaways&lt;/h1&gt;\n\n&lt;p&gt;The  recipe for successful A/B testing is quick computation and high data  integrity. For this purpose, we implement a two-step aggregation method  in Apache Flink, utilize the Aggregate model, materialized view, and  short indexes of Apache Doris. Then we develop a Sink-to-Doris  component, which is realized by the idempotent writing of Apache Doris  and the two-stage commit mechanism of Apache Flink.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?auto=webp&amp;v=enabled&amp;s=ed9468f916100de164d0d4bde72466b599c4cd1f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b6e3934133a26ae55487c7bbcd89b5b26e72e07", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b8498732899bc298095c13caff27f9e29df5080", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51922ef3618bde282118ae560099eb52869c7485", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c24685b8cefde69b831f57afaa42c4da18f5168f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89f303ee2533ad6574df1ef6180d85d04705dc3a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=696f7137a7d46624c2b51624addbf2cb83b2fb89", "width": 1080, "height": 540}], "variants": {}, "id": "D3qhZW6Eh5PWuR0-NDgx2j7uZCBc6ZryuUEfgYXvDIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "144e0cp", "is_robot_indexable": true, "report_reasons": null, "author": "Any_Opportunity1234", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144e0cp/ab_testing_was_a_handful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144e0cp/ab_testing_was_a_handful/", "subreddit_subscribers": 921072, "created_utc": 1686242035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Has anyone here tried Mojo yet? What are your initial views?", "author_fullname": "t2_3wr0pzmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mojo programming language", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143xglf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686193422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here tried Mojo yet? What are your initial views?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143xglf", "is_robot_indexable": true, "report_reasons": null, "author": "sARUcasm", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143xglf/mojo_programming_language/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143xglf/mojo_programming_language/", "subreddit_subscribers": 921072, "created_utc": 1686193422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I asked in a [previous post](https://www.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/) for advice about how to find insight in unstructured text data. Almost everyone [recommended BERTopic](https://www.reddit.com/r/datascience/comments/13ltn9q/comment/jkrwrug/?utm_source=share&amp;utm_medium=web2x&amp;context=3), but I wasn't able to run BERTopic on my machine locally (segmentation fault). Fortunately, I found [Top2Vec](https://github.com/ddangelov/Top2Vec), which uses [HBDSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) and [UMAP](https://umap-learn.readthedocs.io/en/latest/) to quickly find good topics in uncleaned(!) text data.\n\nTo try to get the most out of Top2Vec, I wrote some code to select the best [HDBSCAN hyperparameters](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html), specifically the distance metric and the cluster selection method. My code runs Top2Vec with different distance and cluster selection method parameters and displays the [UMAP points plot](https://umap-learn.readthedocs.io/en/latest/plotting.html), and then I select the one with the clearest looking clusters in that plot (the business needs a good clustering visualization that really *pops* in a slide deck). My plan is to plot the network graph in Plotly when I'm finished.\n\nThe best I can do is get 5-10 topics that are *kind of good*, but not awesome. My plan today is to subset the data based on the topics, and run the documents that are assigned to each topic through Top2Vec again to try to get subtopics, and use the first topics as parent topics.\n\nDoes anyone have experience with doing something like this? Does this approach make sense? What else should I do to get the most out of Top2Vec?", "author_fullname": "t2_6gwxih9e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HBDSCAN hyperparameter tuning for topic modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144jvhx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686255546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I asked in a &lt;a href=\"https://www.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/\"&gt;previous post&lt;/a&gt; for advice about how to find insight in unstructured text data. Almost everyone &lt;a href=\"https://www.reddit.com/r/datascience/comments/13ltn9q/comment/jkrwrug/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;recommended BERTopic&lt;/a&gt;, but I wasn&amp;#39;t able to run BERTopic on my machine locally (segmentation fault). Fortunately, I found &lt;a href=\"https://github.com/ddangelov/Top2Vec\"&gt;Top2Vec&lt;/a&gt;, which uses &lt;a href=\"https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\"&gt;HBDSCAN&lt;/a&gt; and &lt;a href=\"https://umap-learn.readthedocs.io/en/latest/\"&gt;UMAP&lt;/a&gt; to quickly find good topics in uncleaned(!) text data.&lt;/p&gt;\n\n&lt;p&gt;To try to get the most out of Top2Vec, I wrote some code to select the best &lt;a href=\"https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\"&gt;HDBSCAN hyperparameters&lt;/a&gt;, specifically the distance metric and the cluster selection method. My code runs Top2Vec with different distance and cluster selection method parameters and displays the &lt;a href=\"https://umap-learn.readthedocs.io/en/latest/plotting.html\"&gt;UMAP points plot&lt;/a&gt;, and then I select the one with the clearest looking clusters in that plot (the business needs a good clustering visualization that really &lt;em&gt;pops&lt;/em&gt; in a slide deck). My plan is to plot the network graph in Plotly when I&amp;#39;m finished.&lt;/p&gt;\n\n&lt;p&gt;The best I can do is get 5-10 topics that are &lt;em&gt;kind of good&lt;/em&gt;, but not awesome. My plan today is to subset the data based on the topics, and run the documents that are assigned to each topic through Top2Vec again to try to get subtopics, and use the first topics as parent topics.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with doing something like this? Does this approach make sense? What else should I do to get the most out of Top2Vec?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?auto=webp&amp;v=enabled&amp;s=b9dcf2856d881a082ba5b5abe3639f81dc42feb3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5519ba53fd55e1839842d46474d45a432d26d1ff", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c643da33a63d89089445be4dd3c7d6e835dac8e2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ea4612372122bd5404ac4adf308fd8f26099a4f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caf4d6224a869143bce4c0f665c1da57e66720cf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f12d33b764d4d25178b0596fba2120e5e3d3ad6e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce25173c8e5050e94fb690134c076fa95677d205", "width": 1080, "height": 540}], "variants": {}, "id": "aBSVvJdg6ekbq76-m9w74cXXRT4dCIQBjn0MUgj2e9c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144jvhx", "is_robot_indexable": true, "report_reasons": null, "author": "abelEngineer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144jvhx/hbdscan_hyperparameter_tuning_for_topic_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144jvhx/hbdscan_hyperparameter_tuning_for_topic_modeling/", "subreddit_subscribers": 921072, "created_utc": 1686255546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hey fellow Redditors!\n\nI've been working on this exciting project on Kaggle where I delve into data from another Reddit page. Specifically, I've been analyzing the r/Technology subreddit. \n\nI would absolutely love to hear your thoughts and suggestions to enhance this project even further! You can check out the project here:  [\ud83d\udcbe r/Technology EDA + SA \ud83d\udcbe | Kaggle](https://www.kaggle.com/code/curiel/r-technology-eda-sa) \n\nFeel free to delve into the analysis and let me know what you think. Did I overlook anything crucial? Are there any areas where I could improve? I'm all ears!\n\nBy the way, I'm considering incorporating some different analyses using the extracted data. Maybe including around 7 graphs could be a good idea... What do you think?", "author_fullname": "t2_86qi4nrq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback for My Kaggle Project: Exploring data from Reddit posts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144qrli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686272197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Redditors!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on this exciting project on Kaggle where I delve into data from another Reddit page. Specifically, I&amp;#39;ve been analyzing the &lt;a href=\"/r/Technology\"&gt;r/Technology&lt;/a&gt; subreddit. &lt;/p&gt;\n\n&lt;p&gt;I would absolutely love to hear your thoughts and suggestions to enhance this project even further! You can check out the project here:  &lt;a href=\"https://www.kaggle.com/code/curiel/r-technology-eda-sa\"&gt;\ud83d\udcbe r/Technology EDA + SA \ud83d\udcbe | Kaggle&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Feel free to delve into the analysis and let me know what you think. Did I overlook anything crucial? Are there any areas where I could improve? I&amp;#39;m all ears!&lt;/p&gt;\n\n&lt;p&gt;By the way, I&amp;#39;m considering incorporating some different analyses using the extracted data. Maybe including around 7 graphs could be a good idea... What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bAF3nu34sqx9JiQ8p2IC1qpunRvsj1lY6gdZCTglVR4.jpg?auto=webp&amp;v=enabled&amp;s=f9a4ebbdf25c141e393ac0f5de16dfb5de3c35e8", "width": 100, "height": 100}, "resolutions": [], "variants": {}, "id": "nf-qTd7Cm4Nu8vE8A8JoHkZUwetTZo9OzjS48fKRCdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144qrli", "is_robot_indexable": true, "report_reasons": null, "author": "data-dreamr", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144qrli/feedback_for_my_kaggle_project_exploring_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144qrli/feedback_for_my_kaggle_project_exploring_data/", "subreddit_subscribers": 921072, "created_utc": 1686272197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I currently work as a supply chain analyst and I utilize python to automate tasks within SAP as well as analyze large sets of data with IF statements to help me make strategic decisions (inventory rebalancing, forecasting, purchasing). Does this fall more under a software engineer or data scientist skill set?", "author_fullname": "t2_317zg1g3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is using python for supply chain considered data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144068g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686201542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work as a supply chain analyst and I utilize python to automate tasks within SAP as well as analyze large sets of data with IF statements to help me make strategic decisions (inventory rebalancing, forecasting, purchasing). Does this fall more under a software engineer or data scientist skill set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144068g", "is_robot_indexable": true, "report_reasons": null, "author": "C17Wing", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144068g/is_using_python_for_supply_chain_considered_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144068g/is_using_python_for_supply_chain_considered_data/", "subreddit_subscribers": 921072, "created_utc": 1686201542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm finishing my PhD. My research is in the application of special coordinates. I have completed certificates in programming and some in data science. What positions and fields are best for me?\n\n&amp;#x200B;\n\nThanks in advance.", "author_fullname": "t2_9gtr0luc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I get a job in data science if I'm an applied mathematician?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144rjpd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686274357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m finishing my PhD. My research is in the application of special coordinates. I have completed certificates in programming and some in data science. What positions and fields are best for me?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144rjpd", "is_robot_indexable": true, "report_reasons": null, "author": "Different_Breath7502", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144rjpd/how_can_i_get_a_job_in_data_science_if_im_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144rjpd/how_can_i_get_a_job_in_data_science_if_im_an/", "subreddit_subscribers": 921072, "created_utc": 1686274357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all!\n\n&amp;#x200B;\n\nI am building a community of social/data/computer scientists at the University of Mannheim focused on applications and theory of Large Language Models.  The community will be working to make contributions along research, teaching, and enterpreneurial dimensions.  The positions can be at the graduate student, pre-doc (ABD), or post-doc level.  The details for the call are coming, but given the impending summer, I wanted to get an initial announcement out.  \n\n&amp;#x200B;\n\nI am looking for the right people at least as much as the right skills: people  open to providing and receiving feedback, supporting others, and who is committed and looking to grow intellectually.  The more experience with the relevant coding (Python, HuggingFace), mathematical statistics, algorithms, and applied work in the social sciences, the better.  We will have our own infrastructure, so experience setting up and running servers will be a big plus.  \n\n&amp;#x200B;\n\nA formal announcement will follow, but the position will be at the E13 level.   Teaching requirements will depend on the level and skills of the applicant, but will be tied directly to your research interests.  Salary is competitive, and will be at the standard E13 level as set by the state of Baden-W\u00fcrttemberg.\n\n&amp;#x200B;\n\nIf you have any questions, please send them to [MarcRatkovic@gmail.com](mailto:MarcRatkovic@gmail.com).  When you reach out, it would help if you would include a CV and any relevant working papers (at any stage of development). \n\n&amp;#x200B;\n\nAll the best,\n\nMarc Ratkovic\n\nW3 Chair of Social Data Science\n\nUniversity of Mannheim", "author_fullname": "t2_czuym6b7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLMs at the University of Mannheim", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144ecn5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686242817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am building a community of social/data/computer scientists at the University of Mannheim focused on applications and theory of Large Language Models.  The community will be working to make contributions along research, teaching, and enterpreneurial dimensions.  The positions can be at the graduate student, pre-doc (ABD), or post-doc level.  The details for the call are coming, but given the impending summer, I wanted to get an initial announcement out.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am looking for the right people at least as much as the right skills: people  open to providing and receiving feedback, supporting others, and who is committed and looking to grow intellectually.  The more experience with the relevant coding (Python, HuggingFace), mathematical statistics, algorithms, and applied work in the social sciences, the better.  We will have our own infrastructure, so experience setting up and running servers will be a big plus.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A formal announcement will follow, but the position will be at the E13 level.   Teaching requirements will depend on the level and skills of the applicant, but will be tied directly to your research interests.  Salary is competitive, and will be at the standard E13 level as set by the state of Baden-W\u00fcrttemberg.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions, please send them to [&lt;a href=\"mailto:MarcRatkovic@gmail.com\"&gt;MarcRatkovic@gmail.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:MarcRatkovic@gmail.com\"&gt;MarcRatkovic@gmail.com&lt;/a&gt;).  When you reach out, it would help if you would include a CV and any relevant working papers (at any stage of development). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All the best,&lt;/p&gt;\n\n&lt;p&gt;Marc Ratkovic&lt;/p&gt;\n\n&lt;p&gt;W3 Chair of Social Data Science&lt;/p&gt;\n\n&lt;p&gt;University of Mannheim&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144ecn5", "is_robot_indexable": true, "report_reasons": null, "author": "MarcRatkovic", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144ecn5/llms_at_the_university_of_mannheim/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144ecn5/llms_at_the_university_of_mannheim/", "subreddit_subscribers": 921072, "created_utc": 1686242817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TL;DR, is deep learning overrated when applied for multivariate TSAD/ multivariate predictive maintenance? Are there proven statistical methods applied in the field?  \n\n\nI would like to bring here a discussion that recently hit my thoughts. This short yt [video](https://www.youtube.com/watch?v=Vg1p3DouX8w) by [Eamonn Keogh](https://scholar.google.com/citations?user=slVcOQIAAAAJ&amp;hl=en) talks about UNIVARIATE TSAD, and critics the use of DL papers recently diffused in this field (he says that a lot of the ones we have are unprecised). Actually I share his ideas, but It does not talk about multivariate TSAD (extensible also for the reasoning of multivariate predictive maintenance for complex sensor systems). So I would like to know from your side if this reasoning is extensible also for multivariate case. For my personal study case I have found out a lot of LSTM Autoencoders applications when trying to model this problem.", "author_fullname": "t2_i6lyoywf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deep learning papers about Time Series Anomaly Detection (TSAD) are flawed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144a7kb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686233088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR, is deep learning overrated when applied for multivariate TSAD/ multivariate predictive maintenance? Are there proven statistical methods applied in the field?  &lt;/p&gt;\n\n&lt;p&gt;I would like to bring here a discussion that recently hit my thoughts. This short yt &lt;a href=\"https://www.youtube.com/watch?v=Vg1p3DouX8w\"&gt;video&lt;/a&gt; by &lt;a href=\"https://scholar.google.com/citations?user=slVcOQIAAAAJ&amp;amp;hl=en\"&gt;Eamonn Keogh&lt;/a&gt; talks about UNIVARIATE TSAD, and critics the use of DL papers recently diffused in this field (he says that a lot of the ones we have are unprecised). Actually I share his ideas, but It does not talk about multivariate TSAD (extensible also for the reasoning of multivariate predictive maintenance for complex sensor systems). So I would like to know from your side if this reasoning is extensible also for multivariate case. For my personal study case I have found out a lot of LSTM Autoencoders applications when trying to model this problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?auto=webp&amp;v=enabled&amp;s=c25788dc2f5e76b27aafda619fb6c2c4b2c74217", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9244d39957aa5938f36729d257f69064f42a2895", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b32b1acfd332537a6d5a14d46a031736b5464cf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=deb96c7d81f84612c4f1e9bd4ef97a0b5a6a438d", "width": 320, "height": 240}], "variants": {}, "id": "mn1T4iM0XKmCAI0A2QM7l--U5oqj5obH3Tq18NzGQ_Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144a7kb", "is_robot_indexable": true, "report_reasons": null, "author": "ginotherhino1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144a7kb/deep_learning_papers_about_time_series_anomaly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144a7kb/deep_learning_papers_about_time_series_anomaly/", "subreddit_subscribers": 921072, "created_utc": 1686233088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi!\n\nI'm currently working on connecting a React app with a Shiny app (Python), but I'm facing difficulties in doing so. I haven't been able to find any resources or guides that specifically address this integration. As a newcomer to web application development, I am not sure if there's any additional setup I may have missed.\n\nI'm curious if it's possible to connect Shiny (Python) and React. If anyone has experience or knowledge in this area, I would greatly appreciate any advice or insights!\n\nOn the other hand, I'm open to exploring alternative tech stacks that might simplify the whole process. For example, using React with Flask instead of Shiny (Python). If anyone has recommendations or suggestions, I'd love to hear them!\n\nThank you in advance!\n\n&amp;#x200B;\n\nDetails:\n\nCurrently, I have both a React app and a Shiny app (Python) running independently on different localhost ports. In my attempt to connect these two apps, I followed a tutorial available at [**https://github.com/filipakkad/react-shiny-template**](https://github.com/filipakkad/react-shiny-template), which demonstrates the use of websockets to establish communication between Shiny (R) and React.\n\nHowever, I encountered an error message that states \"Uncaught TypeError: Cannot read properties of undefined (reading 'addCustomMessageHandler')\" when using the \n\n    window.Shiny.addCustomMessageHandler\n\nfunction mentioned in the tutorial. \n\n I also tried another tutorial provided in this repository: [**https://github.com/BrandenKeck/shiny\\_react\\_template**](https://github.com/BrandenKeck/shiny_react_template). Unfortunately, the webpack config approach mentioned in this tutorial did not work for me either. I received an error message stating \"Module not found: Can't resolve 'shiny'\".\n\nSorry if the error messages are too vague to troubleshoot.", "author_fullname": "t2_95g1e07t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to connect Shiny for Python with React?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143wno4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on connecting a React app with a Shiny app (Python), but I&amp;#39;m facing difficulties in doing so. I haven&amp;#39;t been able to find any resources or guides that specifically address this integration. As a newcomer to web application development, I am not sure if there&amp;#39;s any additional setup I may have missed.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if it&amp;#39;s possible to connect Shiny (Python) and React. If anyone has experience or knowledge in this area, I would greatly appreciate any advice or insights!&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I&amp;#39;m open to exploring alternative tech stacks that might simplify the whole process. For example, using React with Flask instead of Shiny (Python). If anyone has recommendations or suggestions, I&amp;#39;d love to hear them!&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Details:&lt;/p&gt;\n\n&lt;p&gt;Currently, I have both a React app and a Shiny app (Python) running independently on different localhost ports. In my attempt to connect these two apps, I followed a tutorial available at &lt;a href=\"https://github.com/filipakkad/react-shiny-template\"&gt;&lt;strong&gt;https://github.com/filipakkad/react-shiny-template&lt;/strong&gt;&lt;/a&gt;, which demonstrates the use of websockets to establish communication between Shiny (R) and React.&lt;/p&gt;\n\n&lt;p&gt;However, I encountered an error message that states &amp;quot;Uncaught TypeError: Cannot read properties of undefined (reading &amp;#39;addCustomMessageHandler&amp;#39;)&amp;quot; when using the &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;window.Shiny.addCustomMessageHandler\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;function mentioned in the tutorial. &lt;/p&gt;\n\n&lt;p&gt;I also tried another tutorial provided in this repository: &lt;a href=\"https://github.com/BrandenKeck/shiny_react_template\"&gt;&lt;strong&gt;https://github.com/BrandenKeck/shiny_react_template&lt;/strong&gt;&lt;/a&gt;. Unfortunately, the webpack config approach mentioned in this tutorial did not work for me either. I received an error message stating &amp;quot;Module not found: Can&amp;#39;t resolve &amp;#39;shiny&amp;#39;&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Sorry if the error messages are too vague to troubleshoot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?auto=webp&amp;v=enabled&amp;s=df682b7818bf96a86f346c10684a73b607e7f31c", "width": 651, "height": 320}, "resolutions": [{"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9662c36ed5b3b66e858821f09a9e6003dbc5aa04", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4485b075ebc6a083cd92daa62a76be41bbf526a", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a1478ad2a984ef28466a22393f52d564331efb6", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3132c9ee0abdbf2aaa2ad69c7a89dfd6bdd8fe27", "width": 640, "height": 314}], "variants": {}, "id": "GBFg7Huw_zEKgghuGLuNBfvugskO8_GA2tV8ZiqndUI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143wno4", "is_robot_indexable": true, "report_reasons": null, "author": "Feisty-Temporary4403", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143wno4/is_it_possible_to_connect_shiny_for_python_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143wno4/is_it_possible_to_connect_shiny_for_python_with/", "subreddit_subscribers": 921072, "created_utc": 1686191145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As titled.\n\nTried to compete in a competittion. Hope to find some open-sourced project to use as baseline.\n\nOr do you guys have any high-level thoughts on how to architect such a thing?", "author_fullname": "t2_l2t68", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is There any Open Sourced AI Generated Image Detection Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144j0w9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686253604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As titled.&lt;/p&gt;\n\n&lt;p&gt;Tried to compete in a competittion. Hope to find some open-sourced project to use as baseline.&lt;/p&gt;\n\n&lt;p&gt;Or do you guys have any high-level thoughts on how to architect such a thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144j0w9", "is_robot_indexable": true, "report_reasons": null, "author": "HighlandEvil", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144j0w9/is_there_any_open_sourced_ai_generated_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144j0w9/is_there_any_open_sourced_ai_generated_image/", "subreddit_subscribers": 921072, "created_utc": 1686253604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking to predict the affinity/propensity of my customers to a set of product features using mostly tabular/numeric data (transactional data). I\u2019m making a trade off between creating an individual tree-based model for each propensity I\u2019m planning to calculate and the use of a foundational (embeddings) model, finetuning it to each propensity calculation. While I expect the cost to be higher for foundational model use, I\u2019m still interested to learn side by side performance, examine if the value is worth the cost. Structured data use in FMs/LMMs has historically not been the highlight due to the incremental value FMs bring with unstructured data/text or several modalities (e.g. Images/sound). However, I see some recent papers that are examining this application in particular. Does anyone have any experience/feedback on this use case? The goal of my post is to refine my approach, understand if it\u2019s worth exploring, learn from past experience and identify the best opportunities for value given the expected incremental cost of a FM/LLM. I\u2019m also examining an ensemble architecture of both models and the value of FMs for feature engineering. Thanks.", "author_fullname": "t2_509a5jg5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FM vs individual tree-based models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144huss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686250905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to predict the affinity/propensity of my customers to a set of product features using mostly tabular/numeric data (transactional data). I\u2019m making a trade off between creating an individual tree-based model for each propensity I\u2019m planning to calculate and the use of a foundational (embeddings) model, finetuning it to each propensity calculation. While I expect the cost to be higher for foundational model use, I\u2019m still interested to learn side by side performance, examine if the value is worth the cost. Structured data use in FMs/LMMs has historically not been the highlight due to the incremental value FMs bring with unstructured data/text or several modalities (e.g. Images/sound). However, I see some recent papers that are examining this application in particular. Does anyone have any experience/feedback on this use case? The goal of my post is to refine my approach, understand if it\u2019s worth exploring, learn from past experience and identify the best opportunities for value given the expected incremental cost of a FM/LLM. I\u2019m also examining an ensemble architecture of both models and the value of FMs for feature engineering. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144huss", "is_robot_indexable": true, "report_reasons": null, "author": "SnooMacarons3152", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144huss/fm_vs_individual_treebased_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144huss/fm_vs_individual_treebased_models/", "subreddit_subscribers": 921072, "created_utc": 1686250905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m interested in learning more about signal processing. I\u2019m a chemical engineer by degree so I know a minimal amount about it from tuning control loops in my past life but definitely not enough to find it useful for data science et al. I\u2019m familiar with a decent amount of the terminology and math but not as much how to actually do it. Where would you recommend I start?", "author_fullname": "t2_14hblx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Signal Processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144fbjt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m interested in learning more about signal processing. I\u2019m a chemical engineer by degree so I know a minimal amount about it from tuning control loops in my past life but definitely not enough to find it useful for data science et al. I\u2019m familiar with a decent amount of the terminology and math but not as much how to actually do it. Where would you recommend I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144fbjt", "is_robot_indexable": true, "report_reasons": null, "author": "Ryush806", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144fbjt/signal_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144fbjt/signal_processing/", "subreddit_subscribers": 921072, "created_utc": 1686245026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just pass my third round interview and the next round is panel interview on Zoom.\n\nThe 1st round is simple phone screen.\n\nThe 2nd round is 30 min talk with Hiring Manager which is a quite pleasant conversation.\n\nThe 3rd round is a 45 minutes coderpad interview consisted of python and MYSQL.\n\nThe HR sent me an email talking about the coming round (hopefully final round? I guess) . It got 5 sessions with 9 interviewers! As a non-native speaker, I felt a bit intimidated by it. Besides the first part which is mainly MYSQL and Python with Monte Carlo, I am not sure about what the other parts will look like. As DS is a board topic, I don't know how to get myself well prepared for it.\n\n'''\n\n**Data Scientist interviewer TBD -&gt;**\u00a0this interview is to further evaluate a candidate's programming ability level with a more complicated problem. Similar to the first technical screen, the interviewers will have a CoderPad environment setup and will go through a coding exercise with you (Python and SQL). SQL will be the same format as the first technical screen where\u00a0you write queries against a database, and Python will be related to Monte Carlo.\n\n**Data Scientist interviewer TBD\u00a0\u00a0-&gt;**\u00a0this will be the problem decomposition interview.\u00a0The problem decomposition interview is designed to evaluate a candidate\u2019s ability to solve a data problem similar to one that he or she might encounter at XXXX. We want to understand whether or not a candidate can work through a data problem end to end, from problem statement identification to data source discovery to method identification to delivery.\u00a0\n\n**Analytics Consulting interviewer TBD -&gt;**\u00a0this is the analytics consulting cross-functional interview focused on cross-functional collaboration skills.\n\n**DS Leader TBD -&gt;**\u00a0this is the \"WHO\" interview which is focused on soft skills, interests, as well as some behavioral type questions.\u00a0\n\n**Data Scientist interviewer TBD\u00a0-&gt;**\u00a0this is the analytical interview.\u00a0The analytical interview aims to test for familiarity with core data science techniques, including understanding of the limitations of such techniques. Part of what is tested is the ability to clearly communicate important machine learning/data science concepts.\u00a0\n\n'''", "author_fullname": "t2_gxmoxq43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Last of 5 interviews in the coming week", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144p954", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686268156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just pass my third round interview and the next round is panel interview on Zoom.&lt;/p&gt;\n\n&lt;p&gt;The 1st round is simple phone screen.&lt;/p&gt;\n\n&lt;p&gt;The 2nd round is 30 min talk with Hiring Manager which is a quite pleasant conversation.&lt;/p&gt;\n\n&lt;p&gt;The 3rd round is a 45 minutes coderpad interview consisted of python and MYSQL.&lt;/p&gt;\n\n&lt;p&gt;The HR sent me an email talking about the coming round (hopefully final round? I guess) . It got 5 sessions with 9 interviewers! As a non-native speaker, I felt a bit intimidated by it. Besides the first part which is mainly MYSQL and Python with Monte Carlo, I am not sure about what the other parts will look like. As DS is a board topic, I don&amp;#39;t know how to get myself well prepared for it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Scientist interviewer TBD -&amp;gt;&lt;/strong&gt;\u00a0this interview is to further evaluate a candidate&amp;#39;s programming ability level with a more complicated problem. Similar to the first technical screen, the interviewers will have a CoderPad environment setup and will go through a coding exercise with you (Python and SQL). SQL will be the same format as the first technical screen where\u00a0you write queries against a database, and Python will be related to Monte Carlo.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Scientist interviewer TBD\u00a0\u00a0-&amp;gt;&lt;/strong&gt;\u00a0this will be the problem decomposition interview.\u00a0The problem decomposition interview is designed to evaluate a candidate\u2019s ability to solve a data problem similar to one that he or she might encounter at XXXX. We want to understand whether or not a candidate can work through a data problem end to end, from problem statement identification to data source discovery to method identification to delivery.\u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Analytics Consulting interviewer TBD -&amp;gt;&lt;/strong&gt;\u00a0this is the analytics consulting cross-functional interview focused on cross-functional collaboration skills.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DS Leader TBD -&amp;gt;&lt;/strong&gt;\u00a0this is the &amp;quot;WHO&amp;quot; interview which is focused on soft skills, interests, as well as some behavioral type questions.\u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Scientist interviewer TBD\u00a0-&amp;gt;&lt;/strong&gt;\u00a0this is the analytical interview.\u00a0The analytical interview aims to test for familiarity with core data science techniques, including understanding of the limitations of such techniques. Part of what is tested is the ability to clearly communicate important machine learning/data science concepts.\u00a0&lt;/p&gt;\n\n&lt;p&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144p954", "is_robot_indexable": true, "report_reasons": null, "author": "Somomi_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144p954/last_of_5_interviews_in_the_coming_week/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144p954/last_of_5_interviews_in_the_coming_week/", "subreddit_subscribers": 921072, "created_utc": 1686268156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I've been investigating these ethical dilemmas lately, especially regarding algorithmic bias. What are your opinions on this topic? What do you think about this topic?", "author_fullname": "t2_6wl1sz1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ethical Dilemmas in Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144fouf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve been investigating these ethical dilemmas lately, especially regarding algorithmic bias. What are your opinions on this topic? What do you think about this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144fouf", "is_robot_indexable": true, "report_reasons": null, "author": "Diegoapaps", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144fouf/ethical_dilemmas_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144fouf/ethical_dilemmas_in_data_science/", "subreddit_subscribers": 921072, "created_utc": 1686245894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I asked in a [previous post](https://www.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/) for advice about how to find insight in unstructured text data. Almost everyone [recommended BERTopic](https://www.reddit.com/r/datascience/comments/13ltn9q/comment/jkrwrug/?utm_source=share&amp;utm_medium=web2x&amp;context=3), but I wasn't able to run BERTopic on my machine locally (segmentation fault). Fortunately, I found [Top2Vec](https://github.com/ddangelov/Top2Vec), which uses [HBDSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) and [UMAP](https://umap-learn.readthedocs.io/en/latest/) to quickly find good topics in uncleaned(!) text data.\n\nTo try to get the most out of Top2Vec, I wrote some code to select the best [HDBSCAN hyperparameters](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html), specifically the distance metric and the cluster selection method. My code runs Top2Vec with different distance and cluster selection method parameters and displays the [UMAP points plot](https://umap-learn.readthedocs.io/en/latest/plotting.html), and then I select the one with the clearest looking clusters in that plot (the business needs a good clustering visualization that really *pops* in a slide deck). My plan is to plot the network graph in Plotly when I'm finished.\n\nThe best I can do is get 5-10 topics that are *kind of good*, but not awesome. My plan today is to subset the data based on the topics, and run the documents that are assigned to each topic through Top2Vec again to try to get subtopics, and use the first topics as parent topics.\n\nDoes anyone have experience with doing something like this? Does this approach make sense? What else should I do to get the most out of Top2Vec?", "author_fullname": "t2_6gwxih9e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for best Top2Vec (HDBSCAN) usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1446lm8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686223549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I asked in a &lt;a href=\"https://www.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/\"&gt;previous post&lt;/a&gt; for advice about how to find insight in unstructured text data. Almost everyone &lt;a href=\"https://www.reddit.com/r/datascience/comments/13ltn9q/comment/jkrwrug/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;recommended BERTopic&lt;/a&gt;, but I wasn&amp;#39;t able to run BERTopic on my machine locally (segmentation fault). Fortunately, I found &lt;a href=\"https://github.com/ddangelov/Top2Vec\"&gt;Top2Vec&lt;/a&gt;, which uses &lt;a href=\"https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\"&gt;HBDSCAN&lt;/a&gt; and &lt;a href=\"https://umap-learn.readthedocs.io/en/latest/\"&gt;UMAP&lt;/a&gt; to quickly find good topics in uncleaned(!) text data.&lt;/p&gt;\n\n&lt;p&gt;To try to get the most out of Top2Vec, I wrote some code to select the best &lt;a href=\"https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\"&gt;HDBSCAN hyperparameters&lt;/a&gt;, specifically the distance metric and the cluster selection method. My code runs Top2Vec with different distance and cluster selection method parameters and displays the &lt;a href=\"https://umap-learn.readthedocs.io/en/latest/plotting.html\"&gt;UMAP points plot&lt;/a&gt;, and then I select the one with the clearest looking clusters in that plot (the business needs a good clustering visualization that really &lt;em&gt;pops&lt;/em&gt; in a slide deck). My plan is to plot the network graph in Plotly when I&amp;#39;m finished.&lt;/p&gt;\n\n&lt;p&gt;The best I can do is get 5-10 topics that are &lt;em&gt;kind of good&lt;/em&gt;, but not awesome. My plan today is to subset the data based on the topics, and run the documents that are assigned to each topic through Top2Vec again to try to get subtopics, and use the first topics as parent topics.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with doing something like this? Does this approach make sense? What else should I do to get the most out of Top2Vec?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?auto=webp&amp;v=enabled&amp;s=b9dcf2856d881a082ba5b5abe3639f81dc42feb3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5519ba53fd55e1839842d46474d45a432d26d1ff", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c643da33a63d89089445be4dd3c7d6e835dac8e2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ea4612372122bd5404ac4adf308fd8f26099a4f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caf4d6224a869143bce4c0f665c1da57e66720cf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f12d33b764d4d25178b0596fba2120e5e3d3ad6e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce25173c8e5050e94fb690134c076fa95677d205", "width": 1080, "height": 540}], "variants": {}, "id": "aBSVvJdg6ekbq76-m9w74cXXRT4dCIQBjn0MUgj2e9c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1446lm8", "is_robot_indexable": true, "report_reasons": null, "author": "abelEngineer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1446lm8/tips_for_best_top2vec_hdbscan_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1446lm8/tips_for_best_top2vec_hdbscan_usage/", "subreddit_subscribers": 921072, "created_utc": 1686223549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Typical Machine Learning Engineer Salary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144lq2h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_bso1n30n", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "careerguidance", "selftext": "How much do machine learning engineers usually make? I recently graduated from college about a month ago and started a job as a ML engineer and make 76k a year.  I work for a large, international private company But when I google ML entry engineer salaries, the range falls higher (88-120k). \n\nIs this salary reasonable?\n\nAlso, if any ML engineers are willing to share, what were your starting salaries as an ML engineer?", "author_fullname": "t2_bso1n30n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Typical Machine Learning Engineer Salary?", "link_flair_richtext": [{"e": "text", "t": "Advice"}], "subreddit_name_prefixed": "r/careerguidance", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1447e19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686227474.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686225804.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.careerguidance", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How much do machine learning engineers usually make? I recently graduated from college about a month ago and started a job as a ML engineer and make 76k a year.  I work for a large, international private company But when I google ML entry engineer salaries, the range falls higher (88-120k). &lt;/p&gt;\n\n&lt;p&gt;Is this salary reasonable?&lt;/p&gt;\n\n&lt;p&gt;Also, if any ML engineers are willing to share, what were your starting salaries as an ML engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "682e2d40-78e2-11ea-b9b2-0e4e69ee6911", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2t9i0", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1447e19", "is_robot_indexable": true, "report_reasons": null, "author": "Final_Passion1350", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/careerguidance/comments/1447e19/typical_machine_learning_engineer_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/careerguidance/comments/1447e19/typical_machine_learning_engineer_salary/", "subreddit_subscribers": 1899327, "created_utc": 1686225804.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1686259686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.careerguidance", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/careerguidance/comments/1447e19/typical_machine_learning_engineer_salary/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144lq2h", "is_robot_indexable": true, "report_reasons": null, "author": "Final_Passion1350", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1447e19", "author_flair_text_color": null, "permalink": "/r/datascience/comments/144lq2h/typical_machine_learning_engineer_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/careerguidance/comments/1447e19/typical_machine_learning_engineer_salary/", "subreddit_subscribers": 921072, "created_utc": 1686259686.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know that set ups will vary based on all kinds of variables, but I am looking for a diagram that shows a typical set up. Any help is greatly appreciated!", "author_fullname": "t2_br8ttrfio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi folks! Can anyone help me understand the basic structure of a hyperscale/cloud data center?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144fbco", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that set ups will vary based on all kinds of variables, but I am looking for a diagram that shows a typical set up. Any help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144fbco", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Week1997", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144fbco/hi_folks_can_anyone_help_me_understand_the_basic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144fbco/hi_folks_can_anyone_help_me_understand_the_basic/", "subreddit_subscribers": 921072, "created_utc": 1686245015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_oime0u9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_144ed4x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "author_name": "PSN Academy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OISLU6FWlgc/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@PSNAcademy"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/144ed4x", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JuajMx4-3I1GBwnMEdSZI1cT4RXjWw_0XVP4J0FkZ0U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686242848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=OISLU6FWlgc&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?auto=webp&amp;v=enabled&amp;s=8ac302293204c47e5da89bd2016d45daa4e919d8", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19071c06bdb8aa79fc627188f56d3e31af938307", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e330ea451a896099c9202e5c5ef32ac287b07a4b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1da7e7a4875ef10610c171fd7988a8d00a4593d", "width": 320, "height": 240}], "variants": {}, "id": "r2MUKkg8ca1_s8pyFfcwN3ycxWdpaHhWVrKK98Fyq2o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "144ed4x", "is_robot_indexable": true, "report_reasons": null, "author": "profpsnayak", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144ed4x/\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25_\ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\ud835\udc2c_\ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d_\ud835\udc28\ud835\udc1f_\ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=OISLU6FWlgc&amp;feature=share", "subreddit_subscribers": 921072, "created_utc": 1686242848.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "author_name": "PSN Academy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OISLU6FWlgc/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@PSNAcademy"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "  Everyone talk about space as final front, but sea is the last one. Because can be a great source from valuable resource as oil, minerals and food. World largest corps actually explores and make money from it, being estimated as to be a big industry in next decades.\n  So I guess data professionals can be useful and make money helping to discover where interesting things can be found. So how work in this industry, and what might be useful?", "author_fullname": "t2_c12nt86a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How work in sea exploration as data profissional?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449ohj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone talk about space as final front, but sea is the last one. Because can be a great source from valuable resource as oil, minerals and food. World largest corps actually explores and make money from it, being estimated as to be a big industry in next decades.\n  So I guess data professionals can be useful and make money helping to discover where interesting things can be found. So how work in this industry, and what might be useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1449ohj", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Trifle-2735", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1449ohj/how_work_in_sea_exploration_as_data_profissional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1449ohj/how_work_in_sea_exploration_as_data_profissional/", "subreddit_subscribers": 921072, "created_utc": 1686231795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Before diving into the purpose i.e will\n\nMojo\ud83d\udd25 replace Python\ud83d\udc0d or not, first of all, let's see what is Mojo.  \n[https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521](https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521)", "author_fullname": "t2_icilc2wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review \ud83d\udd25 Mojo: Will Mojo\ud83d\udd25 Replace Python? \ud83e\udd14", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144euf9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.1, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686243925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before diving into the purpose i.e will&lt;/p&gt;\n\n&lt;p&gt;Mojo\ud83d\udd25 replace Python\ud83d\udc0d or not, first of all, let&amp;#39;s see what is Mojo.&lt;br/&gt;\n&lt;a href=\"https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521\"&gt;https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?auto=webp&amp;v=enabled&amp;s=1b6758ef9e33db8344d93a82c9b04aa20dfd3942", "width": 832, "height": 470}, "resolutions": [{"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2edf9f78592752569097c6101967c97f70cea9ae", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9194847bf52a6105e57a7e29e2024a5931b8374", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5354363d198876e0b43805cf2cee8e45cbedf530", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3e95214d4c3b91e4784b9e5a8068f4f0448f733", "width": 640, "height": 361}], "variants": {}, "id": "ufwQMjelZj16nhL0qkZMNIgiNqBpIcGV9sHJFCUVDTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144euf9", "is_robot_indexable": true, "report_reasons": null, "author": "gaodalie", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144euf9/review_mojo_will_mojo_replace_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144euf9/review_mojo_will_mojo_replace_python/", "subreddit_subscribers": 921072, "created_utc": 1686243925.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}