{"kind": "Listing", "data": {"after": "t3_144n175", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, founder at GlareDB here.\n\nWe've just open sourced GlareDB, a database for querying distributed data with SQL. Check out the repo here: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb)\n\nWe have integrations with Postgres, Snowflake, files in S3 (Parquet, CSV), and more. Our goal is to make it easy to run analytics across disparate data sources using just SQL, reducing the need to set up ETL pipelines to move data around.  Take a look at our [docs](https://docs.glaredb.com/docs/working-with-your-data/querying.html#querying-multiple-data-sources) to see what querying multiple data sources looks like. We've also recently merged in a [PR](https://github.com/GlareDB/glaredb/pull/1086) letting you run queries like `select * from read_postgres(...)`.\n\nGlareDB is still early stages, and we have a lot planned the next few months. Have a use case that you think GlareDB is a good fit for? Let us know! And if you have any feature request for things you'd like to see, feel free to open up an issue.", "author_fullname": "t2_butu3hfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GlareDB: An open source SQL database to query and analyze distributed data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144hveq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 125, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 125, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686250941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, founder at GlareDB here.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve just open sourced GlareDB, a database for querying distributed data with SQL. Check out the repo here: &lt;a href=\"https://github.com/GlareDB/glaredb\"&gt;https://github.com/GlareDB/glaredb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We have integrations with Postgres, Snowflake, files in S3 (Parquet, CSV), and more. Our goal is to make it easy to run analytics across disparate data sources using just SQL, reducing the need to set up ETL pipelines to move data around.  Take a look at our &lt;a href=\"https://docs.glaredb.com/docs/working-with-your-data/querying.html#querying-multiple-data-sources\"&gt;docs&lt;/a&gt; to see what querying multiple data sources looks like. We&amp;#39;ve also recently merged in a &lt;a href=\"https://github.com/GlareDB/glaredb/pull/1086\"&gt;PR&lt;/a&gt; letting you run queries like &lt;code&gt;select * from read_postgres(...)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;GlareDB is still early stages, and we have a lot planned the next few months. Have a use case that you think GlareDB is a good fit for? Let us know! And if you have any feature request for things you&amp;#39;d like to see, feel free to open up an issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?auto=webp&amp;v=enabled&amp;s=1f13c31ab3a2e2f3df63a36e4be6217b75f3982c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1e5ac2db1e6e636b3f537b68a1eb1da9731dd14", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d221a4508a9a87c115fa46e5a507d52a498b3ca", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b294cf0181b8486516795475ca969156ed95699a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b08f1cb3ecd212bac39c01b21198b192af02ea7a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=791ea716ac65fab56be13cd201c6f8b288e36036", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/t2KztFW2BBEwoDxZ-bhYO-TU-58YuPAQHbM0NriOaTY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caaeacc15a204febc2068a055848022f2701e72c", "width": 1080, "height": 540}], "variants": {}, "id": "K2VwLOrLqwVN4d-xERv1jNfaQjJr4mEavmIaRBbiLGI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "144hveq", "is_robot_indexable": true, "report_reasons": null, "author": "sean-glaredb", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144hveq/glaredb_an_open_source_sql_database_to_query_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144hveq/glaredb_an_open_source_sql_database_to_query_and/", "subreddit_subscribers": 109798, "created_utc": 1686250941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "That\u2019s all.", "author_fullname": "t2_71x5v4dp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a final job interview in a few days and I\u2019m scared.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144u8fv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686282075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That\u2019s all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "144u8fv", "is_robot_indexable": true, "report_reasons": null, "author": "Emosk8rboi42969", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144u8fv/i_have_a_final_job_interview_in_a_few_days_and_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144u8fv/i_have_a_final_job_interview_in_a_few_days_and_im/", "subreddit_subscribers": 109798, "created_utc": 1686282075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the course of 13+ years, I've become very proficient on SQL. On the technical side, I can do really complex queries, CTEs, window functions, understanding perfomance plans, indices, and I've also learned about DBA regarding file management, logging, and things like that. \n\nI can very well translate business requirements into a relational database model, and build complex tools using SQL + VB.NET or VBA on Excel. For ETL I can use SSIS, and orchestrate everything with VBA, PowerShell, MS Flow/Automate, and different Windows schedulers or jobs. On the report side I can build a PowerBI dashboard or a very complex tool based on Excel with VBA or a Windows application with .NET. I'm starting to learn Python but so far have been able to make do with the tools I know.\n\nI thought I could call myself a Data Engineer.\n\nBut everytime I look at Data Enginer job postings, or even recommendations on this sub, all I see are things like Spark, Hadoop, Snowflake, Databricks, AWS and Azure Cloud. Things that not only I haven't learned yet, but I haven't been able to _see_ in my work environment.\n\nSo... am I not a Data Engineer? Or am I just a different type of DE from what the current trend needs?", "author_fullname": "t2_3k4lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data Engineer\" vs \"SQL Expert\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144ccuu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686239513.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686238242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the course of 13+ years, I&amp;#39;ve become very proficient on SQL. On the technical side, I can do really complex queries, CTEs, window functions, understanding perfomance plans, indices, and I&amp;#39;ve also learned about DBA regarding file management, logging, and things like that. &lt;/p&gt;\n\n&lt;p&gt;I can very well translate business requirements into a relational database model, and build complex tools using SQL + VB.NET or VBA on Excel. For ETL I can use SSIS, and orchestrate everything with VBA, PowerShell, MS Flow/Automate, and different Windows schedulers or jobs. On the report side I can build a PowerBI dashboard or a very complex tool based on Excel with VBA or a Windows application with .NET. I&amp;#39;m starting to learn Python but so far have been able to make do with the tools I know.&lt;/p&gt;\n\n&lt;p&gt;I thought I could call myself a Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;But everytime I look at Data Enginer job postings, or even recommendations on this sub, all I see are things like Spark, Hadoop, Snowflake, Databricks, AWS and Azure Cloud. Things that not only I haven&amp;#39;t learned yet, but I haven&amp;#39;t been able to &lt;em&gt;see&lt;/em&gt; in my work environment.&lt;/p&gt;\n\n&lt;p&gt;So... am I not a Data Engineer? Or am I just a different type of DE from what the current trend needs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_b4ff447e-05a5-42dc-9002-63568807cfe6", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A glowing commendation for all to see", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "All-Seeing Upvote", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=25880d00e4283ff6a3851b64c83fea465c3fac48", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=67fae0c2af26488b9d70cb7afe877086707cf1d1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=3033c6583809a27b998883b7c56c158102cb0420", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=cccb48d7518eaba72894a7ac4214bb70c7120793", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=4101e0eff424bbd818195853387db358bec74ed0", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "144ccuu", "is_robot_indexable": true, "report_reasons": null, "author": "mecartistronico", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144ccuu/data_engineer_vs_sql_expert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144ccuu/data_engineer_vs_sql_expert/", "subreddit_subscribers": 109798, "created_utc": 1686238242.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6lg1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte 0.50: Introducing Checkpointing, Column Selection, and Schema Propagation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_144br5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-DyZAnfRrMkV5t8ipBovTsZAY3U6hdENANp0mqMpsec.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686236784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/announcing-airbyte-0-50-checkpointing-column-selection-and-schema-propagation", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?auto=webp&amp;v=enabled&amp;s=f89bb48d85b033c506d0ec07110eeab11312351f", "width": 2400, "height": 1256}, "resolutions": [{"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb722b51d518ae6f493d0003ce82fa1d44ac547e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=748a6aedb97c7d4512995e93ab98bb027a13840b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d2dda26256ffbc65c6320271db84cb42ae378d2", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=994fdf502f0814c2545e6fdea10e025e19e39ce1", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32b7170ec371b56ae4aceaa92c8fd9ab0de70f25", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/-3U4WuN6oTE3jeqsFl9vdpMaeXFvbDeGTSb1CIwyZg8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fafe44574f8a0b5956aeb84b682c6dc4a22e0643", "width": 1080, "height": 565}], "variants": {}, "id": "HCnX-381bCXm4BTZJDB__gImXB-fG7zq42d3ULpDpPg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "144br5h", "is_robot_indexable": true, "report_reasons": null, "author": "evantahler", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144br5h/airbyte_050_introducing_checkpointing_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/announcing-airbyte-0-50-checkpointing-column-selection-and-schema-propagation", "subreddit_subscribers": 109798, "created_utc": 1686236784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I recently started an analyst job that has taken quite a shift in responsibilities, I started out doing pretty basic stuff excel monkey style administrative duties that involved a fair bit of analysis.\n\nMy responsibilities are becoming a lot more data processing based, which I love, but I'm not sure if it's involved enough to call day engineering. \nI'm pretty much focused I'm automating the data processing, we already have a software system built for data transformations for our industry, but I will be focusing on automating the data processing before and after this system.\n\nFor example I'll be using SQL, alteryx and other tools to get data from our clients (our current methods are not scalable, so they want me to make it scalable to take clients with 10x as much data as our current clients) and do all the data transformations to get it into our software.\n\nI'm also building dimensional OLAP cube models for our reporting systems, currently our software system outputs raw data which would be unusable for our clients, so the analyst uses excel to make it readable. I will be setting the ETL for this reporting system, the model and need to design reports that can handle clients 10x larger than our current clients, while still showing all the necessary info, but while being easy to understand (think thousands of products with almost 100 possible measures that need to be shown to the client per each product)\n\nI'm making a bit under 45k to do this, but I'm sure it will go up a bit in time since I just started taking on these new responsibilities.\n\nAm I doing junior level data engineering? And what type of salary should I be asking for when it comes time for a raise?", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I data engineering? Am I criminally underpaid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144ho5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686250504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I recently started an analyst job that has taken quite a shift in responsibilities, I started out doing pretty basic stuff excel monkey style administrative duties that involved a fair bit of analysis.&lt;/p&gt;\n\n&lt;p&gt;My responsibilities are becoming a lot more data processing based, which I love, but I&amp;#39;m not sure if it&amp;#39;s involved enough to call day engineering. \nI&amp;#39;m pretty much focused I&amp;#39;m automating the data processing, we already have a software system built for data transformations for our industry, but I will be focusing on automating the data processing before and after this system.&lt;/p&gt;\n\n&lt;p&gt;For example I&amp;#39;ll be using SQL, alteryx and other tools to get data from our clients (our current methods are not scalable, so they want me to make it scalable to take clients with 10x as much data as our current clients) and do all the data transformations to get it into our software.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also building dimensional OLAP cube models for our reporting systems, currently our software system outputs raw data which would be unusable for our clients, so the analyst uses excel to make it readable. I will be setting the ETL for this reporting system, the model and need to design reports that can handle clients 10x larger than our current clients, while still showing all the necessary info, but while being easy to understand (think thousands of products with almost 100 possible measures that need to be shown to the client per each product)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making a bit under 45k to do this, but I&amp;#39;m sure it will go up a bit in time since I just started taking on these new responsibilities.&lt;/p&gt;\n\n&lt;p&gt;Am I doing junior level data engineering? And what type of salary should I be asking for when it comes time for a raise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "144ho5p", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144ho5p/am_i_data_engineering_am_i_criminally_underpaid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144ho5p/am_i_data_engineering_am_i_criminally_underpaid/", "subreddit_subscribers": 109798, "created_utc": 1686250504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys ... Could you please suggest me materials / courses where I can see some practical TDD in data engineering ?", "author_fullname": "t2_i7ebjs8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit testing / integration testing learning materials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144q1rc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686270213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys ... Could you please suggest me materials / courses where I can see some practical TDD in data engineering ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "144q1rc", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Enthusiasm_2842", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144q1rc/unit_testing_integration_testing_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144q1rc/unit_testing_integration_testing_learning/", "subreddit_subscribers": 109798, "created_utc": 1686270213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, good community.\n\nLet's say you have some spare time and want to add a side income. Or, you need it to secure better stability for years to come.\n\nWhat are our alternatives? Have you tried something and have numbers to share (income vs investment in hours and money)? Or maybe listened from a friend? Or maybe want to start...?\n\nHope this discussion yields positive ideias and results. :)", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make side revenue as a data/software engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144kigq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686256993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, good community.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you have some spare time and want to add a side income. Or, you need it to secure better stability for years to come.&lt;/p&gt;\n\n&lt;p&gt;What are our alternatives? Have you tried something and have numbers to share (income vs investment in hours and money)? Or maybe listened from a friend? Or maybe want to start...?&lt;/p&gt;\n\n&lt;p&gt;Hope this discussion yields positive ideias and results. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "144kigq", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144kigq/how_to_make_side_revenue_as_a_datasoftware/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144kigq/how_to_make_side_revenue_as_a_datasoftware/", "subreddit_subscribers": 109798, "created_utc": 1686256993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My employer has spent sickening amounts of money on various \u201cself serve\u201d analytics/bi/de tools. I have been successful on some small projects like setting up tools to generate commonly requested excel files and dashboards to let people filter and play around with stuff that been largely defined I.e return on sales\u2026. But that\u2019s akin to letting somebody drive the car. You can drive it however you want, but we built the roads so you can only go where we say you can go and if you want to drive somewhere new, the engineering team is going to have to build you a new road.  \n\nHave you ever witnessed the ever elusive \u201cself serve\u201d where entry level hourly workers are solving complex problems and generating value for companies? Or is somebody doing AVG/SUM in some carefully curated DB view in power bi and calling it ML the absolute ceiling?", "author_fullname": "t2_4nbvm2s0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you ever seen a successful \u201cself-serve\u201d implementation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1454ghq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686315161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My employer has spent sickening amounts of money on various \u201cself serve\u201d analytics/bi/de tools. I have been successful on some small projects like setting up tools to generate commonly requested excel files and dashboards to let people filter and play around with stuff that been largely defined I.e return on sales\u2026. But that\u2019s akin to letting somebody drive the car. You can drive it however you want, but we built the roads so you can only go where we say you can go and if you want to drive somewhere new, the engineering team is going to have to build you a new road.  &lt;/p&gt;\n\n&lt;p&gt;Have you ever witnessed the ever elusive \u201cself serve\u201d where entry level hourly workers are solving complex problems and generating value for companies? Or is somebody doing AVG/SUM in some carefully curated DB view in power bi and calling it ML the absolute ceiling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1454ghq", "is_robot_indexable": true, "report_reasons": null, "author": "Visible-Tennis4144", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1454ghq/have_you_ever_seen_a_successful_selfserve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1454ghq/have_you_ever_seen_a_successful_selfserve/", "subreddit_subscribers": 109798, "created_utc": 1686315161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A lot of code we have in our pipelines is written in R, and I am hoping to swap over to python so that more folks on the team can read/write code.\n\nAny tips for transitioning someone from R to pandas?  I am not an expert in R so trying to find good resources.\n\nAlso what would be the equivalent of shiny in python?  \n\nWe have two apps we would like to port over - has a dropdown that controls data in a timeline.  \n\nThanks!", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from R to Python - any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1450x5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686304362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of code we have in our pipelines is written in R, and I am hoping to swap over to python so that more folks on the team can read/write code.&lt;/p&gt;\n\n&lt;p&gt;Any tips for transitioning someone from R to pandas?  I am not an expert in R so trying to find good resources.&lt;/p&gt;\n\n&lt;p&gt;Also what would be the equivalent of shiny in python?  &lt;/p&gt;\n\n&lt;p&gt;We have two apps we would like to port over - has a dropdown that controls data in a timeline.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1450x5k", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1450x5k/transitioning_from_r_to_python_any_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1450x5k/transitioning_from_r_to_python_any_tips/", "subreddit_subscribers": 109798, "created_utc": 1686304362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " In today's data-driven world, businesses need to have a solid data architecture in place to make informed decisions and drive growth. A well-designed data architecture allows for seamless data integration, efficient data processing, and scalable solutions. In this article, we'll discuss data architecture best practices based on the advice of Dan Sutherland, a managing director focusing on technology consulting. We'll also explore the different roles involved in building a modern data architecture, such as data architects, data modelers, data integration developers, and data engineers.\n\n## Seven Best Practices for Designing a Data Architecture\n\n1. Cloud-native Design: Modern data architecture should be designed to support scaling, high availability, and end-to-end security for data. This design allows for easy scalability without affecting performance.\n2. Scalable Data Pipelines: Data architectures should support real-time data streaming and micro-batch data bursts to handle spikes in data pipelines, such as seasonal fluctuations or quarter-end data flows.\n3. Seamless Data Integration: Data architectures should integrate with legacy applications using standard API interfaces, optimizing data sharing across systems and departments within an organization.\n4. Real-time Data Enablement: Enterprises need the ability to deploy automated and active data validations, classifications, management, and governance with complete and visible data lineage.\n5. Decoupled and Extensible Design: Data services provided to different organizations should not depend on one another, and it should be easy to add new capabilities and functionalities, such as adding data flow from Salesforce into your systems.\n6. Domain-driven Approach: Modern data architecture should be driven by common data domains, events, and microservices, centered around the common business information model.\n7. Balanced Investment: Consider the return on investment for your business when building a data architecture. There's no need to overinvest in modern data architecture environments and features if they're not needed for your business size and growth.\n\n## Roles Involved in Data Architecture\n\nA team of skilled professionals is essential for successfully executing these practices. Each member of the team brings their unique expertise to the table, ensuring that the organization's data strategy aligns with its overall goals. The key roles within a data engineering team include:\n\n* Data Architect: As a senior leader, the Data Architect is responsible for translating business requirements into technology requirements. They define the data architecture framework, standards, principles, and reference architecture, which serve as the foundation for the organization's data strategy. In this role, they collaborate and coordinate with multiple departments, stakeholders, partners, and external vendors to ensure seamless integration of data solutions.\n* Data Modeler: The Data Modeler creates conceptual, logical, or physical models of data sets, which provide a clear and consistent representation of the organization's data. By reverse-engineering databases, they identify standard labels and notations for use across departments, fostering consistency and streamlining communication between teams.\n* Data Integration Developer: These individuals are responsible for designing and implementing integrations between software platforms, programs, and applications. Working closely with the Data Architect, they ensure that the organization's data systems are interconnected and function seamlessly, enabling the extraction of maximum value from data assets.\n* Data Engineer: In situations where a Data Architect may not be present, such as in smaller companies, Data Engineers take on the responsibility of creating the vision designed by the Data Architect. They implement the data architecture framework, building the pipelines and infrastructure necessary to store, process, and analyze data effectively.\n\nBy leveraging the unique skills and expertise of each team member, a data engineering team can effectively execute an organization's data strategy, ultimately driving value and supporting data-driven decision-making across the company.\n\nBuilding a robust data architecture is crucial for businesses of all sizes. By following these best practices and understanding the different roles involved in data architecture, organizations can make better decisions, improve efficiency, and drive growth. When considering your current data architecture, think about which roles are present in your organization and whether they fulfill the responsibilities outlined in this article. As you plan and invest in your data architecture, remember to keep a balance between your business needs and the return on investment.\n\nI share these and other tips on building robust IT architecture in my blog: [https://ainsys.com/blog/2023/04/20/data-architecture-practices/?utm\\_source=reddit&amp;utm\\_medium=social&amp;utm\\_campaign=data\\_engineering&amp;utm\\_content=data\\_architecture&amp;utm\\_term=ITarchitecture](https://ainsys.com/blog/2023/04/20/data-architecture-practices/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=data_engineering&amp;utm_content=data_architecture&amp;utm_term=ITarchitecture)", "author_fullname": "t2_ct09rz3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture Best Practices: How to Build a Robust Data Infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144en5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686243492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In today&amp;#39;s data-driven world, businesses need to have a solid data architecture in place to make informed decisions and drive growth. A well-designed data architecture allows for seamless data integration, efficient data processing, and scalable solutions. In this article, we&amp;#39;ll discuss data architecture best practices based on the advice of Dan Sutherland, a managing director focusing on technology consulting. We&amp;#39;ll also explore the different roles involved in building a modern data architecture, such as data architects, data modelers, data integration developers, and data engineers.&lt;/p&gt;\n\n&lt;h2&gt;Seven Best Practices for Designing a Data Architecture&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Cloud-native Design: Modern data architecture should be designed to support scaling, high availability, and end-to-end security for data. This design allows for easy scalability without affecting performance.&lt;/li&gt;\n&lt;li&gt;Scalable Data Pipelines: Data architectures should support real-time data streaming and micro-batch data bursts to handle spikes in data pipelines, such as seasonal fluctuations or quarter-end data flows.&lt;/li&gt;\n&lt;li&gt;Seamless Data Integration: Data architectures should integrate with legacy applications using standard API interfaces, optimizing data sharing across systems and departments within an organization.&lt;/li&gt;\n&lt;li&gt;Real-time Data Enablement: Enterprises need the ability to deploy automated and active data validations, classifications, management, and governance with complete and visible data lineage.&lt;/li&gt;\n&lt;li&gt;Decoupled and Extensible Design: Data services provided to different organizations should not depend on one another, and it should be easy to add new capabilities and functionalities, such as adding data flow from Salesforce into your systems.&lt;/li&gt;\n&lt;li&gt;Domain-driven Approach: Modern data architecture should be driven by common data domains, events, and microservices, centered around the common business information model.&lt;/li&gt;\n&lt;li&gt;Balanced Investment: Consider the return on investment for your business when building a data architecture. There&amp;#39;s no need to overinvest in modern data architecture environments and features if they&amp;#39;re not needed for your business size and growth.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;Roles Involved in Data Architecture&lt;/h2&gt;\n\n&lt;p&gt;A team of skilled professionals is essential for successfully executing these practices. Each member of the team brings their unique expertise to the table, ensuring that the organization&amp;#39;s data strategy aligns with its overall goals. The key roles within a data engineering team include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Architect: As a senior leader, the Data Architect is responsible for translating business requirements into technology requirements. They define the data architecture framework, standards, principles, and reference architecture, which serve as the foundation for the organization&amp;#39;s data strategy. In this role, they collaborate and coordinate with multiple departments, stakeholders, partners, and external vendors to ensure seamless integration of data solutions.&lt;/li&gt;\n&lt;li&gt;Data Modeler: The Data Modeler creates conceptual, logical, or physical models of data sets, which provide a clear and consistent representation of the organization&amp;#39;s data. By reverse-engineering databases, they identify standard labels and notations for use across departments, fostering consistency and streamlining communication between teams.&lt;/li&gt;\n&lt;li&gt;Data Integration Developer: These individuals are responsible for designing and implementing integrations between software platforms, programs, and applications. Working closely with the Data Architect, they ensure that the organization&amp;#39;s data systems are interconnected and function seamlessly, enabling the extraction of maximum value from data assets.&lt;/li&gt;\n&lt;li&gt;Data Engineer: In situations where a Data Architect may not be present, such as in smaller companies, Data Engineers take on the responsibility of creating the vision designed by the Data Architect. They implement the data architecture framework, building the pipelines and infrastructure necessary to store, process, and analyze data effectively.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;By leveraging the unique skills and expertise of each team member, a data engineering team can effectively execute an organization&amp;#39;s data strategy, ultimately driving value and supporting data-driven decision-making across the company.&lt;/p&gt;\n\n&lt;p&gt;Building a robust data architecture is crucial for businesses of all sizes. By following these best practices and understanding the different roles involved in data architecture, organizations can make better decisions, improve efficiency, and drive growth. When considering your current data architecture, think about which roles are present in your organization and whether they fulfill the responsibilities outlined in this article. As you plan and invest in your data architecture, remember to keep a balance between your business needs and the return on investment.&lt;/p&gt;\n\n&lt;p&gt;I share these and other tips on building robust IT architecture in my blog: &lt;a href=\"https://ainsys.com/blog/2023/04/20/data-architecture-practices/?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=data_engineering&amp;amp;utm_content=data_architecture&amp;amp;utm_term=ITarchitecture\"&gt;https://ainsys.com/blog/2023/04/20/data-architecture-practices/?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=data_engineering&amp;amp;utm_content=data_architecture&amp;amp;utm_term=ITarchitecture&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I3xJj8ELsQOmy2H-NeHKFmGnh2tKVfHX1i7SJ1O1qwE.jpg?auto=webp&amp;v=enabled&amp;s=a857d06fc3f1362c48ce1ce4e18be17b3a3dc051", "width": 79, "height": 86}, "resolutions": [], "variants": {}, "id": "fKkAw45B6Aa1K9gfC0CvmXNzCjb0F-J2wvKUvaKf1Vk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "144en5h", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Speech36", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144en5h/data_architecture_best_practices_how_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144en5h/data_architecture_best_practices_how_to_build_a/", "subreddit_subscribers": 109798, "created_utc": 1686243492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bc2rrdnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Tool for NoSQL Data Modelling to get a visualization like seen here?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_144bqls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YGLZC-Wtwnxxq7lEhGGxM_fS3pHPUwe3D-VXkSYTsTU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686236751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/e5x6cppl7t4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/e5x6cppl7t4b1.png?auto=webp&amp;v=enabled&amp;s=2c9d290ece4bc04a4a37366cebd81ddae57b1506", "width": 1361, "height": 948}, "resolutions": [{"url": "https://preview.redd.it/e5x6cppl7t4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b21ef0b07cda2f37eeaa1ffe82eaf5752ce0d14", "width": 108, "height": 75}, {"url": "https://preview.redd.it/e5x6cppl7t4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd5e43803319b1555fc40f0cffa4647443713c9d", "width": 216, "height": 150}, {"url": "https://preview.redd.it/e5x6cppl7t4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=204d4af49bd604bc2c063c9dca5533a5d6569e8c", "width": 320, "height": 222}, {"url": "https://preview.redd.it/e5x6cppl7t4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b43fbadd2493af1adec74e0d15fa904af947ab91", "width": 640, "height": 445}, {"url": "https://preview.redd.it/e5x6cppl7t4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=771904e03b81f656e9263071ec0624f96c482866", "width": 960, "height": 668}, {"url": "https://preview.redd.it/e5x6cppl7t4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb3c2b594cee466669790b8555a9b149f1f6899c", "width": 1080, "height": 752}], "variants": {}, "id": "TaOjWvvsrofzkwX3kq_CdTOI1vUTv2RIhlc6CMtXSVc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144bqls", "is_robot_indexable": true, "report_reasons": null, "author": "AndyMacht58", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144bqls/free_tool_for_nosql_data_modelling_to_get_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/e5x6cppl7t4b1.png", "subreddit_subscribers": 109798, "created_utc": 1686236751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone got some best practice github gist for a load module/function (using sqlalchemy/etc +x). Assume simple single node compute: dataframe to database table. \n\n- criteria: performant, parameterized/injection safe, requires specification of schema/dtypes (if pandas helper is used), etc.\n\nIf you dont have one at hand, I also welcome some bullet points on best practices. Elaborate implementations welcome. Trying to sanity check.", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Example of best-practice python load function/module", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144e5wo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686249245.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686242389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone got some best practice github gist for a load module/function (using sqlalchemy/etc +x). Assume simple single node compute: dataframe to database table. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;criteria: performant, parameterized/injection safe, requires specification of schema/dtypes (if pandas helper is used), etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you dont have one at hand, I also welcome some bullet points on best practices. Elaborate implementations welcome. Trying to sanity check.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144e5wo", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144e5wo/example_of_bestpractice_python_load_functionmodule/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144e5wo/example_of_bestpractice_python_load_functionmodule/", "subreddit_subscribers": 109798, "created_utc": 1686242389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anybody know if it's possible to automatically execute bootstrap queries when connecting to Snowflake via SQLAlchemy? The way our Snowflake instance is managed requires us to use multiple roles for access to data across schemas. Our primary role remains readable/writable. \n\nTypically, the first thing we do with a new connection is execute:\n\nUSE SECONDARY ROLES ALL;\n\nHowever, I haven't figured out how to automate this with SQLAlchemy. I stumbled across this while testing whether Superset would work for a specific dashboarding need. Superset disallows creating data sets from multiple queries, so it's not possible to include that query above the business logic. Ideally, it would be quietly fired in the background each time Superset connects to the Snowflake. \n\nIs this possible?", "author_fullname": "t2_ysn7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQLAlchemy connection w/Snowflake and auto-executing bootstrap queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144jmmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686254989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody know if it&amp;#39;s possible to automatically execute bootstrap queries when connecting to Snowflake via SQLAlchemy? The way our Snowflake instance is managed requires us to use multiple roles for access to data across schemas. Our primary role remains readable/writable. &lt;/p&gt;\n\n&lt;p&gt;Typically, the first thing we do with a new connection is execute:&lt;/p&gt;\n\n&lt;p&gt;USE SECONDARY ROLES ALL;&lt;/p&gt;\n\n&lt;p&gt;However, I haven&amp;#39;t figured out how to automate this with SQLAlchemy. I stumbled across this while testing whether Superset would work for a specific dashboarding need. Superset disallows creating data sets from multiple queries, so it&amp;#39;s not possible to include that query above the business logic. Ideally, it would be quietly fired in the background each time Superset connects to the Snowflake. &lt;/p&gt;\n\n&lt;p&gt;Is this possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144jmmu", "is_robot_indexable": true, "report_reasons": null, "author": "i_hmm_some", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144jmmu/sqlalchemy_connection_wsnowflake_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144jmmu/sqlalchemy_connection_wsnowflake_and/", "subreddit_subscribers": 109798, "created_utc": 1686254989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for an orchestration tool to add to my stack. Ideally open source. The bulk of the jobs are dbt transformations, I want something that integrates well with dbt cloud.\n\nI use meltano and fivetran for ingestion, snowflake as database. S3/AWS in some pipelines. \n\nI am new to all the mainstream orchestration tools. Currently considering Airflow, Dagster and Prefect", "author_fullname": "t2_vikcbs0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best orchestration tool for dbt-cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144grme", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686248375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for an orchestration tool to add to my stack. Ideally open source. The bulk of the jobs are dbt transformations, I want something that integrates well with dbt cloud.&lt;/p&gt;\n\n&lt;p&gt;I use meltano and fivetran for ingestion, snowflake as database. S3/AWS in some pipelines. &lt;/p&gt;\n\n&lt;p&gt;I am new to all the mainstream orchestration tools. Currently considering Airflow, Dagster and Prefect&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144grme", "is_robot_indexable": true, "report_reasons": null, "author": "RespondOk3068", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144grme/best_orchestration_tool_for_dbtcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144grme/best_orchestration_tool_for_dbtcloud/", "subreddit_subscribers": 109798, "created_utc": 1686248375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to set up a pipeline to load data from Postgres to OpenSearch. The OpenSearch model includes some fields that are lists of IDs from many to many mappings and some other more complex transformations. I am looking for non-expensive solutions that are not super hard to implement and manage. I looked into AWS DMS but the transformations seem super limited? If anyone has any suggestions I'm happy to listen!", "author_fullname": "t2_w1tdhbrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage loading data from RDS to OpenSearch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145098s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686302123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to set up a pipeline to load data from Postgres to OpenSearch. The OpenSearch model includes some fields that are lists of IDs from many to many mappings and some other more complex transformations. I am looking for non-expensive solutions that are not super hard to implement and manage. I looked into AWS DMS but the transformations seem super limited? If anyone has any suggestions I&amp;#39;m happy to listen!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145098s", "is_robot_indexable": true, "report_reasons": null, "author": "data_pie3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145098s/how_do_you_manage_loading_data_from_rds_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145098s/how_do_you_manage_loading_data_from_rds_to/", "subreddit_subscribers": 109798, "created_utc": 1686302123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for join candidates by identifying key columns (common unique identifiers) between tables. Using some existing data sets from non-normalized/structured source data. \n\nThe source of the data is from end users and not already in a database \n\n I can do this the normal way and just scan and manually query the columns and data for each table. However, you guys are always on top of the latest tools so wanted to see if there was anything modern that does some of the initial analysis to speed things up to become more efficient.\n\nExample - I would just dump these tables into Parquet files or Postgres and then point the tool at it, and it can find common data candidates between the tables.\n\nAnything out there that you guys like using ?", "author_fullname": "t2_t2wl82bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any open source tools you guys recommend for discovering relationships between tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144u523", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686286526.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686281782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for join candidates by identifying key columns (common unique identifiers) between tables. Using some existing data sets from non-normalized/structured source data. &lt;/p&gt;\n\n&lt;p&gt;The source of the data is from end users and not already in a database &lt;/p&gt;\n\n&lt;p&gt;I can do this the normal way and just scan and manually query the columns and data for each table. However, you guys are always on top of the latest tools so wanted to see if there was anything modern that does some of the initial analysis to speed things up to become more efficient.&lt;/p&gt;\n\n&lt;p&gt;Example - I would just dump these tables into Parquet files or Postgres and then point the tool at it, and it can find common data candidates between the tables.&lt;/p&gt;\n\n&lt;p&gt;Anything out there that you guys like using ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "144u523", "is_robot_indexable": true, "report_reasons": null, "author": "generic-d-engineer", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144u523/any_open_source_tools_you_guys_recommend_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144u523/any_open_source_tools_you_guys_recommend_for/", "subreddit_subscribers": 109798, "created_utc": 1686281782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a Data Engineer for the last 6 years, and sometimes I feel like I need to try something new.\nBut the thing is, apart from the work itself I value money (who doesn't?) over job satisfaction in general.\nSo I was thinking about the move into Sales\nEngineering but I don't know exactly what would be an average salary for a SE, since every source check have a different average salary...\nSo my point is, money wise, is it worth it to migrate to SE or would that be too much of a pay cut?", "author_fullname": "t2_3ng50ktz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is moving from DE to SE a good idea?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144ixlf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686253387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a Data Engineer for the last 6 years, and sometimes I feel like I need to try something new.\nBut the thing is, apart from the work itself I value money (who doesn&amp;#39;t?) over job satisfaction in general.\nSo I was thinking about the move into Sales\nEngineering but I don&amp;#39;t know exactly what would be an average salary for a SE, since every source check have a different average salary...\nSo my point is, money wise, is it worth it to migrate to SE or would that be too much of a pay cut?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "144ixlf", "is_robot_indexable": true, "report_reasons": null, "author": "BramosR", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/144ixlf/is_moving_from_de_to_se_a_good_idea/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144ixlf/is_moving_from_de_to_se_a_good_idea/", "subreddit_subscribers": 109798, "created_utc": 1686253387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am currently searching an affordable alternative for an aging Hadoop cluster. We already have some stuff in Athena (parquet + S3 + Glue) but it is not very well structured.\n\nToday I tested Trino on EMR vs Athena with and without tables stored in the Iceberg format.\n\nThe overall experience with Athena was much better. Queries where faster and they worked out of the box. Trino was much slower, scaling didn't worked and sometimes it was not able to read the parquet files without specifying a proper error message.\n\nI cannot wrap my head around why most people seem to use Trino over Athena? It seems to be much more expensive, slower and less robust. (Albeit, the last point might originate from my limited experience).\n\nCan someone who is using both technologies give me a more refined report on the experience? We definitely do not want to use other SaaS technologies (we are solely on AWS, so GCP/BigQuery and Azure is not on the table anyways; Snowflake is so intransparent with their pricing; Databricks seems overkill)\n\nCheers,\n\nMatt", "author_fullname": "t2_5o9ebpsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about Athena, Trino and Iceberg", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144gd06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686247408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently searching an affordable alternative for an aging Hadoop cluster. We already have some stuff in Athena (parquet + S3 + Glue) but it is not very well structured.&lt;/p&gt;\n\n&lt;p&gt;Today I tested Trino on EMR vs Athena with and without tables stored in the Iceberg format.&lt;/p&gt;\n\n&lt;p&gt;The overall experience with Athena was much better. Queries where faster and they worked out of the box. Trino was much slower, scaling didn&amp;#39;t worked and sometimes it was not able to read the parquet files without specifying a proper error message.&lt;/p&gt;\n\n&lt;p&gt;I cannot wrap my head around why most people seem to use Trino over Athena? It seems to be much more expensive, slower and less robust. (Albeit, the last point might originate from my limited experience).&lt;/p&gt;\n\n&lt;p&gt;Can someone who is using both technologies give me a more refined report on the experience? We definitely do not want to use other SaaS technologies (we are solely on AWS, so GCP/BigQuery and Azure is not on the table anyways; Snowflake is so intransparent with their pricing; Databricks seems overkill)&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Matt&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144gd06", "is_robot_indexable": true, "report_reasons": null, "author": "mosquitsch", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144gd06/questions_about_athena_trino_and_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144gd06/questions_about_athena_trino_and_iceberg/", "subreddit_subscribers": 109798, "created_utc": 1686247408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Really amazing blog post by some of my colleagues/ClickHouse contributors.\n\nThe TL;DR is that with some of the recent changes in ClickHouse, the possibility of distributing super complex queries containing JOINs across many machines in a cluster using zero-copy replication is now possible. The implications are pretty massive if you're a ClickHouse user. ClickHouse is already really fast even on a single machine, but with these changes, you could feasibly run complex JOINing queries over **trillions** of rows in under a second, which is pretty mind-boggling. We're not there yet, but the work discussed in this blog post is a strong start.\n\nWould love to hear all your thoughts about it: [tbrd.co/joinsrd](https://tbrd.co/joinsrd) \n\nAnd in case it wasn't clear above, I do work for Tinybird, who published this post. But the content is more geared toward ClickHouse in general, so I hope this doesn't come off as a shill!", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding JOIN support to parallel replicas on ClickHouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144eg2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686243033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Really amazing blog post by some of my colleagues/ClickHouse contributors.&lt;/p&gt;\n\n&lt;p&gt;The TL;DR is that with some of the recent changes in ClickHouse, the possibility of distributing super complex queries containing JOINs across many machines in a cluster using zero-copy replication is now possible. The implications are pretty massive if you&amp;#39;re a ClickHouse user. ClickHouse is already really fast even on a single machine, but with these changes, you could feasibly run complex JOINing queries over &lt;strong&gt;trillions&lt;/strong&gt; of rows in under a second, which is pretty mind-boggling. We&amp;#39;re not there yet, but the work discussed in this blog post is a strong start.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear all your thoughts about it: &lt;a href=\"https://tbrd.co/joinsrd\"&gt;tbrd.co/joinsrd&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;And in case it wasn&amp;#39;t clear above, I do work for Tinybird, who published this post. But the content is more geared toward ClickHouse in general, so I hope this doesn&amp;#39;t come off as a shill!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?auto=webp&amp;v=enabled&amp;s=410402aa5be06f11fb58a09bf5ee8b22305ce198", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fa5a294307c9e936f6a098f750a3ac7e49eb986", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aced700afbfd5d047445e1a02f5e235ba4df6c1b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7d231b868191029dede5426a1678bcb375564ef", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b48d0391248d6c74e57633fa8f4337f9d343fc7f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=756443144012f3ca5d8e6e8c0da3285a43f8d2b3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/biDVvmIeRSy8w8mCNC8iwH3BOQLMopm3j7QbJBZlIt8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d33a5dfb603ecdb02117acf564f33d2ca23eb5e", "width": 1080, "height": 567}], "variants": {}, "id": "4mCXT7Z7iPa7UFM0muDZwUheEFVlOu7H1swJZD-iBMs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "144eg2h", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144eg2h/adding_join_support_to_parallel_replicas_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144eg2h/adding_join_support_to_parallel_replicas_on/", "subreddit_subscribers": 109798, "created_utc": 1686243033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently implementing a data mesh on an Azure eco system and a major part of our work is building data products for reusability between multiple use cases. My manager asked me to explore the opportunities of synthetic data generation to help with some of the data product implementation. Only a very high level direction is given. As per my understanding (and whatever further research I did ) , this is mostly used for Data Science space for Model training and similar stuff.\n\nAnyone here had a need to use synthetic data generation mechanism for their data platform? And how did it help you . Please suggest.", "author_fullname": "t2_qshu8mn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone worked on synthetic data generation for Data Product Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14558lq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686317169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently implementing a data mesh on an Azure eco system and a major part of our work is building data products for reusability between multiple use cases. My manager asked me to explore the opportunities of synthetic data generation to help with some of the data product implementation. Only a very high level direction is given. As per my understanding (and whatever further research I did ) , this is mostly used for Data Science space for Model training and similar stuff.&lt;/p&gt;\n\n&lt;p&gt;Anyone here had a need to use synthetic data generation mechanism for their data platform? And how did it help you . Please suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14558lq", "is_robot_indexable": true, "report_reasons": null, "author": "Global_Industry_6801", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14558lq/anyone_worked_on_synthetic_data_generation_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14558lq/anyone_worked_on_synthetic_data_generation_for/", "subreddit_subscribers": 109798, "created_utc": 1686317169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm trying to set up an ETL data pipeline from scratch, and need ideas/suggestions around the **E** &amp; **T** part of **ETL**. Let me explain it right from the data source to the final sink.\n\n# E\n\nAs the data source, what I have is a gRPC stream. I get data in protobuf encoded format from it. This is a fixed part in the overall system, there is no other way to extract the data. We plan to ingest this data in [delta lake](https://github.com/delta-io/delta), but before we do that there are a few problems.\n\nEach protobuf that I get from the stream, contains 4-5 different kinds of entities. Depending on the kind of entity, it should be inserted in the correct table in the delta lake. So, I need to first decode the protobuf and then figure out those entities and then accordingly write to delta. One particular constraint I have here is that a specific kind of entity comes encoded in such a format for which decoding is only possible in Golang, that encoding library doesn't exist in any other languages, and neither do we want to implement it for other langs. Also, to date, there is no good delta writer for Go. So, the way we were thinking to go about this problem was through two passthrough services.\n\n* gRPC Stream ---(protobuf)---&gt; Go decoder service ---(json)---&gt; Java/Scala Delta writer service ---(add entries to 4-5 different tables)---&gt; Delta lake\n* The Go decoder service would decode all the entities from within the protobuf, and put them all in a JSON object.\n* The Java/Scala delta writer service would use the JSON objects from the Go service, and write those entities to their respective tables in delta. If this service can write the entities in the order of the stream, that would be best, as then we won't have to run Z-ordering optimizations on all the different delta lake tables.\n* The problems we see with this are:\n   * The overhead of state management around consuming the gRPC Stream so that we can correctly resume the stream from the right place, if it or any of the downstream services broke in between. The gRPC stream does allow us to resume given we provide it a cursor, but we will have to manage that cursor on our end.\n   * Service management for the decoder and delta writer services. Ensuring that they never go down.\n   * There might be more decoder services in different languages in the future (depending on the libs that exist only in those languages), which will introduce more headache around service management.\n\n***Q1***: Does anyone have ideas or know an existing solution/workflow/framework that can help easily manage writing gRPC streams to delta with some (language agnostic) transformation involved in between? We just started looking into Apache Flink, but we don't know about its full capabilities yet!\n\n# T\n\nOnce we have the data in delta lake, there are two kinds of transformations that we foresee:\n\n1. Complex ones involving joins across tables. Expressing these transforms is feasible in SQL. We plan to use Trino for carrying out these transforms. Also, such transforms would be known in advance, so we plan to use Dagster+DBT for the orchestration part.\n2. Simple ones that map each row in one table to a new row in another table. Expressing these transforms is not feasible in SQL. There are some computations that we need to do on the row content that is easily expressible only in a programming language like Go/Python etc. This is sort of comparable to the decoder service we were previously talking about in the **E** (Extract) part. The only thing is that there would be a lot of such transforms, and we want to spend as less time on a single transform as possible. Any such transform can divide its work into 10 different chunks all of which can run in parallel as the transform of one row doesn't affect the transform of another row. Trino seems to support custom functions in Java, so even expressing these transforms via SQL to trino might be feasible. But, we aren't sure around the orchestration part. Such transforms won't be known upfront. Anyone can request for such transforms anytime. We know Databricks DLT with Python support can be helpful here, but we don't want to stick to any vendor-specific solutions. We want to build it out using open-source tools so that we should be able to manage it ourselves.\n\n**Q2**: Any advice/ideas around the transformation part? Specifically, the 2nd point. Would it be better to merge it within the decoder service in the Extraction phase? If so, any way to parallelize the compute involved in such a transform in an easy way?\n\n# L\n\nThis again is a fixed part of the system. Trino on top of Delta Lake is something that we are finalized with to serve the user queries.", "author_fullname": "t2_5wv31mqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas/Suggestions around setting up a data pipeline from scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14547rb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686314526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to set up an ETL data pipeline from scratch, and need ideas/suggestions around the &lt;strong&gt;E&lt;/strong&gt; &amp;amp; &lt;strong&gt;T&lt;/strong&gt; part of &lt;strong&gt;ETL&lt;/strong&gt;. Let me explain it right from the data source to the final sink.&lt;/p&gt;\n\n&lt;h1&gt;E&lt;/h1&gt;\n\n&lt;p&gt;As the data source, what I have is a gRPC stream. I get data in protobuf encoded format from it. This is a fixed part in the overall system, there is no other way to extract the data. We plan to ingest this data in &lt;a href=\"https://github.com/delta-io/delta\"&gt;delta lake&lt;/a&gt;, but before we do that there are a few problems.&lt;/p&gt;\n\n&lt;p&gt;Each protobuf that I get from the stream, contains 4-5 different kinds of entities. Depending on the kind of entity, it should be inserted in the correct table in the delta lake. So, I need to first decode the protobuf and then figure out those entities and then accordingly write to delta. One particular constraint I have here is that a specific kind of entity comes encoded in such a format for which decoding is only possible in Golang, that encoding library doesn&amp;#39;t exist in any other languages, and neither do we want to implement it for other langs. Also, to date, there is no good delta writer for Go. So, the way we were thinking to go about this problem was through two passthrough services.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;gRPC Stream ---(protobuf)---&amp;gt; Go decoder service ---(json)---&amp;gt; Java/Scala Delta writer service ---(add entries to 4-5 different tables)---&amp;gt; Delta lake&lt;/li&gt;\n&lt;li&gt;The Go decoder service would decode all the entities from within the protobuf, and put them all in a JSON object.&lt;/li&gt;\n&lt;li&gt;The Java/Scala delta writer service would use the JSON objects from the Go service, and write those entities to their respective tables in delta. If this service can write the entities in the order of the stream, that would be best, as then we won&amp;#39;t have to run Z-ordering optimizations on all the different delta lake tables.&lt;/li&gt;\n&lt;li&gt;The problems we see with this are:\n\n&lt;ul&gt;\n&lt;li&gt;The overhead of state management around consuming the gRPC Stream so that we can correctly resume the stream from the right place, if it or any of the downstream services broke in between. The gRPC stream does allow us to resume given we provide it a cursor, but we will have to manage that cursor on our end.&lt;/li&gt;\n&lt;li&gt;Service management for the decoder and delta writer services. Ensuring that they never go down.&lt;/li&gt;\n&lt;li&gt;There might be more decoder services in different languages in the future (depending on the libs that exist only in those languages), which will introduce more headache around service management.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Q1&lt;/em&gt;&lt;/strong&gt;: Does anyone have ideas or know an existing solution/workflow/framework that can help easily manage writing gRPC streams to delta with some (language agnostic) transformation involved in between? We just started looking into Apache Flink, but we don&amp;#39;t know about its full capabilities yet!&lt;/p&gt;\n\n&lt;h1&gt;T&lt;/h1&gt;\n\n&lt;p&gt;Once we have the data in delta lake, there are two kinds of transformations that we foresee:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Complex ones involving joins across tables. Expressing these transforms is feasible in SQL. We plan to use Trino for carrying out these transforms. Also, such transforms would be known in advance, so we plan to use Dagster+DBT for the orchestration part.&lt;/li&gt;\n&lt;li&gt;Simple ones that map each row in one table to a new row in another table. Expressing these transforms is not feasible in SQL. There are some computations that we need to do on the row content that is easily expressible only in a programming language like Go/Python etc. This is sort of comparable to the decoder service we were previously talking about in the &lt;strong&gt;E&lt;/strong&gt; (Extract) part. The only thing is that there would be a lot of such transforms, and we want to spend as less time on a single transform as possible. Any such transform can divide its work into 10 different chunks all of which can run in parallel as the transform of one row doesn&amp;#39;t affect the transform of another row. Trino seems to support custom functions in Java, so even expressing these transforms via SQL to trino might be feasible. But, we aren&amp;#39;t sure around the orchestration part. Such transforms won&amp;#39;t be known upfront. Anyone can request for such transforms anytime. We know Databricks DLT with Python support can be helpful here, but we don&amp;#39;t want to stick to any vendor-specific solutions. We want to build it out using open-source tools so that we should be able to manage it ourselves.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Q2&lt;/strong&gt;: Any advice/ideas around the transformation part? Specifically, the 2nd point. Would it be better to merge it within the decoder service in the Extraction phase? If so, any way to parallelize the compute involved in such a transform in an easy way?&lt;/p&gt;\n\n&lt;h1&gt;L&lt;/h1&gt;\n\n&lt;p&gt;This again is a fixed part of the system. Trino on top of Delta Lake is something that we are finalized with to serve the user queries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?auto=webp&amp;v=enabled&amp;s=fa58310d4ff1d938c13a4bd89e90c9bba275089e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee96d192170a46719444a777d2130ddd3490de45", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=426fe3a6df115d3aa40ffd5b9ecd43206b9840e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d76b33a58cdd34ff3992647ad7e46af5724915ea", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=832d3f585ad218ec3ea821c5bcbe9c79adb9a434", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a6f8c57c9ac45b52413b6a9cf36a40e200822e9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39244e3eb2a9d424a843d53b0e3eb52338b79581", "width": 1080, "height": 540}], "variants": {}, "id": "DVCDhaaj9zVfsdKJ_bz5ixrpUB7CFHkJma1GKr7iYdQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14547rb", "is_robot_indexable": true, "report_reasons": null, "author": "abhimanyusinghgaur", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14547rb/ideassuggestions_around_setting_up_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14547rb/ideassuggestions_around_setting_up_a_data/", "subreddit_subscribers": 109798, "created_utc": 1686314526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Usually in our projects requirements change continuously, even for the same feature. Do you prefer a particular tool for keeping track of this? Can you give me an example of how you would use it?", "author_fullname": "t2_e4qv08k8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which tool do you use to keep track of requirements?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144yp78", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686296633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Usually in our projects requirements change continuously, even for the same feature. Do you prefer a particular tool for keeping track of this? Can you give me an example of how you would use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "144yp78", "is_robot_indexable": true, "report_reasons": null, "author": "Busy_Elderberry8650", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144yp78/which_tool_do_you_use_to_keep_track_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144yp78/which_tool_do_you_use_to_keep_track_of/", "subreddit_subscribers": 109798, "created_utc": 1686296633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,I have some experience in python and c++ (systems engineering)but I want to break into data engineering,what are some of the DO's and DON'Ts i should do?\n\nWill my experience by relevant and helpful while applying?  \nAll suggestions are welcome,thanks in advance  \n", "author_fullname": "t2_akfpmfzn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software Engineering Intern,trying to break into DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144wkfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686289368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,I have some experience in python and c++ (systems engineering)but I want to break into data engineering,what are some of the DO&amp;#39;s and DON&amp;#39;Ts i should do?&lt;/p&gt;\n\n&lt;p&gt;Will my experience by relevant and helpful while applying?&lt;br/&gt;\nAll suggestions are welcome,thanks in advance  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "144wkfs", "is_robot_indexable": true, "report_reasons": null, "author": "homegrown_kb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144wkfs/software_engineering_interntrying_to_break_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144wkfs/software_engineering_interntrying_to_break_into_de/", "subreddit_subscribers": 109798, "created_utc": 1686289368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \n\nGuys, here where I\u2019m work, we\u2019ve a service Data Integration using Azure Function and Service Bus to send data to Dynamics CRM in a low send time (average 120 requests per minute). To explain more about, the company receives opportunities from one of their products and they want to have these opportunities in Dynamics CRM to do business, so the product sends these opportunities by calling our service.\n\n1\u00b0 Function HTTP \u2013 the client calls our service send a JSON data and I change name keys and type the data. I send data to queue in service bus (AVG - 2,53 second server response time)\n\n2\u00b0 Function Service Bus Trigger - A trigger who looks at the queue. When start it I transform the data the data according to what Dynamics must receive, so in this step I do some queries in Dynamics to build the fields. At the end I send to another queue in Service Bus (AVG \u2013 31,9 second server response time)\n\n3\u00b0 Function Service Bus Trigger \u2013 I just insert data in Dynamics with a rule, I check if the data (opportunity) exists in Dynamics using a service principal to insert. After I insert data in Azure SQL as a complement to the information already entered via the API in a Dynamics virtual entity. (AVG \u2013 15,0 second server response time)\n\nIn the process I get the data to save in Azure Data Lake because some areas have interest to analyze these opportunities. \n\nI\u2019ve a log if the opportunity was created or not and if the opportunity went through all the stages.\n\nSome problems: \n\n1 - Recently the consumer complained that he was entering a few opportunities in Dynamics, but I did a validation in the functions, in the service bus and I didn't find anything that justified this delay.\n\n2 \u2013 The consumer say to us that some opportunities hasn\u2019t insert in to Dynamics virtual entity (SQL), when I check in our log I\u2019ve a surprise, the opportunity had been created and I get the ID like evidence, but when the Dev Dynamics went to consult in SQL the opportunity was not found, I don\u2019t know what\u2019s happened because I started a transaction and i end it.\n\nPS: I create the opportunity and after a I consult the database to check if it was created in different transactions\n\n3 \u2013 I\u2019ll need to include update of these opportunities in this process, to not lose performance in one of the functions, I was thinking of creating another function to execute this. Would you do it this way?\n\n&amp;#x200B;\n\n Any suggestion to improve the process I appreciate, feel free to comment and give opinions \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_9dcp68l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Integration Azure Function - Performance, Deleted Data and Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144qftb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686271262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Guys, here where I\u2019m work, we\u2019ve a service Data Integration using Azure Function and Service Bus to send data to Dynamics CRM in a low send time (average 120 requests per minute). To explain more about, the company receives opportunities from one of their products and they want to have these opportunities in Dynamics CRM to do business, so the product sends these opportunities by calling our service.&lt;/p&gt;\n\n&lt;p&gt;1\u00b0 Function HTTP \u2013 the client calls our service send a JSON data and I change name keys and type the data. I send data to queue in service bus (AVG - 2,53 second server response time)&lt;/p&gt;\n\n&lt;p&gt;2\u00b0 Function Service Bus Trigger - A trigger who looks at the queue. When start it I transform the data the data according to what Dynamics must receive, so in this step I do some queries in Dynamics to build the fields. At the end I send to another queue in Service Bus (AVG \u2013 31,9 second server response time)&lt;/p&gt;\n\n&lt;p&gt;3\u00b0 Function Service Bus Trigger \u2013 I just insert data in Dynamics with a rule, I check if the data (opportunity) exists in Dynamics using a service principal to insert. After I insert data in Azure SQL as a complement to the information already entered via the API in a Dynamics virtual entity. (AVG \u2013 15,0 second server response time)&lt;/p&gt;\n\n&lt;p&gt;In the process I get the data to save in Azure Data Lake because some areas have interest to analyze these opportunities. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve a log if the opportunity was created or not and if the opportunity went through all the stages.&lt;/p&gt;\n\n&lt;p&gt;Some problems: &lt;/p&gt;\n\n&lt;p&gt;1 - Recently the consumer complained that he was entering a few opportunities in Dynamics, but I did a validation in the functions, in the service bus and I didn&amp;#39;t find anything that justified this delay.&lt;/p&gt;\n\n&lt;p&gt;2 \u2013 The consumer say to us that some opportunities hasn\u2019t insert in to Dynamics virtual entity (SQL), when I check in our log I\u2019ve a surprise, the opportunity had been created and I get the ID like evidence, but when the Dev Dynamics went to consult in SQL the opportunity was not found, I don\u2019t know what\u2019s happened because I started a transaction and i end it.&lt;/p&gt;\n\n&lt;p&gt;PS: I create the opportunity and after a I consult the database to check if it was created in different transactions&lt;/p&gt;\n\n&lt;p&gt;3 \u2013 I\u2019ll need to include update of these opportunities in this process, to not lose performance in one of the functions, I was thinking of creating another function to execute this. Would you do it this way?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestion to improve the process I appreciate, feel free to comment and give opinions &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Jr Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144qftb", "is_robot_indexable": true, "report_reasons": null, "author": "DPovoa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/144qftb/data_integration_azure_function_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144qftb/data_integration_azure_function_performance/", "subreddit_subscribers": 109798, "created_utc": 1686271262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI'm trying to build CI/CD pipeline in DevOps. I was able to do it for runbooks (PowerShell scripts) but I've got no idea how can I do it for Schedulers and Webhooks. Webhooks have an URI that is visible only one time during the creation of the Webhook.\nAnyone has an idea how can I do it? Thanks a lot in advance", "author_fullname": "t2_pp0zirow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD Pipeline for azure automation account", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144n175", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686262697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI&amp;#39;m trying to build CI/CD pipeline in DevOps. I was able to do it for runbooks (PowerShell scripts) but I&amp;#39;ve got no idea how can I do it for Schedulers and Webhooks. Webhooks have an URI that is visible only one time during the creation of the Webhook.\nAnyone has an idea how can I do it? Thanks a lot in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144n175", "is_robot_indexable": true, "report_reasons": null, "author": "These_Rip_9327", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144n175/cicd_pipeline_for_azure_automation_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144n175/cicd_pipeline_for_azure_automation_account/", "subreddit_subscribers": 109798, "created_utc": 1686262697.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}