{"kind": "Listing", "data": {"after": "t3_149sd0m", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a3kj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The \"Big Three's\" Data Storage Offerings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_149urpv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 183, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 183, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kTLt1vS-piYXFh1mKJXM_Op8tJCHs5sT7_hCkPofglI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686810263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ao5lvu6rk46b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ao5lvu6rk46b1.png?auto=webp&amp;v=enabled&amp;s=27c4f5642ec71a6917eb144a3b186bdb72f620e6", "width": 3000, "height": 4396}, "resolutions": [{"url": "https://preview.redd.it/ao5lvu6rk46b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=267d77192297b3165b5bccfb8e4b9e3c1abeb6a2", "width": 108, "height": 158}, {"url": "https://preview.redd.it/ao5lvu6rk46b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5178e30536621b9a6fe9b56b116d220e115d4f8", "width": 216, "height": 316}, {"url": "https://preview.redd.it/ao5lvu6rk46b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=402d13fab01a7840e1298aaacfcf5ff1b1f954a6", "width": 320, "height": 468}, {"url": "https://preview.redd.it/ao5lvu6rk46b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d34030a3563af43f0422d1a2b95220bd365a1b57", "width": 640, "height": 937}, {"url": "https://preview.redd.it/ao5lvu6rk46b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1eaa1596ba60797c50b966062f88b6681fa1e3c", "width": 960, "height": 1406}, {"url": "https://preview.redd.it/ao5lvu6rk46b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=606373e96a0a8eb4d4f075533460ddf1a84d797f", "width": 1080, "height": 1582}], "variants": {}, "id": "JVIhxd8crozI4pTCAovrd8ipptSOJkbKIIS5_iTA3H8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149urpv", "is_robot_indexable": true, "report_reasons": null, "author": "Kickass_Wizard", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149urpv/the_big_threes_data_storage_offerings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ao5lvu6rk46b1.png", "subreddit_subscribers": 110536, "created_utc": 1686810263.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Started a year ago with microsoft exams, started a minimum wage job doing DE and have been for 10 months and I've been offered an amazing job actually helping people and also exploring analytics/datascience and other stuff too. \n\nComplete DE freedom to engineer and explore and find ways to help people, bonus is its 1.5x my salary and offers senior level relatively quickly. \n\nI've always struggled and felt like an imposter but in such a short time I've come far and I can't wait to learn more. \n\nI suck at doing off job projects, I prefer having  data shoved in my face and told to fix or do something with it!\n\nHad a rough year but this has worked out amazingly and I'm thankful for everyone's support!\n\n(Sorry if it's slight brag)", "author_fullname": "t2_6o5h731v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1 year since I started data engineer and I found the job of my dreams while you guys were gone \ud83d\ude2d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149fhb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 86, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 86, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686767941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Started a year ago with microsoft exams, started a minimum wage job doing DE and have been for 10 months and I&amp;#39;ve been offered an amazing job actually helping people and also exploring analytics/datascience and other stuff too. &lt;/p&gt;\n\n&lt;p&gt;Complete DE freedom to engineer and explore and find ways to help people, bonus is its 1.5x my salary and offers senior level relatively quickly. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always struggled and felt like an imposter but in such a short time I&amp;#39;ve come far and I can&amp;#39;t wait to learn more. &lt;/p&gt;\n\n&lt;p&gt;I suck at doing off job projects, I prefer having  data shoved in my face and told to fix or do something with it!&lt;/p&gt;\n\n&lt;p&gt;Had a rough year but this has worked out amazingly and I&amp;#39;m thankful for everyone&amp;#39;s support!&lt;/p&gt;\n\n&lt;p&gt;(Sorry if it&amp;#39;s slight brag)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "149fhb7", "is_robot_indexable": true, "report_reasons": null, "author": "anonymous6156", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149fhb7/1_year_since_i_started_data_engineer_and_i_found/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149fhb7/1_year_since_i_started_data_engineer_and_i_found/", "subreddit_subscribers": 110536, "created_utc": 1686767941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious, whenever I look for examples and syntax, Apache has these one-liners like \"this is what it is and don't ask anymore questions\" lol.\n\n[https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row\\_number.html?highlight=row\\_number#pyspark.sql.functions.row\\_number](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row_number.html?highlight=row_number#pyspark.sql.functions.row_number)\n\nCompared to Pandas docs, for example, which are more descriptive and useful. Thoughts?", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "why is Apache Pyspark documentation so...sparse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149g5zk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 83, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 83, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686769576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious, whenever I look for examples and syntax, Apache has these one-liners like &amp;quot;this is what it is and don&amp;#39;t ask anymore questions&amp;quot; lol.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row_number.html?highlight=row_number#pyspark.sql.functions.row_number\"&gt;https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row_number.html?highlight=row_number#pyspark.sql.functions.row_number&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Compared to Pandas docs, for example, which are more descriptive and useful. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149g5zk", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149g5zk/why_is_apache_pyspark_documentation_sosparse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149g5zk/why_is_apache_pyspark_documentation_sosparse/", "subreddit_subscribers": 110536, "created_utc": 1686769576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. Just wanted to let you know that there\u2019s a 5 book bundle @ [Humble Bundle](https://www.humblebundle.com/books/popular-programming-languages-2023-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_2_c_popularprogramminglanguages2023oreilly_bookbundle) going for 1\u20ac (or more if you want to donate to Code for America) that includes Scala Cookbook, Robust Python and more. There\u2019s also a 10 and 15 book bundle if you\u2019re interested in those books!\n\nOffer ends in 12 days, hope this helped you!", "author_fullname": "t2_9d8b8hr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Scala Cookbook (+ Robust Python &amp; more books) for 1\u20ac at Humble Bundle", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149gppg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686770873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. Just wanted to let you know that there\u2019s a 5 book bundle @ &lt;a href=\"https://www.humblebundle.com/books/popular-programming-languages-2023-oreilly-books?hmb_source=&amp;amp;hmb_medium=product_tile&amp;amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_2_c_popularprogramminglanguages2023oreilly_bookbundle\"&gt;Humble Bundle&lt;/a&gt; going for 1\u20ac (or more if you want to donate to Code for America) that includes Scala Cookbook, Robust Python and more. There\u2019s also a 10 and 15 book bundle if you\u2019re interested in those books!&lt;/p&gt;\n\n&lt;p&gt;Offer ends in 12 days, hope this helped you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?auto=webp&amp;v=enabled&amp;s=145c5031dd9a76b3a193c59bb037b7f969636343", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff318a3e2d3d005fb2b97fba6815f8e46f0df195", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e533d028be1491f12d960c5d200ac7ab94084045", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdeb2123bb81cdeb7d8445bab1776ec7145007df", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e1fa92d72d541f6ea0ef6099d91cfc9c6cb4509", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42abec49d1559a82ba3501a0721124d8bac2044c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/1_PX3qdTAodSNVui3XcRTHc4dIHtwUTeCGa5IKhL0lM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b179f527a048da516c574ee3436a15d9754f85b", "width": 1080, "height": 607}], "variants": {}, "id": "RyTlK6qn5A-hML7nwk6v7Ctu-pDz9VPLPbeDYzhPn4I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149gppg", "is_robot_indexable": true, "report_reasons": null, "author": "TauIsRC", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149gppg/psa_scala_cookbook_robust_python_more_books_for_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149gppg/psa_scala_cookbook_robust_python_more_books_for_1/", "subreddit_subscribers": 110536, "created_utc": 1686770873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 5 years of experience, currently working at a well known tech company that is not a FAANG for over 2 years.  \n\n\n\n\n\nI interviewed for a more senior engineering position at a small startup recently, and got a rejection mid way through the process without even receiving an invite for the final round.  On paper, I did everything right: did great on the HackerRank and hiring manager interview, my company and this company used the same cloud provider, and I could go on talking about challenging and cool projects that I've worked on at my current and past companies.\n\n\n\n\n\nAre engineering positions that were once less competitive only hiring top tier engineers now?  I'm talking about engineers at top tech companies with 10+ years of experience, maybe with an MSCS too.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any other experienced engineers getting unexplained rejections when interviewing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14a0wto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686831395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 5 years of experience, currently working at a well known tech company that is not a FAANG for over 2 years.  &lt;/p&gt;\n\n&lt;p&gt;I interviewed for a more senior engineering position at a small startup recently, and got a rejection mid way through the process without even receiving an invite for the final round.  On paper, I did everything right: did great on the HackerRank and hiring manager interview, my company and this company used the same cloud provider, and I could go on talking about challenging and cool projects that I&amp;#39;ve worked on at my current and past companies.&lt;/p&gt;\n\n&lt;p&gt;Are engineering positions that were once less competitive only hiring top tier engineers now?  I&amp;#39;m talking about engineers at top tech companies with 10+ years of experience, maybe with an MSCS too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14a0wto", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14a0wto/any_other_experienced_engineers_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a0wto/any_other_experienced_engineers_getting/", "subreddit_subscribers": 110536, "created_utc": 1686831395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing work with a client who sends daily reports of order data to an email data connector on Fivetran. The order records are only 1,000 - 2,000 rows daily, but the issue is that the file also contains \\~100,000 empty rows. I was doing some MAR usage tracking this week and realized that **Fivetran is charging us for ingesting the empty rows**.\n\nDoes anyone know a way around this - to avoid FT from charging for the empty rows / considering those valid records?  I'm trying to solve this on the FT side, and not having to goto the client and ask them to alter the way they're currently sending their files.\n\nThanks in advance!", "author_fullname": "t2_6fg82tze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran Charging for Empty Rows in CSV's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149raph", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686799060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing work with a client who sends daily reports of order data to an email data connector on Fivetran. The order records are only 1,000 - 2,000 rows daily, but the issue is that the file also contains ~100,000 empty rows. I was doing some MAR usage tracking this week and realized that &lt;strong&gt;Fivetran is charging us for ingesting the empty rows&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know a way around this - to avoid FT from charging for the empty rows / considering those valid records?  I&amp;#39;m trying to solve this on the FT side, and not having to goto the client and ask them to alter the way they&amp;#39;re currently sending their files.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "149raph", "is_robot_indexable": true, "report_reasons": null, "author": "plutodoesnotexist", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149raph/fivetran_charging_for_empty_rows_in_csvs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149raph/fivetran_charging_for_empty_rows_in_csvs/", "subreddit_subscribers": 110536, "created_utc": 1686799060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently reading about the Kimball method in The Data Warehousing Toolkit, 3rd edition. I\u2019m familiar with dimensional modeling in data warehouses, but I\u2019m struggling to understand the difference between 3NF and Dimensional models. It says the key is the degree of normalization\u2026 does this mean still PKs and FKs function the same way as they do in star schemas? is the difference essentially context (dimensional models being source system agnostic)? I wanna make sure I understand this before I move further in the book.", "author_fullname": "t2_tm7d06j2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between 3NF and Dimensional models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_149nipn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cB2pCD0m80y1sH-QBsW1iLtQgYZw8h5cf4A8QxfXtug.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686787957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently reading about the Kimball method in The Data Warehousing Toolkit, 3rd edition. I\u2019m familiar with dimensional modeling in data warehouses, but I\u2019m struggling to understand the difference between 3NF and Dimensional models. It says the key is the degree of normalization\u2026 does this mean still PKs and FKs function the same way as they do in star schemas? is the difference essentially context (dimensional models being source system agnostic)? I wanna make sure I understand this before I move further in the book.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pjz6do1mq26b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?auto=webp&amp;v=enabled&amp;s=49e372bda4891f9c71bb69d86c5a2562862a0e37", "width": 1125, "height": 2436}, "resolutions": [{"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=039f931dd2c2ecc420bd43b489ff9683610f1da6", "width": 108, "height": 216}, {"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71d1ea217534f39184ba9088c91353c0f1aac0d8", "width": 216, "height": 432}, {"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e2fd6ddc22ef7a71b52c72c679303a399788fe6", "width": 320, "height": 640}, {"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5866e50e3086d9f9bae458221136d695756d6e0b", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d8f9a3e1cb986d24d83406d72204f3b94179d78", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/pjz6do1mq26b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a157058202ed7f928d7fca9d080b2cbc02b941b6", "width": 1080, "height": 2160}], "variants": {}, "id": "X0Tc-1PlRGcGa4v3Bb4WfjL0W7n4j9rbvnB82ZGwNNc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "149nipn", "is_robot_indexable": true, "report_reasons": null, "author": "oscarwranglin", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149nipn/difference_between_3nf_and_dimensional_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pjz6do1mq26b1.jpg", "subreddit_subscribers": 110536, "created_utc": 1686787957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d8afn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Continuous Slack=&gt;ChatGPT=&gt;Google Sheets Pipeline using Estuary Flow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149h80m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1686772120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "estuary.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://estuary.dev/gpt-real-time-pipeline/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "149h80m", "is_robot_indexable": true, "report_reasons": null, "author": "johnnygraettinger", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149h80m/a_continuous_slackchatgptgoogle_sheets_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://estuary.dev/gpt-real-time-pipeline/", "subreddit_subscribers": 110536, "created_utc": 1686772120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some issues that data teams face in comparison to SWE (typical web/mobile) teams?\n\nOne thing I\u2019ve noticed is that my peers on the SWE side have more dedicated and specialized roles:  project/product managers, architect, principal or staff positions, UI/UX, note-takers, among others.\n\nWhereas every company I\u2019ve worked for has expected data team members to wear all of the hats (design, architecture, implementation, communication, and delivery/analysis).\n\nLike I\u2019m put in situations where I\u2019m given a short timeline, and I\u2019m expected to flesh out all of the Jira tickets, communicate it out to stakeholders, gather all the requirements, think up the design for this new problem, sell the architecture to stakeholders, implement it (including creating tests and deployment infrastructure), configure monitoring, and then follow-up with thorough communications and updates throughout this whole process.\n\nWhereas my SWE counterparts have their tickets handed to them, implement the feature, and merge that jawn to main when it\u2019s ready while providing intermittent updates to the PM.\n\nMy experience seems pretty normal for DE, but is my understanding of SWE correct (at a well-run shop), and should this experience be normal for DE?", "author_fullname": "t2_f16v22yv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problems more unique to DE than SWE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149l0up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686781350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some issues that data teams face in comparison to SWE (typical web/mobile) teams?&lt;/p&gt;\n\n&lt;p&gt;One thing I\u2019ve noticed is that my peers on the SWE side have more dedicated and specialized roles:  project/product managers, architect, principal or staff positions, UI/UX, note-takers, among others.&lt;/p&gt;\n\n&lt;p&gt;Whereas every company I\u2019ve worked for has expected data team members to wear all of the hats (design, architecture, implementation, communication, and delivery/analysis).&lt;/p&gt;\n\n&lt;p&gt;Like I\u2019m put in situations where I\u2019m given a short timeline, and I\u2019m expected to flesh out all of the Jira tickets, communicate it out to stakeholders, gather all the requirements, think up the design for this new problem, sell the architecture to stakeholders, implement it (including creating tests and deployment infrastructure), configure monitoring, and then follow-up with thorough communications and updates throughout this whole process.&lt;/p&gt;\n\n&lt;p&gt;Whereas my SWE counterparts have their tickets handed to them, implement the feature, and merge that jawn to main when it\u2019s ready while providing intermittent updates to the PM.&lt;/p&gt;\n\n&lt;p&gt;My experience seems pretty normal for DE, but is my understanding of SWE correct (at a well-run shop), and should this experience be normal for DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149l0up", "is_robot_indexable": true, "report_reasons": null, "author": "etl_boi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149l0up/problems_more_unique_to_de_than_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149l0up/problems_more_unique_to_de_than_swe/", "subreddit_subscribers": 110536, "created_utc": 1686781350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some ideas on how to proceed with a situation I\u2019ve found myself in. Just started a new job with a company that\u2019s making its first foray into BI and analytics. They\u2019ve got some very basic DB views reporting to an internal web page but that\u2019s about it. They want to move to embrace modern data models and Power BI.\n\nProblem is the data environment is a hot mess and they don\u2019t know enough to know it\u2019s bad. Database has been build in house by self taught admin, reacting to requests from Exec, sales etc. They\u2019ve survived by patching and creating thousands of views with 101 where clauses to try and sort the data. Currently everyone thinks it\u2019s pretty robust and gives good figures. But I\u2019ve been in two weeks and I\u2019m fighting a rising panic that it\u2019s not worth anything. Primary Key seems to be totally uncontrolled, \u201c1,2,3 cheddar,4\u201d type thing. Most of the data is being entered as free text and there is not a single bit of normalisation so it\u2019s just lots of megatables and views. There\u2019s no dev or test, changes are just made direct to prod out of hours.\n\nHad a sit down with my boss (finance) and tried to explain the issues. He\u2019s confused why we don\u2019t have a lovely real time dashboard already and I\u2019m trying to explain that we\u2019ve got 46 versions of \u201cSt Albans\u201d in the db. \n\nI\u2019m trying to untangle all this and figure out exactly what\u2019s gone on but it\u2019s seeming like an insurmountable task. \n\nHas anyone ever come across a situation like this and if so how did you manage to get it under control. I know it\u2019s a long shot but if anyone\u2019s got a \u201cUnfuck a data stack for dummies\u201d article please hit me up with a link!", "author_fullname": "t2_scnmi5ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So you\u2019ve started a new job and the data estate is a total binfire", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14a6c40", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686845318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some ideas on how to proceed with a situation I\u2019ve found myself in. Just started a new job with a company that\u2019s making its first foray into BI and analytics. They\u2019ve got some very basic DB views reporting to an internal web page but that\u2019s about it. They want to move to embrace modern data models and Power BI.&lt;/p&gt;\n\n&lt;p&gt;Problem is the data environment is a hot mess and they don\u2019t know enough to know it\u2019s bad. Database has been build in house by self taught admin, reacting to requests from Exec, sales etc. They\u2019ve survived by patching and creating thousands of views with 101 where clauses to try and sort the data. Currently everyone thinks it\u2019s pretty robust and gives good figures. But I\u2019ve been in two weeks and I\u2019m fighting a rising panic that it\u2019s not worth anything. Primary Key seems to be totally uncontrolled, \u201c1,2,3 cheddar,4\u201d type thing. Most of the data is being entered as free text and there is not a single bit of normalisation so it\u2019s just lots of megatables and views. There\u2019s no dev or test, changes are just made direct to prod out of hours.&lt;/p&gt;\n\n&lt;p&gt;Had a sit down with my boss (finance) and tried to explain the issues. He\u2019s confused why we don\u2019t have a lovely real time dashboard already and I\u2019m trying to explain that we\u2019ve got 46 versions of \u201cSt Albans\u201d in the db. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to untangle all this and figure out exactly what\u2019s gone on but it\u2019s seeming like an insurmountable task. &lt;/p&gt;\n\n&lt;p&gt;Has anyone ever come across a situation like this and if so how did you manage to get it under control. I know it\u2019s a long shot but if anyone\u2019s got a \u201cUnfuck a data stack for dummies\u201d article please hit me up with a link!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14a6c40", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Aardvark258", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a6c40/so_youve_started_a_new_job_and_the_data_estate_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a6c40/so_youve_started_a_new_job_and_the_data_estate_is/", "subreddit_subscribers": 110536, "created_utc": 1686845318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been brought in at this company to help them with their reporting. In 1.5 years we are on the 4th iteration due to lack of data engineering capacity and now the holy grail that is ringing around the water cooler is a new data model, based on untrustworthy sources will solve all reporting issues.\n\nI am pretty fed up but do not have enough DE expertise to convince the manager that we are taking a wrong turn here and that moving to a new data model will actually make the reporting quality worse. Any suggestions what to do?\n\n1st iteration: moved transformations from visualization tool (Power BI) to reporting schema in views in Serverless SQL - quick win\n\n2nd iteration: rewrote SQL to be more optimized due to data volume being too large (which sounds weird since we are talking about 5/6 sources with # rows varying between 500k - 9m)\n\n3rd iteration (current): had a freelancer create a pipeline to write SQL views to external tables using parquet\n\n4th iteration (to be): new Serverless SQL environment where I am tasked with creating a new data model in views that will be written to external tables overnight\n\nThe base of these are csv files that are processed overnight using ADF and these sources are all historized in parquet files using an SCD2 type mechanism: they are given a startdate, but enddate is solved in a view.\n\nAll columns are processed as maxed varchars, there are no data quality checks, source\\_a that is fed into the production environment of source\\_b does not include the unique id from source\\_a so matching these things is arbitrary on similar characteristics. The DE consultants are pushing different environments for development, testing and production while our analysts are drowning in reporting work and are not allowed to create their own tables and/or files.", "author_fullname": "t2_2sss2inq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I wrong here? Tasked with data modeling using only views, 4th iteration in 1.5 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149xd8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686819604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been brought in at this company to help them with their reporting. In 1.5 years we are on the 4th iteration due to lack of data engineering capacity and now the holy grail that is ringing around the water cooler is a new data model, based on untrustworthy sources will solve all reporting issues.&lt;/p&gt;\n\n&lt;p&gt;I am pretty fed up but do not have enough DE expertise to convince the manager that we are taking a wrong turn here and that moving to a new data model will actually make the reporting quality worse. Any suggestions what to do?&lt;/p&gt;\n\n&lt;p&gt;1st iteration: moved transformations from visualization tool (Power BI) to reporting schema in views in Serverless SQL - quick win&lt;/p&gt;\n\n&lt;p&gt;2nd iteration: rewrote SQL to be more optimized due to data volume being too large (which sounds weird since we are talking about 5/6 sources with # rows varying between 500k - 9m)&lt;/p&gt;\n\n&lt;p&gt;3rd iteration (current): had a freelancer create a pipeline to write SQL views to external tables using parquet&lt;/p&gt;\n\n&lt;p&gt;4th iteration (to be): new Serverless SQL environment where I am tasked with creating a new data model in views that will be written to external tables overnight&lt;/p&gt;\n\n&lt;p&gt;The base of these are csv files that are processed overnight using ADF and these sources are all historized in parquet files using an SCD2 type mechanism: they are given a startdate, but enddate is solved in a view.&lt;/p&gt;\n\n&lt;p&gt;All columns are processed as maxed varchars, there are no data quality checks, source_a that is fed into the production environment of source_b does not include the unique id from source_a so matching these things is arbitrary on similar characteristics. The DE consultants are pushing different environments for development, testing and production while our analysts are drowning in reporting work and are not allowed to create their own tables and/or files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "149xd8e", "is_robot_indexable": true, "report_reasons": null, "author": "foldingtoiletpaper", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149xd8e/am_i_wrong_here_tasked_with_data_modeling_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149xd8e/am_i_wrong_here_tasked_with_data_modeling_using/", "subreddit_subscribers": 110536, "created_utc": 1686819604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello beautiful people,\n\nMy organization is considering to migrate from dbt to dataform. We are using GBQ as the data warehouse. Is it a good idea to move from dbt to dataform? What are the pros and cons? \n\nI searched this forum for the same questions. The answers were pretty old. I was thinking if the attitude towards dataform could have changed in the couple of months. Maybe.\n\nCan you give you ideas, please?\n\nThanks", "author_fullname": "t2_auriunhuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt vs Dataform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149rv4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686800826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello beautiful people,&lt;/p&gt;\n\n&lt;p&gt;My organization is considering to migrate from dbt to dataform. We are using GBQ as the data warehouse. Is it a good idea to move from dbt to dataform? What are the pros and cons? &lt;/p&gt;\n\n&lt;p&gt;I searched this forum for the same questions. The answers were pretty old. I was thinking if the attitude towards dataform could have changed in the couple of months. Maybe.&lt;/p&gt;\n\n&lt;p&gt;Can you give you ideas, please?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149rv4a", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Rub-3984", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/149rv4a/dbt_vs_dataform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149rv4a/dbt_vs_dataform/", "subreddit_subscribers": 110536, "created_utc": 1686800826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As of now our team is pretty old-school and everyone creates their own local dev environment using a `requirements.txt` file and a bootstrapping doc that outlines any other steps necessary to recreate prod.  We have images for our Airflow servers for easy redeployment, and we're considering containerizing these to use for local dev work.  This seems like a natural progression, and will reduce any room for issues or inconsistencies in setting up a dev environment.  For anyone who's done or considered this, what are your thoughts? And what best practices should we be following?", "author_fullname": "t2_22ksp1z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone here use a container (Docker or otherwise) as their development environment? What are the best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149igv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686775839.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686775092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As of now our team is pretty old-school and everyone creates their own local dev environment using a &lt;code&gt;requirements.txt&lt;/code&gt; file and a bootstrapping doc that outlines any other steps necessary to recreate prod.  We have images for our Airflow servers for easy redeployment, and we&amp;#39;re considering containerizing these to use for local dev work.  This seems like a natural progression, and will reduce any room for issues or inconsistencies in setting up a dev environment.  For anyone who&amp;#39;s done or considered this, what are your thoughts? And what best practices should we be following?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149igv5", "is_robot_indexable": true, "report_reasons": null, "author": "x1084", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149igv5/does_anyone_here_use_a_container_docker_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149igv5/does_anyone_here_use_a_container_docker_or/", "subreddit_subscribers": 110536, "created_utc": 1686775092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, maybe a bit of a naive question. Our company traditionally uses Informatica as our ETL for a long time. \n\nOur team is looking for something a bit more UI friendly to do ETL or ELT. My question is can we use Fivetran to ingest from various sources (I know fivetran offers over 200+). Then use Fivetran to parse xml, json, etc into .csv for analytics ready? Or do we need a different tool? \n\nHow much volume fivetran can handle? What are the pros and cons?", "author_fullname": "t2_vlbvypwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Fivetran for ingestion and transforming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149ibrz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686774743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, maybe a bit of a naive question. Our company traditionally uses Informatica as our ETL for a long time. &lt;/p&gt;\n\n&lt;p&gt;Our team is looking for something a bit more UI friendly to do ETL or ELT. My question is can we use Fivetran to ingest from various sources (I know fivetran offers over 200+). Then use Fivetran to parse xml, json, etc into .csv for analytics ready? Or do we need a different tool? &lt;/p&gt;\n\n&lt;p&gt;How much volume fivetran can handle? What are the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "149ibrz", "is_robot_indexable": true, "report_reasons": null, "author": "dataeng_328", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149ibrz/using_fivetran_for_ingestion_and_transforming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149ibrz/using_fivetran_for_ingestion_and_transforming/", "subreddit_subscribers": 110536, "created_utc": 1686774743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Folks, I recently joined in a senior leadership position in an organization. While I am yet to hire next line of team mananagers, meanwhile I am navigating the teams through the day to day challenges and issues.\n\nWe moved some MySQL tables to Databricks Delta Lake(few months before I joined). Currently the data is flowing to both sides but recently one report that was moved to read the data from Delta tables was painfully slow. Little bit of investigation showed that the databricks cluster took good time to be functional(shuts down after 45 mins of inactivity as per config). First run of report is too slow- like 2 mins as compared to 2 seconds from MySQL.\n\nPlease suggest what questions  can be asked in open discussion that could lead to a resolution.", "author_fullname": "t2_9yg41d75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake Table Querying in AWS Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14a60fx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686844571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Folks, I recently joined in a senior leadership position in an organization. While I am yet to hire next line of team mananagers, meanwhile I am navigating the teams through the day to day challenges and issues.&lt;/p&gt;\n\n&lt;p&gt;We moved some MySQL tables to Databricks Delta Lake(few months before I joined). Currently the data is flowing to both sides but recently one report that was moved to read the data from Delta tables was painfully slow. Little bit of investigation showed that the databricks cluster took good time to be functional(shuts down after 45 mins of inactivity as per config). First run of report is too slow- like 2 mins as compared to 2 seconds from MySQL.&lt;/p&gt;\n\n&lt;p&gt;Please suggest what questions  can be asked in open discussion that could lead to a resolution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14a60fx", "is_robot_indexable": true, "report_reasons": null, "author": "Mental-Matter-4370", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a60fx/delta_lake_table_querying_in_aws_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a60fx/delta_lake_table_querying_in_aws_databricks/", "subreddit_subscribers": 110536, "created_utc": 1686844571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Due to all kinds of reasons like failure of collection sensors, communication error, and unexpected malfunction, missing values are common to see in time series from the real-world environment. This makes partially-observed time series (POTS) a pervasive problem in open-world modeling and prevents advanced data analysis. Although this problem is important, the area of data mining on POTS still lacks a dedicated toolkit. PyPOTS is created to fill in this gap.\n\nPyPOTS (pronounced \"Pie Pots\") is the first (and so far the only) Python toolbox/library specifically designed for data mining and machine learning on partially-observed time series (POTS), namely, incomplete time series with missing values, A.K.A. irregularly-sampled time series, supporting tasks of imputation, classification, clustering, and forecasting on POTS datasets. It is born to become a handy toolbox that is going to make data mining on POTS easy rather than tedious, to help engineers and researchers focus more on the core problems in their hands rather than on how to deal with the missing parts in their data. PyPOTS will keep integrating classical and the latest state-of-the-art data mining algorithms for partially-observed multivariate time series. For sure, besides various algorithms, PyPOTS has unified APIs together with detailed documentation and interactive examples across algorithms as tutorials.\n\nFeedback and contributions are very welcome!\n\n&amp;#x200B;\n\nWebsite: [https://pypots.com](https://pypots.com)\n\nPaper link: [https://arxiv.org/abs/2305.18811](https://arxiv.org/abs/2305.18811)\n\nGitHub repo: [https://github.com/WenjieDu/PyPOTS](https://github.com/WenjieDu/PyPOTS)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/142p76p5y56b1.jpg?width=1801&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=82af5ad3fad54a5b648358c53595c78b7c024cb9", "author_fullname": "t2_4iz6qtg8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We built PyPOTS: an open-source toolbox for data mining on partially-observed time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 38, "top_awarded_type": null, "hide_score": false, "media_metadata": {"142p76p5y56b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 29, "x": 108, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f32d25d84c1d1fd9ac4c5b3e4b294cfdd2a55d7c"}, {"y": 59, "x": 216, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74de307650d14626df7cb99bab68203b458c5727"}, {"y": 88, "x": 320, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=590e02fd8ad613920722ec25e16853abe54535a7"}, {"y": 177, "x": 640, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4bbf4566f5e2bf250d869d187898cea4caea339e"}, {"y": 265, "x": 960, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d94b0827444cd76ee38d72a3f0f3d7d8d15146c9"}, {"y": 299, "x": 1080, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a63c8f9a1a71ad90b78c24e16b92ae3a161497fd"}], "s": {"y": 499, "x": 1801, "u": "https://preview.redd.it/142p76p5y56b1.jpg?width=1801&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=82af5ad3fad54a5b648358c53595c78b7c024cb9"}, "id": "142p76p5y56b1"}}, "name": "t3_149zoc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HO6S3WZBe1Tnl1fmVExA3ZtNe-tb7Mvoh6JeMkZViaE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686827549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to all kinds of reasons like failure of collection sensors, communication error, and unexpected malfunction, missing values are common to see in time series from the real-world environment. This makes partially-observed time series (POTS) a pervasive problem in open-world modeling and prevents advanced data analysis. Although this problem is important, the area of data mining on POTS still lacks a dedicated toolkit. PyPOTS is created to fill in this gap.&lt;/p&gt;\n\n&lt;p&gt;PyPOTS (pronounced &amp;quot;Pie Pots&amp;quot;) is the first (and so far the only) Python toolbox/library specifically designed for data mining and machine learning on partially-observed time series (POTS), namely, incomplete time series with missing values, A.K.A. irregularly-sampled time series, supporting tasks of imputation, classification, clustering, and forecasting on POTS datasets. It is born to become a handy toolbox that is going to make data mining on POTS easy rather than tedious, to help engineers and researchers focus more on the core problems in their hands rather than on how to deal with the missing parts in their data. PyPOTS will keep integrating classical and the latest state-of-the-art data mining algorithms for partially-observed multivariate time series. For sure, besides various algorithms, PyPOTS has unified APIs together with detailed documentation and interactive examples across algorithms as tutorials.&lt;/p&gt;\n\n&lt;p&gt;Feedback and contributions are very welcome!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Website: &lt;a href=\"https://pypots.com\"&gt;https://pypots.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper link: &lt;a href=\"https://arxiv.org/abs/2305.18811\"&gt;https://arxiv.org/abs/2305.18811&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub repo: &lt;a href=\"https://github.com/WenjieDu/PyPOTS\"&gt;https://github.com/WenjieDu/PyPOTS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/142p76p5y56b1.jpg?width=1801&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=82af5ad3fad54a5b648358c53595c78b7c024cb9\"&gt;https://preview.redd.it/142p76p5y56b1.jpg?width=1801&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=82af5ad3fad54a5b648358c53595c78b7c024cb9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "149zoc5", "is_robot_indexable": true, "report_reasons": null, "author": "WenjayDu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149zoc5/we_built_pypots_an_opensource_toolbox_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149zoc5/we_built_pypots_an_opensource_toolbox_for_data/", "subreddit_subscribers": 110536, "created_utc": 1686827549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.\n\n  \nMergers and acquisitions (M&amp;A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.\n\n  \nMergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&amp;A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&amp;A, and provide tools for addressing them effectively.\n\n  \nDuring and after M&amp;A, two companies\u2019 tech stacks essentially collide, and IT management has to deal with a number of challenges:\n\n# 1. Integrating disparate systems and platforms\n\nIntegrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.  \nHere are some of the key issues that arise during this integration process:\n\n* Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;\n* Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;\n* Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;\n* Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;\n* User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;\n* System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.\n\n2. Reconciling different data formats and standards\n\nWhen two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.\n\n3. Significant differences in the technological maturity between the two companies\n\nIf one of the companies has a significant technological advantage over the other, several issues may arise:\n\n* Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;\n* Operational Disruptions: if one company\u2019s systems are automated and the other\u2019s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;\n* Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It\u2019s crucial to assess and address these risks as part of the integration process.\n\n4. Considering the human element\n\nThis is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&amp;A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&amp;A.\n\n  \nThe problem is, employees are conservative and don\u2019t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.  \nAnd it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things \u2013 they need to start profiting from the merger or acquisition as soon as possible.\n\n  \nMerging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal\u2019s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.\n\n  \nIn fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&amp;As), as they help increase acceptance among employees and minimize potential disruption. Here\u2019s how these strategies can be applied:  \n\n\n* Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company\u2019s situation;\n* Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;\n* Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;\n* Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive \u2018us versus them\u2019 mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.  \nOther issues can be solved via the right tool that addresses every issue mentioned.\n\n# How to effectively address data-related M&amp;A challenges?\n\nTo effectively address the challenges posed by M&amp;A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.\n\n  \nA central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&amp;A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.\n\n  \nBy transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.\n\n  \nMoreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&amp;A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.\n\n# No-Code Data Solutions\n\nNo-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&amp;A right away.  \nFurthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&amp;A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.\n\nIn addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&amp;A process.  \nThe rest of research can be found here:  https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm\\_source=linkedin&amp;utm\\_medium=social&amp;utm\\_campaign=data\\_engineering&amp;utm\\_content=migrations\\_acquistions&amp;utm\\_term=ITarchitecture", "author_fullname": "t2_ct09rz3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data management challenges in M&amp;A", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149yxju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686825092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.&lt;/p&gt;\n\n&lt;p&gt;Mergers and acquisitions (M&amp;amp;A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.&lt;/p&gt;\n\n&lt;p&gt;Mergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&amp;amp;A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&amp;amp;A, and provide tools for addressing them effectively.&lt;/p&gt;\n\n&lt;p&gt;During and after M&amp;amp;A, two companies\u2019 tech stacks essentially collide, and IT management has to deal with a number of challenges:&lt;/p&gt;\n\n&lt;h1&gt;1. Integrating disparate systems and platforms&lt;/h1&gt;\n\n&lt;p&gt;Integrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.&lt;br/&gt;\nHere are some of the key issues that arise during this integration process:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;&lt;/li&gt;\n&lt;li&gt;Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;&lt;/li&gt;\n&lt;li&gt;Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;&lt;/li&gt;\n&lt;li&gt;Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;&lt;/li&gt;\n&lt;li&gt;User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;&lt;/li&gt;\n&lt;li&gt;System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Reconciling different data formats and standards&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;When two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Significant differences in the technological maturity between the two companies&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If one of the companies has a significant technological advantage over the other, several issues may arise:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;&lt;/li&gt;\n&lt;li&gt;Operational Disruptions: if one company\u2019s systems are automated and the other\u2019s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;&lt;/li&gt;\n&lt;li&gt;Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It\u2019s crucial to assess and address these risks as part of the integration process.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Considering the human element&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&amp;amp;A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&amp;amp;A.&lt;/p&gt;\n\n&lt;p&gt;The problem is, employees are conservative and don\u2019t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.&lt;br/&gt;\nAnd it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things \u2013 they need to start profiting from the merger or acquisition as soon as possible.&lt;/p&gt;\n\n&lt;p&gt;Merging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal\u2019s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.&lt;/p&gt;\n\n&lt;p&gt;In fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&amp;amp;As), as they help increase acceptance among employees and minimize potential disruption. Here\u2019s how these strategies can be applied:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company\u2019s situation;&lt;/li&gt;\n&lt;li&gt;Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;&lt;/li&gt;\n&lt;li&gt;Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;&lt;/li&gt;\n&lt;li&gt;Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive \u2018us versus them\u2019 mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.&lt;br/&gt;\nOther issues can be solved via the right tool that addresses every issue mentioned.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;How to effectively address data-related M&amp;amp;A challenges?&lt;/h1&gt;\n\n&lt;p&gt;To effectively address the challenges posed by M&amp;amp;A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.&lt;/p&gt;\n\n&lt;p&gt;A central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&amp;amp;A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.&lt;/p&gt;\n\n&lt;p&gt;By transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.&lt;/p&gt;\n\n&lt;p&gt;Moreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&amp;amp;A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.&lt;/p&gt;\n\n&lt;h1&gt;No-Code Data Solutions&lt;/h1&gt;\n\n&lt;p&gt;No-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&amp;amp;A right away.&lt;br/&gt;\nFurthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&amp;amp;A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.&lt;/p&gt;\n\n&lt;p&gt;In addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&amp;amp;A process.&lt;br/&gt;\nThe rest of research can be found here:  &lt;a href=\"https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm%5C_source=linkedin&amp;amp;utm%5C_medium=social&amp;amp;utm%5C_campaign=data%5C_engineering&amp;amp;utm%5C_content=migrations%5C_acquistions&amp;amp;utm%5C_term=ITarchitecture\"&gt;https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm\\_source=linkedin&amp;amp;utm\\_medium=social&amp;amp;utm\\_campaign=data\\_engineering&amp;amp;utm\\_content=migrations\\_acquistions&amp;amp;utm\\_term=ITarchitecture&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I3xJj8ELsQOmy2H-NeHKFmGnh2tKVfHX1i7SJ1O1qwE.jpg?auto=webp&amp;v=enabled&amp;s=a857d06fc3f1362c48ce1ce4e18be17b3a3dc051", "width": 79, "height": 86}, "resolutions": [], "variants": {}, "id": "fKkAw45B6Aa1K9gfC0CvmXNzCjb0F-J2wvKUvaKf1Vk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "149yxju", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Speech36", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149yxju/data_management_challenges_in_ma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149yxju/data_management_challenges_in_ma/", "subreddit_subscribers": 110536, "created_utc": 1686825092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://signalfire.com/modern-data-stack-investor/](https://signalfire.com/modern-data-stack-investor/)", "author_fullname": "t2_cj7rqjodt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Q&amp;A about the modern data stack with the founder of prophecy.io", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14a8b37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686850182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://signalfire.com/modern-data-stack-investor/\"&gt;https://signalfire.com/modern-data-stack-investor/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?auto=webp&amp;v=enabled&amp;s=8fb9f83ec7ee1bd3071331f60b4477c36ff89973", "width": 1374, "height": 773}, "resolutions": [{"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b483ccbbd29516288074922196e9840ff6a0b98", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71f73b914a0c4dc0804b04b6ee294f62f203001f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=288c2db366e6054ebf24ea2699c7b500d2092080", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa571f5503ddcfb2d8c615c7f036f01579c93233", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b1eb21f5dc31ccbab2dbc5cb61db6d7ceb5f905", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/R8DLQZtq-Ct8VhYccodvqqMT9oLbvtqDZK27gz0qCHc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89c3885e09971340bb63d8a048d58b0ad0e05f77", "width": 1080, "height": 607}], "variants": {}, "id": "cgQ22dyCgMlHQTVNxuKhLRGm2hcY1du6mcAmD2jgCJk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14a8b37", "is_robot_indexable": true, "report_reasons": null, "author": "DefiantSleep7506", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a8b37/qa_about_the_modern_data_stack_with_the_founder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a8b37/qa_about_the_modern_data_stack_with_the_founder/", "subreddit_subscribers": 110536, "created_utc": 1686850182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DEs,\n\nRecently, the French senate voted [a law to allow Facial Recognition in public](https://www.biometricupdate.com/202306/french-senate-votes-in-favor-of-public-facial-recognition-pilot), at scale, in real time. Leaving the ethic topic aside, I can't help but to think, there's no way the French government can pull that off.  Technically, first, this would require a huge team of great engineers (how can they attract talents with the salary they offer), but also from a budget perspective. Running this in the Cloud, will cost a lot?\n\nI work for a GAFAM, and some of our data pipelines process around 0.5 to 0.7 TB/s. We have a few of these and they're not easy. As a small team - not all of our stuff is efficient  - and the only reason we're able to pull this off is because we pay millions of dollars in AWS bill per month.\n\nHow can the French government (known for its shit tech) can go about architecting a real-time facial recognition system at Country scale? Is this bill just marketing and the reality is they'll fail to deliver?\n\nCurious to hear your thoughts as Data Engineer? Here are some over-simplified calculations:\n\n* So, there are 1.6M CCTV cameras in France, assuming all of them will stream into the system. Let's assume: resolution (2MP), average 15 FPS, H.264, 2MBs bitrate, that's at least **3 MB/s throughput per camera**, or a **total of 4.8 TB/s** for all cameras.\n* Assuming all 1.6 millions cameras stream 24 hours/day, it is **38.4M hours of stream p/day** (or, **663 PB/day).** To give an idea of the scale, that is **19x more hours of stream than Twitch** yesterday (according to twitchtracker).\n* Now suppose they use AWS Rekognition (which they won't) for inference and face matching. Price is $0.10 per minute of video (it's actually way more because you need multiple API calls and tons of application code around it, but it doesn't matter), that is: 38.4M hours\\*60 minutes\\*$0.10\\*24h, **$5 billion dollar per day** (ie, the full 2023 French Intelligence Budget, **in a singe day**). Even with massive discounts, there's just no way. Even with crazy optimisations, dividing this by 10, 20, 30, this would still be way too much given their annual budget.\n* Also, there's no way on earth they can fit that load in any Kinesis region.\n* Let's not even speak about them purchasing their own GPU mega-clusters, instead of using the Cloud.\n\nNow surely, there are better ways of designing and architecting this. But even then, it doesn't add up. I don't understand. They don't have enough employees, their engineers are not as good, they're under-paid, they don't have the infrastructure.\n\nTwo options: either, French people are told to be scared of Big Brother, when the reality is the French government is far far from being able to pull this off.\n\nOr, I am completely wrong, and there are better ways to design this for a fraction of the cost. Did I get the math wrong?", "author_fullname": "t2_2vtcnbh6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facial Recognition at Government scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14a88j1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686850001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DEs,&lt;/p&gt;\n\n&lt;p&gt;Recently, the French senate voted &lt;a href=\"https://www.biometricupdate.com/202306/french-senate-votes-in-favor-of-public-facial-recognition-pilot\"&gt;a law to allow Facial Recognition in public&lt;/a&gt;, at scale, in real time. Leaving the ethic topic aside, I can&amp;#39;t help but to think, there&amp;#39;s no way the French government can pull that off.  Technically, first, this would require a huge team of great engineers (how can they attract talents with the salary they offer), but also from a budget perspective. Running this in the Cloud, will cost a lot?&lt;/p&gt;\n\n&lt;p&gt;I work for a GAFAM, and some of our data pipelines process around 0.5 to 0.7 TB/s. We have a few of these and they&amp;#39;re not easy. As a small team - not all of our stuff is efficient  - and the only reason we&amp;#39;re able to pull this off is because we pay millions of dollars in AWS bill per month.&lt;/p&gt;\n\n&lt;p&gt;How can the French government (known for its shit tech) can go about architecting a real-time facial recognition system at Country scale? Is this bill just marketing and the reality is they&amp;#39;ll fail to deliver?&lt;/p&gt;\n\n&lt;p&gt;Curious to hear your thoughts as Data Engineer? Here are some over-simplified calculations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;So, there are 1.6M CCTV cameras in France, assuming all of them will stream into the system. Let&amp;#39;s assume: resolution (2MP), average 15 FPS, H.264, 2MBs bitrate, that&amp;#39;s at least &lt;strong&gt;3 MB/s throughput per camera&lt;/strong&gt;, or a &lt;strong&gt;total of 4.8 TB/s&lt;/strong&gt; for all cameras.&lt;/li&gt;\n&lt;li&gt;Assuming all 1.6 millions cameras stream 24 hours/day, it is &lt;strong&gt;38.4M hours of stream p/day&lt;/strong&gt; (or, &lt;strong&gt;663 PB/day).&lt;/strong&gt; To give an idea of the scale, that is &lt;strong&gt;19x more hours of stream than Twitch&lt;/strong&gt; yesterday (according to twitchtracker).&lt;/li&gt;\n&lt;li&gt;Now suppose they use AWS Rekognition (which they won&amp;#39;t) for inference and face matching. Price is $0.10 per minute of video (it&amp;#39;s actually way more because you need multiple API calls and tons of application code around it, but it doesn&amp;#39;t matter), that is: 38.4M hours*60 minutes*$0.10*24h, &lt;strong&gt;$5 billion dollar per day&lt;/strong&gt; (ie, the full 2023 French Intelligence Budget, &lt;strong&gt;in a singe day&lt;/strong&gt;). Even with massive discounts, there&amp;#39;s just no way. Even with crazy optimisations, dividing this by 10, 20, 30, this would still be way too much given their annual budget.&lt;/li&gt;\n&lt;li&gt;Also, there&amp;#39;s no way on earth they can fit that load in any Kinesis region.&lt;/li&gt;\n&lt;li&gt;Let&amp;#39;s not even speak about them purchasing their own GPU mega-clusters, instead of using the Cloud.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now surely, there are better ways of designing and architecting this. But even then, it doesn&amp;#39;t add up. I don&amp;#39;t understand. They don&amp;#39;t have enough employees, their engineers are not as good, they&amp;#39;re under-paid, they don&amp;#39;t have the infrastructure.&lt;/p&gt;\n\n&lt;p&gt;Two options: either, French people are told to be scared of Big Brother, when the reality is the French government is far far from being able to pull this off.&lt;/p&gt;\n\n&lt;p&gt;Or, I am completely wrong, and there are better ways to design this for a fraction of the cost. Did I get the math wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?auto=webp&amp;v=enabled&amp;s=ee5581ccd7783aede7250f4cbc80dcd46797d351", "width": 2048, "height": 1152}, "resolutions": [{"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4da6745aef447e1080ad9cad07e43a566d1fa2ce", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ec0d2885c75f2ad5c297414ba04118a30a9e04f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccd91579a8d5fbf9588f66f6cf8bf889e792dc5b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddc39cf32a821212533c75c5cc519767cc562771", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09cc9de0dc0b1ca4dbaf127b22755e462a9618e1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/qIZac7kecs8YL2Nde9sjhrSGx04toA_ozJUyRa7-SiQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=166bb3414a7d05389e4bc8e31d009b046c505c1b", "width": 1080, "height": 607}], "variants": {}, "id": "-euwekUlJ9cpRMToUSpJ01-jPrFBzqxL6lQwFAHchoM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14a88j1", "is_robot_indexable": true, "report_reasons": null, "author": "davidlequin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a88j1/facial_recognition_at_government_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a88j1/facial_recognition_at_government_scale/", "subreddit_subscribers": 110536, "created_utc": 1686850001.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm in search of advice on how to navigate your first role as data engineer.\n\n\n\nA few months ago, I started working as a junior data engineer at a huge company that sponsored my place on a coding bootcamp. I successfully completed the bootcamp and passed the technical interview, which I was really stoked about, and was very excited about getting started, but when I actually started, I got discouraged pretty quickly, and I'm still struggling to get by.\n\n\nA bit of background, I'm a girl, I'm almost 30 and I'm a 'career switcher'. I come from a completely non-technical background, and while I have experience with the SDLC from working as part of a product team in my previous role, I am a total novice when it comes to data and analytics, let alone data engineering. I am also quite shy and have a chronic anxiety problem, which, suffice to say, doesn't help with being a functioning corporate employee (it took me four years to feel at ease in my previous role, speak up in meetings, etc.).\n\n\nGoing back to the job itself -- things were pretty different than what I expected. First of all, the team I joined was not aware that I was actually going to join (I had interviewed with various managers, and the point of the interview was to decide under whom to place these people from the bootcamp), which accounted for a pretty crappy first day. As a consequence, nobody approached me, and I, only having one other experience prior to that one, was not aware I was supposed to schedule intro meetings with the various people in the team to understand their role and what they were working on. I guess I was expecting a more proactive approach from their side. My manager was also out of office, at the time, which didn't help. It took me a while to start interacting with my coworkers; I had a mentor appointed, but this person, while being great at their job and very patient, did not have a plan on how to onboard a junior out of a bootcamp, and most of our interactions consisted of me asking what I could learn/do. \n\n\nA few months in, I am finally able to get a (limited) understanding of the project we're working on, I was able to contribute a little bit (some bug fixing), but I often find myself sitting there with nothing to do and asking the other engineers how I can support, reading documentation in the meantime. I don't get involved much in their discussions, they don't invite me to calls, mostly because they're limited in capacity, and when bugs come up, it's quicker for them to solve them without having to go through things step by step with me. I would like to be more involved, but at the same time, I know I won't understand everything. I feel like a burden every day, I don't want to be a bottleneck, and I don't know what to do. Of course, impostor syndrome is kicking in, especially since I'm aware that I'm basically a diversity hire. I hate not having much on my plate, and I fear that, one year from now, I will still be useless and ignorant.\n\n\nI have clearly missed the mark on having a successful start; therefore, I figured I'd come to you. Any advice or resources (also tech stack-related) on navigating your first data engineering role would be appreciated. Thanks for your attention.", "author_fullname": "t2_14zmwo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for a struggling junior data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14a87mf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686849942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m in search of advice on how to navigate your first role as data engineer.&lt;/p&gt;\n\n&lt;p&gt;A few months ago, I started working as a junior data engineer at a huge company that sponsored my place on a coding bootcamp. I successfully completed the bootcamp and passed the technical interview, which I was really stoked about, and was very excited about getting started, but when I actually started, I got discouraged pretty quickly, and I&amp;#39;m still struggling to get by.&lt;/p&gt;\n\n&lt;p&gt;A bit of background, I&amp;#39;m a girl, I&amp;#39;m almost 30 and I&amp;#39;m a &amp;#39;career switcher&amp;#39;. I come from a completely non-technical background, and while I have experience with the SDLC from working as part of a product team in my previous role, I am a total novice when it comes to data and analytics, let alone data engineering. I am also quite shy and have a chronic anxiety problem, which, suffice to say, doesn&amp;#39;t help with being a functioning corporate employee (it took me four years to feel at ease in my previous role, speak up in meetings, etc.).&lt;/p&gt;\n\n&lt;p&gt;Going back to the job itself -- things were pretty different than what I expected. First of all, the team I joined was not aware that I was actually going to join (I had interviewed with various managers, and the point of the interview was to decide under whom to place these people from the bootcamp), which accounted for a pretty crappy first day. As a consequence, nobody approached me, and I, only having one other experience prior to that one, was not aware I was supposed to schedule intro meetings with the various people in the team to understand their role and what they were working on. I guess I was expecting a more proactive approach from their side. My manager was also out of office, at the time, which didn&amp;#39;t help. It took me a while to start interacting with my coworkers; I had a mentor appointed, but this person, while being great at their job and very patient, did not have a plan on how to onboard a junior out of a bootcamp, and most of our interactions consisted of me asking what I could learn/do. &lt;/p&gt;\n\n&lt;p&gt;A few months in, I am finally able to get a (limited) understanding of the project we&amp;#39;re working on, I was able to contribute a little bit (some bug fixing), but I often find myself sitting there with nothing to do and asking the other engineers how I can support, reading documentation in the meantime. I don&amp;#39;t get involved much in their discussions, they don&amp;#39;t invite me to calls, mostly because they&amp;#39;re limited in capacity, and when bugs come up, it&amp;#39;s quicker for them to solve them without having to go through things step by step with me. I would like to be more involved, but at the same time, I know I won&amp;#39;t understand everything. I feel like a burden every day, I don&amp;#39;t want to be a bottleneck, and I don&amp;#39;t know what to do. Of course, impostor syndrome is kicking in, especially since I&amp;#39;m aware that I&amp;#39;m basically a diversity hire. I hate not having much on my plate, and I fear that, one year from now, I will still be useless and ignorant.&lt;/p&gt;\n\n&lt;p&gt;I have clearly missed the mark on having a successful start; therefore, I figured I&amp;#39;d come to you. Any advice or resources (also tech stack-related) on navigating your first data engineering role would be appreciated. Thanks for your attention.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14a87mf", "is_robot_indexable": true, "report_reasons": null, "author": "silcap", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a87mf/advice_for_a_struggling_junior_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a87mf/advice_for_a_struggling_junior_data_engineer/", "subreddit_subscribers": 110536, "created_utc": 1686849942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any on-premise data stack options (Lake, Warehouse, ELT + Lineage) that come with paid support?\n\nOther than Tableau (which it seems to let you organize your data like Power BI DataSets), it seems like most vendors only offer their products on the cloud.\n\nApache, MinIO, etc. are all nifty, but they're completely open source w/out a company that you can get paid support from.", "author_fullname": "t2_ahf8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On-Premise Data Stack Options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14a5634", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686842773.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686842481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any on-premise data stack options (Lake, Warehouse, ELT + Lineage) that come with paid support?&lt;/p&gt;\n\n&lt;p&gt;Other than Tableau (which it seems to let you organize your data like Power BI DataSets), it seems like most vendors only offer their products on the cloud.&lt;/p&gt;\n\n&lt;p&gt;Apache, MinIO, etc. are all nifty, but they&amp;#39;re completely open source w/out a company that you can get paid support from.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14a5634", "is_robot_indexable": true, "report_reasons": null, "author": "PencilBoy99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a5634/onpremise_data_stack_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a5634/onpremise_data_stack_options/", "subreddit_subscribers": 110536, "created_utc": 1686842481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am attempting to read s3 globs (part files) using polars and rust - I tried to look at documentation most of it references Python - would anyone know of this is supported in rust?", "author_fullname": "t2_qvq8jmd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars s3 reads globs of parquet in Rust", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14a4fed", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686840724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to read s3 globs (part files) using polars and rust - I tried to look at documentation most of it references Python - would anyone know of this is supported in rust?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14a4fed", "is_robot_indexable": true, "report_reasons": null, "author": "ricky-programmer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a4fed/polars_s3_reads_globs_of_parquet_in_rust/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a4fed/polars_s3_reads_globs_of_parquet_in_rust/", "subreddit_subscribers": 110536, "created_utc": 1686840724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am going through an application process for a BI/Data Analyst position. Got a take away task to complete as a part of the process. The task is about parsing the data from a log file, cleansing, creating some queries and some analysis, which is not an issue. One point of the task is to store parsed and cleaned data in any database engine of a free choice.  \nAny advice on DB engine to use for such a small case?", "author_fullname": "t2_1mi7ryw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Task Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14a2mpt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686836142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am going through an application process for a BI/Data Analyst position. Got a take away task to complete as a part of the process. The task is about parsing the data from a log file, cleansing, creating some queries and some analysis, which is not an issue. One point of the task is to store parsed and cleaned data in any database engine of a free choice.&lt;br/&gt;\nAny advice on DB engine to use for such a small case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14a2mpt", "is_robot_indexable": true, "report_reasons": null, "author": "dimpopo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14a2mpt/interview_task_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14a2mpt/interview_task_suggestions/", "subreddit_subscribers": 110536, "created_utc": 1686836142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I used AWS for 3 years and found it so easy to loop through files in python and move them about after processing. Fast forward to now I'm in the Azure space and using blob storage. I feel like you have to do so much messing around to do anything with files.   \n\n\nIs it just me, am I the drama? Or is this a common thing?", "author_fullname": "t2_al2yxww8e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is working with Blob Storage so difficult compared to working with S3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149wad0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686815634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used AWS for 3 years and found it so easy to loop through files in python and move them about after processing. Fast forward to now I&amp;#39;m in the Azure space and using blob storage. I feel like you have to do so much messing around to do anything with files.   &lt;/p&gt;\n\n&lt;p&gt;Is it just me, am I the drama? Or is this a common thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149wad0", "is_robot_indexable": true, "report_reasons": null, "author": "The-Engineer-93", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149wad0/why_is_working_with_blob_storage_so_difficult/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149wad0/why_is_working_with_blob_storage_so_difficult/", "subreddit_subscribers": 110536, "created_utc": 1686815634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone worked here with ETL tool called Ab Initio? What are your impressions and any company you would recommend. Thank you", "author_fullname": "t2_9navdxud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ab Initio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_149sd0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686802386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone worked here with ETL tool called Ab Initio? What are your impressions and any company you would recommend. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "149sd0m", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Grade2960", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/149sd0m/ab_initio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/149sd0m/ab_initio/", "subreddit_subscribers": 110536, "created_utc": 1686802386.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}