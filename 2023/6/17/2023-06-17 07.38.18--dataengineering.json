{"kind": "Listing", "data": {"after": "t3_14at7ca", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dagster Master Plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14az4gz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cLNw5l3GlrASSy-nBqRgrjOQ0usKiV_A1lL-vXs-W4c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686927674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/dagster-master-plan", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?auto=webp&amp;v=enabled&amp;s=5e576129ef6e809ae03398b59e5eac6af438af01", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8653bf88106073b8fc1cc77ebdb44700ce26eac0", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a934b4c7ad4d724aee64b10e1e790a16ba152768", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b40c3d49af780ca4a5d309f3c434d7f8fefa4c58", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fc90d78ba482a53d867382ceeb73d85d678e5db", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=167d911182695702969305badfff1b4b1cf51aab", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=accd0a6eb7acf9b76dca5e4191e51b903a4aa227", "width": 1080, "height": 567}], "variants": {}, "id": "D2sMhqXf73jaVMZw01DtRTchkeGVvrYwOOR-HI2Bzhs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14az4gz", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14az4gz/the_dagster_master_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/dagster-master-plan", "subreddit_subscribers": 110847, "created_utc": 1686927674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m going to guess early to mid 20s.", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How old were you when you landed your first real data engineering job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b5t87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686944009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m going to guess early to mid 20s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14b5t87", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 94, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b5t87/how_old_were_you_when_you_landed_your_first_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b5t87/how_old_were_you_when_you_landed_your_first_real/", "subreddit_subscribers": 110847, "created_utc": 1686944009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happy Father's Day, data engineers!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14b7yl2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Sl5mzne5DYIbwhxTi2qEII3ZkEaQz1Ou2HXwJC7WSI0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686949266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/gv8fyeo72g6b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?auto=webp&amp;v=enabled&amp;s=a405764be344f4174597e60ae04e32667967016e", "width": 1200, "height": 1500}, "resolutions": [{"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0059acfcb203aa81fffbd0158ac07fea739a35d", "width": 108, "height": 135}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c28feaea9d5706438dc1fb501df46b59cd73a717", "width": 216, "height": 270}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2503bca3436d416646111d0f00ed51f7f2ad470d", "width": 320, "height": 400}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=941df641c128f37b6a37de589577db7a2f6c7e80", "width": 640, "height": 800}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=328b95e457ff6dbfdb6da2f9629d9dae69e7cf89", "width": 960, "height": 1200}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13151f090b4e7af9a6d05bf59e22293f1d2e617a", "width": 1080, "height": 1350}], "variants": {}, "id": "HPCRKmZuUHk-w5Gl_juNUl5814xnwImdyOk_Cp85yOI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14b7yl2", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b7yl2/happy_fathers_day_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/gv8fyeo72g6b1.png", "subreddit_subscribers": 110847, "created_utc": 1686949266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I just started my new project and I have a problem with data cleaning. I  don't know which path I'm supposed to go. Cleaning data in Python  Pandas or importing CSV into the database, I must say that cleaning data  in the database is so much easier. I don't know why people do analysis  in Python", "author_fullname": "t2_c4wcir5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Cleaning (Pyhton vs Database)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14atps1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686912845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started my new project and I have a problem with data cleaning. I  don&amp;#39;t know which path I&amp;#39;m supposed to go. Cleaning data in Python  Pandas or importing CSV into the database, I must say that cleaning data  in the database is so much easier. I don&amp;#39;t know why people do analysis  in Python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14atps1", "is_robot_indexable": true, "report_reasons": null, "author": "AdOrnery1159", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14atps1/data_cleaning_pyhton_vs_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14atps1/data_cleaning_pyhton_vs_database/", "subreddit_subscribers": 110847, "created_utc": 1686912845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, right now I am working in finance (credit risk, customer selection models etc), one of the top world banks. My job involves compilance, analyzing the models, whether the data is correct and so on. It involves also writing reports and conclusions about the models for upper managment. I do have some ML, statistics, risk, python knowledge but I thought I would move to more coding job so it would be more stimulating for me. Are this skills actually possible to leverege in data engineer/analytics engineer role (I would like to do something like this). I would not like to start completely at junior level because my background is kind of flexible and some parts of the job do have intersections with DE job. I am right now doing some courses on the GCP and will start building projects using docker etc for the future. Are there some roles that would have intersections involving my job and possible DE job? Thanks!", "author_fullname": "t2_2oo6rj79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job transition - finance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14arw5e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686906594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, right now I am working in finance (credit risk, customer selection models etc), one of the top world banks. My job involves compilance, analyzing the models, whether the data is correct and so on. It involves also writing reports and conclusions about the models for upper managment. I do have some ML, statistics, risk, python knowledge but I thought I would move to more coding job so it would be more stimulating for me. Are this skills actually possible to leverege in data engineer/analytics engineer role (I would like to do something like this). I would not like to start completely at junior level because my background is kind of flexible and some parts of the job do have intersections with DE job. I am right now doing some courses on the GCP and will start building projects using docker etc for the future. Are there some roles that would have intersections involving my job and possible DE job? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14arw5e", "is_robot_indexable": true, "report_reasons": null, "author": "Omnetfh", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14arw5e/job_transition_finance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14arw5e/job_transition_finance/", "subreddit_subscribers": 110847, "created_utc": 1686906594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. Looking for opinions on pulling data for a reasonably large api source. \n\nThe dataset is essentially 1000 requests (at daily intervals, or hourly, doesn\u2019t really matter) that I need to poll independently. \n\nEach call gets the latest date time in the db then requests from then forward.\n\nI can write this pretty easily in python but curious how to optimize this in dagster. Should I be sending off each stream to its own process (essentially spinning up 1000 processes), or is there a better way? \n\nEach state is essentially independent of all others so it\u2019s all parallelized easily, I just need to check for each stream in the db what it\u2019s current latest time is then gimmie gimmie data. \n\nThoughts? We also use Airbyte, but it doesn\u2019t seem very clean to do this without creating some duplication.", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API with 1000 endpoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bdhvs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686964462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Looking for opinions on pulling data for a reasonably large api source. &lt;/p&gt;\n\n&lt;p&gt;The dataset is essentially 1000 requests (at daily intervals, or hourly, doesn\u2019t really matter) that I need to poll independently. &lt;/p&gt;\n\n&lt;p&gt;Each call gets the latest date time in the db then requests from then forward.&lt;/p&gt;\n\n&lt;p&gt;I can write this pretty easily in python but curious how to optimize this in dagster. Should I be sending off each stream to its own process (essentially spinning up 1000 processes), or is there a better way? &lt;/p&gt;\n\n&lt;p&gt;Each state is essentially independent of all others so it\u2019s all parallelized easily, I just need to check for each stream in the db what it\u2019s current latest time is then gimmie gimmie data. &lt;/p&gt;\n\n&lt;p&gt;Thoughts? We also use Airbyte, but it doesn\u2019t seem very clean to do this without creating some duplication.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14bdhvs", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bdhvs/api_with_1000_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bdhvs/api_with_1000_endpoints/", "subreddit_subscribers": 110847, "created_utc": 1686964462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have been playing with dbt SCD using snapshot and stuck making relationship. I understood snapshot part of it, however snapshot scripts do not belong to model folder. If that's the case how do I make the relationship between fact table with SCD II (snapshot) dim table because I can't reference it?  \n\n\nAlso, Fivetran does not support running dbt snapshot in integrated mode ( I guess). Only option I could see is deploy deployment.yml which is really annoying because it runs on a fixed schedule. I want to run my transformation after the all referenced sources has been loaded which I can't gurantee via scheduled deployment.yml. Can someone help?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran, dbt SCD II and Fact table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bf8cb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686969639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have been playing with dbt SCD using snapshot and stuck making relationship. I understood snapshot part of it, however snapshot scripts do not belong to model folder. If that&amp;#39;s the case how do I make the relationship between fact table with SCD II (snapshot) dim table because I can&amp;#39;t reference it?  &lt;/p&gt;\n\n&lt;p&gt;Also, Fivetran does not support running dbt snapshot in integrated mode ( I guess). Only option I could see is deploy deployment.yml which is really annoying because it runs on a fixed schedule. I want to run my transformation after the all referenced sources has been loaded which I can&amp;#39;t gurantee via scheduled deployment.yml. Can someone help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14bf8cb", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bf8cb/fivetran_dbt_scd_ii_and_fact_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bf8cb/fivetran_dbt_scd_ii_and_fact_table/", "subreddit_subscribers": 110847, "created_utc": 1686969639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm building a pipeline to extract data from a number of tables in a mysql database, transform some of the data and run some logic before storing the result in another database for use by a Looker Studio dashboard. In theory all very simple stuff, so having created something similar at a previous job using Airflow, I thought i'd look around for the best practises for a modern data stack. \n\nThis brought me to dagster. Really cool software, similar to Airflow in many ways but feels more modern. I've paired it with Airbyte to extract data from the production database to a reporting database. The dagster assets then run their magic and I wrote a final asset to write the information to another database (using SQLAlchemy in the end because I can't for the life of me figure out how to upsert data into a database using dataframes.)\n\nSo this all mostly works, but my issue now is regarding partitioning of the data. In the source database, there's a table tracking number of sales by cashier. The assets are configured with the DailyPartitionsDefinition partitions definition to allow for backfilling. My assets work but my long winded question is: when backfilling multiple days of data, the sync request on my airbyte resources is triggered for each backfill, leading to a timeout by the airbyte server from the later backfill requests. How should I best structure my assets to prevent this?\n\nOptions:\n\n* Remove airbyte from dagsterand figure out a way to connect the assets without the integration\n* Do everything manually in Python and dagster. Read from the database taking into account partitioning, e.g. select \\* from sales where date = 2023-06-16\n* Rip it all out and go back to Airflow\n\nI really like dagster, I just can't get my head around this. Every example I see is either really simple or way too complicated.\n\nSorry for the rambling question, I hope it makes sense but any direction would be very much appreciated!", "author_fullname": "t2_3zctr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster/Airbyte ELT - Feeling like i'm losing my mind!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14au1rj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686913872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a pipeline to extract data from a number of tables in a mysql database, transform some of the data and run some logic before storing the result in another database for use by a Looker Studio dashboard. In theory all very simple stuff, so having created something similar at a previous job using Airflow, I thought i&amp;#39;d look around for the best practises for a modern data stack. &lt;/p&gt;\n\n&lt;p&gt;This brought me to dagster. Really cool software, similar to Airflow in many ways but feels more modern. I&amp;#39;ve paired it with Airbyte to extract data from the production database to a reporting database. The dagster assets then run their magic and I wrote a final asset to write the information to another database (using SQLAlchemy in the end because I can&amp;#39;t for the life of me figure out how to upsert data into a database using dataframes.)&lt;/p&gt;\n\n&lt;p&gt;So this all mostly works, but my issue now is regarding partitioning of the data. In the source database, there&amp;#39;s a table tracking number of sales by cashier. The assets are configured with the DailyPartitionsDefinition partitions definition to allow for backfilling. My assets work but my long winded question is: when backfilling multiple days of data, the sync request on my airbyte resources is triggered for each backfill, leading to a timeout by the airbyte server from the later backfill requests. How should I best structure my assets to prevent this?&lt;/p&gt;\n\n&lt;p&gt;Options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Remove airbyte from dagsterand figure out a way to connect the assets without the integration&lt;/li&gt;\n&lt;li&gt;Do everything manually in Python and dagster. Read from the database taking into account partitioning, e.g. select * from sales where date = 2023-06-16&lt;/li&gt;\n&lt;li&gt;Rip it all out and go back to Airflow&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I really like dagster, I just can&amp;#39;t get my head around this. Every example I see is either really simple or way too complicated.&lt;/p&gt;\n\n&lt;p&gt;Sorry for the rambling question, I hope it makes sense but any direction would be very much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14au1rj", "is_robot_indexable": true, "report_reasons": null, "author": "dixonl90", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14au1rj/dagsterairbyte_elt_feeling_like_im_losing_my_mind/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14au1rj/dagsterairbyte_elt_feeling_like_im_losing_my_mind/", "subreddit_subscribers": 110847, "created_utc": 1686913872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work more in the Analytics Engineering space so my question might not make complete sense however I would appreciate any clarity than can be provided.\n\nMy understanding is a common way for data to flow is as follows:\n\nApplication database (MySQL) &gt;&gt; Datalake (S3) &gt;&gt; Data Warehouse (Snowflake).\n\nAs an Analytics Eng I do many transformations in the Data Warehouse. \n\nWhy does the data need to go into S3 first? \n\nAre additional transformations happening in there done by the Data Engineer? \n\nCould S3 be removed and the data can go directly from the application database to the data warehouse?\n\nThanks", "author_fullname": "t2_fwqwbjia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Flow Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14attqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686913208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work more in the Analytics Engineering space so my question might not make complete sense however I would appreciate any clarity than can be provided.&lt;/p&gt;\n\n&lt;p&gt;My understanding is a common way for data to flow is as follows:&lt;/p&gt;\n\n&lt;p&gt;Application database (MySQL) &amp;gt;&amp;gt; Datalake (S3) &amp;gt;&amp;gt; Data Warehouse (Snowflake).&lt;/p&gt;\n\n&lt;p&gt;As an Analytics Eng I do many transformations in the Data Warehouse. &lt;/p&gt;\n\n&lt;p&gt;Why does the data need to go into S3 first? &lt;/p&gt;\n\n&lt;p&gt;Are additional transformations happening in there done by the Data Engineer? &lt;/p&gt;\n\n&lt;p&gt;Could S3 be removed and the data can go directly from the application database to the data warehouse?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14attqq", "is_robot_indexable": true, "report_reasons": null, "author": "space-trader-92", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14attqq/data_flow_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14attqq/data_flow_question/", "subreddit_subscribers": 110847, "created_utc": 1686913208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title suggests, i have been trying to get into data engineering and have no relevant experience of it. Everywhere I see, they expect the applicants to have certain years of experience in data engineering. I want to start from a junior role and then climb the ladder later to more senior positions. But it seems, in countries like India, you can't change your role mid way in your career. And it is the same for other roles as well. Please suggest, how I should tackle these situations. I really appreciate that in some countries you can freely choose your career and start afresh even in your late 30s.", "author_fullname": "t2_t09x3o4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is role change difficult in some countries like India?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bgxk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686974909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests, i have been trying to get into data engineering and have no relevant experience of it. Everywhere I see, they expect the applicants to have certain years of experience in data engineering. I want to start from a junior role and then climb the ladder later to more senior positions. But it seems, in countries like India, you can&amp;#39;t change your role mid way in your career. And it is the same for other roles as well. Please suggest, how I should tackle these situations. I really appreciate that in some countries you can freely choose your career and start afresh even in your late 30s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14bgxk7", "is_robot_indexable": true, "report_reasons": null, "author": "frustratedhu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bgxk7/why_is_role_change_difficult_in_some_countries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bgxk7/why_is_role_change_difficult_in_some_countries/", "subreddit_subscribers": 110847, "created_utc": 1686974909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to implement data sanity check for two databases, also return the inconsistent results(most prob extact rows) and since I have to implement this for a very huge database(let\u2019s say each database have min. 20 tables each with 2.5M+ data) so what techniques would you suggest? Or do you use in your company?", "author_fullname": "t2_aahomxjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you perform data sanity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ay92f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686925544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to implement data sanity check for two databases, also return the inconsistent results(most prob extact rows) and since I have to implement this for a very huge database(let\u2019s say each database have min. 20 tables each with 2.5M+ data) so what techniques would you suggest? Or do you use in your company?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ay92f", "is_robot_indexable": true, "report_reasons": null, "author": "Icraveviolencemother", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ay92f/how_do_you_perform_data_sanity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ay92f/how_do_you_perform_data_sanity/", "subreddit_subscribers": 110847, "created_utc": 1686925544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I haven't found much on this topic. To me this seems an oversight/developer marketing opportunity, as I can see a lot of companies/projects wanting to do this in the near future.\n\nAnyway, do you have any good resources you can point me to?  \n\nHow would you approach it?  \n\nHave a CDC pipeline with Debezium/Kafka to consume the Postgres events and then build a custom connector to Qdrant (in my case)?  \n\nDo you have any learning materials that would make it easier for me to get going?", "author_fullname": "t2_c056b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to keep a vector database in sync with a source-of-truth SQL database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14asu6u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686909874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t found much on this topic. To me this seems an oversight/developer marketing opportunity, as I can see a lot of companies/projects wanting to do this in the near future.&lt;/p&gt;\n\n&lt;p&gt;Anyway, do you have any good resources you can point me to?  &lt;/p&gt;\n\n&lt;p&gt;How would you approach it?  &lt;/p&gt;\n\n&lt;p&gt;Have a CDC pipeline with Debezium/Kafka to consume the Postgres events and then build a custom connector to Qdrant (in my case)?  &lt;/p&gt;\n\n&lt;p&gt;Do you have any learning materials that would make it easier for me to get going?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14asu6u", "is_robot_indexable": true, "report_reasons": null, "author": "retendo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14asu6u/how_to_keep_a_vector_database_in_sync_with_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14asu6u/how_to_keep_a_vector_database_in_sync_with_a/", "subreddit_subscribers": 110847, "created_utc": 1686909874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHey everyone,\n\nI've been exploring the world of big data and cloud computing lately, and I've come across two powerful tools: Databricks and AWS Glue. However, I'm still trying to understand how these two services work together.\n\nI know that Databricks is a unified analytics platform built on Apache Spark, which allows you to process big data and perform advanced analytics. On the other hand, AWS Glue is an ETL (Extract, Transform, Load) service provided by Amazon Web Services (AWS).\n\nWhat I'm particularly interested in is how Databricks utilizes AWS Glue. I've heard that Glue can help with data preparation and integration tasks, but I'm curious about the specific use cases and benefits of using Glue within the Databricks ecosystem.\n\nDoes anyone have experience or insights into how Databricks integrates with AWS Glue? How does Glue enhance the data processing capabilities of Databricks? Are there any specific features or functionalities that make this integration advantageous?\n\nI'm looking to understand the synergy between these two services and how they can work together to streamline big data workflows. Any real-world examples, tutorials, or recommended resources would be greatly appreciated.\n\nThanks in advance for your help!", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Databricks utilize AWS Glue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14aslc7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686909052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring the world of big data and cloud computing lately, and I&amp;#39;ve come across two powerful tools: Databricks and AWS Glue. However, I&amp;#39;m still trying to understand how these two services work together.&lt;/p&gt;\n\n&lt;p&gt;I know that Databricks is a unified analytics platform built on Apache Spark, which allows you to process big data and perform advanced analytics. On the other hand, AWS Glue is an ETL (Extract, Transform, Load) service provided by Amazon Web Services (AWS).&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m particularly interested in is how Databricks utilizes AWS Glue. I&amp;#39;ve heard that Glue can help with data preparation and integration tasks, but I&amp;#39;m curious about the specific use cases and benefits of using Glue within the Databricks ecosystem.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience or insights into how Databricks integrates with AWS Glue? How does Glue enhance the data processing capabilities of Databricks? Are there any specific features or functionalities that make this integration advantageous?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to understand the synergy between these two services and how they can work together to streamline big data workflows. Any real-world examples, tutorials, or recommended resources would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14aslc7", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14aslc7/how_does_databricks_utilize_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14aslc7/how_does_databricks_utilize_aws_glue/", "subreddit_subscribers": 110847, "created_utc": 1686909052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Concepts #1 \u2014 Slowly Changing Dimensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_14aptlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/j33fu5QP0HvPZZMWG2BYVM1eRcbeejhl-7jE8f5YNkc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686899178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdev.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdev.com/data-engineering-concepts-1-slowly-changing-dimensions-851d52446ccd", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lRA2iUt5uW51DNS4l3EsJufC2u06h7hOPG20t4fljNk.jpg?auto=webp&amp;v=enabled&amp;s=ff330e6e2e5785bb8424797f7c39d4b74c190af8", "width": 1042, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/lRA2iUt5uW51DNS4l3EsJufC2u06h7hOPG20t4fljNk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de25db31e40a22646c5c596b81063b75c3023763", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/lRA2iUt5uW51DNS4l3EsJufC2u06h7hOPG20t4fljNk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc4bb5daa5da77bc5cb6835c507556dcf88406cb", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/lRA2iUt5uW51DNS4l3EsJufC2u06h7hOPG20t4fljNk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=262252caf96f6c965bdebbacfe2b0aab892bb2e2", "width": 320, "height": 235}, {"url": "https://external-preview.redd.it/lRA2iUt5uW51DNS4l3EsJufC2u06h7hOPG20t4fljNk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=460c0fd68ca540a96b5ba363148877845d82447a", "width": 640, "height": 471}, {"url": "https://external-preview.redd.it/lRA2iUt5uW51DNS4l3EsJufC2u06h7hOPG20t4fljNk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65d1b0eb76070d9857982a27cb530dc1902310e6", "width": 960, "height": 707}], "variants": {}, "id": "5kaNvsWQE49zdI4YIIyHTEvB1Quj-zmYZ9zpTSBU7Og"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14aptlk", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14aptlk/data_engineering_concepts_1_slowly_changing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdev.com/data-engineering-concepts-1-slowly-changing-dimensions-851d52446ccd", "subreddit_subscribers": 110847, "created_utc": 1686899178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team and I are trying to implement slim CI jobs on our pull requests into github and I've been tasked with setting up the configuration. I've read through the documentation on DBT's site and I believe things are set up correctly:\n\n* DBT account and github account are linked, DBT environments are also set up (DEV, UAT, PROD)\n* in the execution settings we're deferring to our production batch job that runs every 15 minutes\n* dbt command is \" dbt build --select state:modified+ \"\n* Triggers are set to run on PRs\n\nAll these settings according to DBT's documentation are correct and I'm able to initiate the slim CI job, but it's still attempting to run every single model within our project. Rather than just the modified model and the downstream dependencies like slim CI is designed to function. My only thought is that it's running all models because this job is set up in a separate DBT environment than our production run, which is what the deferral state is based off of.\n\nAnyone else experience similar issues, thanks in advance!", "author_fullname": "t2_71weyfry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Cloud Slim CI Job Not Working as Expected", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bbgrg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686959031.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686958795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team and I are trying to implement slim CI jobs on our pull requests into github and I&amp;#39;ve been tasked with setting up the configuration. I&amp;#39;ve read through the documentation on DBT&amp;#39;s site and I believe things are set up correctly:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DBT account and github account are linked, DBT environments are also set up (DEV, UAT, PROD)&lt;/li&gt;\n&lt;li&gt;in the execution settings we&amp;#39;re deferring to our production batch job that runs every 15 minutes&lt;/li&gt;\n&lt;li&gt;dbt command is &amp;quot; dbt build --select state:modified+ &amp;quot;&lt;/li&gt;\n&lt;li&gt;Triggers are set to run on PRs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All these settings according to DBT&amp;#39;s documentation are correct and I&amp;#39;m able to initiate the slim CI job, but it&amp;#39;s still attempting to run every single model within our project. Rather than just the modified model and the downstream dependencies like slim CI is designed to function. My only thought is that it&amp;#39;s running all models because this job is set up in a separate DBT environment than our production run, which is what the deferral state is based off of.&lt;/p&gt;\n\n&lt;p&gt;Anyone else experience similar issues, thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14bbgrg", "is_robot_indexable": true, "report_reasons": null, "author": "DrunkenWhaler136", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bbgrg/dbt_cloud_slim_ci_job_not_working_as_expected/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bbgrg/dbt_cloud_slim_ci_job_not_working_as_expected/", "subreddit_subscribers": 110847, "created_utc": 1686958795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are setting up a pipeline from AWS RDBMS to Databricks and evaluating how to continuously transform the data into a columnar store: [Databricks DLT jobs](https://www.databricks.com/blog/2022/04/25/simplifying-change-data-capture-with-databricks-delta-live-tables.html) or [AWS Glue/Lambdas/Kinesis/etc](https://aws.amazon.com/blogs/big-data/build-incremental-data-pipelines-to-load-transactional-data-changes-using-aws-dms-delta-2-0-and-amazon-emr-serverless/).\n\nDoes anyone have experience with fully automating the columnar transformation part of the pipeline? Is this possible?\n\nOverall, what we want to achieve is a low-maintenance system that can handle our ongoing RDBMS schema changes without additional intervention if possible. We don't have strong latency requirements at this time.\n\nThanks.", "author_fullname": "t2_dk4dxkd3g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC schema automation with Databricks DLT vs. AWS Glue/Lambdas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b9cm6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686978624.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686953317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are setting up a pipeline from AWS RDBMS to Databricks and evaluating how to continuously transform the data into a columnar store: &lt;a href=\"https://www.databricks.com/blog/2022/04/25/simplifying-change-data-capture-with-databricks-delta-live-tables.html\"&gt;Databricks DLT jobs&lt;/a&gt; or &lt;a href=\"https://aws.amazon.com/blogs/big-data/build-incremental-data-pipelines-to-load-transactional-data-changes-using-aws-dms-delta-2-0-and-amazon-emr-serverless/\"&gt;AWS Glue/Lambdas/Kinesis/etc&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with fully automating the columnar transformation part of the pipeline? Is this possible?&lt;/p&gt;\n\n&lt;p&gt;Overall, what we want to achieve is a low-maintenance system that can handle our ongoing RDBMS schema changes without additional intervention if possible. We don&amp;#39;t have strong latency requirements at this time.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?auto=webp&amp;v=enabled&amp;s=ddff45fae01c06370c759b9931b9077847c6d361", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6f2eb1bf83d14e6bd589bff4c1e7b1c5e47bcd8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afb6dbcfc1b4d767051677ffe770f7e58aebf520", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c98b115acd54fb4ad57766e9784aa21285058754", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c32ee7f066f7b62484afd6decd874910f19c893f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c67e25da31639aab18847655871a22985eea1d1", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03e7895da40e6d1c4c85a0e7c5efb2eb892f4d91", "width": 1080, "height": 565}], "variants": {}, "id": "ygY9JKKNhmxY0oZd2uwBDVmGqTQce4B1UZt1ribYLPM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b9cm6", "is_robot_indexable": true, "report_reasons": null, "author": "ComprehensivePart288", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b9cm6/cdc_schema_automation_with_databricks_dlt_vs_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b9cm6/cdc_schema_automation_with_databricks_dlt_vs_aws/", "subreddit_subscribers": 110847, "created_utc": 1686953317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand the appeal of airbyte and have successfully deployed it on my laptop for testing, but now I wanted to move what I have deployed into AWS and possibly the cloud offering. Anyone knows if that is possible (e.g. to have a config file with what we develop locally, send to github to be reviewed/versioned and then apply that same config for testing/production) or would I need to test the extraction locally and then need to do the same process all over again (manually) to deploy it in a different environment ?", "author_fullname": "t2_hz0e0qby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b31sc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686937279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the appeal of airbyte and have successfully deployed it on my laptop for testing, but now I wanted to move what I have deployed into AWS and possibly the cloud offering. Anyone knows if that is possible (e.g. to have a config file with what we develop locally, send to github to be reviewed/versioned and then apply that same config for testing/production) or would I need to test the extraction locally and then need to do the same process all over again (manually) to deploy it in a different environment ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14b31sc", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Ratio1642", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b31sc/airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b31sc/airbyte/", "subreddit_subscribers": 110847, "created_utc": 1686937279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm not sure if this is the right sub, but what I'm working on is somewhere between backend and data eng'g. Anyw, I\u2019m currently setting up a log-based cdc with debezium that feeds a message broker for streams. The thing is, I want to only capture the record if the command is an update. However, I can't find any good resource that tackles the configuration for debezium to function like that. If \"directly altering debezium behavior\" is not an option, then I can only think of one solution - create an outbox table to store updated records, that of which can be implemented w either of the ff:\n\n(a) Let some middleman (stored procedures) trigger the insertion of record to the outbox table and it gets deleted as soon as the connector publishes it to the broker to keep the db clean. This is a process that happens at the db level; I am abstracting the work away from the application layer. The drawbacks now would be: trouble with keeping this procedure as part of the entire update transaction since the update is handled by an ORM tool from the app layer. This can be solved by (b) though.\n\n(b) To keep the entire update + insertion to outbox table an entire transaction, I perform the write on the service that deals with update operation and wrap these into one transactional method. Now the drawbacks would be, it adds unnecessary configs and load to the app layer. \n\nAre there better workarounds for this? If there are configurable alternatives to debezium, please let me know. Thank you very much.", "author_fullname": "t2_t73a89x8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium for Postgres CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14azwp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686929551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m not sure if this is the right sub, but what I&amp;#39;m working on is somewhere between backend and data eng&amp;#39;g. Anyw, I\u2019m currently setting up a log-based cdc with debezium that feeds a message broker for streams. The thing is, I want to only capture the record if the command is an update. However, I can&amp;#39;t find any good resource that tackles the configuration for debezium to function like that. If &amp;quot;directly altering debezium behavior&amp;quot; is not an option, then I can only think of one solution - create an outbox table to store updated records, that of which can be implemented w either of the ff:&lt;/p&gt;\n\n&lt;p&gt;(a) Let some middleman (stored procedures) trigger the insertion of record to the outbox table and it gets deleted as soon as the connector publishes it to the broker to keep the db clean. This is a process that happens at the db level; I am abstracting the work away from the application layer. The drawbacks now would be: trouble with keeping this procedure as part of the entire update transaction since the update is handled by an ORM tool from the app layer. This can be solved by (b) though.&lt;/p&gt;\n\n&lt;p&gt;(b) To keep the entire update + insertion to outbox table an entire transaction, I perform the write on the service that deals with update operation and wrap these into one transactional method. Now the drawbacks would be, it adds unnecessary configs and load to the app layer. &lt;/p&gt;\n\n&lt;p&gt;Are there better workarounds for this? If there are configurable alternatives to debezium, please let me know. Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14azwp3", "is_robot_indexable": true, "report_reasons": null, "author": "midnight_babyyy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14azwp3/debezium_for_postgres_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14azwp3/debezium_for_postgres_cdc/", "subreddit_subscribers": 110847, "created_utc": 1686929551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I overheard a conversation that data-oriented programming is old school and Data oriented programming is the better way to programming. \n\nCan I use DOP outside of game development? I really want to understand more", "author_fullname": "t2_8skbp1qj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "?? Server efficiency through Data Oriented Design??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b7nu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686948543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I overheard a conversation that data-oriented programming is old school and Data oriented programming is the better way to programming. &lt;/p&gt;\n\n&lt;p&gt;Can I use DOP outside of game development? I really want to understand more&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b7nu4", "is_robot_indexable": true, "report_reasons": null, "author": "Positive-Fly4773", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b7nu4/server_efficiency_through_data_oriented_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b7nu4/server_efficiency_through_data_oriented_design/", "subreddit_subscribers": 110847, "created_utc": 1686948543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Disclaimer: I'm not a data engineer. I'm evaluating my concept and want to validate its feasibility. Please let me know if this makes sense or can be performed differently. I don't believe these are technical bug / error questions. If anything I'm determining if I understand the concept (s).\n\n\nHypothetical Scenario: \n1. Dataset created from output of EVA Work package  Progress measurements \n2. Excel CSV file extracted from shared folder- Leads submit files for work packages daily for work packages they are responsible for. \n3. Data within range values validated by type of data and number of characters. (Expected ranges are based on a 4 tier scale defined by an upper and lower control limit.)\n4. Business rules based on 17 sql logic statements using fuzzy adjectives. \n5. Staging database is \"Temp_table\" located in MS SQL server.\n6. Target table is \"WPPM_Production_Table\" in MS SQL server data warehouse.\n\nQuestions: \n\nA. Would this be considered an ETL pipeline \nwith data processed in batch from source databases?\n\nB. Is data extraction from CSV files necessary? Or would the MDS add on for Excel make this easier with numerous leads and work packages?\n\nC. Data validation--is it better to validate at the application level or data warehouse based on the scenario? And is one better than the other? Would use of MDS invalidate the need for data validation in Excel? \n\nD. Would this scenario require creating aggregates? \n\nE. Are triggers more advisable if MDS is not used? Or does that even matter? \n\nF. Is there a specific data model that describes this scenario? \n\nG. Is it good practice / acceptable to maintain the Staging database and data warehouse on the same platform (i.e. MS SQL?)", "author_fullname": "t2_ph1t3ewm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice from experts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bg2jh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686972257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I&amp;#39;m not a data engineer. I&amp;#39;m evaluating my concept and want to validate its feasibility. Please let me know if this makes sense or can be performed differently. I don&amp;#39;t believe these are technical bug / error questions. If anything I&amp;#39;m determining if I understand the concept (s).&lt;/p&gt;\n\n&lt;p&gt;Hypothetical Scenario: \n1. Dataset created from output of EVA Work package  Progress measurements \n2. Excel CSV file extracted from shared folder- Leads submit files for work packages daily for work packages they are responsible for. \n3. Data within range values validated by type of data and number of characters. (Expected ranges are based on a 4 tier scale defined by an upper and lower control limit.)\n4. Business rules based on 17 sql logic statements using fuzzy adjectives. \n5. Staging database is &amp;quot;Temp_table&amp;quot; located in MS SQL server.\n6. Target table is &amp;quot;WPPM_Production_Table&amp;quot; in MS SQL server data warehouse.&lt;/p&gt;\n\n&lt;p&gt;Questions: &lt;/p&gt;\n\n&lt;p&gt;A. Would this be considered an ETL pipeline \nwith data processed in batch from source databases?&lt;/p&gt;\n\n&lt;p&gt;B. Is data extraction from CSV files necessary? Or would the MDS add on for Excel make this easier with numerous leads and work packages?&lt;/p&gt;\n\n&lt;p&gt;C. Data validation--is it better to validate at the application level or data warehouse based on the scenario? And is one better than the other? Would use of MDS invalidate the need for data validation in Excel? &lt;/p&gt;\n\n&lt;p&gt;D. Would this scenario require creating aggregates? &lt;/p&gt;\n\n&lt;p&gt;E. Are triggers more advisable if MDS is not used? Or does that even matter? &lt;/p&gt;\n\n&lt;p&gt;F. Is there a specific data model that describes this scenario? &lt;/p&gt;\n\n&lt;p&gt;G. Is it good practice / acceptable to maintain the Staging database and data warehouse on the same platform (i.e. MS SQL?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14bg2jh", "is_robot_indexable": true, "report_reasons": null, "author": "Kobalt13mm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bg2jh/need_advice_from_experts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bg2jh/need_advice_from_experts/", "subreddit_subscribers": 110847, "created_utc": 1686972257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the fastest way to run a small etl pipeline on a single node databricks cluster. \n\nUsing sqlalchemy and polars to read and wrangle is quite a bit faster than using the overhead of pyspark. The bottleneck is writing if data is smallish yet not tiny (e.g. 5 to 10 million rows).\n\nWhat is the best approach here? Pandas to_sql with sqlalchemy engine set to fast_executemany=True works okayish. Is molding the pandas df into a spark df and then using pyspark to export faster? Are there other faster options?", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing to Azure SQL server from single node DBX cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b7475", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686947215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the fastest way to run a small etl pipeline on a single node databricks cluster. &lt;/p&gt;\n\n&lt;p&gt;Using sqlalchemy and polars to read and wrangle is quite a bit faster than using the overhead of pyspark. The bottleneck is writing if data is smallish yet not tiny (e.g. 5 to 10 million rows).&lt;/p&gt;\n\n&lt;p&gt;What is the best approach here? Pandas to_sql with sqlalchemy engine set to fast_executemany=True works okayish. Is molding the pandas df into a spark df and then using pyspark to export faster? Are there other faster options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b7475", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b7475/writing_to_azure_sql_server_from_single_node_dbx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b7475/writing_to_azure_sql_server_from_single_node_dbx/", "subreddit_subscribers": 110847, "created_utc": 1686947215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_54sig", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping dynamic websites using cloud based Scraper API and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14b3yer", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": "transparent", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Jm-7mvf5ZyeJb0WsiK_5XC4MMsa3FlzqfTIDEmG5cXA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686939474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.adnansiddiqi.me", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.adnansiddiqi.me/scraping-dynamic-websites-using-scraper-api-and-python/?utm_source=reddit_de&amp;utm_medium=reddit&amp;utm_campaign=c_reddit_de_scraperapi_1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?auto=webp&amp;v=enabled&amp;s=0be9678b4647c2d6408183cce9b9aa0529608b7a", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5f86e79c73632718a0192d0770cef735ef71afe", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6036e446628cf00e86d298ac6686dc79c73c1300", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70b62381374986e58ee72a922723f53997296918", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d178dd6248e816a1a25776439716e1b990c0508", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90afa6ec317edb114bad30f7f0f876ebf1e9ba29", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9817f3cab85ca2f9aa6f96bba6673b4654369567", "width": 1080, "height": 607}], "variants": {}, "id": "0ZTLqnRC7I-OtWL4YPQWymXBI_n4kH4DoAVlbgZ_Wrw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14b3yer", "is_robot_indexable": true, "report_reasons": null, "author": "pknerd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14b3yer/scraping_dynamic_websites_using_cloud_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.adnansiddiqi.me/scraping-dynamic-websites-using-scraper-api-and-python/?utm_source=reddit_de&amp;utm_medium=reddit&amp;utm_campaign=c_reddit_de_scraperapi_1", "subreddit_subscribers": 110847, "created_utc": 1686939474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow Data Engineers,\n\nI'm currently working at a tech company as a Senior Data Engineer; I was laid off last year by another company, and while looking for my current job, I noticed many hiring managers like more to have a Software Engineer title in the resume.\n\nMy current job has lots of Software Engineering work (e.g., developing AWS lambdas in TypeScript), so I don't feel too distant from an SWE. I also want to always target more technical DE roles instead of analytical ones (like my previous role at FAANG).\n\nSo I was wondering if it's okay changing the title in my resume (for future jobs), from Senior Data Engineer to Senior Software Engineer (Data).\n\nIs there any potential downside when doing a background check?\n\n[View Poll](https://www.reddit.com/poll/14b12o5)", "author_fullname": "t2_5dn8ds4e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ok to change the title in my resume from Data Engineer to Software Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b12o5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686932387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working at a tech company as a Senior Data Engineer; I was laid off last year by another company, and while looking for my current job, I noticed many hiring managers like more to have a Software Engineer title in the resume.&lt;/p&gt;\n\n&lt;p&gt;My current job has lots of Software Engineering work (e.g., developing AWS lambdas in TypeScript), so I don&amp;#39;t feel too distant from an SWE. I also want to always target more technical DE roles instead of analytical ones (like my previous role at FAANG).&lt;/p&gt;\n\n&lt;p&gt;So I was wondering if it&amp;#39;s okay changing the title in my resume (for future jobs), from Senior Data Engineer to Senior Software Engineer (Data).&lt;/p&gt;\n\n&lt;p&gt;Is there any potential downside when doing a background check?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/14b12o5\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b12o5", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Muffin-8079", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1687364387266, "options": [{"text": "Changing title to Software Engineer, Data", "id": "23495915"}, {"text": "Changing title to Software Engineer", "id": "23495916"}, {"text": "Leaving title as Data Engineer", "id": "23495917"}, {"text": "View results", "id": "23495918"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 309, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b12o5/is_it_ok_to_change_the_title_in_my_resume_from/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/14b12o5/is_it_ok_to_change_the_title_in_my_resume_from/", "subreddit_subscribers": 110847, "created_utc": 1686932387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Model and Data Versioning: An Introduction to mlflow and DVC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14avpao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/18UUk4H6u7huOYzLrAp0EU-9F4fNSuZX-CRR_ny0mHQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686918876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/walmartglobaltech/model-and-data-versioning-an-introduction-to-mlflow-and-dvc-260347cd0f6e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SmP166aNO4sLv1hicYp9gtTRxiQjbyfz4zUCI6ja9Ks.jpg?auto=webp&amp;v=enabled&amp;s=c6f576dea82fe4d4c467239ef411c86b8f85de28", "width": 934, "height": 524}, "resolutions": [{"url": "https://external-preview.redd.it/SmP166aNO4sLv1hicYp9gtTRxiQjbyfz4zUCI6ja9Ks.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=022ff67292ee0d96700d98e250dfdb2490907eb9", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/SmP166aNO4sLv1hicYp9gtTRxiQjbyfz4zUCI6ja9Ks.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d50e59cb44b20a9630c6fd1add53bc436244d51", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/SmP166aNO4sLv1hicYp9gtTRxiQjbyfz4zUCI6ja9Ks.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5615b277d788185f428fee0b1a0dab672078241d", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/SmP166aNO4sLv1hicYp9gtTRxiQjbyfz4zUCI6ja9Ks.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8465c379f3313251030dc1c2cc78ba050aa56e76", "width": 640, "height": 359}], "variants": {}, "id": "3VBNHf_29E4hJ2ZK7F-hdoRprCfV28NKadAC3E-K_Cg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14avpao", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14avpao/model_and_data_versioning_an_introduction_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/walmartglobaltech/model-and-data-versioning-an-introduction-to-mlflow-and-dvc-260347cd0f6e", "subreddit_subscribers": 110847, "created_utc": 1686918876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've recently started to study some of the tools necessary to become a data engineer: airflow, spark, etc. However, while some things are starting to make sense, I'm still trying to figure out the beggining of it all: how do you even start to incorporate this tools in a company that does not use them at all?\n\nWhat I'm trying to understand, and I am failing to do, is what should be the first steps of implementation. For instance, my company has a lot of IoT data that they don't use - but want to start doing so, and I would love to be part of it and learn how to build pipelines, understand more about how we should storage the data, etc. \n\nDoes anyone here have this initial implementation experience and would like to share? Books, articles, anything is valid.\n\n&amp;#x200B;\n\nThank you for your time.", "author_fullname": "t2_2117hvx8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does one starts applying data engineering in a company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14at7ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686911097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently started to study some of the tools necessary to become a data engineer: airflow, spark, etc. However, while some things are starting to make sense, I&amp;#39;m still trying to figure out the beggining of it all: how do you even start to incorporate this tools in a company that does not use them at all?&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m trying to understand, and I am failing to do, is what should be the first steps of implementation. For instance, my company has a lot of IoT data that they don&amp;#39;t use - but want to start doing so, and I would love to be part of it and learn how to build pipelines, understand more about how we should storage the data, etc. &lt;/p&gt;\n\n&lt;p&gt;Does anyone here have this initial implementation experience and would like to share? Books, articles, anything is valid.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14at7ca", "is_robot_indexable": true, "report_reasons": null, "author": "tryingnewhabits", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14at7ca/how_does_one_starts_applying_data_engineering_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14at7ca/how_does_one_starts_applying_data_engineering_in/", "subreddit_subscribers": 110847, "created_utc": 1686911097.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}