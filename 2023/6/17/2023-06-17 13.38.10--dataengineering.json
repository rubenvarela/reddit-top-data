{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dagster Master Plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14az4gz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cLNw5l3GlrASSy-nBqRgrjOQ0usKiV_A1lL-vXs-W4c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686927674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/dagster-master-plan", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?auto=webp&amp;v=enabled&amp;s=5e576129ef6e809ae03398b59e5eac6af438af01", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8653bf88106073b8fc1cc77ebdb44700ce26eac0", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a934b4c7ad4d724aee64b10e1e790a16ba152768", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b40c3d49af780ca4a5d309f3c434d7f8fefa4c58", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fc90d78ba482a53d867382ceeb73d85d678e5db", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=167d911182695702969305badfff1b4b1cf51aab", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/VTnl1-iAcevGqv-mpuqbX_nf3KpnSxR9vKFLM9RWXm8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=accd0a6eb7acf9b76dca5e4191e51b903a4aa227", "width": 1080, "height": 567}], "variants": {}, "id": "D2sMhqXf73jaVMZw01DtRTchkeGVvrYwOOR-HI2Bzhs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14az4gz", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14az4gz/the_dagster_master_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/dagster-master-plan", "subreddit_subscribers": 110881, "created_utc": 1686927674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m going to guess early to mid 20s.", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How old were you when you landed your first real data engineering job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b5t87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686944009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m going to guess early to mid 20s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14b5t87", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 117, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b5t87/how_old_were_you_when_you_landed_your_first_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b5t87/how_old_were_you_when_you_landed_your_first_real/", "subreddit_subscribers": 110881, "created_utc": 1686944009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Happy Father's Day, data engineers!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14b7yl2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Sl5mzne5DYIbwhxTi2qEII3ZkEaQz1Ou2HXwJC7WSI0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686949266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/gv8fyeo72g6b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?auto=webp&amp;v=enabled&amp;s=a405764be344f4174597e60ae04e32667967016e", "width": 1200, "height": 1500}, "resolutions": [{"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0059acfcb203aa81fffbd0158ac07fea739a35d", "width": 108, "height": 135}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c28feaea9d5706438dc1fb501df46b59cd73a717", "width": 216, "height": 270}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2503bca3436d416646111d0f00ed51f7f2ad470d", "width": 320, "height": 400}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=941df641c128f37b6a37de589577db7a2f6c7e80", "width": 640, "height": 800}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=328b95e457ff6dbfdb6da2f9629d9dae69e7cf89", "width": 960, "height": 1200}, {"url": "https://preview.redd.it/gv8fyeo72g6b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13151f090b4e7af9a6d05bf59e22293f1d2e617a", "width": 1080, "height": 1350}], "variants": {}, "id": "HPCRKmZuUHk-w5Gl_juNUl5814xnwImdyOk_Cp85yOI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14b7yl2", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b7yl2/happy_fathers_day_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/gv8fyeo72g6b1.png", "subreddit_subscribers": 110881, "created_utc": 1686949266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI was trying to read excel files residing on **AWS S3**. As I already had pyspark pipelines setup, I attempted to use **com.crealytics.spark.excel** for excel. It worked fine for files **&lt;10MB** however, with large files (50 to 150 MB excel files) I started getting job failure as follows:\n\n&gt;\"java.lang.OutOfMemoryError: Java heap space\"\n\nI referred to AWS Glue's docs and found the following troubleshooting guide: [AWS Glue OOM Heap Space](https://repost.aws/knowledge-center/glue-oom-java-heap-space-error)\n\nThis, however, only dealt with **large number of small file problems**, or other **driver intensive operations**, and the only suggestion it had for my situation is to **scale up**.\n\nFor **50 MB files**, I scaled up to **20-30 workers** and the job was successful, however, the **150 MB file** still could not be read.\n\nI approached the problem with a different toolset i.e. **boto3 &amp; pandas** or **awswrangler**. That did the job with just 4 workers in under 10 mins. I bet not even 3 are required.\n\nI wanted to know if I had done something incorrectly with crealytics, considering pyspark is supposed to be much more powerful, compute wise, considering its distributed nature. Also, if the above result is conclusive, could anyone guide me as to why this happened, based on how both work? Would be grateful for your responses.  \nI had posted a question on [stackoverflow](https://stackoverflow.com/questions/76464770/com-crealytics-spark-excel-vs-pandas-awswrangler/76465167#76465167), however, there was no comprehensive response.", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas was faster and less memory intensive then crealytics pyspark. How is it possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bn5fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686996210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to read excel files residing on &lt;strong&gt;AWS S3&lt;/strong&gt;. As I already had pyspark pipelines setup, I attempted to use &lt;strong&gt;com.crealytics.spark.excel&lt;/strong&gt; for excel. It worked fine for files &lt;strong&gt;&amp;lt;10MB&lt;/strong&gt; however, with large files (50 to 150 MB excel files) I started getting job failure as follows:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&amp;quot;java.lang.OutOfMemoryError: Java heap space&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I referred to AWS Glue&amp;#39;s docs and found the following troubleshooting guide: &lt;a href=\"https://repost.aws/knowledge-center/glue-oom-java-heap-space-error\"&gt;AWS Glue OOM Heap Space&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This, however, only dealt with &lt;strong&gt;large number of small file problems&lt;/strong&gt;, or other &lt;strong&gt;driver intensive operations&lt;/strong&gt;, and the only suggestion it had for my situation is to &lt;strong&gt;scale up&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;For &lt;strong&gt;50 MB files&lt;/strong&gt;, I scaled up to &lt;strong&gt;20-30 workers&lt;/strong&gt; and the job was successful, however, the &lt;strong&gt;150 MB file&lt;/strong&gt; still could not be read.&lt;/p&gt;\n\n&lt;p&gt;I approached the problem with a different toolset i.e. &lt;strong&gt;boto3 &amp;amp; pandas&lt;/strong&gt; or &lt;strong&gt;awswrangler&lt;/strong&gt;. That did the job with just 4 workers in under 10 mins. I bet not even 3 are required.&lt;/p&gt;\n\n&lt;p&gt;I wanted to know if I had done something incorrectly with crealytics, considering pyspark is supposed to be much more powerful, compute wise, considering its distributed nature. Also, if the above result is conclusive, could anyone guide me as to why this happened, based on how both work? Would be grateful for your responses.&lt;br/&gt;\nI had posted a question on &lt;a href=\"https://stackoverflow.com/questions/76464770/com-crealytics-spark-excel-vs-pandas-awswrangler/76465167#76465167\"&gt;stackoverflow&lt;/a&gt;, however, there was no comprehensive response.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;v=enabled&amp;s=96305e240e32ef61b596395e1bd74cdd2b5ddc9f", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=668115d5116a9321c9d15551b8d546a68f75112b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488bc7dc46930436614152debfa68b6926b8196b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=181b543f38a37bffa9070ec5c73a2cbcd66cc8f4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d81e093ae625b1e109af99de0c72a0ebea0f0088", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10b559f8887295a4ee5c2790c34a42f8e3690847", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=836e2bf75ddac523506ae28bf3813f47697700e2", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14bn5fw", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bn5fw/pandas_was_faster_and_less_memory_intensive_then/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bn5fw/pandas_was_faster_and_less_memory_intensive_then/", "subreddit_subscribers": 110881, "created_utc": 1686996210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title suggests, i have been trying to get into data engineering and have no relevant experience of it. Everywhere I see, they expect the applicants to have certain years of experience in data engineering. I want to start from a junior role and then climb the ladder later to more senior positions. But it seems, in countries like India, you can't change your role mid way in your career. And it is the same for other roles as well. Please suggest, how I should tackle these situations. I really appreciate that in some countries you can freely choose your career and start afresh even in your late 30s.", "author_fullname": "t2_t09x3o4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is role change difficult in some countries like India?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bgxk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686974909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests, i have been trying to get into data engineering and have no relevant experience of it. Everywhere I see, they expect the applicants to have certain years of experience in data engineering. I want to start from a junior role and then climb the ladder later to more senior positions. But it seems, in countries like India, you can&amp;#39;t change your role mid way in your career. And it is the same for other roles as well. Please suggest, how I should tackle these situations. I really appreciate that in some countries you can freely choose your career and start afresh even in your late 30s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14bgxk7", "is_robot_indexable": true, "report_reasons": null, "author": "frustratedhu", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bgxk7/why_is_role_change_difficult_in_some_countries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bgxk7/why_is_role_change_difficult_in_some_countries/", "subreddit_subscribers": 110881, "created_utc": 1686974909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. Looking for opinions on pulling data for a reasonably large api source. \n\nThe dataset is essentially 1000 requests (at daily intervals, or hourly, doesn\u2019t really matter) that I need to poll independently. \n\nEach call gets the latest date time in the db then requests from then forward.\n\nI can write this pretty easily in python but curious how to optimize this in dagster. Should I be sending off each stream to its own process (essentially spinning up 1000 processes), or is there a better way? \n\nEach state is essentially independent of all others so it\u2019s all parallelized easily, I just need to check for each stream in the db what it\u2019s current latest time is then gimmie gimmie data. \n\nThoughts? We also use Airbyte, but it doesn\u2019t seem very clean to do this without creating some duplication.", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API with 1000 endpoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bdhvs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686964462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Looking for opinions on pulling data for a reasonably large api source. &lt;/p&gt;\n\n&lt;p&gt;The dataset is essentially 1000 requests (at daily intervals, or hourly, doesn\u2019t really matter) that I need to poll independently. &lt;/p&gt;\n\n&lt;p&gt;Each call gets the latest date time in the db then requests from then forward.&lt;/p&gt;\n\n&lt;p&gt;I can write this pretty easily in python but curious how to optimize this in dagster. Should I be sending off each stream to its own process (essentially spinning up 1000 processes), or is there a better way? &lt;/p&gt;\n\n&lt;p&gt;Each state is essentially independent of all others so it\u2019s all parallelized easily, I just need to check for each stream in the db what it\u2019s current latest time is then gimmie gimmie data. &lt;/p&gt;\n\n&lt;p&gt;Thoughts? We also use Airbyte, but it doesn\u2019t seem very clean to do this without creating some duplication.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14bdhvs", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bdhvs/api_with_1000_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bdhvs/api_with_1000_endpoints/", "subreddit_subscribers": 110881, "created_utc": 1686964462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have been playing with dbt SCD using snapshot and stuck making relationship. I understood snapshot part of it, however snapshot scripts do not belong to model folder. If that's the case how do I make the relationship between fact table with SCD II (snapshot) dim table because I can't reference it?  \n\n\nAlso, Fivetran does not support running dbt snapshot in integrated mode ( I guess). Only option I could see is deploy deployment.yml which is really annoying because it runs on a fixed schedule. I want to run my transformation after the all referenced sources has been loaded which I can't gurantee via scheduled deployment.yml. Can someone help?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran, dbt SCD II and Fact table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bf8cb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686969639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have been playing with dbt SCD using snapshot and stuck making relationship. I understood snapshot part of it, however snapshot scripts do not belong to model folder. If that&amp;#39;s the case how do I make the relationship between fact table with SCD II (snapshot) dim table because I can&amp;#39;t reference it?  &lt;/p&gt;\n\n&lt;p&gt;Also, Fivetran does not support running dbt snapshot in integrated mode ( I guess). Only option I could see is deploy deployment.yml which is really annoying because it runs on a fixed schedule. I want to run my transformation after the all referenced sources has been loaded which I can&amp;#39;t gurantee via scheduled deployment.yml. Can someone help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14bf8cb", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bf8cb/fivetran_dbt_scd_ii_and_fact_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bf8cb/fivetran_dbt_scd_ii_and_fact_table/", "subreddit_subscribers": 110881, "created_utc": 1686969639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to implement data sanity check for two databases, also return the inconsistent results(most prob extact rows) and since I have to implement this for a very huge database(let\u2019s say each database have min. 20 tables each with 2.5M+ data) so what techniques would you suggest? Or do you use in your company?", "author_fullname": "t2_aahomxjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you perform data sanity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ay92f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686925544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to implement data sanity check for two databases, also return the inconsistent results(most prob extact rows) and since I have to implement this for a very huge database(let\u2019s say each database have min. 20 tables each with 2.5M+ data) so what techniques would you suggest? Or do you use in your company?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ay92f", "is_robot_indexable": true, "report_reasons": null, "author": "Icraveviolencemother", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ay92f/how_do_you_perform_data_sanity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ay92f/how_do_you_perform_data_sanity/", "subreddit_subscribers": 110881, "created_utc": 1686925544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand the appeal of airbyte and have successfully deployed it on my laptop for testing, but now I wanted to move what I have deployed into AWS and possibly the cloud offering. Anyone knows if that is possible (e.g. to have a config file with what we develop locally, send to github to be reviewed/versioned and then apply that same config for testing/production) or would I need to test the extraction locally and then need to do the same process all over again (manually) to deploy it in a different environment ?", "author_fullname": "t2_hz0e0qby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b31sc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686937279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the appeal of airbyte and have successfully deployed it on my laptop for testing, but now I wanted to move what I have deployed into AWS and possibly the cloud offering. Anyone knows if that is possible (e.g. to have a config file with what we develop locally, send to github to be reviewed/versioned and then apply that same config for testing/production) or would I need to test the extraction locally and then need to do the same process all over again (manually) to deploy it in a different environment ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14b31sc", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Ratio1642", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b31sc/airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b31sc/airbyte/", "subreddit_subscribers": 110881, "created_utc": 1686937279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team and I are trying to implement slim CI jobs on our pull requests into github and I've been tasked with setting up the configuration. I've read through the documentation on DBT's site and I believe things are set up correctly:\n\n* DBT account and github account are linked, DBT environments are also set up (DEV, UAT, PROD)\n* in the execution settings we're deferring to our production batch job that runs every 15 minutes\n* dbt command is \" dbt build --select state:modified+ \"\n* Triggers are set to run on PRs\n\nAll these settings according to DBT's documentation are correct and I'm able to initiate the slim CI job, but it's still attempting to run every single model within our project. Rather than just the modified model and the downstream dependencies like slim CI is designed to function. My only thought is that it's running all models because this job is set up in a separate DBT environment than our production run, which is what the deferral state is based off of.\n\nAnyone else experience similar issues, thanks in advance!", "author_fullname": "t2_71weyfry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Cloud Slim CI Job Not Working as Expected", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bbgrg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686959031.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686958795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team and I are trying to implement slim CI jobs on our pull requests into github and I&amp;#39;ve been tasked with setting up the configuration. I&amp;#39;ve read through the documentation on DBT&amp;#39;s site and I believe things are set up correctly:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DBT account and github account are linked, DBT environments are also set up (DEV, UAT, PROD)&lt;/li&gt;\n&lt;li&gt;in the execution settings we&amp;#39;re deferring to our production batch job that runs every 15 minutes&lt;/li&gt;\n&lt;li&gt;dbt command is &amp;quot; dbt build --select state:modified+ &amp;quot;&lt;/li&gt;\n&lt;li&gt;Triggers are set to run on PRs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All these settings according to DBT&amp;#39;s documentation are correct and I&amp;#39;m able to initiate the slim CI job, but it&amp;#39;s still attempting to run every single model within our project. Rather than just the modified model and the downstream dependencies like slim CI is designed to function. My only thought is that it&amp;#39;s running all models because this job is set up in a separate DBT environment than our production run, which is what the deferral state is based off of.&lt;/p&gt;\n\n&lt;p&gt;Anyone else experience similar issues, thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14bbgrg", "is_robot_indexable": true, "report_reasons": null, "author": "DrunkenWhaler136", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bbgrg/dbt_cloud_slim_ci_job_not_working_as_expected/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bbgrg/dbt_cloud_slim_ci_job_not_working_as_expected/", "subreddit_subscribers": 110881, "created_utc": 1686958795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are setting up a pipeline from AWS RDBMS to Databricks and evaluating how to continuously transform the data into a columnar store: [Databricks DLT jobs](https://www.databricks.com/blog/2022/04/25/simplifying-change-data-capture-with-databricks-delta-live-tables.html) or [AWS Glue/Lambdas/Kinesis/etc](https://aws.amazon.com/blogs/big-data/build-incremental-data-pipelines-to-load-transactional-data-changes-using-aws-dms-delta-2-0-and-amazon-emr-serverless/).\n\nDoes anyone have experience with fully automating the columnar transformation part of the pipeline? Is this possible?\n\nOverall, what we want to achieve is a low-maintenance system that can handle our ongoing RDBMS schema changes without additional intervention if possible. We don't have strong latency requirements at this time.\n\nThanks.", "author_fullname": "t2_dk4dxkd3g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC schema automation with Databricks DLT vs. AWS Glue/Lambdas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b9cm6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686978624.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686953317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are setting up a pipeline from AWS RDBMS to Databricks and evaluating how to continuously transform the data into a columnar store: &lt;a href=\"https://www.databricks.com/blog/2022/04/25/simplifying-change-data-capture-with-databricks-delta-live-tables.html\"&gt;Databricks DLT jobs&lt;/a&gt; or &lt;a href=\"https://aws.amazon.com/blogs/big-data/build-incremental-data-pipelines-to-load-transactional-data-changes-using-aws-dms-delta-2-0-and-amazon-emr-serverless/\"&gt;AWS Glue/Lambdas/Kinesis/etc&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with fully automating the columnar transformation part of the pipeline? Is this possible?&lt;/p&gt;\n\n&lt;p&gt;Overall, what we want to achieve is a low-maintenance system that can handle our ongoing RDBMS schema changes without additional intervention if possible. We don&amp;#39;t have strong latency requirements at this time.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?auto=webp&amp;v=enabled&amp;s=ddff45fae01c06370c759b9931b9077847c6d361", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6f2eb1bf83d14e6bd589bff4c1e7b1c5e47bcd8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afb6dbcfc1b4d767051677ffe770f7e58aebf520", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c98b115acd54fb4ad57766e9784aa21285058754", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c32ee7f066f7b62484afd6decd874910f19c893f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c67e25da31639aab18847655871a22985eea1d1", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/XAc38jEPeQ2bo7Lal5Fy_p9QRa6z5eaI6wohF5POKOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03e7895da40e6d1c4c85a0e7c5efb2eb892f4d91", "width": 1080, "height": 565}], "variants": {}, "id": "ygY9JKKNhmxY0oZd2uwBDVmGqTQce4B1UZt1ribYLPM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b9cm6", "is_robot_indexable": true, "report_reasons": null, "author": "ComprehensivePart288", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b9cm6/cdc_schema_automation_with_databricks_dlt_vs_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b9cm6/cdc_schema_automation_with_databricks_dlt_vs_aws/", "subreddit_subscribers": 110881, "created_utc": 1686953317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm not sure if this is the right sub, but what I'm working on is somewhere between backend and data eng'g. Anyw, I\u2019m currently setting up a log-based cdc with debezium that feeds a message broker for streams. The thing is, I want to only capture the record if the command is an update. However, I can't find any good resource that tackles the configuration for debezium to function like that. If \"directly altering debezium behavior\" is not an option, then I can only think of one solution - create an outbox table to store updated records, that of which can be implemented w either of the ff:\n\n(a) Let some middleman (stored procedures) trigger the insertion of record to the outbox table and it gets deleted as soon as the connector publishes it to the broker to keep the db clean. This is a process that happens at the db level; I am abstracting the work away from the application layer. The drawbacks now would be: trouble with keeping this procedure as part of the entire update transaction since the update is handled by an ORM tool from the app layer. This can be solved by (b) though.\n\n(b) To keep the entire update + insertion to outbox table an entire transaction, I perform the write on the service that deals with update operation and wrap these into one transactional method. Now the drawbacks would be, it adds unnecessary configs and load to the app layer. \n\nAre there better workarounds for this? If there are configurable alternatives to debezium, please let me know. Thank you very much.", "author_fullname": "t2_t73a89x8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium for Postgres CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14azwp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686929551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m not sure if this is the right sub, but what I&amp;#39;m working on is somewhere between backend and data eng&amp;#39;g. Anyw, I\u2019m currently setting up a log-based cdc with debezium that feeds a message broker for streams. The thing is, I want to only capture the record if the command is an update. However, I can&amp;#39;t find any good resource that tackles the configuration for debezium to function like that. If &amp;quot;directly altering debezium behavior&amp;quot; is not an option, then I can only think of one solution - create an outbox table to store updated records, that of which can be implemented w either of the ff:&lt;/p&gt;\n\n&lt;p&gt;(a) Let some middleman (stored procedures) trigger the insertion of record to the outbox table and it gets deleted as soon as the connector publishes it to the broker to keep the db clean. This is a process that happens at the db level; I am abstracting the work away from the application layer. The drawbacks now would be: trouble with keeping this procedure as part of the entire update transaction since the update is handled by an ORM tool from the app layer. This can be solved by (b) though.&lt;/p&gt;\n\n&lt;p&gt;(b) To keep the entire update + insertion to outbox table an entire transaction, I perform the write on the service that deals with update operation and wrap these into one transactional method. Now the drawbacks would be, it adds unnecessary configs and load to the app layer. &lt;/p&gt;\n\n&lt;p&gt;Are there better workarounds for this? If there are configurable alternatives to debezium, please let me know. Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14azwp3", "is_robot_indexable": true, "report_reasons": null, "author": "midnight_babyyy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14azwp3/debezium_for_postgres_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14azwp3/debezium_for_postgres_cdc/", "subreddit_subscribers": 110881, "created_utc": 1686929551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi, i'm a college cs student who just started my first internship in de. i'm learning/using oracle pl/sql, informatica, and some other stuff. i have 2 questions:\n\n1. is oracle pl/sql and informatica still worth learning, it seems like more and more companies are moving away from it (just my general impression).  a lot of more job descriptions have different dbs/tools. the pl/sql language and the gui of informatica specifically feel kind of outdated\n2. i was initially looking for swe internships but landed a de internship instead. don't mind it, but given the more opportunities/higher pay, I'd prefer to go into swe. I've heard de can be seen as a subset of backend/swe. is this still true for oracle pl/sql and informatica? if not, what is this true for. kind of wanting to see if i can use this as an opportunity to get into backend dbs and pipelines in swe backend\n\nany advice is much appreciated, thanks", "author_fullname": "t2_4f7u7b3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "first DE internship, lots of questions (are pl/sql &amp; informatica worth learning? de and swe?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bi3og", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686978781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi, i&amp;#39;m a college cs student who just started my first internship in de. i&amp;#39;m learning/using oracle pl/sql, informatica, and some other stuff. i have 2 questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;is oracle pl/sql and informatica still worth learning, it seems like more and more companies are moving away from it (just my general impression).  a lot of more job descriptions have different dbs/tools. the pl/sql language and the gui of informatica specifically feel kind of outdated&lt;/li&gt;\n&lt;li&gt;i was initially looking for swe internships but landed a de internship instead. don&amp;#39;t mind it, but given the more opportunities/higher pay, I&amp;#39;d prefer to go into swe. I&amp;#39;ve heard de can be seen as a subset of backend/swe. is this still true for oracle pl/sql and informatica? if not, what is this true for. kind of wanting to see if i can use this as an opportunity to get into backend dbs and pipelines in swe backend&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;any advice is much appreciated, thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14bi3og", "is_robot_indexable": true, "report_reasons": null, "author": "yellowcoloredcrayon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bi3og/first_de_internship_lots_of_questions_are_plsql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bi3og/first_de_internship_lots_of_questions_are_plsql/", "subreddit_subscribers": 110881, "created_utc": 1686978781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I overheard a conversation that data-oriented programming is old school and Data oriented programming is the better way to programming. \n\nCan I use DOP outside of game development? I really want to understand more", "author_fullname": "t2_8skbp1qj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "?? Server efficiency through Data Oriented Design??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b7nu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686948543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I overheard a conversation that data-oriented programming is old school and Data oriented programming is the better way to programming. &lt;/p&gt;\n\n&lt;p&gt;Can I use DOP outside of game development? I really want to understand more&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b7nu4", "is_robot_indexable": true, "report_reasons": null, "author": "Positive-Fly4773", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b7nu4/server_efficiency_through_data_oriented_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b7nu4/server_efficiency_through_data_oriented_design/", "subreddit_subscribers": 110881, "created_utc": 1686948543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the fastest way to run a small etl pipeline on a single node databricks cluster. \n\nUsing sqlalchemy and polars to read and wrangle is quite a bit faster than using the overhead of pyspark. The bottleneck is writing if data is smallish yet not tiny (e.g. 5 to 10 million rows).\n\nWhat is the best approach here? Pandas to_sql with sqlalchemy engine set to fast_executemany=True works okayish. Is molding the pandas df into a spark df and then using pyspark to export faster? Are there other faster options?", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing to Azure SQL server from single node DBX cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b7475", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686947215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the fastest way to run a small etl pipeline on a single node databricks cluster. &lt;/p&gt;\n\n&lt;p&gt;Using sqlalchemy and polars to read and wrangle is quite a bit faster than using the overhead of pyspark. The bottleneck is writing if data is smallish yet not tiny (e.g. 5 to 10 million rows).&lt;/p&gt;\n\n&lt;p&gt;What is the best approach here? Pandas to_sql with sqlalchemy engine set to fast_executemany=True works okayish. Is molding the pandas df into a spark df and then using pyspark to export faster? Are there other faster options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b7475", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b7475/writing_to_azure_sql_server_from_single_node_dbx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14b7475/writing_to_azure_sql_server_from_single_node_dbx/", "subreddit_subscribers": 110881, "created_utc": 1686947215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow Data Engineers,\n\nI'm currently working at a tech company as a Senior Data Engineer; I was laid off last year by another company, and while looking for my current job, I noticed many hiring managers like more to have a Software Engineer title in the resume.\n\nMy current job has lots of Software Engineering work (e.g., developing AWS lambdas in TypeScript), so I don't feel too distant from an SWE. I also want to always target more technical DE roles instead of analytical ones (like my previous role at FAANG).\n\nSo I was wondering if it's okay changing the title in my resume (for future jobs), from Senior Data Engineer to Senior Software Engineer (Data).\n\nIs there any potential downside when doing a background check?\n\n[View Poll](https://www.reddit.com/poll/14b12o5)", "author_fullname": "t2_5dn8ds4e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ok to change the title in my resume from Data Engineer to Software Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14b12o5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686932387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working at a tech company as a Senior Data Engineer; I was laid off last year by another company, and while looking for my current job, I noticed many hiring managers like more to have a Software Engineer title in the resume.&lt;/p&gt;\n\n&lt;p&gt;My current job has lots of Software Engineering work (e.g., developing AWS lambdas in TypeScript), so I don&amp;#39;t feel too distant from an SWE. I also want to always target more technical DE roles instead of analytical ones (like my previous role at FAANG).&lt;/p&gt;\n\n&lt;p&gt;So I was wondering if it&amp;#39;s okay changing the title in my resume (for future jobs), from Senior Data Engineer to Senior Software Engineer (Data).&lt;/p&gt;\n\n&lt;p&gt;Is there any potential downside when doing a background check?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/14b12o5\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14b12o5", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Muffin-8079", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1687364387266, "options": [{"text": "Changing title to Software Engineer, Data", "id": "23495915"}, {"text": "Changing title to Software Engineer", "id": "23495916"}, {"text": "Leaving title as Data Engineer", "id": "23495917"}, {"text": "View results", "id": "23495918"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 342, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14b12o5/is_it_ok_to_change_the_title_in_my_resume_from/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/14b12o5/is_it_ok_to_change_the_title_in_my_resume_from/", "subreddit_subscribers": 110881, "created_utc": 1686932387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Disclaimer: I'm not a data engineer. I'm evaluating my concept and want to validate its feasibility. Please let me know if this makes sense or can be performed differently. I don't believe these are technical bug / error questions. If anything I'm determining if I understand the concept (s).\n\n\nHypothetical Scenario: \n1. Dataset created from output of EVA Work package  Progress measurements \n2. Excel CSV file extracted from shared folder- Leads submit files for work packages daily for work packages they are responsible for. \n3. Data within range values validated by type of data and number of characters. (Expected ranges are based on a 4 tier scale defined by an upper and lower control limit.)\n4. Business rules based on 17 sql logic statements using fuzzy adjectives. \n5. Staging database is \"Temp_table\" located in MS SQL server.\n6. Target table is \"WPPM_Production_Table\" in MS SQL server data warehouse.\n\nQuestions: \n\nA. Would this be considered an ETL pipeline \nwith data processed in batch from source databases?\n\nB. Is data extraction from CSV files necessary? Or would the MDS add on for Excel make this easier with numerous leads and work packages?\n\nC. Data validation--is it better to validate at the application level or data warehouse based on the scenario? And is one better than the other? Would use of MDS invalidate the need for data validation in Excel? \n\nD. Would this scenario require creating aggregates? \n\nE. Are triggers more advisable if MDS is not used? Or does that even matter? \n\nF. Is there a specific data model that describes this scenario? \n\nG. Is it good practice / acceptable to maintain the Staging database and data warehouse on the same platform (i.e. MS SQL?)", "author_fullname": "t2_ph1t3ewm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice from experts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14bg2jh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686972257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I&amp;#39;m not a data engineer. I&amp;#39;m evaluating my concept and want to validate its feasibility. Please let me know if this makes sense or can be performed differently. I don&amp;#39;t believe these are technical bug / error questions. If anything I&amp;#39;m determining if I understand the concept (s).&lt;/p&gt;\n\n&lt;p&gt;Hypothetical Scenario: \n1. Dataset created from output of EVA Work package  Progress measurements \n2. Excel CSV file extracted from shared folder- Leads submit files for work packages daily for work packages they are responsible for. \n3. Data within range values validated by type of data and number of characters. (Expected ranges are based on a 4 tier scale defined by an upper and lower control limit.)\n4. Business rules based on 17 sql logic statements using fuzzy adjectives. \n5. Staging database is &amp;quot;Temp_table&amp;quot; located in MS SQL server.\n6. Target table is &amp;quot;WPPM_Production_Table&amp;quot; in MS SQL server data warehouse.&lt;/p&gt;\n\n&lt;p&gt;Questions: &lt;/p&gt;\n\n&lt;p&gt;A. Would this be considered an ETL pipeline \nwith data processed in batch from source databases?&lt;/p&gt;\n\n&lt;p&gt;B. Is data extraction from CSV files necessary? Or would the MDS add on for Excel make this easier with numerous leads and work packages?&lt;/p&gt;\n\n&lt;p&gt;C. Data validation--is it better to validate at the application level or data warehouse based on the scenario? And is one better than the other? Would use of MDS invalidate the need for data validation in Excel? &lt;/p&gt;\n\n&lt;p&gt;D. Would this scenario require creating aggregates? &lt;/p&gt;\n\n&lt;p&gt;E. Are triggers more advisable if MDS is not used? Or does that even matter? &lt;/p&gt;\n\n&lt;p&gt;F. Is there a specific data model that describes this scenario? &lt;/p&gt;\n\n&lt;p&gt;G. Is it good practice / acceptable to maintain the Staging database and data warehouse on the same platform (i.e. MS SQL?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14bg2jh", "is_robot_indexable": true, "report_reasons": null, "author": "Kobalt13mm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14bg2jh/need_advice_from_experts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14bg2jh/need_advice_from_experts/", "subreddit_subscribers": 110881, "created_utc": 1686972257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_54sig", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping dynamic websites using cloud based Scraper API and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_14b3yer", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": "transparent", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Jm-7mvf5ZyeJb0WsiK_5XC4MMsa3FlzqfTIDEmG5cXA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686939474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.adnansiddiqi.me", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.adnansiddiqi.me/scraping-dynamic-websites-using-scraper-api-and-python/?utm_source=reddit_de&amp;utm_medium=reddit&amp;utm_campaign=c_reddit_de_scraperapi_1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?auto=webp&amp;v=enabled&amp;s=0be9678b4647c2d6408183cce9b9aa0529608b7a", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5f86e79c73632718a0192d0770cef735ef71afe", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6036e446628cf00e86d298ac6686dc79c73c1300", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70b62381374986e58ee72a922723f53997296918", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d178dd6248e816a1a25776439716e1b990c0508", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90afa6ec317edb114bad30f7f0f876ebf1e9ba29", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/yQgcQcuDQDOMy--Ho_nWMPEBbfkEZoy4IZMsfxOYB8U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9817f3cab85ca2f9aa6f96bba6673b4654369567", "width": 1080, "height": 607}], "variants": {}, "id": "0ZTLqnRC7I-OtWL4YPQWymXBI_n4kH4DoAVlbgZ_Wrw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14b3yer", "is_robot_indexable": true, "report_reasons": null, "author": "pknerd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14b3yer/scraping_dynamic_websites_using_cloud_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.adnansiddiqi.me/scraping-dynamic-websites-using-scraper-api-and-python/?utm_source=reddit_de&amp;utm_medium=reddit&amp;utm_campaign=c_reddit_de_scraperapi_1", "subreddit_subscribers": 110881, "created_utc": 1686939474.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}