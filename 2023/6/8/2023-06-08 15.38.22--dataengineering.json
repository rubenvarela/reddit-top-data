{"kind": "Listing", "data": {"after": "t3_143wu16", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_al1087x2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"We have great datasets\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14442pi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 345, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 345, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ga_7RAoN8V433_JbVht6gqgSz82qr9tW8WqHe5FUDoE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686215302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0bv7bxlsfr4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0bv7bxlsfr4b1.jpg?auto=webp&amp;v=enabled&amp;s=5299cc7847620be17e95e54c423ddb6a4ec889ef", "width": 657, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/0bv7bxlsfr4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a252fcec0f4d26871e545eed29c5e4526d4dab5", "width": 108, "height": 168}, {"url": "https://preview.redd.it/0bv7bxlsfr4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43de77bc0dd46aa1a632fd532b0d693b1b77d803", "width": 216, "height": 336}, {"url": "https://preview.redd.it/0bv7bxlsfr4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=996a092469a1fc28b11d9c4156d2680b6b613a7a", "width": 320, "height": 498}, {"url": "https://preview.redd.it/0bv7bxlsfr4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6563c0ae0a82d35d03c3054b9019d50d184b1cb9", "width": 640, "height": 997}], "variants": {}, "id": "2SEkdAR-fDYOrIf2zGGCVLGBI4-9-IByrrCjsdsFqhI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14442pi", "is_robot_indexable": true, "report_reasons": null, "author": "OverratedDataScience", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14442pi/we_have_great_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0bv7bxlsfr4b1.jpg", "subreddit_subscribers": 109565, "created_utc": 1686215302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our dashboards at work basically have powerbi doing all the merging and aggregation.\n\nI am not positive if it's done by query or after pointed to the sql tables.\n\nIs that normal, or best practices? I would have built a view and pointed to the view.", "author_fullname": "t2_m3d4ku9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dashboards best practices - how much transformation should PowerBI or Tableau be doing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143t8vc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686182089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our dashboards at work basically have powerbi doing all the merging and aggregation.&lt;/p&gt;\n\n&lt;p&gt;I am not positive if it&amp;#39;s done by query or after pointed to the sql tables.&lt;/p&gt;\n\n&lt;p&gt;Is that normal, or best practices? I would have built a view and pointed to the view.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143t8vc", "is_robot_indexable": true, "report_reasons": null, "author": "DifficultyNext7666", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143t8vc/dashboards_best_practices_how_much_transformation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143t8vc/dashboards_best_practices_how_much_transformation/", "subreddit_subscribers": 109565, "created_utc": 1686182089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just finished a technical interview for a lead data engineer position. It is an hour long interview and spent the first half of it  going through SQL leetcode with complex window functions.\n\nAt around 40 mins mark I realised that they are just looking for a SQL guru and ignoring the facts that I have more to offers eg knowledge about AWS services, Terraforming infrastructure, data architecture, etc.\n\nIs this data engineering all about (being great with SQL) or did i make a good decision and asked to stop the interview at minute 45? What are your thoughts?", "author_fullname": "t2_8n43xmar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviewing for lead data engineer position.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143wn2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just finished a technical interview for a lead data engineer position. It is an hour long interview and spent the first half of it  going through SQL leetcode with complex window functions.&lt;/p&gt;\n\n&lt;p&gt;At around 40 mins mark I realised that they are just looking for a SQL guru and ignoring the facts that I have more to offers eg knowledge about AWS services, Terraforming infrastructure, data architecture, etc.&lt;/p&gt;\n\n&lt;p&gt;Is this data engineering all about (being great with SQL) or did i make a good decision and asked to stop the interview at minute 45? What are your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "143wn2y", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-River1467", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143wn2y/interviewing_for_lead_data_engineer_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143wn2y/interviewing_for_lead_data_engineer_position/", "subreddit_subscribers": 109565, "created_utc": 1686191104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nMy employer is offering 2k towards a class/certification.  I have extensive experience in SQL and have done data migrations the old school way of using stored procedures etc. I really want to get a better understanding of the tools and methods of running a data pipeline. I have limited experience with Python. Is there a recommendation for a good \u201call-in-one\u201d data engineering course I can take? I\u2019d really like to take up the offer of using the 2k towards this. \n\nThanks!", "author_fullname": "t2_7y30rgjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2k towards a course in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143gt1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686152909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;My employer is offering 2k towards a class/certification.  I have extensive experience in SQL and have done data migrations the old school way of using stored procedures etc. I really want to get a better understanding of the tools and methods of running a data pipeline. I have limited experience with Python. Is there a recommendation for a good \u201call-in-one\u201d data engineering course I can take? I\u2019d really like to take up the offer of using the 2k towards this. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143gt1s", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Fox940", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143gt1s/2k_towards_a_course_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143gt1s/2k_towards_a_course_in_data_engineering/", "subreddit_subscribers": 109565, "created_utc": 1686152909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "While there are some projects where you  can make a use case of their product and write a blog about it and you are a contributor. But are there any projects where you can contribute by building ETL pipelines for them?", "author_fullname": "t2_ens6kw85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it impossible to contribute to open source as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143us7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686186094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While there are some projects where you  can make a use case of their product and write a blog about it and you are a contributor. But are there any projects where you can contribute by building ETL pipelines for them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143us7i", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent-Tadpole-564", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143us7i/is_it_impossible_to_contribute_to_open_source_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143us7i/is_it_impossible_to_contribute_to_open_source_as/", "subreddit_subscribers": 109565, "created_utc": 1686186094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm learning about Presto and Trino, and this term \"Interactive querying\" comes up a lot. I have a hard time wrapping my head around it.   \n\n\nIf you are using Presto or Trino, why do you use it, compared to Simple hive query, for example?", "author_fullname": "t2_8r6amwln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What exactly is interactive querying? When do you need it? How is it different from running a couple of Hive SQL commands?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ts6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686183465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m learning about Presto and Trino, and this term &amp;quot;Interactive querying&amp;quot; comes up a lot. I have a hard time wrapping my head around it.   &lt;/p&gt;\n\n&lt;p&gt;If you are using Presto or Trino, why do you use it, compared to Simple hive query, for example?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143ts6n", "is_robot_indexable": true, "report_reasons": null, "author": "money_noob_007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143ts6n/what_exactly_is_interactive_querying_when_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143ts6n/what_exactly_is_interactive_querying_when_do_you/", "subreddit_subscribers": 109565, "created_utc": 1686183465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, I am a data/platform/software engineer with 3 YOE. I want to share my experience and challenges at work, and hopefully get some advice for my career in the next **2-4 years** from the community.\n\nI am hiding some of information to stay anonymous, but I think they should give a good idea of what my work is like.\n\n# Work experience / tech stack\n\nI was hired as a data engineer at first, but I only seriously worked on one data engineering project. In this 3 years, I worked on **a lot of stuff:**\n\n1. Serverless data serving system on AWS\n2. Airflow deployment / development / plugins / pipelines, I am the main guy supporting this platform \n3. CICD\n4. Python / Docker application\n5. FastAPI and ElasticSearch backend\n6. Python CPU/GPU image classification backend dev/testing (**I really enjoyed it** although it was mainly a previous senior's work)\n7. Webscraping \n8. Some Spark for unstructured data (batch)\n9. Snowflake / dbt (batch)\n10. Terraform / AWS CDK\n\nMy manager has a very wide range of work scope, so I follow his scope and worked on a lot of stuff, which is truly a blessing but I don't get very deep into these skills. Recently my work has been more and more on platform capabilities and enabling other engineers.\n\n# Team background / culture\n\nThe entire department is big enough for my work to make impact, but we are not a team with strong engineering background, most people are analytics/data science background. People/management are friendly and supportive.\n\nMy immediate team is a central platform team. My immediate teammates/managers are very good engineers, but our team's work scope is too wide so we are not focused enough. There are some more good engineers but we don't have a lot of knowledge sharing.\n\n# My challenges / concerns / struggles\n\n1. Starting to feel some politics\n2. Tech debts are not usually prioritised, not like some companies which will regularly dedicate sprints to fix tech debts. This leads to problems / inefficiency down the road\n3. Projects are not well planned, and designs can change on a daily basis, I was really stressed on some projects, because they progress slowly and time became tighter and tighter\n5. Tickets story points poorly estimated, usually because we are bringing new stuff to our projects and have less experience \n4. (might be the reason of 3) my immediate team has no project manager, and no proper analyst\n5. My work scope is too damn wide. I am not sure this is good or bad, I certainly learn so so much, and I continue to become important to the team. But I don't get a chance to be very good at something. And I get very distracted in context switching, and eventually stressed out.\n6. My manager is giving less code/implementation review, because he's working on higher level design, and some tech stack is also new for him.\n\n# Things I enjoy more\n\n1. Writing code, testing it, running it. **Things with short feedback loop (who doesn't like it)**.\n2. Building applications.\n\n# Things I enjoy less\n\nAnalytics and looking at graphs. I think analytical thinking is a crucial skill for any one any job at a high level, but actually doing analytic work is painful for me.\n\n# Salary\n\nAfter all, the paycheque is what ultimately matters for a job. According to \\[this collection\\]([https://www.reddit.com/r/dataengineering/comments/npxcqc/quarterly\\_salary\\_discussion/](https://www.reddit.com/r/dataengineering/comments/npxcqc/quarterly_salary_discussion/)), I am around / above median for engineers with my YOE in my country. I am quite happy with my pay now.\n\n# Things I want to get advice on\n\n1. Should I change job? If yes, should I aim for FANG/tier 1 companies, or smaller companies / consulting, or let's be bold, trading firms?\n2. Should I stay in data platform career? Or move into more pure data engineering? And I am interested in software/backend engineer role too\n   1. I actually got an offer as a software/backend engineer from a biggish tech company, but I rejected it due to multiple reasons, mostly job security. \n3. Stay or leave, what should I ask from manager to help me progress?\n4. Is it better for me to be in a more independent situation, or get more guidance / review from senior engineers?\n5. Should I keep a wide scope or more focused scope?\n6. Any other thought or advice you have\n\n# Finally\n\nThank you for reading and providing advice! Feel free to ask anything as well I will try to answer what I know.", "author_fullname": "t2_8zp9kjxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice wanted - data/platform/software engineer with 3 YOE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449g7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686233339.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, I am a data/platform/software engineer with 3 YOE. I want to share my experience and challenges at work, and hopefully get some advice for my career in the next &lt;strong&gt;2-4 years&lt;/strong&gt; from the community.&lt;/p&gt;\n\n&lt;p&gt;I am hiding some of information to stay anonymous, but I think they should give a good idea of what my work is like.&lt;/p&gt;\n\n&lt;h1&gt;Work experience / tech stack&lt;/h1&gt;\n\n&lt;p&gt;I was hired as a data engineer at first, but I only seriously worked on one data engineering project. In this 3 years, I worked on &lt;strong&gt;a lot of stuff:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Serverless data serving system on AWS&lt;/li&gt;\n&lt;li&gt;Airflow deployment / development / plugins / pipelines, I am the main guy supporting this platform &lt;/li&gt;\n&lt;li&gt;CICD&lt;/li&gt;\n&lt;li&gt;Python / Docker application&lt;/li&gt;\n&lt;li&gt;FastAPI and ElasticSearch backend&lt;/li&gt;\n&lt;li&gt;Python CPU/GPU image classification backend dev/testing (&lt;strong&gt;I really enjoyed it&lt;/strong&gt; although it was mainly a previous senior&amp;#39;s work)&lt;/li&gt;\n&lt;li&gt;Webscraping &lt;/li&gt;\n&lt;li&gt;Some Spark for unstructured data (batch)&lt;/li&gt;\n&lt;li&gt;Snowflake / dbt (batch)&lt;/li&gt;\n&lt;li&gt;Terraform / AWS CDK&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My manager has a very wide range of work scope, so I follow his scope and worked on a lot of stuff, which is truly a blessing but I don&amp;#39;t get very deep into these skills. Recently my work has been more and more on platform capabilities and enabling other engineers.&lt;/p&gt;\n\n&lt;h1&gt;Team background / culture&lt;/h1&gt;\n\n&lt;p&gt;The entire department is big enough for my work to make impact, but we are not a team with strong engineering background, most people are analytics/data science background. People/management are friendly and supportive.&lt;/p&gt;\n\n&lt;p&gt;My immediate team is a central platform team. My immediate teammates/managers are very good engineers, but our team&amp;#39;s work scope is too wide so we are not focused enough. There are some more good engineers but we don&amp;#39;t have a lot of knowledge sharing.&lt;/p&gt;\n\n&lt;h1&gt;My challenges / concerns / struggles&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Starting to feel some politics&lt;/li&gt;\n&lt;li&gt;Tech debts are not usually prioritised, not like some companies which will regularly dedicate sprints to fix tech debts. This leads to problems / inefficiency down the road&lt;/li&gt;\n&lt;li&gt;Projects are not well planned, and designs can change on a daily basis, I was really stressed on some projects, because they progress slowly and time became tighter and tighter&lt;/li&gt;\n&lt;li&gt;Tickets story points poorly estimated, usually because we are bringing new stuff to our projects and have less experience &lt;/li&gt;\n&lt;li&gt;(might be the reason of 3) my immediate team has no project manager, and no proper analyst&lt;/li&gt;\n&lt;li&gt;My work scope is too damn wide. I am not sure this is good or bad, I certainly learn so so much, and I continue to become important to the team. But I don&amp;#39;t get a chance to be very good at something. And I get very distracted in context switching, and eventually stressed out.&lt;/li&gt;\n&lt;li&gt;My manager is giving less code/implementation review, because he&amp;#39;s working on higher level design, and some tech stack is also new for him.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Things I enjoy more&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Writing code, testing it, running it. &lt;strong&gt;Things with short feedback loop (who doesn&amp;#39;t like it)&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Building applications.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Things I enjoy less&lt;/h1&gt;\n\n&lt;p&gt;Analytics and looking at graphs. I think analytical thinking is a crucial skill for any one any job at a high level, but actually doing analytic work is painful for me.&lt;/p&gt;\n\n&lt;h1&gt;Salary&lt;/h1&gt;\n\n&lt;p&gt;After all, the paycheque is what ultimately matters for a job. According to [this collection](&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/npxcqc/quarterly_salary_discussion/\"&gt;https://www.reddit.com/r/dataengineering/comments/npxcqc/quarterly_salary_discussion/&lt;/a&gt;), I am around / above median for engineers with my YOE in my country. I am quite happy with my pay now.&lt;/p&gt;\n\n&lt;h1&gt;Things I want to get advice on&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I change job? If yes, should I aim for FANG/tier 1 companies, or smaller companies / consulting, or let&amp;#39;s be bold, trading firms?&lt;/li&gt;\n&lt;li&gt;Should I stay in data platform career? Or move into more pure data engineering? And I am interested in software/backend engineer role too\n\n&lt;ol&gt;\n&lt;li&gt;I actually got an offer as a software/backend engineer from a biggish tech company, but I rejected it due to multiple reasons, mostly job security. &lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Stay or leave, what should I ask from manager to help me progress?&lt;/li&gt;\n&lt;li&gt;Is it better for me to be in a more independent situation, or get more guidance / review from senior engineers?&lt;/li&gt;\n&lt;li&gt;Should I keep a wide scope or more focused scope?&lt;/li&gt;\n&lt;li&gt;Any other thought or advice you have&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Finally&lt;/h1&gt;\n\n&lt;p&gt;Thank you for reading and providing advice! Feel free to ask anything as well I will try to answer what I know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1449g7y", "is_robot_indexable": true, "report_reasons": null, "author": "Jazzlike-Pollution-4", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1449g7y/career_advice_wanted_dataplatformsoftware/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1449g7y/career_advice_wanted_dataplatformsoftware/", "subreddit_subscribers": 109565, "created_utc": 1686231228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most companies are rushing to build or incorporate #gpt in their value chain. #genai. Do you agree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 135, "top_awarded_type": null, "hide_score": false, "name": "t3_1449ezb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ql36a1u5XiI16sn5HqRzdhG8zw_inVMLnQL9oCUH2rM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686231137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/228ijxavqs4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/228ijxavqs4b1.jpg?auto=webp&amp;v=enabled&amp;s=ade6d01ef12f1c754f0f21503c425dabd1f616e7", "width": 508, "height": 491}, "resolutions": [{"url": "https://preview.redd.it/228ijxavqs4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5987f1e009464ea5b5f7dcc2cb75030621336611", "width": 108, "height": 104}, {"url": "https://preview.redd.it/228ijxavqs4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ef2b8d9f551a5174cc3c049aa498b7ea63e1485", "width": 216, "height": 208}, {"url": "https://preview.redd.it/228ijxavqs4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf109415d50e8e1b020bdc9840e1e30f9b0cea6a", "width": 320, "height": 309}], "variants": {}, "id": "ef1ShIJxSr2mp7vHw6SFkjRj1YADOVuW0kbsWj4Buyo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1449ezb", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1449ezb/most_companies_are_rushing_to_build_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/228ijxavqs4b1.jpg", "subreddit_subscribers": 109565, "created_utc": 1686231137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious - how big are the data lakes you're actively maintaining? Daily input/output? Mostly static or always being queried? What are you using for it? What works, what doesn't?\n\nForget the buzzwords - data lake, data lakehouse, data marsh, whatever you call it - the thing that isn't a data warehouse that people use to query", "author_fullname": "t2_40buwhod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time to Flex - How Big is your Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143x8kz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686196635.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686192800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious - how big are the data lakes you&amp;#39;re actively maintaining? Daily input/output? Mostly static or always being queried? What are you using for it? What works, what doesn&amp;#39;t?&lt;/p&gt;\n\n&lt;p&gt;Forget the buzzwords - data lake, data lakehouse, data marsh, whatever you call it - the thing that isn&amp;#39;t a data warehouse that people use to query&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143x8kz", "is_robot_indexable": true, "report_reasons": null, "author": "DrZachman", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143x8kz/time_to_flex_how_big_is_your_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143x8kz/time_to_flex_how_big_is_your_data_lake/", "subreddit_subscribers": 109565, "created_utc": 1686192800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently learning elastic search..", "author_fullname": "t2_ur0ro3ju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good resource or blogs for Elastic Search ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ws10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently learning elastic search..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143ws10", "is_robot_indexable": true, "report_reasons": null, "author": "chaddlb0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143ws10/any_good_resource_or_blogs_for_elastic_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143ws10/any_good_resource_or_blogs_for_elastic_search/", "subreddit_subscribers": 109565, "created_utc": 1686191476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My background is mostly in Microsoft and Azure products, but I have a potential contract opportunity working with GCP.  Not sure if I'll be successful or not, so it's potentially a moot point.  But the opportunity to get away from the Microsoft treadmill is appealing... I'd be more than happy if I never saw ADF/Synapse ever again.  \n\nCan anyone talk about the pros and cons of Google's stack... and particularly how it lines up against Azure?", "author_fullname": "t2_5txrt2ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ubuc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686184898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My background is mostly in Microsoft and Azure products, but I have a potential contract opportunity working with GCP.  Not sure if I&amp;#39;ll be successful or not, so it&amp;#39;s potentially a moot point.  But the opportunity to get away from the Microsoft treadmill is appealing... I&amp;#39;d be more than happy if I never saw ADF/Synapse ever again.  &lt;/p&gt;\n\n&lt;p&gt;Can anyone talk about the pros and cons of Google&amp;#39;s stack... and particularly how it lines up against Azure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "143ubuc", "is_robot_indexable": true, "report_reasons": null, "author": "tarzanboy76", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143ubuc/azure_to_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143ubuc/azure_to_gcp/", "subreddit_subscribers": 109565, "created_utc": 1686184898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tech stack: \nAzure data lake gen 2 storage ;\nAzure data factory ;\nDatabricks premium ;\nMedallion Architecture ;\n\nTo preface: my organization has 20 different source systems across 15 different servers. Each server has a SQL DB with like data.  That data is extracted from the source sql db and stored in bronze layer as parquet files. \n\n\nI then have 1 silver databricks transformation notebook that the source system name is passed through from an ADF parameter to filter the source and target data.  \n\nAt the end of the procedure, I complete a merge into (update and insert) into a delta table that is partitioned by source system.  \n\nIn the merge into routine I am filtering the source AND target by the source system parameter. \n\nI have 1 pipeline that is triggered by the source system name (20 different triggers that all start at the same time, but with a different source system parameter). \n\nEven though my delta table is partitioned by source system and my notebook is being passed a source system, I still receive an error: files were added to partition source system = x , by a concurrent update. \n\n#my question\nIs there any way around this without creating a silver.deltatable_sourcesystem?\n\n\n\n\nThis is my redacted merge into code: \n\np_sourceSystemName = 'SourceSystem1'\n\ndeltaTable =  f\"delta.`abfss://pathtotable/Silver_mytable`\"\n\n-Get the list of column names for the source table\nsource_cols = spark.table(\"test\").columns\n\n-Create a temporary table with distinct records from the source table\nspark.table(\"test\").select(source_cols).distinct().createOrReplaceTempView(\"temp_source\")\n\n\n-Update existing records\nsql_update = f\"\"\"MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = '{p_sourceSystemName}'\n) AS source\n\nON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue != source.HashValue\nAND target.currentIndicator = 'Y'\nAND target.SourceSystem = '{p_sourceSystemName}'\n\nWHEN MATCHED THEN\n    UPDATE SET\n        currentIndicator = 'N',\n        ValidToTs = current_timestamp,\n        actionType = 'Update'\n\"\"\"\n- insert new records\nsql_insert = f\"\"\"MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = '{p_sourceSystemName}'\n) AS source\nON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue = source.HashValue\nAND target.currentIndicator = 'Y'\nAND target.SourceSystem = '{p_sourceSystemName}'\n\nWHEN NOT MATCHED THEN\n    INSERT ({\", \".join(source_cols)}) -- dynamically construct the column list\n    VALUES ({\", \".join([\"source.\" + col for col in source_cols])});  -- dynamically construct the value list\n\"\"\"\n\n- Execute the update and insert statements\nspark.sql(sql_update)\nspark.sql(sql_insert)", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Merge into", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143pi7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686173956.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686173087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tech stack: \nAzure data lake gen 2 storage ;\nAzure data factory ;\nDatabricks premium ;\nMedallion Architecture ;&lt;/p&gt;\n\n&lt;p&gt;To preface: my organization has 20 different source systems across 15 different servers. Each server has a SQL DB with like data.  That data is extracted from the source sql db and stored in bronze layer as parquet files. &lt;/p&gt;\n\n&lt;p&gt;I then have 1 silver databricks transformation notebook that the source system name is passed through from an ADF parameter to filter the source and target data.  &lt;/p&gt;\n\n&lt;p&gt;At the end of the procedure, I complete a merge into (update and insert) into a delta table that is partitioned by source system.  &lt;/p&gt;\n\n&lt;p&gt;In the merge into routine I am filtering the source AND target by the source system parameter. &lt;/p&gt;\n\n&lt;p&gt;I have 1 pipeline that is triggered by the source system name (20 different triggers that all start at the same time, but with a different source system parameter). &lt;/p&gt;\n\n&lt;p&gt;Even though my delta table is partitioned by source system and my notebook is being passed a source system, I still receive an error: files were added to partition source system = x , by a concurrent update. &lt;/p&gt;\n\n&lt;h1&gt;my question&lt;/h1&gt;\n\n&lt;p&gt;Is there any way around this without creating a silver.deltatable_sourcesystem?&lt;/p&gt;\n\n&lt;p&gt;This is my redacted merge into code: &lt;/p&gt;\n\n&lt;p&gt;p_sourceSystemName = &amp;#39;SourceSystem1&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;deltaTable =  f&amp;quot;delta.&lt;code&gt;abfss://pathtotable/Silver_mytable&lt;/code&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;-Get the list of column names for the source table\nsource_cols = spark.table(&amp;quot;test&amp;quot;).columns&lt;/p&gt;\n\n&lt;p&gt;-Create a temporary table with distinct records from the source table\nspark.table(&amp;quot;test&amp;quot;).select(source_cols).distinct().createOrReplaceTempView(&amp;quot;temp_source&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;-Update existing records\nsql_update = f&amp;quot;&amp;quot;&amp;quot;MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;\n) AS source&lt;/p&gt;\n\n&lt;p&gt;ON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue != source.HashValue\nAND target.currentIndicator = &amp;#39;Y&amp;#39;\nAND target.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;WHEN MATCHED THEN\n    UPDATE SET\n        currentIndicator = &amp;#39;N&amp;#39;,\n        ValidToTs = current_timestamp,\n        actionType = &amp;#39;Update&amp;#39;\n&amp;quot;&amp;quot;&amp;quot;\n- insert new records\nsql_insert = f&amp;quot;&amp;quot;&amp;quot;MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;\n) AS source\nON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue = source.HashValue\nAND target.currentIndicator = &amp;#39;Y&amp;#39;\nAND target.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;WHEN NOT MATCHED THEN\n    INSERT ({&amp;quot;, &amp;quot;.join(source_cols)}) -- dynamically construct the column list\n    VALUES ({&amp;quot;, &amp;quot;.join([&amp;quot;source.&amp;quot; + col for col in source_cols])});  -- dynamically construct the value list\n&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Execute the update and insert statements\nspark.sql(sql_update)\nspark.sql(sql_insert)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143pi7b", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143pi7b/databricks_merge_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143pi7b/databricks_merge_into/", "subreddit_subscribers": 109565, "created_utc": 1686173087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ayc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Daft: A High-Performance Distributed Dataframe Library for Multimodal Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "name": "t3_143g5la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jwGyLjzZbEIgBtBgbuKaWsGg1LxyH7-fX-cny7P6PQc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686151394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.getdaft.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.getdaft.io/p/introducing-daft-a-high-performance", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?auto=webp&amp;v=enabled&amp;s=af44267447f9459af1d070fc65d1957009020626", "width": 946, "height": 369}, "resolutions": [{"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a645bf20db63c3833dd03a6b7c5637d856189d42", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=266bc632d794474fd89bbdc267fdb0f9b6165094", "width": 216, "height": 84}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3da78b4472f60f2f820e4e7beb924ec94e3c53b", "width": 320, "height": 124}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8df22713f1d50180e70199fb801c3b8e2c0dd566", "width": 640, "height": 249}], "variants": {}, "id": "kEXEo96ULUyl0I7ys3DrbgOGio_zYSY3_GKj6DhNg2Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143g5la", "is_robot_indexable": true, "report_reasons": null, "author": "fridder", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143g5la/introducing_daft_a_highperformance_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.getdaft.io/p/introducing-daft-a-high-performance", "subreddit_subscribers": 109565, "created_utc": 1686151394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I was thinking about using MySQL to host a database, but wanted to know if it was server based / on premise or could be cloud based?\n\nI've heard people reference Google Cloud services for serverless DBMS? ", "author_fullname": "t2_7ox5swxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I share a MySQL database over the cloud with someone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449cpz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686231274.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686230989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I was thinking about using MySQL to host a database, but wanted to know if it was server based / on premise or could be cloud based?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard people reference Google Cloud services for serverless DBMS? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1449cpz", "is_robot_indexable": true, "report_reasons": null, "author": "Use_Clean", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1449cpz/can_i_share_a_mysql_database_over_the_cloud_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1449cpz/can_i_share_a_mysql_database_over_the_cloud_with/", "subreddit_subscribers": 109565, "created_utc": 1686230989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks, I'm a data engineer worked on both cloud and on-prem solutions, my current mission is to Migrate from Hortonworks to Cloudera (Hadoop storage , Spark workloads, Kafka brokers, Flink, ... ) and wondering what is the future of On-prem solution in the Data Engineering Space ?\nI'm assuming that with big companies like banks, insurance, ... that are very picky about owning their data instead of hosting it in a 3 party storage provider will still use it. \nAlso is it true that sometimes Cloud Data Architecture might become more expensive that those On-prem ?", "author_fullname": "t2_dkcukl0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future of On-prem Big Data Solution ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1444ote", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686217430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks, I&amp;#39;m a data engineer worked on both cloud and on-prem solutions, my current mission is to Migrate from Hortonworks to Cloudera (Hadoop storage , Spark workloads, Kafka brokers, Flink, ... ) and wondering what is the future of On-prem solution in the Data Engineering Space ?\nI&amp;#39;m assuming that with big companies like banks, insurance, ... that are very picky about owning their data instead of hosting it in a 3 party storage provider will still use it. \nAlso is it true that sometimes Cloud Data Architecture might become more expensive that those On-prem ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1444ote", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional-Wheel-7854", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1444ote/future_of_onprem_big_data_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1444ote/future_of_onprem_big_data_solution/", "subreddit_subscribers": 109565, "created_utc": 1686217430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As in what the resources could be.  \nI want to learn more about the back story from the ground up. I want to learn about parallel programming and  high efficient processing of large data.  Any comprehensive theoretical textbook would be fine with me, although not limited to not just 0theory.", "author_fullname": "t2_ftuqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I learn in depth about distributed systems and distributed computing from a traditional computer science perspective?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1442sti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686210642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in what the resources could be.&lt;br/&gt;\nI want to learn more about the back story from the ground up. I want to learn about parallel programming and  high efficient processing of large data.  Any comprehensive theoretical textbook would be fine with me, although not limited to not just 0theory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1442sti", "is_robot_indexable": true, "report_reasons": null, "author": "philonoist", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1442sti/where_can_i_learn_in_depth_about_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1442sti/where_can_i_learn_in_depth_about_distributed/", "subreddit_subscribers": 109565, "created_utc": 1686210642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found this article describing an ELT pipeline.  It\u2019s a good read, but I\u2019m struggling to understand why you would store a data frame as parquet and store on Google Cloud Storage and then use a BQ Load Job to put it into BQ to transform and store in a BQ table.  Why not cut out the Cloud Storage step and put it directly into BQ?  Any ideas?", "author_fullname": "t2_5065w9mg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sky-Pipe: Prefect-GCP-dbt Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_143xvwc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TDhxRwWd7jcI-b7f-ZC7F5SXgnQBJuw5gYXtCXmDPWw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686194639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jackskylord.medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this article describing an ELT pipeline.  It\u2019s a good read, but I\u2019m struggling to understand why you would store a data frame as parquet and store on Google Cloud Storage and then use a BQ Load Job to put it into BQ to transform and store in a BQ table.  Why not cut out the Cloud Storage step and put it directly into BQ?  Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jackskylord.medium.com/sky-pipe-prefect-gcp-dbt-data-pipeline-80561107c38f", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?auto=webp&amp;v=enabled&amp;s=6a57dfcad039d0ec448dad3dbd92f7a5a5aa2c2a", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89a2f638491c9ddae01840225c197c4359dab5ce", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02237bea5a4ec78593ba28526a6ad716e1b65d7d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e16472eb10e9a958c2e3a0c07053b944d0a9e0df", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed293de1c2af0ca87d5a1cbc954125aad24a4d03", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a1e127fea200ccf0eab8fb901c422825c05ede4", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4ff12c625dc37c1a955168358b0644794f12c0b", "width": 1080, "height": 810}], "variants": {}, "id": "6V3XW5iiLUmun0rfq3Lz5QlX4QVzL7-Y8JQI4TRZuDI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143xvwc", "is_robot_indexable": true, "report_reasons": null, "author": "kvotheTHEinquisitor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143xvwc/skypipe_prefectgcpdbt_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jackskylord.medium.com/sky-pipe-prefect-gcp-dbt-data-pipeline-80561107c38f", "subreddit_subscribers": 109565, "created_utc": 1686194639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using this custom data validation library called Cerberus. Cerberus has a pre-built function, validate(), that validates data based on a table schema and set of validation rules that you give it. The function can either take in a row(in the form of a dictionary), or an entire pandas dataframe. \n\nThe function itself can take a really long time if the amount of data is large. Is there any way that I can partition the data and run the function against the partitions on multiple clusters in parallel?\n\nI've tried looking at many of Sparks features and couldn't figure out a solution ):", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help integrating Spark with an open source data validation library.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143meqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686166010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using this custom data validation library called Cerberus. Cerberus has a pre-built function, validate(), that validates data based on a table schema and set of validation rules that you give it. The function can either take in a row(in the form of a dictionary), or an entire pandas dataframe. &lt;/p&gt;\n\n&lt;p&gt;The function itself can take a really long time if the amount of data is large. Is there any way that I can partition the data and run the function against the partitions on multiple clusters in parallel?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried looking at many of Sparks features and couldn&amp;#39;t figure out a solution ):&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143meqi", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/143meqi/need_some_help_integrating_spark_with_an_open/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143meqi/need_some_help_integrating_spark_with_an_open/", "subreddit_subscribers": 109565, "created_utc": 1686166010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I did check with some of my DE friends. I asked them If they do any checks they didn't.\n\nHence, I'm wondering how do you guys minimally check what you ingest? I'd be using Databricks soon, is there any guide that I should be looking at.\n\n&amp;#x200B;\n\nThanks.", "author_fullname": "t2_ecope", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data integrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144avk3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686234704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did check with some of my DE friends. I asked them If they do any checks they didn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;Hence, I&amp;#39;m wondering how do you guys minimally check what you ingest? I&amp;#39;d be using Databricks soon, is there any guide that I should be looking at.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "144avk3", "is_robot_indexable": true, "report_reasons": null, "author": "snip3r77", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144avk3/data_integrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144avk3/data_integrity/", "subreddit_subscribers": 109565, "created_utc": 1686234704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Backdrop (a student) working in a startup with a small dev team.\n\nWe query for data (time series with updates every 50ms) from different sources (WebSockets, REST API, etc.) and store it in a database (PostgreSQL), then display it via Grafana with alerts. All of this is behind our aws vpc.\n\nThe current idea is to write a Python script, connect to all these data sources, then store it in our aws rds.\n\nWonder if there's a better way to do things. I heard recommendations about Kafka and Prometheus. I would appreciate some guidance/articles on whether there's a way this kinda work is usually done, esp on the query and store side. \n\nI would appreciate low maintenance, batteries-included, one-person buildable solution. And also a \"we need 5 data engineers and 3 dev ops to build and maintain this\" solution.", "author_fullname": "t2_29dbxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saas solution for a project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144aujj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686234635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Backdrop (a student) working in a startup with a small dev team.&lt;/p&gt;\n\n&lt;p&gt;We query for data (time series with updates every 50ms) from different sources (WebSockets, REST API, etc.) and store it in a database (PostgreSQL), then display it via Grafana with alerts. All of this is behind our aws vpc.&lt;/p&gt;\n\n&lt;p&gt;The current idea is to write a Python script, connect to all these data sources, then store it in our aws rds.&lt;/p&gt;\n\n&lt;p&gt;Wonder if there&amp;#39;s a better way to do things. I heard recommendations about Kafka and Prometheus. I would appreciate some guidance/articles on whether there&amp;#39;s a way this kinda work is usually done, esp on the query and store side. &lt;/p&gt;\n\n&lt;p&gt;I would appreciate low maintenance, batteries-included, one-person buildable solution. And also a &amp;quot;we need 5 data engineers and 3 dev ops to build and maintain this&amp;quot; solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "144aujj", "is_robot_indexable": true, "report_reasons": null, "author": "On3non1y", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144aujj/saas_solution_for_a_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144aujj/saas_solution_for_a_project/", "subreddit_subscribers": 109565, "created_utc": 1686234635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a very junir data engineer and need to set up a flow in ADF, but im having a hard time finding a solid enough logic for identifying unique rows and how the update logic will work.  \n\n\nEssentially what we get from our API query is: \n\n\\- Article\\_ID   \n\\- Store\\_Id  \n\\- ChckReasonCode  \n\\- Reception\\_Date  \n\\- Days  \n\n\nWhat we want to measure is \"Days\", which basically tells us how many days an Article has been in the stores list.   \n\n\nEvery day we get new data from an API. Problem is that one unique row can have the same Article\\_id, Store\\_id and CheckReasonCode, but the Reception\\_Date value will always reflect current day.  \n\n\nSo how can I update the dwh row, if I cannot link the newly added data already exisiting rows?  \n\n\nI was thinking of doing something of a check where Article\\_ID,Store\\_Id, and ChkReasonCode is grouped, and something like a \"If Reception date is only +1 from already existing dwh row\", update dwh row with new \"Days\" value.  \n\n\nHowever, this is not very good in terms of error handling, but I cannot find a different approach.  \n\n\nAny help will be much appreciated.  If anything is unclear ill be happy to try to explain it more thoroughly.", "author_fullname": "t2_xcbzsw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly Changing Dimensions Update logic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1447vf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686227173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a very junir data engineer and need to set up a flow in ADF, but im having a hard time finding a solid enough logic for identifying unique rows and how the update logic will work.  &lt;/p&gt;\n\n&lt;p&gt;Essentially what we get from our API query is: &lt;/p&gt;\n\n&lt;p&gt;- Article_ID&lt;br/&gt;\n- Store_Id&lt;br/&gt;\n- ChckReasonCode&lt;br/&gt;\n- Reception_Date&lt;br/&gt;\n- Days  &lt;/p&gt;\n\n&lt;p&gt;What we want to measure is &amp;quot;Days&amp;quot;, which basically tells us how many days an Article has been in the stores list.   &lt;/p&gt;\n\n&lt;p&gt;Every day we get new data from an API. Problem is that one unique row can have the same Article_id, Store_id and CheckReasonCode, but the Reception_Date value will always reflect current day.  &lt;/p&gt;\n\n&lt;p&gt;So how can I update the dwh row, if I cannot link the newly added data already exisiting rows?  &lt;/p&gt;\n\n&lt;p&gt;I was thinking of doing something of a check where Article_ID,Store_Id, and ChkReasonCode is grouped, and something like a &amp;quot;If Reception date is only +1 from already existing dwh row&amp;quot;, update dwh row with new &amp;quot;Days&amp;quot; value.  &lt;/p&gt;\n\n&lt;p&gt;However, this is not very good in terms of error handling, but I cannot find a different approach.  &lt;/p&gt;\n\n&lt;p&gt;Any help will be much appreciated.  If anything is unclear ill be happy to try to explain it more thoroughly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1447vf6", "is_robot_indexable": true, "report_reasons": null, "author": "useyourname89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1447vf6/slowly_changing_dimensions_update_logic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1447vf6/slowly_changing_dimensions_update_logic/", "subreddit_subscribers": 109565, "created_utc": 1686227173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \n\n\njust thinking about developer experience of building data pipeline(s). Nowadays, if I want to build a new data pipeline, I use Meltano to extract data, and dbt to transform it. I load everything to PostgreSQL, or Snowflake.   \n\n\nI usually build quite simple pipelines (just a few data sources - hubspot, GA, etc.), and therefore I run them in GitLab CI/CD, or GitHub actions.    \n\n\nI wanted to create some tool for it where I would just say \"hey, i want this data source and this data source, i want to load it there, and connect there\" but I ended up that it would be just \"configuration for configuration\". \ud83d\ude03 Maybe, I have just tunel view, or not enough experience w/ data engineering (since I was full stack dev for \\~6 years and to data engineering I switched recently) but is there a batter way on how to start projects without need to write tons of \"boilerplate\" code/configuration in tools like Meltano, dbt, YAML(s) for GitLab/GitHub, etc.?", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best experience for building data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1443xy8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686214858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,   &lt;/p&gt;\n\n&lt;p&gt;just thinking about developer experience of building data pipeline(s). Nowadays, if I want to build a new data pipeline, I use Meltano to extract data, and dbt to transform it. I load everything to PostgreSQL, or Snowflake.   &lt;/p&gt;\n\n&lt;p&gt;I usually build quite simple pipelines (just a few data sources - hubspot, GA, etc.), and therefore I run them in GitLab CI/CD, or GitHub actions.    &lt;/p&gt;\n\n&lt;p&gt;I wanted to create some tool for it where I would just say &amp;quot;hey, i want this data source and this data source, i want to load it there, and connect there&amp;quot; but I ended up that it would be just &amp;quot;configuration for configuration&amp;quot;. \ud83d\ude03 Maybe, I have just tunel view, or not enough experience w/ data engineering (since I was full stack dev for ~6 years and to data engineering I switched recently) but is there a batter way on how to start projects without need to write tons of &amp;quot;boilerplate&amp;quot; code/configuration in tools like Meltano, dbt, YAML(s) for GitLab/GitHub, etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1443xy8", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1443xy8/the_best_experience_for_building_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1443xy8/the_best_experience_for_building_data_pipelines/", "subreddit_subscribers": 109565, "created_utc": 1686214858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A brief history of Data Engineering: From IDS to Real-Time streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1441qvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/SR0Oh18pN3KeWgpiAw_eBx1gosrbq3kDTd7F3R3nqo0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686206908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "artificialcorner.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://artificialcorner.com/a-brief-history-of-data-engineering-from-ids-to-real-time-streaming-6d5380237255", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pp-5_mQDuXGXnNnnAr0-aZ6lYTylPjPbEgSycpbw6mk.jpg?auto=webp&amp;v=enabled&amp;s=8b2ed456eccfd757f155c237120476a99abee652", "width": 800, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/pp-5_mQDuXGXnNnnAr0-aZ6lYTylPjPbEgSycpbw6mk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=258e0abe9d423cc0cc8e6d3cfd429624475fbf19", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/pp-5_mQDuXGXnNnnAr0-aZ6lYTylPjPbEgSycpbw6mk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aaef67ee8f707f1a8c9d9cbe5ed0c9b70adffc3b", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/pp-5_mQDuXGXnNnnAr0-aZ6lYTylPjPbEgSycpbw6mk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbc9bca948a08b682303f0fca77493fc8b7e4b53", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/pp-5_mQDuXGXnNnnAr0-aZ6lYTylPjPbEgSycpbw6mk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f00bfdb0d17a2e0383b833ece994ffb296d702a0", "width": 640, "height": 1280}], "variants": {}, "id": "ZmhxaYOoAQYFFYBrfCVN2ue857ScEvkPWaX7ngrqflk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1441qvf", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1441qvf/a_brief_history_of_data_engineering_from_ids_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://artificialcorner.com/a-brief-history-of-data-engineering-from-ids-to-real-time-streaming-6d5380237255", "subreddit_subscribers": 109565, "created_utc": 1686206908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI have an ADF question regarding Mapping data flow where CDC is ticked.\n\nSuppose I ticked the option that says \u201cFull on first run, then incremental.\u201d I tried running the dataflow within a pipeline for the first two runs and it worked fine. First load = load everything. Second load = didn\u2019t insert or do anything since I didn\u2019t change anything from source.\n\nNow, I know ADF manages its own checkpoint keys. I tried rerunning the dataflow, this time overriding the checkpoint key with a different value. What I\u2019m expecting here is that ADF will read again all rows from source, but won\u2019t insert whatever ID is already present in the sink. However, it looks like ADF still inserts leading to duplicate keys. \n\nI thought when I set a KEY column in the sink settings, it will know which rows already exist in the sink.\n\nHope it makes sense. TIA", "author_fullname": "t2_64zxitsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF Mapping Data Flow CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143yg5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686196273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I have an ADF question regarding Mapping data flow where CDC is ticked.&lt;/p&gt;\n\n&lt;p&gt;Suppose I ticked the option that says \u201cFull on first run, then incremental.\u201d I tried running the dataflow within a pipeline for the first two runs and it worked fine. First load = load everything. Second load = didn\u2019t insert or do anything since I didn\u2019t change anything from source.&lt;/p&gt;\n\n&lt;p&gt;Now, I know ADF manages its own checkpoint keys. I tried rerunning the dataflow, this time overriding the checkpoint key with a different value. What I\u2019m expecting here is that ADF will read again all rows from source, but won\u2019t insert whatever ID is already present in the sink. However, it looks like ADF still inserts leading to duplicate keys. &lt;/p&gt;\n\n&lt;p&gt;I thought when I set a KEY column in the sink settings, it will know which rows already exist in the sink.&lt;/p&gt;\n\n&lt;p&gt;Hope it makes sense. TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143yg5s", "is_robot_indexable": true, "report_reasons": null, "author": "vinsanity1603", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143yg5s/adf_mapping_data_flow_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143yg5s/adf_mapping_data_flow_cdc/", "subreddit_subscribers": 109565, "created_utc": 1686196273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im trying to roll back some failed dags in production. Our dags are deployed on airflow and composer on google cloud. only my manager has access to deploy to prod. production deployment is manual via jenkins. we ue github for our code version management. I'd like to learn more about airflow dag deployment in production - im very junior and this is my first time on production. I have been tasked with figuring out how to roll back dags which fail in production and set them up for rerun - i looked this up in chat gpt and it recommends a pythons  script using gitpython  which rolls back to the last git version using the commit id of the last stable checkout. I've never rolled back naything before - it recommends to checkout the previous commit. Can someone point me to resources on  rollbacks of dags on production - how did you guys learn how to do this - where from? Any resources/tutorials would be helpful", "author_fullname": "t2_3w8i6ry97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rolling back dags in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143wu16", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to roll back some failed dags in production. Our dags are deployed on airflow and composer on google cloud. only my manager has access to deploy to prod. production deployment is manual via jenkins. we ue github for our code version management. I&amp;#39;d like to learn more about airflow dag deployment in production - im very junior and this is my first time on production. I have been tasked with figuring out how to roll back dags which fail in production and set them up for rerun - i looked this up in chat gpt and it recommends a pythons  script using gitpython  which rolls back to the last git version using the commit id of the last stable checkout. I&amp;#39;ve never rolled back naything before - it recommends to checkout the previous commit. Can someone point me to resources on  rollbacks of dags on production - how did you guys learn how to do this - where from? Any resources/tutorials would be helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143wu16", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless-Soup-2583", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143wu16/rolling_back_dags_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143wu16/rolling_back_dags_in_production/", "subreddit_subscribers": 109565, "created_utc": 1686191620.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}