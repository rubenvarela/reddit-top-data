{"kind": "Listing", "data": {"after": "t3_143bt4z", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": " Reddit user [/u/TheArstaInventor](https://www.reddit.com/u/TheArstaInventor/) was recently banned from Reddit, alongside a subreddit they created [r/LemmyMigration](https://www.reddit.com/r/LemmyMigration/) which was promoting Lemmy.   \n\n\nLemmy is a self-hosted social link sharing and discussion platform, offering an alternative experience to Reddit. Considering recent issues with Reddit API changes, and the impending hemorrhage to Reddit's userbase, this is a sign they're panicking.\n\nThe account and subreddit have since been reinstated, but this doesn't look good for Reddit.\n\n[Full Story Here](https://www.videogamer.com/news/reddit-ban-subreddit-user-for-alternative-platforms/)", "author_fullname": "t2_323wrsms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit temporarily ban subreddit and user advertising rival self-hosted platform (Lemmy)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143diuj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1820, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1820, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686145007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reddit user &lt;a href=\"https://www.reddit.com/u/TheArstaInventor/\"&gt;/u/TheArstaInventor&lt;/a&gt; was recently banned from Reddit, alongside a subreddit they created &lt;a href=\"https://www.reddit.com/r/LemmyMigration/\"&gt;r/LemmyMigration&lt;/a&gt; which was promoting Lemmy.   &lt;/p&gt;\n\n&lt;p&gt;Lemmy is a self-hosted social link sharing and discussion platform, offering an alternative experience to Reddit. Considering recent issues with Reddit API changes, and the impending hemorrhage to Reddit&amp;#39;s userbase, this is a sign they&amp;#39;re panicking.&lt;/p&gt;\n\n&lt;p&gt;The account and subreddit have since been reinstated, but this doesn&amp;#39;t look good for Reddit.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.videogamer.com/news/reddit-ban-subreddit-user-for-alternative-platforms/\"&gt;Full Story Here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?auto=webp&amp;v=enabled&amp;s=b8eefb80e051ba5398a798c3a229f32db754c076", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0561d12bc98eb5f1b8d0d944a5c060c864ab8623", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9df6672753cabfd8d6415b81932b4e9cdb47ddd8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef98ae9ad8c8655be51f274feff1b59ef96d0243", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a0d880e3969f6248f2c83387e65a8f196c8c24f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef106070d31ec92bfaac7be0f6ef9be04a758ff8", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3baf5840512a29988e474a7940626d2606c072fd", "width": 1080, "height": 607}], "variants": {}, "id": "Gm61LbO2xeYFv46HtomUc2iJNR9HfwuX54_1zHApFg0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143diuj", "is_robot_indexable": true, "report_reasons": null, "author": "aDogWithoutABone", "discussion_type": null, "num_comments": 309, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143diuj/reddit_temporarily_ban_subreddit_and_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143diuj/reddit_temporarily_ban_subreddit_and_user/", "subreddit_subscribers": 253191, "created_utc": 1686145007.0, "num_crossposts": 7, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hey folks,\n\nToday we are launching OpenObserve. An open source Elasticsearch/Splunk/Datadog alternative written in rust and vue that is super easy to get started with and has 140x lower storage cost. It offers logs, metrics, traces, dashboards, alerts, functions (run aws lambda like functions during ingestion and query to enrich, redact, transform, normalize and whatever else you want to do. Think redacting email IDs from logs, adding geolocation based on IP address, etc). You can do all of this from the UI; no messing up with configuration files.\n\nOpenObserve can use local disk for storage in single node mode or s3/gc/minio/azure blob or any s3 compatible store in HA mode.\n\nWe found that setting up observability often involved setting up 4 different tools (grafana for dashboarding, elasticsearch/loki/etc for logs, jaeger for tracing, thanos, cortex etc for metics) and its not simple to do these things.\n\nHere is a blog on why we built OpenObserve - [https://openobserve.ai/blog/launching-openobserve](https://openobserve.ai/blog/launching-openobserve).\n\nWe are in early days and would love to get feedback and suggestions.\n\nHere is the github page. [https://github.com/openobserve/openobserve](https://github.com/openobserve/openobserve)\n\nYou can run it in your raspberry pi and in a 300 node cluster ingesting a petabyte of data per day.", "author_fullname": "t2_7bsgws6q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenObserve: Open source Elasticsearch alternative in Rust for logs. 140x lower storage cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1435zxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 206, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 206, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686121257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;Today we are launching OpenObserve. An open source Elasticsearch/Splunk/Datadog alternative written in rust and vue that is super easy to get started with and has 140x lower storage cost. It offers logs, metrics, traces, dashboards, alerts, functions (run aws lambda like functions during ingestion and query to enrich, redact, transform, normalize and whatever else you want to do. Think redacting email IDs from logs, adding geolocation based on IP address, etc). You can do all of this from the UI; no messing up with configuration files.&lt;/p&gt;\n\n&lt;p&gt;OpenObserve can use local disk for storage in single node mode or s3/gc/minio/azure blob or any s3 compatible store in HA mode.&lt;/p&gt;\n\n&lt;p&gt;We found that setting up observability often involved setting up 4 different tools (grafana for dashboarding, elasticsearch/loki/etc for logs, jaeger for tracing, thanos, cortex etc for metics) and its not simple to do these things.&lt;/p&gt;\n\n&lt;p&gt;Here is a blog on why we built OpenObserve - &lt;a href=\"https://openobserve.ai/blog/launching-openobserve\"&gt;https://openobserve.ai/blog/launching-openobserve&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We are in early days and would love to get feedback and suggestions.&lt;/p&gt;\n\n&lt;p&gt;Here is the github page. &lt;a href=\"https://github.com/openobserve/openobserve\"&gt;https://github.com/openobserve/openobserve&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can run it in your raspberry pi and in a 300 node cluster ingesting a petabyte of data per day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?auto=webp&amp;v=enabled&amp;s=7af5d4a7c0c23b43d6468eed6a9865232ffa57c1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d33bedec1eedc64b24099befee2610e36c1a80", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=586642648cb9af20ea78deb9cde7d79077e2e0d0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cdf2de17fd1043b683e41e84fe5fb28c2f0659c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1383d36d9feeafc549b1450e47c1f89b407d8bd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488da2967b97a31c9c5be251dbb1a1075bb3d933", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=baa4f68a31fa7a7db0ae4647c7ea0236f6f85d6f", "width": 1080, "height": 567}], "variants": {}, "id": "4otMKjAedw7YCHVFlFfFjck6QDT82noeIjZFvL9idEA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1435zxl", "is_robot_indexable": true, "report_reasons": null, "author": "the_ml_guy", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1435zxl/openobserve_open_source_elasticsearch_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1435zxl/openobserve_open_source_elasticsearch_alternative/", "subreddit_subscribers": 253191, "created_utc": 1686121257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "So in lieu of Reddit's recent API changes, it seems people will want to have ways to dump their data and move elsewhere if the announced pricing plan isn't adjusted. Since I wanted to dump my own Reddit messages, I came up with a script that makes this possible.\n\nReddit's new chat infrastructure is based on Matrix, allowing us to use standard Matrix clients to access the message history.\n\nAs I used Golang for this, I used the Mautrix client, and came up with the following:\n\n    func FetchMessages(client *mautrix.Client, roomID id.RoomID, callback func(messages []*event.Event)) error {\n    \tr, err := client.CreateFilter(mautrix.NewDefaultSyncer().FilterJSON)\n    \n    \tif err != nil {\n    \t\treturn err\n    \t}\n    \n    \tresp, err := client.SyncRequest(0, \"\", r.FilterID, true, event.PresenceOnline, context.TODO())\n    \n    \tif err != nil {\n    \t\treturn err\n    \t}\n    \n    \tvar room *mautrix.SyncJoinedRoom\n    \n    \tfor id, r := range resp.Rooms.Join {\n    \t\tif id == roomID {\n    \t\t\troom = r\n    \t\t\tbreak\n    \t\t}\n    \t}\n    \n    \tvar messages []*event.Event\n    \n    \tfor _, m := range room.Timeline.Events {\n    \t\tif m.Type == event.EventMessage {\n    \t\t\tmessages = append(messages, m)\n    \t\t}\n    \t}\n    \n    \tcallback(messages)\n    \n    \tend := room.Timeline.PrevBatch\n    \n    \tfor {\n    \t\tif end == \"\" {\n    \t\t\tbreak\n    \t\t}\n    \n    \t\tvar messages []*event.Event\n    \n    \t\tmsgs, err := client.Messages(roomID, end, \"\", mautrix.DirectionBackward, &amp;mautrix.FilterPart{}, 100)\n    \n    \t\tif err != nil {\n    \t\t\tlog.Fatalf(err.Error())\n    \t\t}\n    \n    \t\tmessages = append(messages, msgs.Chunk...)\n    \t\tcallback(messages)\n    \n    \t\tend = msgs.End\n    \n    \t\tif len(messages) == 0 {\n    \t\t\tcontinue\n    \t\t}\n    \t}\n    \n    \treturn nil\n    }\n\nThis method will fetch all the messages from a given room ID, and call the `callback()` function in batches. From there you can use the events to dump as JSON, store in a DB, or anything else. \n\nTo create the Mautrix client and `roomID` argument, the following snippet can be used:\n\n    client, err := mautrix.NewClient(\"https://matrix.redditspace.com/\", id.NewUserID(\"t2_&lt;userID&gt;\", \"reddit.com\"), \"&lt;redditAccessToken\"\")\n    roomID := id.RoomID(\"&lt;roomID&gt;\")\n\nTo fill out the above variables, you'll need to use your browser's network tab to inspect requests and get the IDs and access token. For that head to Reddit's chat at [https://chat.reddit.com](https://chat.reddit.com) and reload the window with the network tab open. \n\n**User ID**\n\nYour user ID is visible in the request to [https://matrix.redditspace.com/\\_matrix/client/r0/login](https://matrix.redditspace.com/_matrix/client/r0/login). It will be part of the response as `user_id`.\n\n**Room ID**\n\nThe room ID will be part of the URL when you select a chat room. Simply copy the entire path after [https://chat.reddit.com/room](https://chat.reddit.com/room) and URL decode it.\n\n**Access Token**\n\nYour access token will be included in all requests after the login. I used the request to /filter and copy the value from the `Authorization` header without \"Bearer \".\n\nNow, depending on what you want to do with the messages you'll want to write your own parsing and mapping logic, as well as saving, but a fairly straightforward `main()` method to save all the messages in JSON can look like this:\n\n    package main\n    \n    type Message struct {\n        Source      string    `bson:\"source\"`\n        ChatID      string    `bson:\"chat_id\"`\n        Author      string    `bson:\"author\"`\n        Timestamp   time.Time `bson:\"timestamp\"`\n        SourceID    string    `bson:\"source_id\"`\n        Body        string    `bson:\"body\"`\n        Attachments []string  `bson:\"attachments\"`\n    }\n    \n    func parseMsg(message *event.Event, roomId id.RoomID) *model.Message {\n    \tts := time.Unix(message.Timestamp, 0)\n    \n    \tmsg := &amp;model.Message{\n    \t\tSource:    \"reddit\",\n    \t\tChatID:    roomId.String(),\n    \t\tAuthor:    message.Sender.String(),\n    \t\tTimestamp: ts,\n    \t\tSourceID:  message.ID.String(),\n    \t}\n    \n    \tswitch message.Content.Raw[\"msgtype\"] {\n    \tcase \"m.text\":\n    \t\tif message.Content.Raw[\"body\"] == nil {\n    \t\t\tfmt.Println(\"Empty message body:\", message.Content.Raw)\n    \t\t\treturn nil\n    \t\t} else {\n    \t\t\tmsg.Body = message.Content.Raw[\"body\"].(string)\n    \t\t}\n    \tcase \"m.image\":\n    \t\tmsg.Attachments = []string{\n    \t\t\tmessage.Content.Raw[\"url\"].(string),\n    \t\t}\n    \tcase nil:\n    \t\tif message.Content.Raw[\"m.relates_to\"] != nil &amp;&amp; message.Content.Raw[\"m.relates_to\"].(map[string]interface{})[\"rel_type\"] == \"com.reddit.potentially_toxic\" {\n    \t\t} else {\n    \t\t\tfmt.Println(\"No message type:\", message.Content.Raw)\n    \t\t}\n    \t\treturn nil\n    \tdefault:\n    \t\tfmt.Println(\"Unknown message type:\", message.Content.Raw)\n    \t}\n    \n    \treturn msg\n    }\n    \n    func main() {\n        var allMessages []*Message\n        \n        err = reddit.FetchMessages(client, roomId, func(messages []*event.Event) {\n            for _, msg := range messages {\n                m := parseMsg(msg, roomId)\n                if m == nil {\n                    continue\n                }\n                messages = append(messages, m)\n            }\n        }\n    \n        if err != nil {\n            log.Fatalf(err.Error())\n        }\n    \n        file, _ := json.MarshalIndent(allMessages, \"\", \" \")\n        _ = os.WriteFile(\"events.json\", file, 0644)\n    }\n\nHappy dumping!", "author_fullname": "t2_9w1ryk11p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created some Go scripts to dump Reddit chats!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "chatsystem", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143bl09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Chat System", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686140052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So in lieu of Reddit&amp;#39;s recent API changes, it seems people will want to have ways to dump their data and move elsewhere if the announced pricing plan isn&amp;#39;t adjusted. Since I wanted to dump my own Reddit messages, I came up with a script that makes this possible.&lt;/p&gt;\n\n&lt;p&gt;Reddit&amp;#39;s new chat infrastructure is based on Matrix, allowing us to use standard Matrix clients to access the message history.&lt;/p&gt;\n\n&lt;p&gt;As I used Golang for this, I used the Mautrix client, and came up with the following:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;func FetchMessages(client *mautrix.Client, roomID id.RoomID, callback func(messages []*event.Event)) error {\n    r, err := client.CreateFilter(mautrix.NewDefaultSyncer().FilterJSON)\n\n    if err != nil {\n        return err\n    }\n\n    resp, err := client.SyncRequest(0, &amp;quot;&amp;quot;, r.FilterID, true, event.PresenceOnline, context.TODO())\n\n    if err != nil {\n        return err\n    }\n\n    var room *mautrix.SyncJoinedRoom\n\n    for id, r := range resp.Rooms.Join {\n        if id == roomID {\n            room = r\n            break\n        }\n    }\n\n    var messages []*event.Event\n\n    for _, m := range room.Timeline.Events {\n        if m.Type == event.EventMessage {\n            messages = append(messages, m)\n        }\n    }\n\n    callback(messages)\n\n    end := room.Timeline.PrevBatch\n\n    for {\n        if end == &amp;quot;&amp;quot; {\n            break\n        }\n\n        var messages []*event.Event\n\n        msgs, err := client.Messages(roomID, end, &amp;quot;&amp;quot;, mautrix.DirectionBackward, &amp;amp;mautrix.FilterPart{}, 100)\n\n        if err != nil {\n            log.Fatalf(err.Error())\n        }\n\n        messages = append(messages, msgs.Chunk...)\n        callback(messages)\n\n        end = msgs.End\n\n        if len(messages) == 0 {\n            continue\n        }\n    }\n\n    return nil\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This method will fetch all the messages from a given room ID, and call the &lt;code&gt;callback()&lt;/code&gt; function in batches. From there you can use the events to dump as JSON, store in a DB, or anything else. &lt;/p&gt;\n\n&lt;p&gt;To create the Mautrix client and &lt;code&gt;roomID&lt;/code&gt; argument, the following snippet can be used:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;client, err := mautrix.NewClient(&amp;quot;https://matrix.redditspace.com/&amp;quot;, id.NewUserID(&amp;quot;t2_&amp;lt;userID&amp;gt;&amp;quot;, &amp;quot;reddit.com&amp;quot;), &amp;quot;&amp;lt;redditAccessToken&amp;quot;&amp;quot;)\nroomID := id.RoomID(&amp;quot;&amp;lt;roomID&amp;gt;&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To fill out the above variables, you&amp;#39;ll need to use your browser&amp;#39;s network tab to inspect requests and get the IDs and access token. For that head to Reddit&amp;#39;s chat at &lt;a href=\"https://chat.reddit.com\"&gt;https://chat.reddit.com&lt;/a&gt; and reload the window with the network tab open. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;User ID&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Your user ID is visible in the request to &lt;a href=\"https://matrix.redditspace.com/_matrix/client/r0/login\"&gt;https://matrix.redditspace.com/_matrix/client/r0/login&lt;/a&gt;. It will be part of the response as &lt;code&gt;user_id&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Room ID&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The room ID will be part of the URL when you select a chat room. Simply copy the entire path after &lt;a href=\"https://chat.reddit.com/room\"&gt;https://chat.reddit.com/room&lt;/a&gt; and URL decode it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Access Token&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Your access token will be included in all requests after the login. I used the request to /filter and copy the value from the &lt;code&gt;Authorization&lt;/code&gt; header without &amp;quot;Bearer &amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Now, depending on what you want to do with the messages you&amp;#39;ll want to write your own parsing and mapping logic, as well as saving, but a fairly straightforward &lt;code&gt;main()&lt;/code&gt; method to save all the messages in JSON can look like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;package main\n\ntype Message struct {\n    Source      string    `bson:&amp;quot;source&amp;quot;`\n    ChatID      string    `bson:&amp;quot;chat_id&amp;quot;`\n    Author      string    `bson:&amp;quot;author&amp;quot;`\n    Timestamp   time.Time `bson:&amp;quot;timestamp&amp;quot;`\n    SourceID    string    `bson:&amp;quot;source_id&amp;quot;`\n    Body        string    `bson:&amp;quot;body&amp;quot;`\n    Attachments []string  `bson:&amp;quot;attachments&amp;quot;`\n}\n\nfunc parseMsg(message *event.Event, roomId id.RoomID) *model.Message {\n    ts := time.Unix(message.Timestamp, 0)\n\n    msg := &amp;amp;model.Message{\n        Source:    &amp;quot;reddit&amp;quot;,\n        ChatID:    roomId.String(),\n        Author:    message.Sender.String(),\n        Timestamp: ts,\n        SourceID:  message.ID.String(),\n    }\n\n    switch message.Content.Raw[&amp;quot;msgtype&amp;quot;] {\n    case &amp;quot;m.text&amp;quot;:\n        if message.Content.Raw[&amp;quot;body&amp;quot;] == nil {\n            fmt.Println(&amp;quot;Empty message body:&amp;quot;, message.Content.Raw)\n            return nil\n        } else {\n            msg.Body = message.Content.Raw[&amp;quot;body&amp;quot;].(string)\n        }\n    case &amp;quot;m.image&amp;quot;:\n        msg.Attachments = []string{\n            message.Content.Raw[&amp;quot;url&amp;quot;].(string),\n        }\n    case nil:\n        if message.Content.Raw[&amp;quot;m.relates_to&amp;quot;] != nil &amp;amp;&amp;amp; message.Content.Raw[&amp;quot;m.relates_to&amp;quot;].(map[string]interface{})[&amp;quot;rel_type&amp;quot;] == &amp;quot;com.reddit.potentially_toxic&amp;quot; {\n        } else {\n            fmt.Println(&amp;quot;No message type:&amp;quot;, message.Content.Raw)\n        }\n        return nil\n    default:\n        fmt.Println(&amp;quot;Unknown message type:&amp;quot;, message.Content.Raw)\n    }\n\n    return msg\n}\n\nfunc main() {\n    var allMessages []*Message\n\n    err = reddit.FetchMessages(client, roomId, func(messages []*event.Event) {\n        for _, msg := range messages {\n            m := parseMsg(msg, roomId)\n            if m == nil {\n                continue\n            }\n            messages = append(messages, m)\n        }\n    }\n\n    if err != nil {\n        log.Fatalf(err.Error())\n    }\n\n    file, _ := json.MarshalIndent(allMessages, &amp;quot;&amp;quot;, &amp;quot; &amp;quot;)\n    _ = os.WriteFile(&amp;quot;events.json&amp;quot;, file, 0644)\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Happy dumping!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0d52d76a-7e68-11e9-977c-0eabae61418e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143bl09", "is_robot_indexable": true, "report_reasons": null, "author": "Dan6erbond2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143bl09/i_created_some_go_scripts_to_dump_reddit_chats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143bl09/i_created_some_go_scripts_to_dump_reddit_chats/", "subreddit_subscribers": 253191, "created_utc": 1686140052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I'm never clear on what counts as self promotion, so I won't include the link until someone tells I can. Besides that, I haven't run it anywhere but my own server so there are probably bugs.\n\nI made a web-based photo gallery for my home photos and I would like it if some people would help test it and share feedback.\n\nKey features: \n \n * It handles 200,000+ images and videos well, showing them all in a date-sorted scrolling wall of thumbnails, which you can click to view full sized. \n * \"Rewind\" button to show all photos taken on today's date in past years\n * Jump-to-date calendar popup to navigate your large photo library\n * Shows photo locations on a map, clicking on the map takes you to the photo\n * Can toggle map filter mode so that only photos on the current map view are shown in the scrollable area. \n * Reads tags from metadata and lets you filter photos by tags\n * Generates streamable copies of your videos so you can watch them all in the browser\n * Basic username and password protection\n * Progressive Web App so you can put an icon on your home page and it will cache thumbnails and resources\n * Lastly, it should be easy to set up\n\nThe code is a bit old-school. Written in PHP and JavaScript. It needs ffmpeg for video thumbnails and either vipsthumbnail, imagemagic or gd for image thumbnails.\n\nIt's 100% open source and I don't (and won't) have a business around it. I just put a lot of work in to it, think it's kind of cool and would love to get some feedback.", "author_fullname": "t2_k2sbwvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a few testers for a self-hosted image gallery project (free &amp; open source, of course)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "mediaserving", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ajs9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Media Serving", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686137094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m never clear on what counts as self promotion, so I won&amp;#39;t include the link until someone tells I can. Besides that, I haven&amp;#39;t run it anywhere but my own server so there are probably bugs.&lt;/p&gt;\n\n&lt;p&gt;I made a web-based photo gallery for my home photos and I would like it if some people would help test it and share feedback.&lt;/p&gt;\n\n&lt;p&gt;Key features: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It handles 200,000+ images and videos well, showing them all in a date-sorted scrolling wall of thumbnails, which you can click to view full sized. &lt;/li&gt;\n&lt;li&gt;&amp;quot;Rewind&amp;quot; button to show all photos taken on today&amp;#39;s date in past years&lt;/li&gt;\n&lt;li&gt;Jump-to-date calendar popup to navigate your large photo library&lt;/li&gt;\n&lt;li&gt;Shows photo locations on a map, clicking on the map takes you to the photo&lt;/li&gt;\n&lt;li&gt;Can toggle map filter mode so that only photos on the current map view are shown in the scrollable area. &lt;/li&gt;\n&lt;li&gt;Reads tags from metadata and lets you filter photos by tags&lt;/li&gt;\n&lt;li&gt;Generates streamable copies of your videos so you can watch them all in the browser&lt;/li&gt;\n&lt;li&gt;Basic username and password protection&lt;/li&gt;\n&lt;li&gt;Progressive Web App so you can put an icon on your home page and it will cache thumbnails and resources&lt;/li&gt;\n&lt;li&gt;Lastly, it should be easy to set up&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The code is a bit old-school. Written in PHP and JavaScript. It needs ffmpeg for video thumbnails and either vipsthumbnail, imagemagic or gd for image thumbnails.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 100% open source and I don&amp;#39;t (and won&amp;#39;t) have a business around it. I just put a lot of work in to it, think it&amp;#39;s kind of cool and would love to get some feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "cb71ccc0-7e67-11e9-841a-0e67038620c2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143ajs9", "is_robot_indexable": true, "report_reasons": null, "author": "cspybbq", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143ajs9/looking_for_a_few_testers_for_a_selfhosted_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143ajs9/looking_for_a_few_testers_for_a_selfhosted_image/", "subreddit_subscribers": 253191, "created_utc": 1686137094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi, I have the following task on my desk: I want to backup my emails (with attachments) in an automated fashion. As of now I am using gmail using the web interface via browser.\n\nOfficial google backups are quite cumbersome to deal with. I read that the easiest way to keep a copy is to have a mail client (like thunderbird) and keep a backup of it. I tested thunderbird and it works fine (it can really mirror gmail via IMAP and you can import/export the backups).\n\nHowever, I want something similar that can run on my server. In particular, I am looking for:\n\n* something that can pull changes automatically every N-hours (not when opening the app)\n* something that can be self-contained in a docker that run on a headless server\n* has a web interface (like zimbra/roundcube/...) just to check if everything is working. However, this is not mandatory, but just nice to have.\n\nAny recommended workflow here?", "author_fullname": "t2_9mioauuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for suggestions: selfhosted server based mirror of gmail for backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1438gam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686130241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have the following task on my desk: I want to backup my emails (with attachments) in an automated fashion. As of now I am using gmail using the web interface via browser.&lt;/p&gt;\n\n&lt;p&gt;Official google backups are quite cumbersome to deal with. I read that the easiest way to keep a copy is to have a mail client (like thunderbird) and keep a backup of it. I tested thunderbird and it works fine (it can really mirror gmail via IMAP and you can import/export the backups).&lt;/p&gt;\n\n&lt;p&gt;However, I want something similar that can run on my server. In particular, I am looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;something that can pull changes automatically every N-hours (not when opening the app)&lt;/li&gt;\n&lt;li&gt;something that can be self-contained in a docker that run on a headless server&lt;/li&gt;\n&lt;li&gt;has a web interface (like zimbra/roundcube/...) just to check if everything is working. However, this is not mandatory, but just nice to have.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any recommended workflow here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1438gam", "is_robot_indexable": true, "report_reasons": null, "author": "-elmuz-", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1438gam/asking_for_suggestions_selfhosted_server_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1438gam/asking_for_suggestions_selfhosted_server_based/", "subreddit_subscribers": 253191, "created_utc": 1686130241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Google Finance has recently started requiring \"prove it's you\" 2FA's all over the place. Looking for a portal with the basics of the current market, some news feeds, tagging specific stocks, etc.  I knew there are hundreds of commercial ones, but this is selfhosting!", "author_fullname": "t2_16199w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Selfhosted finance portal like Google/Yahoo Finance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143r32m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686176748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google Finance has recently started requiring &amp;quot;prove it&amp;#39;s you&amp;quot; 2FA&amp;#39;s all over the place. Looking for a portal with the basics of the current market, some news feeds, tagging specific stocks, etc.  I knew there are hundreds of commercial ones, but this is selfhosting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143r32m", "is_robot_indexable": true, "report_reasons": null, "author": "d662", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143r32m/selfhosted_finance_portal_like_googleyahoo_finance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143r32m/selfhosted_finance_portal_like_googleyahoo_finance/", "subreddit_subscribers": 253191, "created_utc": 1686176748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi there!  \n\nI've been looking around for an app that could help me achieve something pretty specific. I like having \"reusable checklists\", that is checklists that I can open, complete and reset whenever I want. Useful for when I'm traveling for instance, I can keep an NFC tag in my suitcase that redirects me to my packing checklist in order to make sure everything is ready.  \n\nI already have a Vikunja instance running, but I feel like it's more of a todo list than a \"reusable checklist\" kind of app. I stumbled upon lllist ([https://github.com/xvvvyz/llist](https://github.com/xvvvyz/llist)) which seems pretty cool, but I wanted to see if you guys had any good suggestions for my use case. If it doesn't exist, I might try and get something done, if it could help some of you :)  \n\nCheers!", "author_fullname": "t2_10xcsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A good app for \"reusable\" checklists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143re6j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686177493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking around for an app that could help me achieve something pretty specific. I like having &amp;quot;reusable checklists&amp;quot;, that is checklists that I can open, complete and reset whenever I want. Useful for when I&amp;#39;m traveling for instance, I can keep an NFC tag in my suitcase that redirects me to my packing checklist in order to make sure everything is ready.  &lt;/p&gt;\n\n&lt;p&gt;I already have a Vikunja instance running, but I feel like it&amp;#39;s more of a todo list than a &amp;quot;reusable checklist&amp;quot; kind of app. I stumbled upon lllist (&lt;a href=\"https://github.com/xvvvyz/llist\"&gt;https://github.com/xvvvyz/llist&lt;/a&gt;) which seems pretty cool, but I wanted to see if you guys had any good suggestions for my use case. If it doesn&amp;#39;t exist, I might try and get something done, if it could help some of you :)  &lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?auto=webp&amp;v=enabled&amp;s=e6a05b705906af93ae3c2f46e958624b1909204a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22d477c9b6d384bcf74747516a1be444454e751e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e90430c068a330bc0e401fdadfe653c733d69c9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55c6c0914d42d08246dba5d0a5a7228935652e84", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7ecdf2f433fed41240f2177107eb02924db4e78", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=587b2a3ca7dc0b417c47cfc02724117c139e2f1d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tuevU2_rTTtQTmtuICOPdxkqWUhXtCqr20aBERW8PYY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae6c0e7808fb416daba244d7a1a64e8d02f8735d", "width": 1080, "height": 540}], "variants": {}, "id": "uldBW6MwXrqFH0dy4mVo2sOrICxthW0UdV-MINspVfE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143re6j", "is_robot_indexable": true, "report_reasons": null, "author": "paulchartres", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143re6j/a_good_app_for_reusable_checklists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143re6j/a_good_app_for_reusable_checklists/", "subreddit_subscribers": 253191, "created_utc": 1686177493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Which one do you folks prefer?\n\nI'd consider ditching Jackett for Prowlarr, if Prowlarr was runnable without any dependencies like Jackett can, from what I understand.\n\n[View Poll](https://www.reddit.com/poll/143o3ep)", "author_fullname": "t2_m788zj64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jackett vs Prowlarr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "mediaserving", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143o3ep", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Media Serving", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686169926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which one do you folks prefer?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d consider ditching Jackett for Prowlarr, if Prowlarr was runnable without any dependencies like Jackett can, from what I understand.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/143o3ep\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "cb71ccc0-7e67-11e9-841a-0e67038620c2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "143o3ep", "is_robot_indexable": true, "report_reasons": null, "author": "stop_lying_good_god", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1686774726103, "options": [{"text": "Jackett", "id": "23383001"}, {"text": "Prowlarr", "id": "23383002"}, {"text": "I don't know about them.", "id": "23383003"}, {"text": "Just show me the results.", "id": "23383004"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 215, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143o3ep/jackett_vs_prowlarr/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/selfhosted/comments/143o3ep/jackett_vs_prowlarr/", "subreddit_subscribers": 253191, "created_utc": 1686169926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Majorly my notes consist of lists and some are about certain list items. I will try to elaborate with an example, suppose the note structure is the following:\n\nnotes/reading-lists/fiction and the note has 3 items\n\n* casebook of sherlock holmes\n* iliad\n* it\n\nNow i want to tag these as \n\n* casebook of sherlock holmes #mystery #detective\n* iliad #epic #fantasy\n* it #horror\n\nThe note taking apps i have seen so far only allow page level tags, but i need inline. And these items can be links to notes about these books. Even if its something like a table with col2 being author(s) and col3 being tags, it would work. I would prefer it to be self hosted but Syncthing will not be a problem.", "author_fullname": "t2_14cn0s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a note taking app with inline tags.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143nnqp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686168939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Majorly my notes consist of lists and some are about certain list items. I will try to elaborate with an example, suppose the note structure is the following:&lt;/p&gt;\n\n&lt;p&gt;notes/reading-lists/fiction and the note has 3 items&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;casebook of sherlock holmes&lt;/li&gt;\n&lt;li&gt;iliad&lt;/li&gt;\n&lt;li&gt;it&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now i want to tag these as &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;casebook of sherlock holmes #mystery #detective&lt;/li&gt;\n&lt;li&gt;iliad #epic #fantasy&lt;/li&gt;\n&lt;li&gt;it #horror&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The note taking apps i have seen so far only allow page level tags, but i need inline. And these items can be links to notes about these books. Even if its something like a table with col2 being author(s) and col3 being tags, it would work. I would prefer it to be self hosted but Syncthing will not be a problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143nnqp", "is_robot_indexable": true, "report_reasons": null, "author": "daddyodevil", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143nnqp/looking_for_a_note_taking_app_with_inline_tags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143nnqp/looking_for_a_note_taking_app_with_inline_tags/", "subreddit_subscribers": 253191, "created_utc": 1686168939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I am not a developer and I have tried the alternatives, but I've been looking for a simple paste-bin like app though that wasn't super complicated to set up but also supported syntax highlighting and more than just text files. I've tried some, but I really wanted a simple clean interface for me alone.\n\nCode here: [https://github.com/skibare87/postit](https://github.com/skibare87/postit)\n\nPostIt! is a simple paste manager that allows for text to be pasted in and saved to the filename selected. It also supports arbitrary file upload with the upload file button.\n\nThe editor supports syntax highlighting. All saved files appear at the bottom. Clicking on the file will download it. Right clicking will present a menu that will allow you to load the file back into the editor or delete the file from the server.\n\nIf a file already exists, the UI will ask for confirmation before overwriting.\n\nTo run:\n\n&gt;docker run -v ./files:/files -p 8080:80 skibare87/postit:latest\n\n&amp;#x200B;\n\n[Example Post It!](https://preview.redd.it/ufajgvq03n4b1.png?width=1504&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=341fa2ed9408b037af47f7d80755bd4b5597345f)\n\nNOTE: It is recommended to host this behind a reverse proxy with https support. Also, adding authentication is important if this is publicly available. There are **no** restrictions on file size or content, so use at your own risk.\n\n&amp;#x200B;\n\nEDIT: If a extension is not given, it will force a .txt. Otherwise it will recognize ps1,py,sh,and txt as loadable text files. All other files are download only. ", "author_fullname": "t2_8s1uh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostIt! Paste Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ufajgvq03n4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecca045a1d81066dec2f3a2f5c00272095fdd0f4"}, {"y": 171, "x": 216, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21a36f4ca7ba53dd4f70f9b85a0ef6c9ffb95e03"}, {"y": 254, "x": 320, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57f49ffdcf9f41e20b242750e99454da2c50f567"}, {"y": 508, "x": 640, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d28ef00b2980fa6d4f90485cb799612d064411be"}, {"y": 762, "x": 960, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cc563a2dfd9430bc87afafdbd1241a36e3851ef"}, {"y": 857, "x": 1080, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e774f867d5ebae8cfe5cd2bd62a0e6291e999f2"}], "s": {"y": 1194, "x": 1504, "u": "https://preview.redd.it/ufajgvq03n4b1.png?width=1504&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=341fa2ed9408b037af47f7d80755bd4b5597345f"}, "id": "ufajgvq03n4b1"}}, "name": "t3_143hpl9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nbDc1ax9pfqwps1CAOuy043ZhdIO7DEJGuKMACFGaBw.jpg", "edited": 1686167144.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686154973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not a developer and I have tried the alternatives, but I&amp;#39;ve been looking for a simple paste-bin like app though that wasn&amp;#39;t super complicated to set up but also supported syntax highlighting and more than just text files. I&amp;#39;ve tried some, but I really wanted a simple clean interface for me alone.&lt;/p&gt;\n\n&lt;p&gt;Code here: &lt;a href=\"https://github.com/skibare87/postit\"&gt;https://github.com/skibare87/postit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;PostIt! is a simple paste manager that allows for text to be pasted in and saved to the filename selected. It also supports arbitrary file upload with the upload file button.&lt;/p&gt;\n\n&lt;p&gt;The editor supports syntax highlighting. All saved files appear at the bottom. Clicking on the file will download it. Right clicking will present a menu that will allow you to load the file back into the editor or delete the file from the server.&lt;/p&gt;\n\n&lt;p&gt;If a file already exists, the UI will ask for confirmation before overwriting.&lt;/p&gt;\n\n&lt;p&gt;To run:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;docker run -v ./files:/files -p 8080:80 skibare87/postit:latest&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ufajgvq03n4b1.png?width=1504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=341fa2ed9408b037af47f7d80755bd4b5597345f\"&gt;Example Post It!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;NOTE: It is recommended to host this behind a reverse proxy with https support. Also, adding authentication is important if this is publicly available. There are &lt;strong&gt;no&lt;/strong&gt; restrictions on file size or content, so use at your own risk.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT: If a extension is not given, it will force a .txt. Otherwise it will recognize ps1,py,sh,and txt as loadable text files. All other files are download only. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?auto=webp&amp;v=enabled&amp;s=f012a97f539dab29af98b4667a24354f21fd24a4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9a4cbd14e79efbd2d09c538d13033293cd2588d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fce78c53d634c42e8bd12fa72da055fbcbb7ad25", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6608e690f5519df0e358445b2afe7cc55486975", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc1ec374d72d132117e56c8702971bcb92a2d987", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57a65c8d55e45770e224047097926ca7a1c854e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Xhe20Sbq9o6eEtkOlD3VuWYbYsiiuAoQiEmSeyIXsDs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fb9d08da5468b7bd7148ac94a6b420c95f45ff7", "width": 1080, "height": 540}], "variants": {}, "id": "n4AAY0I80BPwKPdyI3QUQjrHcSyPe5H53FvMy2LVMqw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143hpl9", "is_robot_indexable": true, "report_reasons": null, "author": "skibare87", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143hpl9/postit_paste_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143hpl9/postit_paste_manager/", "subreddit_subscribers": 253191, "created_utc": 1686154973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi everyone, I hope you're doing well. I recently changed my isp and they're only providing ipv6 rather than ipv4. Can I continue self host using ipv6? Thank you.", "author_fullname": "t2_71rz1kxz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I Self Host Using ipv6?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143hkpn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686154660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I hope you&amp;#39;re doing well. I recently changed my isp and they&amp;#39;re only providing ipv6 rather than ipv4. Can I continue self host using ipv6? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143hkpn", "is_robot_indexable": true, "report_reasons": null, "author": "Kaziopu123", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143hkpn/can_i_self_host_using_ipv6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143hkpn/can_i_self_host_using_ipv6/", "subreddit_subscribers": 253191, "created_utc": 1686154660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hey everyone,  \ni recently created my Paperless instance and so far it works \"okay\" ;)  \n\n\nI have 2 Problems:  \n1. If i scan multiple Documents with Barcodes on each document, Paperless didnt split the PDF file.  \nThe Barcodes get recognised\n\nIn the Log the Barcode with Code128 are the ones from my Sticker.  \n\n\n    [2023-05-25 14:55:03,087] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/Gewerbe/Eingangs_Rechnung/20230525_134142.pdf to the task queue.\n    [2023-05-25 14:55:03,094] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/20230525_134142.pdf to the task queue.\n    [2023-05-25 14:55:07,891] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,004] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,165] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n    [2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n    [2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,290] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n    [2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n    [2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,331] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,458] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,522] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n    [2023-05-25 14:55:08,523] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,648] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n    [2023-05-25 14:55:08,649] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,668] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,796] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,849] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n    [2023-05-25 14:55:08,850] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n    [2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,014] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,136] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,196] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,306] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,338] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,441] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n    [2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n    [2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n    [2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n    [2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,689] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,779] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n    002\n    1\n    SCT\n    \n    RG119608\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n    002\n    1\n    SCT\n    \n    RG119608\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,091] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,257] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,326] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,456] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n    [2023-05-25 14:55:10,460] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n    [2023-05-25 14:55:10,462] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n    [2023-05-25 14:55:10,464] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n    [2023-05-25 14:55:10,531] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n    [2023-05-25 14:55:10,535] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n    [2023-05-25 14:55:10,536] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n    [2023-05-25 14:55:10,539] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n    [2023-05-25 14:55:10,573] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {'input_file': PosixPath('/tmp/paperless/paperless-ngxzbpeluif/20230525_134142.pdf'), 'output_file': PosixPath('/tmp/paperless/paperless-1wqpexro/archive.pdf'), 'use_threads': True, 'jobs': '2', 'language': 'eng', 'output_type': 'pdfa', 'progress_bar': False, 'skip_text': True, 'clean': True, 'deskew': True, 'rotate_pages': True, 'rotate_pages_threshold': 12.0, 'sidecar': PosixPath('/tmp/paperless/paperless-1wqpexro/sidecar.txt')}\n    [2023-05-25 14:55:10,627] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {'input_file': PosixPath('/tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf'), 'output_file': PosixPath('/tmp/paperless/paperless-djjj8xny/archive.pdf'), 'use_threads': True, 'jobs': '2', 'language': 'eng', 'output_type': 'pdfa', 'progress_bar': False, 'skip_text': True, 'clean': True, 'deskew': True, 'rotate_pages': True, 'rotate_pages_threshold': 12.0, 'sidecar': PosixPath('/tmp/paperless/paperless-djjj8xny/sidecar.txt')}\n    [2023-05-25 14:56:11,255] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:11,267] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:11,273] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:11,277] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,398] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,433] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,442] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,455] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:57:07,981] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n    [2023-05-25 14:57:07,982] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n    [2023-05-25 14:57:07,986] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-djjj8xny/archive.pdf[0] /tmp/paperless/paperless-djjj8xny/convert.webp\n    [2023-05-25 14:57:08,009] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n    [2023-05-25 14:57:08,011] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n    [2023-05-25 14:57:08,014] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-1wqpexro/archive.pdf[0] /tmp/paperless/paperless-1wqpexro/convert.webp\n    [2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Saving record to database\n    [2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.529478+02:00\n    [2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Saving record to database\n    [2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.457478+02:00\n    [2023-05-25 14:57:10,125] [INFO] [paperless.handlers] Assigning document type Eingangs Rechnungen to 2023-05-25 20230525_134142\n    [2023-05-25 14:57:10,143] [INFO] [paperless.handlers] Tagging \"2023-05-25 20230525_134142\" with \"Gewerbe\"\n    [2023-05-25 14:57:10,161] [INFO] [paperless.handlers] Assigning storage path Gewerbe_Eingangsrechnung to 2023-05-25 20230525_134142\n    [2023-05-25 14:57:10,223] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:57:10,236] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:57:10,238] [DEBUG] [paperless.consumer] Deleting file /tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf\n    [2023-05-25 14:57:10,241] [DEBUG] [paperless.parsing.tesseract] Deleting directory /tmp/paperless/paperless-djjj8xny\n    [2023-05-25 14:57:10,242] [INFO] [paperless.consumer] Document 2023-05-25 20230525_134142 consumption finished\n\nDid you know what i am doing wrong?  \n\n\nAnd my second \"Problem\" in that case i was thinking that Paperless put the Barcode ID in the Document ID automatically?  \nOr does it need more \"Training\" data to know that it should do that?  \n\n\nThanks for your Help ;)", "author_fullname": "t2_4vwsum16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paperless-ngx (Docker) Barcode problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "business", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1436e8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Business Tools", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686122658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\ni recently created my Paperless instance and so far it works &amp;quot;okay&amp;quot; ;)  &lt;/p&gt;\n\n&lt;p&gt;I have 2 Problems:&lt;br/&gt;\n1. If i scan multiple Documents with Barcodes on each document, Paperless didnt split the PDF file.&lt;br/&gt;\nThe Barcodes get recognised&lt;/p&gt;\n\n&lt;p&gt;In the Log the Barcode with Code128 are the ones from my Sticker.  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[2023-05-25 14:55:03,087] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/Gewerbe/Eingangs_Rechnung/20230525_134142.pdf to the task queue.\n[2023-05-25 14:55:03,094] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/20230525_134142.pdf to the task queue.\n[2023-05-25 14:55:07,891] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,004] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,165] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n[2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n[2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,290] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n[2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n[2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,331] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,458] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,522] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n[2023-05-25 14:55:08,523] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,648] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n[2023-05-25 14:55:08,649] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,668] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,796] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,849] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n[2023-05-25 14:55:08,850] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n[2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,014] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,136] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,196] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,306] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,338] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,441] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n[2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n[2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n[2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n[2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,689] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,779] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n002\n1\nSCT\n\nRG119608\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n002\n1\nSCT\n\nRG119608\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,091] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,257] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,326] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,456] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n[2023-05-25 14:55:10,460] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n[2023-05-25 14:55:10,462] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n[2023-05-25 14:55:10,464] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n[2023-05-25 14:55:10,531] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n[2023-05-25 14:55:10,535] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n[2023-05-25 14:55:10,536] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n[2023-05-25 14:55:10,539] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n[2023-05-25 14:55:10,573] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {&amp;#39;input_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-ngxzbpeluif/20230525_134142.pdf&amp;#39;), &amp;#39;output_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-1wqpexro/archive.pdf&amp;#39;), &amp;#39;use_threads&amp;#39;: True, &amp;#39;jobs&amp;#39;: &amp;#39;2&amp;#39;, &amp;#39;language&amp;#39;: &amp;#39;eng&amp;#39;, &amp;#39;output_type&amp;#39;: &amp;#39;pdfa&amp;#39;, &amp;#39;progress_bar&amp;#39;: False, &amp;#39;skip_text&amp;#39;: True, &amp;#39;clean&amp;#39;: True, &amp;#39;deskew&amp;#39;: True, &amp;#39;rotate_pages&amp;#39;: True, &amp;#39;rotate_pages_threshold&amp;#39;: 12.0, &amp;#39;sidecar&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-1wqpexro/sidecar.txt&amp;#39;)}\n[2023-05-25 14:55:10,627] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {&amp;#39;input_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf&amp;#39;), &amp;#39;output_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-djjj8xny/archive.pdf&amp;#39;), &amp;#39;use_threads&amp;#39;: True, &amp;#39;jobs&amp;#39;: &amp;#39;2&amp;#39;, &amp;#39;language&amp;#39;: &amp;#39;eng&amp;#39;, &amp;#39;output_type&amp;#39;: &amp;#39;pdfa&amp;#39;, &amp;#39;progress_bar&amp;#39;: False, &amp;#39;skip_text&amp;#39;: True, &amp;#39;clean&amp;#39;: True, &amp;#39;deskew&amp;#39;: True, &amp;#39;rotate_pages&amp;#39;: True, &amp;#39;rotate_pages_threshold&amp;#39;: 12.0, &amp;#39;sidecar&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-djjj8xny/sidecar.txt&amp;#39;)}\n[2023-05-25 14:56:11,255] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:11,267] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:11,273] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:11,277] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,398] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,433] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,442] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,455] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:57:07,981] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n[2023-05-25 14:57:07,982] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n[2023-05-25 14:57:07,986] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&amp;gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-djjj8xny/archive.pdf[0] /tmp/paperless/paperless-djjj8xny/convert.webp\n[2023-05-25 14:57:08,009] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n[2023-05-25 14:57:08,011] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n[2023-05-25 14:57:08,014] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&amp;gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-1wqpexro/archive.pdf[0] /tmp/paperless/paperless-1wqpexro/convert.webp\n[2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Saving record to database\n[2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.529478+02:00\n[2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Saving record to database\n[2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.457478+02:00\n[2023-05-25 14:57:10,125] [INFO] [paperless.handlers] Assigning document type Eingangs Rechnungen to 2023-05-25 20230525_134142\n[2023-05-25 14:57:10,143] [INFO] [paperless.handlers] Tagging &amp;quot;2023-05-25 20230525_134142&amp;quot; with &amp;quot;Gewerbe&amp;quot;\n[2023-05-25 14:57:10,161] [INFO] [paperless.handlers] Assigning storage path Gewerbe_Eingangsrechnung to 2023-05-25 20230525_134142\n[2023-05-25 14:57:10,223] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:57:10,236] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:57:10,238] [DEBUG] [paperless.consumer] Deleting file /tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf\n[2023-05-25 14:57:10,241] [DEBUG] [paperless.parsing.tesseract] Deleting directory /tmp/paperless/paperless-djjj8xny\n[2023-05-25 14:57:10,242] [INFO] [paperless.consumer] Document 2023-05-25 20230525_134142 consumption finished\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Did you know what i am doing wrong?  &lt;/p&gt;\n\n&lt;p&gt;And my second &amp;quot;Problem&amp;quot; in that case i was thinking that Paperless put the Barcode ID in the Document ID automatically?&lt;br/&gt;\nOr does it need more &amp;quot;Training&amp;quot; data to know that it should do that?  &lt;/p&gt;\n\n&lt;p&gt;Thanks for your Help ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0ac01dca-53ce-11ed-9fce-c6cd629e2d85", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1436e8p", "is_robot_indexable": true, "report_reasons": null, "author": "Heartbeats_1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1436e8p/paperlessngx_docker_barcode_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1436e8p/paperlessngx_docker_barcode_problems/", "subreddit_subscribers": 253191, "created_utc": 1686122658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "## Intro\n\n[Zeabur](https://zeabur.com) is a platform that help developers deploy their services with one click.\n\nNo matter what programming language or framework is used.\n\nhttps://preview.redd.it/vbo400g0fp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ae42568639e81b0926f9087704bda4b9bac58e1c\n\n## Features\n\n### One click deployment\n\nZeabur can automatically analyze the code to determine what language and framework the project uses. No more dockerfile or docker-compose.yml!\n\nAll you need to do is to push your code to the repository, and Zeabur will automatically deploy and update your services.\n\nhttps://preview.redd.it/aazwwhcpfp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3152dfbbddfb5cdf6c09f6af15569f2ad0ff2e36\n\n### Powerful configuration\n\nZeabur provides a powerful configuration system, which allows you to easily configure your services.\n\n* Configure your environment variables easily, without .env files.\n* Add domains for your services with one click. You can use your own domain or just use a subdomain of zeabur.app\n* Multiple environment support, such as production, staging, development and so on. Bind different domains and branches to different environments to get a separate environment easily.\n\n### Pay as you go\n\nAre you still 'renting a server'? Zeabur uses a pay-as-you-go model, so you only pay for the resources your service actually uses, and you don't pay for what you don't use!\n\nhttps://preview.redd.it/y8vltgtnfp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0af160a1c794bb6f16ff63159a027a72b8748962\n\n### Integration\n\nYour powerful system needs many different services to work together, such as MySQL, Redis, Kafka, Elastic Search, and so on.\n\nZeabur can help you easily deploy them in the same place, reduce management costs, and improve transmission speeds.\n\nhttps://preview.redd.it/qjv3k1yjfp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=049714d423bbab8a2dffb5e74626f802ccc8b519\n\n### Easy to manage\n\nAfter deployment, you can easily manage your services through the dashboard.\n\n* You can check the status or usages of your services, your them.\n* Rollback, suspend or restart your services with one click.\n* Invite your team members to manage your services together!\n* You can also see a detailed log of your services.", "author_fullname": "t2_7rofok50", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zeabur - Deploy Your Services with One Click", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "official", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"y8vltgtnfp4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7adcb6618517f1554ff6f8bb13c90131f81cbd3e"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1479ebafd5727c67aae546166cea484a08daa53"}, {"y": 188, "x": 320, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b000ae42bbd9776358332beb9ca676440289e7c3"}, {"y": 376, "x": 640, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92b0f14ce98239c931f07e7947150440aa2406ea"}, {"y": 565, "x": 960, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb73c3da600bb607fb0583a1e28334a6f10ebf76"}, {"y": 635, "x": 1080, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab63ff75ba00629834a4df2694aab15e8b21e97e"}], "s": {"y": 1780, "x": 3024, "u": "https://preview.redd.it/y8vltgtnfp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0af160a1c794bb6f16ff63159a027a72b8748962"}, "id": "y8vltgtnfp4b1"}, "aazwwhcpfp4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f83188e74a37b1c4b50548b017728a90eade08f"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3b83b1459b39f878d2d6d8e9ed414f6b7d09bc2"}, {"y": 188, "x": 320, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84b39691956d6523454690e0ab08a1f04bfd5ceb"}, {"y": 376, "x": 640, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0878af6c8b19a4d8ad561e4d93721b1d47c9aef6"}, {"y": 565, "x": 960, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b09880b039b1f56357a63fc3419ee17576d66b4"}, {"y": 635, "x": 1080, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db7fbcc3581dbdf5b8131ff3e004a5b2427404b1"}], "s": {"y": 1780, "x": 3024, "u": "https://preview.redd.it/aazwwhcpfp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3152dfbbddfb5cdf6c09f6af15569f2ad0ff2e36"}, "id": "aazwwhcpfp4b1"}, "qjv3k1yjfp4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19de4f6ca90f75ee806c5e175c960f77f132bf56"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1da34465ea331851efb4322bda163930f760ad64"}, {"y": 188, "x": 320, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f370780ffa39907a8f3a3381deab3bf9a88e26b"}, {"y": 376, "x": 640, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6151f6a77afba005ef12f950434a03a86bed0bda"}, {"y": 565, "x": 960, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=998808b0af96518830efd8b50263f51627cb1395"}, {"y": 635, "x": 1080, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fc811003af09abdb2c10cf135c7d97dfca43b16"}], "s": {"y": 1780, "x": 3024, "u": "https://preview.redd.it/qjv3k1yjfp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=049714d423bbab8a2dffb5e74626f802ccc8b519"}, "id": "qjv3k1yjfp4b1"}, "vbo400g0fp4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76aaf8bb6fe7a339b0547a8cdbde49fec65eecef"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ceb3deabeed2b928d9b3d7aa7bec8ffd048af5c"}, {"y": 188, "x": 320, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10e885c33ed810b945614bea72602a09a188b7a6"}, {"y": 376, "x": 640, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75ee2851059f189f35b3a2563a25a8a3574bd896"}, {"y": 565, "x": 960, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8c82b3de284f5a545360ad923339f7adcf8c68f"}, {"y": 635, "x": 1080, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba8570edf2f4466ad039fd88fa2372ebdfb5b893"}], "s": {"y": 1780, "x": 3024, "u": "https://preview.redd.it/vbo400g0fp4b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ae42568639e81b0926f9087704bda4b9bac58e1c"}, "id": "vbo400g0fp4b1"}}, "name": "t3_143wngp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Official", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fxAFAiZxAojq0mjqScdG-T6YRNHF9HeTxr9OT6IvOQI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1686191129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Intro&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://zeabur.com\"&gt;Zeabur&lt;/a&gt; is a platform that help developers deploy their services with one click.&lt;/p&gt;\n\n&lt;p&gt;No matter what programming language or framework is used.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vbo400g0fp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ae42568639e81b0926f9087704bda4b9bac58e1c\"&gt;https://preview.redd.it/vbo400g0fp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ae42568639e81b0926f9087704bda4b9bac58e1c&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Features&lt;/h2&gt;\n\n&lt;h3&gt;One click deployment&lt;/h3&gt;\n\n&lt;p&gt;Zeabur can automatically analyze the code to determine what language and framework the project uses. No more dockerfile or docker-compose.yml!&lt;/p&gt;\n\n&lt;p&gt;All you need to do is to push your code to the repository, and Zeabur will automatically deploy and update your services.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aazwwhcpfp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3152dfbbddfb5cdf6c09f6af15569f2ad0ff2e36\"&gt;https://preview.redd.it/aazwwhcpfp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3152dfbbddfb5cdf6c09f6af15569f2ad0ff2e36&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Powerful configuration&lt;/h3&gt;\n\n&lt;p&gt;Zeabur provides a powerful configuration system, which allows you to easily configure your services.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Configure your environment variables easily, without .env files.&lt;/li&gt;\n&lt;li&gt;Add domains for your services with one click. You can use your own domain or just use a subdomain of zeabur.app&lt;/li&gt;\n&lt;li&gt;Multiple environment support, such as production, staging, development and so on. Bind different domains and branches to different environments to get a separate environment easily.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Pay as you go&lt;/h3&gt;\n\n&lt;p&gt;Are you still &amp;#39;renting a server&amp;#39;? Zeabur uses a pay-as-you-go model, so you only pay for the resources your service actually uses, and you don&amp;#39;t pay for what you don&amp;#39;t use!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y8vltgtnfp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0af160a1c794bb6f16ff63159a027a72b8748962\"&gt;https://preview.redd.it/y8vltgtnfp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0af160a1c794bb6f16ff63159a027a72b8748962&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Integration&lt;/h3&gt;\n\n&lt;p&gt;Your powerful system needs many different services to work together, such as MySQL, Redis, Kafka, Elastic Search, and so on.&lt;/p&gt;\n\n&lt;p&gt;Zeabur can help you easily deploy them in the same place, reduce management costs, and improve transmission speeds.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qjv3k1yjfp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=049714d423bbab8a2dffb5e74626f802ccc8b519\"&gt;https://preview.redd.it/qjv3k1yjfp4b1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=049714d423bbab8a2dffb5e74626f802ccc8b519&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Easy to manage&lt;/h3&gt;\n\n&lt;p&gt;After deployment, you can easily manage your services through the dashboard.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You can check the status or usages of your services, your them.&lt;/li&gt;\n&lt;li&gt;Rollback, suspend or restart your services with one click.&lt;/li&gt;\n&lt;li&gt;Invite your team members to manage your services together!&lt;/li&gt;\n&lt;li&gt;You can also see a detailed log of your services.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?auto=webp&amp;v=enabled&amp;s=ae2e4b9457ddb06b2e266e07fa0f10c563716874", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b34593276821161c8833282fc62905cb01f96c8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2f0c59f7da1486fe58ff0c7fb2d62ab0324d93a", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e730ab4f91bf5774015383e9586468618c52281", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8535ef9c0dfb91894c584e6230fd84f0b44a5f92", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=385a57d55ff6f8f9aa7619e19937112d84a34f0a", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/DXPAHSuZQEfR3Alda_IciZbNetDdfxt7jaqv5uB_gA0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a6b854920cbbf0a5562ed5cbb1f906e79965682", "width": 1080, "height": 564}], "variants": {}, "id": "HgvfYiuHVMrXZoIcGi_spNazFJNbHgaQ9GF7PPxpO4Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "88beb69e-05bc-11eb-92c6-0e8fdcdbab83", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "143wngp", "is_robot_indexable": true, "report_reasons": null, "author": "PayAffectionate4055", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143wngp/zeabur_deploy_your_services_with_one_click/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143wngp/zeabur_deploy_your_services_with_one_click/", "subreddit_subscribers": 253191, "created_utc": 1686191129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi there, I\u2019m looking for clarification on something I\u2019ve been looking into lately. So I wanted to look into selfhosting simplelogin but noticed it required port 25, then I noticed my ISP blocks inbound and outbound port 25 and recommended I instead look into using port 587 but I can\u2019t get a definitive answer on weather this is a viable option. I\u2019d appreciate any knowledge on the subject, thanks!", "author_fullname": "t2_lgxhw4la", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Email server ports (25 and 587)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143nk4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686168702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I\u2019m looking for clarification on something I\u2019ve been looking into lately. So I wanted to look into selfhosting simplelogin but noticed it required port 25, then I noticed my ISP blocks inbound and outbound port 25 and recommended I instead look into using port 587 but I can\u2019t get a definitive answer on weather this is a viable option. I\u2019d appreciate any knowledge on the subject, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143nk4g", "is_robot_indexable": true, "report_reasons": null, "author": "insert_username99", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143nk4g/email_server_ports_25_and_587/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143nk4g/email_server_ports_25_and_587/", "subreddit_subscribers": 253191, "created_utc": 1686168702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "EDIT:\n\nIt was as simple as enabling AIO threads.\n\naio threads; to location /\n\nI still have this issue though\n\n[https://www.reddit.com/r/nginx/comments/141pl9d/nginx\\_proxy\\_cache\\_speeds\\_slower\\_on\\_httphttps/](https://www.reddit.com/r/nginx/comments/141pl9d/nginx_proxy_cache_speeds_slower_on_httphttps/)\n\nThanks!\n\n\\---\n\nI am trying to use nginx\\_proxy cache to serve static objects. It doesn't matter to me what caching software I use. I prefer Apache Traffic Server, but this doesn't seem to be an issue with a specific software. It seems to be an issue with settings, and NGINX is used WAY more than ats.\n\nI have a 100Mbps ethernet connection.\n\nIf you want to test each cdn, I made a demonstration page where you just need to click the name of the cdn, and it'll load the correct images.\n\n[https://demonstrattion.neocities.org/](https://demonstrattion.neocities.org/)\n\nBelow are the waterfalls of different solutions sending the same size objects.\n\nMy NGINX server (proxy cache) and CacheFly (nginx-&gt;varnish) waterfalls look like:\n\n[My NGINX server waterfall for 5 1.5MB images. 24ms away. Fastest possible image download speed is 121ms. \\(145ms including server response\\)](https://preview.redd.it/f5lmzjmh3n4b1.png?width=774&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e2f5e78f0f17a4ec6a499081373e6e25fe0fd4fd)\n\nThe first object is sent at full speed, and the next ones are sent sequentially, except the last 2 seem different but they're still sequential? I'm not sure how to describe that behavior.\n\n&amp;#x200B;\n\nWith CDN77 (nginx), Gcore (NGINX), Edgio (haproxy-&gt;varnish) and Automattic (NGINX), the waterfalls look like:\n\n[CDN77 waterfall for 5 1.5MB images. 12ms away. fastest possible image download speed is 121ms. \\(133ms including server response\\)](https://preview.redd.it/vof01c843n4b1.png?width=772&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9c62ca9e164f4c61f4bb4b6dd221b8e709796c12)\n\nThe server responds to all requests and then sends the images sequentially? The first object is not downloaded at full speed though.\n\n&amp;#x200B;\n\nWith Apple CDN (Apache traffic server), Akamai (?), and Bunny CDN (NGINX), the waterfalls look like:\n\n[Apple CDN waterfall for 5 1.5mb images. 12ms away. fastest possible image download speed is 121ms. \\(133ms including server responses\\)](https://preview.redd.it/x9qfkquc3n4b1.png?width=763&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=857675d8d0df1a630f9108253dbba2e1b41199b0)\n\nThe server responds to all requests, then sends every object at once.\n\n&amp;#x200B;\n\nFrom what I can tell, it is most efficient to serve objects the way Apple CDN, Akamai and Bunny do.\n\nI'm not sure which waterfall reflects the most efficient way to serving objects, nor how to achieve any behavior other than my current NGINX instance or CacheFly.\n\nDoes anybody have any insight? Anything is helpful. Thank you!", "author_fullname": "t2_8lpllb0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to force NGINX to serve all files at the same time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "mediaserving", "downs": 0, "thumbnail_height": 16, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vof01c843n4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 12, "x": 108, "u": "https://preview.redd.it/vof01c843n4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e30e57dfe55a2f3dd09490da9204876567c1def"}, {"y": 25, "x": 216, "u": "https://preview.redd.it/vof01c843n4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a45afe2cc386cfac225fe3461e9a10703e62880a"}, {"y": 37, "x": 320, "u": "https://preview.redd.it/vof01c843n4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1122c04cc7b9d7ba3925558a8009549ef9b8e9a"}, {"y": 75, "x": 640, "u": "https://preview.redd.it/vof01c843n4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd7389fc53f6d18ab47f74ed649685e5bea7a683"}], "s": {"y": 91, "x": 772, "u": "https://preview.redd.it/vof01c843n4b1.png?width=772&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9c62ca9e164f4c61f4bb4b6dd221b8e709796c12"}, "id": "vof01c843n4b1"}, "x9qfkquc3n4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 12, "x": 108, "u": "https://preview.redd.it/x9qfkquc3n4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d7a9b2c2d9467e694a1e4d5f0be4e6e50600886"}, {"y": 25, "x": 216, "u": "https://preview.redd.it/x9qfkquc3n4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd8024bee9d5a463e5d4b384407a2c22abbd3e09"}, {"y": 38, "x": 320, "u": "https://preview.redd.it/x9qfkquc3n4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da40e3e90908f2e249d6e069ee32ed10d4a2bf61"}, {"y": 76, "x": 640, "u": "https://preview.redd.it/x9qfkquc3n4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=491858ace54f001dfcd796c54133b6510a3f64d7"}], "s": {"y": 91, "x": 763, "u": "https://preview.redd.it/x9qfkquc3n4b1.png?width=763&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=857675d8d0df1a630f9108253dbba2e1b41199b0"}, "id": "x9qfkquc3n4b1"}, "f5lmzjmh3n4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 12, "x": 108, "u": "https://preview.redd.it/f5lmzjmh3n4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e401ac52ee495ae401add840560ebc1e076ab38a"}, {"y": 25, "x": 216, "u": "https://preview.redd.it/f5lmzjmh3n4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cf26a3a61c00a1c1dcfa09e97547c35100c8613"}, {"y": 37, "x": 320, "u": "https://preview.redd.it/f5lmzjmh3n4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0999d2d63ae67dae261a157f173b278fadcd2356"}, {"y": 74, "x": 640, "u": "https://preview.redd.it/f5lmzjmh3n4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36261afacd3494fa7e8591442f20ffe4b6183b61"}], "s": {"y": 90, "x": 774, "u": "https://preview.redd.it/f5lmzjmh3n4b1.png?width=774&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e2f5e78f0f17a4ec6a499081373e6e25fe0fd4fd"}, "id": "f5lmzjmh3n4b1"}}, "name": "t3_143mano", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Media Serving", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/q3_uz2a2ofMlybcpBu1i87MmeGhprrl0Zx2biymjtN4.jpg", "edited": 1686183727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686165748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT:&lt;/p&gt;\n\n&lt;p&gt;It was as simple as enabling AIO threads.&lt;/p&gt;\n\n&lt;p&gt;aio threads; to location /&lt;/p&gt;\n\n&lt;p&gt;I still have this issue though&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/nginx/comments/141pl9d/nginx_proxy_cache_speeds_slower_on_httphttps/\"&gt;https://www.reddit.com/r/nginx/comments/141pl9d/nginx_proxy_cache_speeds_slower_on_httphttps/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;I am trying to use nginx_proxy cache to serve static objects. It doesn&amp;#39;t matter to me what caching software I use. I prefer Apache Traffic Server, but this doesn&amp;#39;t seem to be an issue with a specific software. It seems to be an issue with settings, and NGINX is used WAY more than ats.&lt;/p&gt;\n\n&lt;p&gt;I have a 100Mbps ethernet connection.&lt;/p&gt;\n\n&lt;p&gt;If you want to test each cdn, I made a demonstration page where you just need to click the name of the cdn, and it&amp;#39;ll load the correct images.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://demonstrattion.neocities.org/\"&gt;https://demonstrattion.neocities.org/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Below are the waterfalls of different solutions sending the same size objects.&lt;/p&gt;\n\n&lt;p&gt;My NGINX server (proxy cache) and CacheFly (nginx-&amp;gt;varnish) waterfalls look like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f5lmzjmh3n4b1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e2f5e78f0f17a4ec6a499081373e6e25fe0fd4fd\"&gt;My NGINX server waterfall for 5 1.5MB images. 24ms away. Fastest possible image download speed is 121ms. (145ms including server response)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The first object is sent at full speed, and the next ones are sent sequentially, except the last 2 seem different but they&amp;#39;re still sequential? I&amp;#39;m not sure how to describe that behavior.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With CDN77 (nginx), Gcore (NGINX), Edgio (haproxy-&amp;gt;varnish) and Automattic (NGINX), the waterfalls look like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vof01c843n4b1.png?width=772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9c62ca9e164f4c61f4bb4b6dd221b8e709796c12\"&gt;CDN77 waterfall for 5 1.5MB images. 12ms away. fastest possible image download speed is 121ms. (133ms including server response)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The server responds to all requests and then sends the images sequentially? The first object is not downloaded at full speed though.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With Apple CDN (Apache traffic server), Akamai (?), and Bunny CDN (NGINX), the waterfalls look like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x9qfkquc3n4b1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=857675d8d0df1a630f9108253dbba2e1b41199b0\"&gt;Apple CDN waterfall for 5 1.5mb images. 12ms away. fastest possible image download speed is 121ms. (133ms including server responses)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The server responds to all requests, then sends every object at once.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;From what I can tell, it is most efficient to serve objects the way Apple CDN, Akamai and Bunny do.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure which waterfall reflects the most efficient way to serving objects, nor how to achieve any behavior other than my current NGINX instance or CacheFly.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have any insight? Anything is helpful. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "cb71ccc0-7e67-11e9-841a-0e67038620c2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143mano", "is_robot_indexable": true, "report_reasons": null, "author": "Trick_Algae5810", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143mano/how_to_force_nginx_to_serve_all_files_at_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143mano/how_to_force_nginx_to_serve_all_files_at_the_same/", "subreddit_subscribers": 253191, "created_utc": 1686165748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "", "author_fullname": "t2_7q1bce91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discourse people are seeking feedback on ActivityPub Implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_143jhhk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qABCSDfBFcOuNmhPtiKkJ0-v43VbSJ_vigKCIBQLYqA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686159126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "socialhub.activitypub.rocks", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://socialhub.activitypub.rocks/t/adding-federation-support-to-discourse/2966/7", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/msR6u55Wb86P0ae7ZuJwWBL-BcjrrlidBo8Y8TA0YEs.jpg?auto=webp&amp;v=enabled&amp;s=ab24c5ddf8fea0854a88c5dca82cd8d110dbc680", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/msR6u55Wb86P0ae7ZuJwWBL-BcjrrlidBo8Y8TA0YEs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1064ee804c3b8df336dce8e3954595fb8aa953d", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/msR6u55Wb86P0ae7ZuJwWBL-BcjrrlidBo8Y8TA0YEs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e6f798733409ac00c8312860d8db09a51d288fb", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/msR6u55Wb86P0ae7ZuJwWBL-BcjrrlidBo8Y8TA0YEs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b4ff8bb2aded9e1eba6994f87dbd45ade14da7d", "width": 320, "height": 320}], "variants": {}, "id": "XpHSqUx4eA4-2hD8IQku15kljP6HrTzsLdik-4IeT9I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "143jhhk", "is_robot_indexable": true, "report_reasons": null, "author": "ex_06", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143jhhk/discourse_people_are_seeking_feedback_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://socialhub.activitypub.rocks/t/adding-federation-support-to-discourse/2966/7", "subreddit_subscribers": 253191, "created_utc": 1686159126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "TL;DR Latest version of Docker seems to be incompatible with LXC on Proxmox, and I need help to set up Docker, Compose, and Portainer on versions that do work in an LXC.\n\nI'm using Proxmox and I have an Ubuntu server 22.04.2 LXC. Following Docker's own install guides like [this one](https://docs.docker.com/engine/install/ubuntu/) as well as other guides like [this one](https://docs.fuga.cloud/how-to-install-portainer-docker-ui-manager-on-ubuntu-20.04-18.04-16.04) run into errors. Here is the error from trying a docker hello-world\n\n`sudo docker run hello-world`\n\n`docker: Error response from daemon: AppArmor enabled on system but the docker-default profile could not be loaded: running \\`/usr/sbin/apparmor\\_parser apparmor\\_parser -Kr /var/lib/docker/tmp/docker-default1024565780\\` failed with output: apparmor\\_parser: Unable to replace \"docker-default\".  Permission denied; attempted to load a profile while confined?\\`\n\n&amp;#x200B;\n\n`error: exit status 243.`\n\n`ERRO[0000] error waiting for container:`\n\nI have been chasing a rabbit hole of errors for a while, and bug posts like [this one](https://github.com/moby/moby/pull/44902) and [this one](https://github.com/moby/moby/issues/44900) as well as other posts [like this one](https://forum.proxmox.com/threads/priviledge-container-disabling-apparmor-does-not-work.122168/) have led me to believe that the problem I'm facing is that my LXC does not support Apparmor because as an LXC, it is using the kernel from the host OS (Proxmox) and the team behind one of the docker packages is trying to force Apparmor because of safety concerns. This seems to have come up at Docker v. 23, and was somehow sorted out for subsequent 23.x versions (apparently by making Apparmor no longer required, at least for the time being), but the latest install version is now 24.0.2 and the errors related to Apparmor are back. I'm all for safety, and if there's some way to get Apparmor actually working in Proxmox so that my LXC container can use it, that'd be great. But if not, I need a way to go back to an older version of docker, I guess.\n\nI don't know very much about installing packages that are older than the latest apt versions, so I can't figure out how to just install older versions of docker and related packages like portainer. That's why I'm asking here for help. Is there a CT Template somewhere (from a reputable source) that includes docker, compose, and portainer? Or could someone point me at a guide or even just the console command to install the working (older) versions of docker and its related packages? I can't just use latest because it seems that the problems are with the latest versions.\n\nThanks, and I hope this also helps anyone else who is banging their heads against the wall trying to get Docker to work in an LXC container in Portainer, and following guides that used to work no longer works because of these Apparmor errors.", "author_fullname": "t2_5jpmatk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for LXC-friendly Docker install instructions for use with Compose and Portainer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1436ekf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686122691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR Latest version of Docker seems to be incompatible with LXC on Proxmox, and I need help to set up Docker, Compose, and Portainer on versions that do work in an LXC.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Proxmox and I have an Ubuntu server 22.04.2 LXC. Following Docker&amp;#39;s own install guides like &lt;a href=\"https://docs.docker.com/engine/install/ubuntu/\"&gt;this one&lt;/a&gt; as well as other guides like &lt;a href=\"https://docs.fuga.cloud/how-to-install-portainer-docker-ui-manager-on-ubuntu-20.04-18.04-16.04\"&gt;this one&lt;/a&gt; run into errors. Here is the error from trying a docker hello-world&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sudo docker run hello-world&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker: Error response from daemon: AppArmor enabled on system but the docker-default profile could not be loaded: running \\&lt;/code&gt;/usr/sbin/apparmor_parser apparmor_parser -Kr /var/lib/docker/tmp/docker-default1024565780` failed with output: apparmor_parser: Unable to replace &amp;quot;docker-default&amp;quot;.  Permission denied; attempted to load a profile while confined?`&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;error: exit status 243.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ERRO[0000] error waiting for container:&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I have been chasing a rabbit hole of errors for a while, and bug posts like &lt;a href=\"https://github.com/moby/moby/pull/44902\"&gt;this one&lt;/a&gt; and &lt;a href=\"https://github.com/moby/moby/issues/44900\"&gt;this one&lt;/a&gt; as well as other posts &lt;a href=\"https://forum.proxmox.com/threads/priviledge-container-disabling-apparmor-does-not-work.122168/\"&gt;like this one&lt;/a&gt; have led me to believe that the problem I&amp;#39;m facing is that my LXC does not support Apparmor because as an LXC, it is using the kernel from the host OS (Proxmox) and the team behind one of the docker packages is trying to force Apparmor because of safety concerns. This seems to have come up at Docker v. 23, and was somehow sorted out for subsequent 23.x versions (apparently by making Apparmor no longer required, at least for the time being), but the latest install version is now 24.0.2 and the errors related to Apparmor are back. I&amp;#39;m all for safety, and if there&amp;#39;s some way to get Apparmor actually working in Proxmox so that my LXC container can use it, that&amp;#39;d be great. But if not, I need a way to go back to an older version of docker, I guess.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know very much about installing packages that are older than the latest apt versions, so I can&amp;#39;t figure out how to just install older versions of docker and related packages like portainer. That&amp;#39;s why I&amp;#39;m asking here for help. Is there a CT Template somewhere (from a reputable source) that includes docker, compose, and portainer? Or could someone point me at a guide or even just the console command to install the working (older) versions of docker and its related packages? I can&amp;#39;t just use latest because it seems that the problems are with the latest versions.&lt;/p&gt;\n\n&lt;p&gt;Thanks, and I hope this also helps anyone else who is banging their heads against the wall trying to get Docker to work in an LXC container in Portainer, and following guides that used to work no longer works because of these Apparmor errors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1436ekf", "is_robot_indexable": true, "report_reasons": null, "author": "ResearchTLDR", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1436ekf/looking_for_lxcfriendly_docker_install/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1436ekf/looking_for_lxcfriendly_docker_install/", "subreddit_subscribers": 253191, "created_utc": 1686122691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I am looking for a tool to upload and encrypt data to Backblaze B2. So far, I\u2019ve found rclone, Duplicati, Borg, Duplicacy, Restic, Duplicity, Kopia, etc. From what I see, it seems that rclone is good for transferring data, while the other programs are more for versioning/file history.\n\nMy use case however, is simply updating (not syncing) new files to the cloud. And the files are videos, images, and other things that I don\u2019t care to have versioning for. This leads me to think that rclone would work best for me, however I keep seeing comments warning that rclone is not a backup tool. \n\nBased on my use cases, does rclone make the most sense? Or am I too noob to see everything I would be missing by using a dedicated backup program? \n\nMy wants for uploading tool:\n\n* *updates* data, not sync. I don\u2019t want things I delete locally to be deleted in the cloud. I just want new/changed files to be transferred to the cloud. \n* can encrypt data before it hits the cloud\n* validates checksums of files (while uploading?) to ensure data integrity -- even with data that is encrypted before uploading\n* I don\u2019t think I need deduplication or versioning\n\nThanks for the help!", "author_fullname": "t2_heaw12g7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which cloud backup software/tool fits my use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "cloudstorage", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1434j70", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Cloud Storage", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686116226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a tool to upload and encrypt data to Backblaze B2. So far, I\u2019ve found rclone, Duplicati, Borg, Duplicacy, Restic, Duplicity, Kopia, etc. From what I see, it seems that rclone is good for transferring data, while the other programs are more for versioning/file history.&lt;/p&gt;\n\n&lt;p&gt;My use case however, is simply updating (not syncing) new files to the cloud. And the files are videos, images, and other things that I don\u2019t care to have versioning for. This leads me to think that rclone would work best for me, however I keep seeing comments warning that rclone is not a backup tool. &lt;/p&gt;\n\n&lt;p&gt;Based on my use cases, does rclone make the most sense? Or am I too noob to see everything I would be missing by using a dedicated backup program? &lt;/p&gt;\n\n&lt;p&gt;My wants for uploading tool:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;updates&lt;/em&gt; data, not sync. I don\u2019t want things I delete locally to be deleted in the cloud. I just want new/changed files to be transferred to the cloud. &lt;/li&gt;\n&lt;li&gt;can encrypt data before it hits the cloud&lt;/li&gt;\n&lt;li&gt;validates checksums of files (while uploading?) to ensure data integrity -- even with data that is encrypted before uploading&lt;/li&gt;\n&lt;li&gt;I don\u2019t think I need deduplication or versioning&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bc5f6346-7e67-11e9-a0fe-0e631119683e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1434j70", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway52075", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1434j70/which_cloud_backup_softwaretool_fits_my_use_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1434j70/which_cloud_backup_softwaretool_fits_my_use_case/", "subreddit_subscribers": 253191, "created_utc": 1686116226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi, I always come across posts about Jellyfin or Plex, however, I fail to understand why people use it and how they source the content to be stored there.\n\nIn the age of streaming, most of the content we consume or have the time to consume is already available. Occasionally, there is a movie/series which is not available in a particular region, but, for me personally, those situations are pretty rare and the subscriptions are also pretty cheap, like Netflix - $7, YT Premium (family) - $2, Disney - $2-3, Prime Video - $12/year. So, it makes no financial sense to purchase Blu-Rays, DVDs instead of just getting a subscription.\n\nAs I understood, Jellyfin allows people to basically rip their collections and make them available in a nice and easy to use interface. Is that right?\n\nIf yes, How do you source the content? Do you still end up purchasing physical disks? or torrents? I come from a nation where DVDs and Blu-Ray disks were never popular because they were and still are expensive.\n\nSo, I don't understand what use-cases Jellyfin, Plex solve; why are they so popular? Or am I missing something very obvious.", "author_fullname": "t2_jj0h1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why people use Jellyfin or Plex?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_143zpmt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686200086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I always come across posts about Jellyfin or Plex, however, I fail to understand why people use it and how they source the content to be stored there.&lt;/p&gt;\n\n&lt;p&gt;In the age of streaming, most of the content we consume or have the time to consume is already available. Occasionally, there is a movie/series which is not available in a particular region, but, for me personally, those situations are pretty rare and the subscriptions are also pretty cheap, like Netflix - $7, YT Premium (family) - $2, Disney - $2-3, Prime Video - $12/year. So, it makes no financial sense to purchase Blu-Rays, DVDs instead of just getting a subscription.&lt;/p&gt;\n\n&lt;p&gt;As I understood, Jellyfin allows people to basically rip their collections and make them available in a nice and easy to use interface. Is that right?&lt;/p&gt;\n\n&lt;p&gt;If yes, How do you source the content? Do you still end up purchasing physical disks? or torrents? I come from a nation where DVDs and Blu-Ray disks were never popular because they were and still are expensive.&lt;/p&gt;\n\n&lt;p&gt;So, I don&amp;#39;t understand what use-cases Jellyfin, Plex solve; why are they so popular? Or am I missing something very obvious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143zpmt", "is_robot_indexable": true, "report_reasons": null, "author": "_ppaliwal", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143zpmt/why_people_use_jellyfin_or_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143zpmt/why_people_use_jellyfin_or_plex/", "subreddit_subscribers": 253191, "created_utc": 1686200086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi all,\n\nI've recently moved to a new location with a router that I do not control. My server is still in the same location and everything appears fine when connecting to my hosted websites from most networks. However, when I attempt to connect directly from my new apartments router to any of my self hosted websites, I get the following error (on all browsers and devices):\n\n# This site can\u2019t provide a secure connection\n\n`proxy.domain.com sent an invalid response.`\n\n`ERR_SSL_PROTOCOL_ERROR` \n\n&amp;#x200B;\n\nOriginally I thought it was because the new network exclusively uses ipv6, but this does not seem to be the case after testing my domains with this site: [https://www.ssllabs.com/ssltest/](https://www.ssllabs.com/ssltest). It also wouldn't make sense that connecting from an ipv6 network should affect my ability to securely connect to the same server with the same network infrastructure. \n\n&amp;#x200B;\n\nI don't think I can access the router management portal in the new apartment, but is there anything I can try from my end to diagnose what might be causing this issue? I've also tried clearing my SSL cache on my laptop and tested using my phone with the same results. I haven't encountered this issue when trying to visit any other websites besides my own.", "author_fullname": "t2_2zpt98o4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connection issues related to SSL on new network", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143t4s9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686186803.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686181802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently moved to a new location with a router that I do not control. My server is still in the same location and everything appears fine when connecting to my hosted websites from most networks. However, when I attempt to connect directly from my new apartments router to any of my self hosted websites, I get the following error (on all browsers and devices):&lt;/p&gt;\n\n&lt;h1&gt;This site can\u2019t provide a secure connection&lt;/h1&gt;\n\n&lt;p&gt;&lt;code&gt;proxy.domain.com sent an invalid response.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ERR_SSL_PROTOCOL_ERROR&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Originally I thought it was because the new network exclusively uses ipv6, but this does not seem to be the case after testing my domains with this site: &lt;a href=\"https://www.ssllabs.com/ssltest\"&gt;https://www.ssllabs.com/ssltest/&lt;/a&gt;. It also wouldn&amp;#39;t make sense that connecting from an ipv6 network should affect my ability to securely connect to the same server with the same network infrastructure. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think I can access the router management portal in the new apartment, but is there anything I can try from my end to diagnose what might be causing this issue? I&amp;#39;ve also tried clearing my SSL cache on my laptop and tested using my phone with the same results. I haven&amp;#39;t encountered this issue when trying to visit any other websites besides my own.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143t4s9", "is_robot_indexable": true, "report_reasons": null, "author": "nrgbistro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143t4s9/connection_issues_related_to_ssl_on_new_network/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143t4s9/connection_issues_related_to_ssl_on_new_network/", "subreddit_subscribers": 253191, "created_utc": 1686181802.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Firebase dynamic linking is going away very soon, we use them strictly for opening our native apps (or if not installed, opening the correct app store). [Branch.io](https://Branch.io) can do it, but they want &gt; $40k/year for what we are getting for free from Firebase. \n\nCan anyone suggest a simple self hosted service that can handle our use-case out of the box?", "author_fullname": "t2_n99ktuk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for firebase dynamic linking or branch.io alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143livk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686163953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Firebase dynamic linking is going away very soon, we use them strictly for opening our native apps (or if not installed, opening the correct app store). &lt;a href=\"https://Branch.io\"&gt;Branch.io&lt;/a&gt; can do it, but they want &amp;gt; $40k/year for what we are getting for free from Firebase. &lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest a simple self hosted service that can handle our use-case out of the box?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?auto=webp&amp;v=enabled&amp;s=88a012e940606d59d3e4b225074f4331713e20b3", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9dda845c422af24c1a863557cb9594ba6de3aeef", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=927a10197377a19b511c60136b1c1f24156c8f54", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b64c9ce93cf0b60b0ffc139e50f23a4b54d6509", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=521ba36644edd392c318937a02bfb0b323037aaa", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f123292a6818cac96c0f2745131f67984e23dd1", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/jzbt_b09-eNRxsGIrtm8RjE1l74S1dkQXlyESzGvYU8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af839e381ab74a5606998fa49521140667e062db", "width": 1080, "height": 565}], "variants": {}, "id": "y5e4KacMBbW2bMeFNvgIDp8qmpAaa5xCs6Ui-8qRg_0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143livk", "is_robot_indexable": true, "report_reasons": null, "author": "SGlokta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143livk/looking_for_firebase_dynamic_linking_or_branchio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143livk/looking_for_firebase_dynamic_linking_or_branchio/", "subreddit_subscribers": 253191, "created_utc": 1686163953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I've been building web apps in different languages since 1994, and I've gone through basically every version of hosting: self hosting, dedicated servers in data centers, running my own data center, using VPS, cloud, etc. (I was one of the founders of OpenStack and really took that to the logical extreme). These days, I try and use my own laptop for as much and as long as possible - and it's always been a hassle to set up and manage a reverse proxy, with DNS entries and the appropriate SSL certificates, for each local project.\n\n[Loophost](https://loophost.dev) is a cross-platform reverse proxy, with a web-based management interface and built-in certificate management, that makes this easy. It also integrates with an SSH-tunneling service so that I can share access to my laptop with my teammates and clients. (If you want to use NGrok or some other tunneling service of your own, that works seamlessly too).\n\nThis is our first beta release - it works well on Mac, but we're still ironing out the bugs on windows and linux. (In particular, the tunneling doesn't work on windows yet).\n\n\\[Technical details: The proxy itself is in Go for performance; the web interface and tunnel management is in Python, and the entire package is bundled as a pip-installable python library for convenience. Certificates are issued by LetsEncrypt.\\]", "author_fullname": "t2_91gcjawy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Tool for Easier SSL-Secured Local Hosting: LoopHost.dev", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "proxy", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143lirv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Proxy", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686163947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been building web apps in different languages since 1994, and I&amp;#39;ve gone through basically every version of hosting: self hosting, dedicated servers in data centers, running my own data center, using VPS, cloud, etc. (I was one of the founders of OpenStack and really took that to the logical extreme). These days, I try and use my own laptop for as much and as long as possible - and it&amp;#39;s always been a hassle to set up and manage a reverse proxy, with DNS entries and the appropriate SSL certificates, for each local project.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://loophost.dev\"&gt;Loophost&lt;/a&gt; is a cross-platform reverse proxy, with a web-based management interface and built-in certificate management, that makes this easy. It also integrates with an SSH-tunneling service so that I can share access to my laptop with my teammates and clients. (If you want to use NGrok or some other tunneling service of your own, that works seamlessly too).&lt;/p&gt;\n\n&lt;p&gt;This is our first beta release - it works well on Mac, but we&amp;#39;re still ironing out the bugs on windows and linux. (In particular, the tunneling doesn&amp;#39;t work on windows yet).&lt;/p&gt;\n\n&lt;p&gt;[Technical details: The proxy itself is in Go for performance; the web interface and tunnel management is in Python, and the entire package is bundled as a pip-installable python library for convenience. Certificates are issued by LetsEncrypt.]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5e46c26c-7e68-11e9-8d4e-0e3bbb559e74", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143lirv", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Blacksmith5678", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143lirv/open_source_tool_for_easier_sslsecured_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143lirv/open_source_tool_for_easier_sslsecured_local/", "subreddit_subscribers": 253191, "created_utc": 1686163947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "As the title says mostly. I wasn't sure if this was the best place to post but I figured I'd start here.\n\n&amp;#x200B;\n\nSo we use our phone system through through Vox Implant, &amp; use lin phone as the softphone, &amp; we use one YeaLink. The Phone system work as they should.\n\n&amp;#x200B;\n\nWhat I saw is that LinPhone has an option for the contacts to read from an LDAP so we can know whose calling when they call. \n\n&amp;#x200B;\n\nMy question is there any way to setup an LDAP server with our Windows 10 computer (not windows server) so that our linphone can tell us whose calling? I'd  like the contacts to preferably be hosted on our computer in house (hence why I posted in selfhosted) I have some Pi3+ laying around also incase I'd need linux instead. \n\n&amp;#x200B;\n\nI an open to other options as well, &amp; can give more info if needed. Thank you for your time.", "author_fullname": "t2_f4ykt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "way to sync contacts with windows, &amp; then setup an LDAP directory to use with an SIP to look up names.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "calendarcontacts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ke6v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Calendar and Contacts", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686161270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says mostly. I wasn&amp;#39;t sure if this was the best place to post but I figured I&amp;#39;d start here.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So we use our phone system through through Vox Implant, &amp;amp; use lin phone as the softphone, &amp;amp; we use one YeaLink. The Phone system work as they should.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I saw is that LinPhone has an option for the contacts to read from an LDAP so we can know whose calling when they call. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is there any way to setup an LDAP server with our Windows 10 computer (not windows server) so that our linphone can tell us whose calling? I&amp;#39;d  like the contacts to preferably be hosted on our computer in house (hence why I posted in selfhosted) I have some Pi3+ laying around also incase I&amp;#39;d need linux instead. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I an open to other options as well, &amp;amp; can give more info if needed. Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2037fc84-7e68-11e9-a512-0ec2fc8fba1e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143ke6v", "is_robot_indexable": true, "report_reasons": null, "author": "XxkoolloserxX", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143ke6v/way_to_sync_contacts_with_windows_then_setup_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143ke6v/way_to_sync_contacts_with_windows_then_setup_an/", "subreddit_subscribers": 253191, "created_utc": 1686161270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "", "author_fullname": "t2_gn7qgeza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Lessons Learned Connecting Every IdP to OIDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "softwaredev", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_143gjty", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Software Development", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/OC1fSOg1a771Ahw8INBKGUjS-c7ukSp8QhnHBdzj-gw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686152318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pomerium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.pomerium.com/blog/5-lessons-learned-connecting-every-idp-to-oidc/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GnSElKJaEHIQyp3PTCP05FHfg2PMLkwY2xU1YpIccyM.jpg?auto=webp&amp;v=enabled&amp;s=dc365f02eee9bc213c62ca651ef1d8275e200ffe", "width": 800, "height": 765}, "resolutions": [{"url": "https://external-preview.redd.it/GnSElKJaEHIQyp3PTCP05FHfg2PMLkwY2xU1YpIccyM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e80881e5df951834dafed2a11660c832117b089d", "width": 108, "height": 103}, {"url": "https://external-preview.redd.it/GnSElKJaEHIQyp3PTCP05FHfg2PMLkwY2xU1YpIccyM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20c1cbbc29e5617338e3de2289078a9d3b644208", "width": 216, "height": 206}, {"url": "https://external-preview.redd.it/GnSElKJaEHIQyp3PTCP05FHfg2PMLkwY2xU1YpIccyM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8caf105a88ad44f0a7282072091af79751e99bc7", "width": 320, "height": 306}, {"url": "https://external-preview.redd.it/GnSElKJaEHIQyp3PTCP05FHfg2PMLkwY2xU1YpIccyM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39a623401ac66871cafb3ee4fb7d7a9c79b436f8", "width": 640, "height": 612}], "variants": {}, "id": "pE8MglBw_nvn_h-nmsP5UNbfPjcmVIMcD2HJOGLbaSE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "684a1552-7e68-11e9-a4b1-0e84949b6a4e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "143gjty", "is_robot_indexable": true, "report_reasons": null, "author": "Pomerium_CMo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143gjty/5_lessons_learned_connecting_every_idp_to_oidc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.pomerium.com/blog/5-lessons-learned-connecting-every-idp-to-oidc/", "subreddit_subscribers": 253191, "created_utc": 1686152318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I have a proxmox server running on a ssd and a 5tb raid where's the data directory located and stuff like that.\n\nI have a jellyfin server running on one container and nextcloud on another one. The video files are stored on /data/smb on the host machine and are shared to the jellyfin container. Nextcloud stores its files on /data/nextcloud but I can't copy or move them to another directory (eg. /data/smb). I tried to change the permission of the nextcloud files directory so the host machine can access but that didn't work.\n\nCan someone help me with that?", "author_fullname": "t2_vl9z9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Proxmox copying files to another lxc container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143bt4z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686140672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a proxmox server running on a ssd and a 5tb raid where&amp;#39;s the data directory located and stuff like that.&lt;/p&gt;\n\n&lt;p&gt;I have a jellyfin server running on one container and nextcloud on another one. The video files are stored on /data/smb on the host machine and are shared to the jellyfin container. Nextcloud stores its files on /data/nextcloud but I can&amp;#39;t copy or move them to another directory (eg. /data/smb). I tried to change the permission of the nextcloud files directory so the host machine can access but that didn&amp;#39;t work.&lt;/p&gt;\n\n&lt;p&gt;Can someone help me with that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143bt4z", "is_robot_indexable": true, "report_reasons": null, "author": "FAKERHOCH10000", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143bt4z/proxmox_copying_files_to_another_lxc_container/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143bt4z/proxmox_copying_files_to_another_lxc_container/", "subreddit_subscribers": 253191, "created_utc": 1686140672.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}