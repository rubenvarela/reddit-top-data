{"kind": "Listing", "data": {"after": "t3_143mg7o", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I came across these guys when working to save all of pikamee's youtube channel before it was deleted. They had already been saving everything as it was released. The site hosts over 200,000 videos. \n\nThe stated closure date is \"on or before July 24, 2023.\"\n\nAccording to the letter posted about the issue, they were using a Google Workspaces team drive with no backup. Apparently, Google is cracking down on their storage policies and a 1.38 PB teams drive is understandably pretty high up on their priority list.\n\nThis post is my warning to check for and save anything you care about that may be lost before the site is gone forever.\n\n[this is their website](https://archive.ragtag.moe/)", "author_fullname": "t2_tglchi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ragtag Archive is going offline - 1.38 PB of vtuber archives will be gone, many of which do not exist elsewhere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143zvuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 440, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 440, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686200596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across these guys when working to save all of pikamee&amp;#39;s youtube channel before it was deleted. They had already been saving everything as it was released. The site hosts over 200,000 videos. &lt;/p&gt;\n\n&lt;p&gt;The stated closure date is &amp;quot;on or before July 24, 2023.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;According to the letter posted about the issue, they were using a Google Workspaces team drive with no backup. Apparently, Google is cracking down on their storage policies and a 1.38 PB teams drive is understandably pretty high up on their priority list.&lt;/p&gt;\n\n&lt;p&gt;This post is my warning to check for and save anything you care about that may be lost before the site is gone forever.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.ragtag.moe/\"&gt;this is their website&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?auto=webp&amp;v=enabled&amp;s=6965c765e274449bdf42d3e1ee8713323b9e4172", "width": 1606, "height": 1625}, "resolutions": [{"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb55a6beb91f37fd66ed1f6d5a5832ad175f44ea", "width": 108, "height": 109}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8bd3ab6a5847f5a67c0df948a8539981014dc1f", "width": 216, "height": 218}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19caf8d55f164ee26441b77fc627cf22bb60f290", "width": 320, "height": 323}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7c64f85d5cf1290838552a8c1801d5138faa55b", "width": 640, "height": 647}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9df107ea0a4fa460ee2d124246813a1c9aa132fa", "width": 960, "height": 971}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b774fd9ccae178fe28e60ff981e6cf41ff694714", "width": 1080, "height": 1092}], "variants": {}, "id": "xjNVVn9Si3HT9w46y-2tNSDqP-BvogIS9gJZJgKG7XI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143zvuh", "is_robot_indexable": true, "report_reasons": null, "author": "avypath", "discussion_type": null, "num_comments": 141, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143zvuh/ragtag_archive_is_going_offline_138_pb_of_vtuber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143zvuh/ragtag_archive_is_going_offline_138_pb_of_vtuber/", "subreddit_subscribers": 686633, "created_utc": 1686200596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 8tb drive that is probably 5+ years old that appears to be corrupt. I have tons of files on it and this morning when I open a folder and click on a video file, I get a \"file missing\" message. However I can see the file and click on its properties as well. Another odd thing, I some files simply vanished, I clicked on a certain movie and got that error message, when I search for the file again its gone, as if i deleted it. My best guess is that this HD just crapped out. Luckily I have a back up, my questions are, is there a way to fix this damaged HD? can formatting it fix it? is is it time to buy a  new one? or is there another option ?\n\n&amp;#x200B;\n\nupdate: Crystal Disk info [https://ibb.co/VH9W7q4](https://ibb.co/VH9W7q4)", "author_fullname": "t2_8x1jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8tb hard drive just died on me.....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1442krz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686224586.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686209812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 8tb drive that is probably 5+ years old that appears to be corrupt. I have tons of files on it and this morning when I open a folder and click on a video file, I get a &amp;quot;file missing&amp;quot; message. However I can see the file and click on its properties as well. Another odd thing, I some files simply vanished, I clicked on a certain movie and got that error message, when I search for the file again its gone, as if i deleted it. My best guess is that this HD just crapped out. Luckily I have a back up, my questions are, is there a way to fix this damaged HD? can formatting it fix it? is is it time to buy a  new one? or is there another option ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;update: Crystal Disk info &lt;a href=\"https://ibb.co/VH9W7q4\"&gt;https://ibb.co/VH9W7q4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?auto=webp&amp;v=enabled&amp;s=2f8bdfe5473e821beafe8890b10931458ccb2f8b", "width": 1017, "height": 982}, "resolutions": [{"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d0e929568bcf474623d885d0cc9ad342bea504b", "width": 108, "height": 104}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f097589251a1479fae1088ee8022b226747c85b", "width": 216, "height": 208}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3516d4e879b225c078f593e080428101b4ea3788", "width": 320, "height": 308}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8a83255272b42b868c62bfc79990044f13088a6", "width": 640, "height": 617}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=885d0a36484824a76843295711e9864cbc486c34", "width": 960, "height": 926}], "variants": {}, "id": "eVDXWjO_hVosDSjRMsDHbyUi2L7SWPUtnocTGivdmD8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1442krz", "is_robot_indexable": true, "report_reasons": null, "author": "FackJooBish", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1442krz/8tb_hard_drive_just_died_on_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1442krz/8tb_hard_drive_just_died_on_me/", "subreddit_subscribers": 686633, "created_utc": 1686209812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8gagi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Reveals HAMR HDD Roadmap: 32TB First, 40TB Follows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_144bzwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5NwS5LWEc5DZovhPMX0Y_cm0yoyue-CcAQ8ir8jnL4Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686237377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tomshardware.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tomshardware.com/news/seagate-reveals-hamr-roadmap-32-tb-comes-first", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?auto=webp&amp;v=enabled&amp;s=c5102b8e1443e127b8716303781b9dbf2a76e617", "width": 970, "height": 545}, "resolutions": [{"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c80c107734a2fdcd19d264b6e270ee237e6dcd0", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6643066a44079afb3e75a19dcb8aa14c5798bb", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c8db52fe100928120d215d22f0a08c2ad76d641", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2f0972a0be6f367b3dd6a91fb8e8cd17e1c67c2", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a697a1fa85d558dbec8265b573c9b4cfb326b421", "width": 960, "height": 539}], "variants": {}, "id": "kRg5RyowODqqOJ7ePMaiyS-M5KEDFFcqtEoWi3ur4iI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144bzwt", "is_robot_indexable": true, "report_reasons": null, "author": "ben7337", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144bzwt/seagate_reveals_hamr_hdd_roadmap_32tb_first_40tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tomshardware.com/news/seagate-reveals-hamr-roadmap-32-tb-comes-first", "subreddit_subscribers": 686633, "created_utc": 1686237377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nWhat solution are you guys using to host/browse downloaded reddit content?\n\nI am working with the ZST files downloaded from Pushshift and sorted into subreddits by the lovely u/watchful1 [here](https://academictorrents.com/details/c398a571976c78d346c325bd75c47b82edf6124e). ZST is too compressed to browse on its own but using scripts like [this one](https://github.com/Watchful1/PushshiftDumps) you can process them into readable NDJSON files. From there im not sure what to do. I would like to have a self hosted reddit-clone that i can import these dumps into and browse freely.\n\nI'm thinking i will have to get a project like [redarc](https://github.com/Yakabuff/redarc) or [BDFR-to-HTML](https://github.com/BlipRanger/bdfr-html) or much more likely [Pushshift-Importer](https://github.com/Paul-E/Pushshift-Importer) which allows you to import pushshift downloads into a SQLite database. From there i would have to hook up the database to a reddit-like frontend.\n\nIs there a solution for this already?", "author_fullname": "t2_gn46ie9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you using to browse/self host downloaded reddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143vpsm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686188612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;What solution are you guys using to host/browse downloaded reddit content?&lt;/p&gt;\n\n&lt;p&gt;I am working with the ZST files downloaded from Pushshift and sorted into subreddits by the lovely &lt;a href=\"/u/watchful1\"&gt;u/watchful1&lt;/a&gt; &lt;a href=\"https://academictorrents.com/details/c398a571976c78d346c325bd75c47b82edf6124e\"&gt;here&lt;/a&gt;. ZST is too compressed to browse on its own but using scripts like &lt;a href=\"https://github.com/Watchful1/PushshiftDumps\"&gt;this one&lt;/a&gt; you can process them into readable NDJSON files. From there im not sure what to do. I would like to have a self hosted reddit-clone that i can import these dumps into and browse freely.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking i will have to get a project like &lt;a href=\"https://github.com/Yakabuff/redarc\"&gt;redarc&lt;/a&gt; or &lt;a href=\"https://github.com/BlipRanger/bdfr-html\"&gt;BDFR-to-HTML&lt;/a&gt; or much more likely &lt;a href=\"https://github.com/Paul-E/Pushshift-Importer\"&gt;Pushshift-Importer&lt;/a&gt; which allows you to import pushshift downloads into a SQLite database. From there i would have to hook up the database to a reddit-like frontend.&lt;/p&gt;\n\n&lt;p&gt;Is there a solution for this already?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?auto=webp&amp;v=enabled&amp;s=918b2eeb77d118fd8f4ba293fa5244eb3286a9c4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff569e6656835558df2eb771cd6c91a6cf219c17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b26d33631721450780f7ec8348b184ec52bb60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6563332be0778922ae7d781b4de71ae36c5970", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0a85df145bd3584118bc59eecc1952652f8e1e5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bb05ca3c8192d78215c635f3bff8a98f332659c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=427bc2290a0c6c1b2004e5dadd7d6573f8b74411", "width": 1080, "height": 567}], "variants": {}, "id": "9ZQzIXgHwWUMmDwxygJ-VDFa1eAMjAGg-cgqxWhg4Js"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143vpsm", "is_robot_indexable": true, "report_reasons": null, "author": "pm_me_xenomorphs", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143vpsm/what_are_you_using_to_browseself_host_downloaded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143vpsm/what_are_you_using_to_browseself_host_downloaded/", "subreddit_subscribers": 686633, "created_utc": 1686188612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Synology system with about 160TB, and back up 100tb of that to google, until recently.  Deep archive sounded like a solution until I looked at restore and download costs, so I want to take the plunge into tape.\n\nLooks like a tape library would be the best option.  Since transfer speed is not a big issue to me (anything onsite will be faster than going to the cloud), I thought a refurbished LTO-5 library would be good, but looks like I might need 3 of those to back everything up.   I could go bigger with HPE LTO-7 with library that holds 40 tapes and expandable, but before the tape cost, that is $8k.  \n\nAre there other options I have not found yet?\n\nWhat tape system are you using?", "author_fullname": "t2_tix5tw2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "moving from google to tape, there are SO many options tho.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143oesz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686170633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synology system with about 160TB, and back up 100tb of that to google, until recently.  Deep archive sounded like a solution until I looked at restore and download costs, so I want to take the plunge into tape.&lt;/p&gt;\n\n&lt;p&gt;Looks like a tape library would be the best option.  Since transfer speed is not a big issue to me (anything onsite will be faster than going to the cloud), I thought a refurbished LTO-5 library would be good, but looks like I might need 3 of those to back everything up.   I could go bigger with HPE LTO-7 with library that holds 40 tapes and expandable, but before the tape cost, that is $8k.  &lt;/p&gt;\n\n&lt;p&gt;Are there other options I have not found yet?&lt;/p&gt;\n\n&lt;p&gt;What tape system are you using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143oesz", "is_robot_indexable": true, "report_reasons": null, "author": "webshammo", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143oesz/moving_from_google_to_tape_there_are_so_many/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143oesz/moving_from_google_to_tape_there_are_so_many/", "subreddit_subscribers": 686633, "created_utc": 1686170633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't have a NAS. My Plex setup is simply my 5TB external seagate plugged to my laptop. My 5TB drive is full, so I'm looking to expand my storage. I know that all drives will fail eventually, but I still want the best I can get. I was originally looking for another external HDD (around 10-15 TB), but then I started reading more about them and how the drive that's inside them isn't usually the best. So I'm starting to think that it'd be better to get something like a 12TB WD gold or red pro, and put it in an enclosure, for close to the same price (compared to the external hdd).\n\nWhat do you guys think? Also if getting an internal HDD with an enclosure is better, what would be the \"best\" HDD? I'm mainly looking at the WD red pro (they're supposedly made for NAS in mind, whatever that means, and are quiet), the WD gold (supposedly the best WD has, but loud), or something from seagate. But I'm a bit confused by the seagate naming convention of their drives. They have their X and E series which seem to be their best (?), or I could get an ironwolf, but it's apparently a lot louder than the wd red pro. I also don't know how much noise is too noisy. My 5TB seagate external makes some noise but I need to focus on it to hear it. But I'm not sure how much louder a WD gold is.\n\nMoney isn't too much an issue, and I don't care about read or write speed, or any other metrics (except longevity). 95% of my data are .mkv files that will be written once, and only read after with plex, and the remaining 5% are photos, documents, and game setup files (not the actual game, just the files for the setup, the game installation will be on another drive) that will also be written once, and rarely read after.", "author_fullname": "t2_4bxhznee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD vs internal HDD with enclosure mainly for plex.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143mb6t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686165782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t have a NAS. My Plex setup is simply my 5TB external seagate plugged to my laptop. My 5TB drive is full, so I&amp;#39;m looking to expand my storage. I know that all drives will fail eventually, but I still want the best I can get. I was originally looking for another external HDD (around 10-15 TB), but then I started reading more about them and how the drive that&amp;#39;s inside them isn&amp;#39;t usually the best. So I&amp;#39;m starting to think that it&amp;#39;d be better to get something like a 12TB WD gold or red pro, and put it in an enclosure, for close to the same price (compared to the external hdd).&lt;/p&gt;\n\n&lt;p&gt;What do you guys think? Also if getting an internal HDD with an enclosure is better, what would be the &amp;quot;best&amp;quot; HDD? I&amp;#39;m mainly looking at the WD red pro (they&amp;#39;re supposedly made for NAS in mind, whatever that means, and are quiet), the WD gold (supposedly the best WD has, but loud), or something from seagate. But I&amp;#39;m a bit confused by the seagate naming convention of their drives. They have their X and E series which seem to be their best (?), or I could get an ironwolf, but it&amp;#39;s apparently a lot louder than the wd red pro. I also don&amp;#39;t know how much noise is too noisy. My 5TB seagate external makes some noise but I need to focus on it to hear it. But I&amp;#39;m not sure how much louder a WD gold is.&lt;/p&gt;\n\n&lt;p&gt;Money isn&amp;#39;t too much an issue, and I don&amp;#39;t care about read or write speed, or any other metrics (except longevity). 95% of my data are .mkv files that will be written once, and only read after with plex, and the remaining 5% are photos, documents, and game setup files (not the actual game, just the files for the setup, the game installation will be on another drive) that will also be written once, and rarely read after.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143mb6t", "is_robot_indexable": true, "report_reasons": null, "author": "MoonlessNightss", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143mb6t/external_hdd_vs_internal_hdd_with_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143mb6t/external_hdd_vs_internal_hdd_with_enclosure/", "subreddit_subscribers": 686633, "created_utc": 1686165782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I started archiving my favorite shows a few months ago. Then I started recording live TV from a couple networks. *Now* Im curious if there's a list that well, *lists*, every company that produces television shows in America (for now, I'd love to do other countries later).\n\nI'm obviously not trying to archive every TV company's content. That would be insane. But having a spreadsheet with some basic info on them would be nice!\n\nI found [this](https://en.wikipedia.org/wiki/List_of_television_production_companies#United_States) page but was wondering if there's something more authoritative than Wikipedia. Anyone ever try to put something together like this?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a comprehensive list of every company that produces TV shows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144a6pm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686233034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started archiving my favorite shows a few months ago. Then I started recording live TV from a couple networks. &lt;em&gt;Now&lt;/em&gt; Im curious if there&amp;#39;s a list that well, &lt;em&gt;lists&lt;/em&gt;, every company that produces television shows in America (for now, I&amp;#39;d love to do other countries later).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m obviously not trying to archive every TV company&amp;#39;s content. That would be insane. But having a spreadsheet with some basic info on them would be nice!&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://en.wikipedia.org/wiki/List_of_television_production_companies#United_States\"&gt;this&lt;/a&gt; page but was wondering if there&amp;#39;s something more authoritative than Wikipedia. Anyone ever try to put something together like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144a6pm", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144a6pm/is_there_a_comprehensive_list_of_every_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144a6pm/is_there_a_comprehensive_list_of_every_company/", "subreddit_subscribers": 686633, "created_utc": 1686233034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My children's school regularly posts photos and videos of school life to their twitter account. I have been using gallery-dl to archive these images. I tend to donwload a month at a time, but recently it has stopped working, can anyone please help me?  \n\n\nHere is the command I usually issue (using google instead of real account):\n\n    PS C:\\Users\\me\\Downloads&gt; .\\gallery-dl --dest gallery-dl/twitter/2023-04 \"https://twitter.com/search?q=(from%3Agoogle)%20until%3A2023-05-01%20since%3A2023-04-01\"\n    [twitter][info] Requesting guest token\n    [twitter][error] 403 Forbidden (Forbidden.)\n\nAnd here is one without the date constraints:\n\n    PS C:\\Users\\me\\Downloads\\&gt; .\\gallery-dl --dest gallery-dl/twitter \"https://twitter.com/search?q=from:google\"\n    [twitter][info] Requesting guest token\n    [twitter][error] 403 Forbidden (Forbidden.)\n\nNeither command works anymore.\n\nHowever, if I run it without using the search, it still works, but then I can't filter a month or year at a time.\n\n    PS C:\\Users\\me\\Downloads&gt; .\\gallery-dl --dest gallery-dl/twitter \"http://twitter.com/google\"\n    * gallery-dl\\twitter\\2023-04\\twitter\\Google\\1666183917177536512_1.mp4\n\nIs this something to do with recent changes to the Twitter API?  \n\n\nThank you", "author_fullname": "t2_stw7a9lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gallery-dl Twitter no longer working", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1447t5w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686226996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My children&amp;#39;s school regularly posts photos and videos of school life to their twitter account. I have been using gallery-dl to archive these images. I tend to donwload a month at a time, but recently it has stopped working, can anyone please help me?  &lt;/p&gt;\n\n&lt;p&gt;Here is the command I usually issue (using google instead of real account):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PS C:\\Users\\me\\Downloads&amp;gt; .\\gallery-dl --dest gallery-dl/twitter/2023-04 &amp;quot;https://twitter.com/search?q=(from%3Agoogle)%20until%3A2023-05-01%20since%3A2023-04-01&amp;quot;\n[twitter][info] Requesting guest token\n[twitter][error] 403 Forbidden (Forbidden.)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And here is one without the date constraints:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PS C:\\Users\\me\\Downloads\\&amp;gt; .\\gallery-dl --dest gallery-dl/twitter &amp;quot;https://twitter.com/search?q=from:google&amp;quot;\n[twitter][info] Requesting guest token\n[twitter][error] 403 Forbidden (Forbidden.)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Neither command works anymore.&lt;/p&gt;\n\n&lt;p&gt;However, if I run it without using the search, it still works, but then I can&amp;#39;t filter a month or year at a time.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PS C:\\Users\\me\\Downloads&amp;gt; .\\gallery-dl --dest gallery-dl/twitter &amp;quot;http://twitter.com/google&amp;quot;\n* gallery-dl\\twitter\\2023-04\\twitter\\Google\\1666183917177536512_1.mp4\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is this something to do with recent changes to the Twitter API?  &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1447t5w", "is_robot_indexable": true, "report_reasons": null, "author": "InfamousLibrarian518", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1447t5w/gallerydl_twitter_no_longer_working/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1447t5w/gallerydl_twitter_no_longer_working/", "subreddit_subscribers": 686633, "created_utc": 1686226996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows (11/10) incremental backup solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143tz5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_a65nr", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "WindowsHelp", "selftext": "What backup solution do you use for windows and why? If it\u2019s still sold, how much does a lifetime license cost and does it get free upgrades?\n\nGot fed up with Windows Backup and looking for a replacement that doesn\u2019t cost too much.", "author_fullname": "t2_a65nr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows (11/10) incremental backup solution", "link_flair_richtext": [{"e": "text", "t": "Windows 11"}], "subreddit_name_prefixed": "r/WindowsHelp", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143m6cj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Windows 11", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686165481.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WindowsHelp", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What backup solution do you use for windows and why? If it\u2019s still sold, how much does a lifetime license cost and does it get free upgrades?&lt;/p&gt;\n\n&lt;p&gt;Got fed up with Windows Backup and looking for a replacement that doesn\u2019t cost too much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1a63a5a8-d28d-11eb-b580-0e2e73aaec5d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_38hjl", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00adef", "id": "143m6cj", "is_robot_indexable": true, "report_reasons": null, "author": "Gymnastboatman", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "subreddit_subscribers": 18317, "created_utc": 1686165481.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686183974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WindowsHelp", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "20TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143tz5f", "is_robot_indexable": true, "report_reasons": null, "author": "Gymnastboatman", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_143m6cj", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/143tz5f/windows_1110_incremental_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "subreddit_subscribers": 686633, "created_utc": 1686183974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! Sorry for my bad English in advance, *TLDR down below.*I'm on the lookout for a cloud storage system that  fits my needs without breaking the bank. I want something that is not just a backup solution, offers affordable pricing, and is kinda working in a normal cloud system way. Here are my main considerations:\n\n* I require a cloud storage service that offers unlimited storage capacity. I need to back up at least 12TB of data, and I also plan to back up my family' data in the future. Also I know when I say cloud and not a backup system, because after what I had searched on the web, kinda all backup services have two problem, one with super low speed and another with hide warnings about deletion of data.\n* Since I'm located in Europe, it's essential for the cloud storage service to provide **fast upload speeds.**\n* I'm searching for something inexpensive cloud storage, like not so expensive system that provides value for money.\n* Maybe a nice system to see your pictures, because most of my data are Raw/DNG photos\n* I don't have amazon in my country, so no Amazon Photos  unfortunately\n\nAfter some research, I came across **sync.com**, which seems promising. They offer an unlimited storage plan at a slightly lower price than Dropbox *($18 per month as of June 2023)*. However, during my tests, I found the upload speed to be quite slow, but I think is quite good, tooking in to the consideration that their servers are based in Canada.It took around **12 minutes to upload just 1GB** of data, which is a bit concerning, regarding that  I have quite a lot to upload to the cloud.  Also, the interface of sync.com is a bit not so advanced, with such a wide range of options, as Google Drive. I miss the ability to choose which folders to sync and control the synchronization settings. Or maybe I don't know how??\n\nAnother services that I have found is JottaCloud, being an interesting option, since they are based in Europe and offer good speeds. However, their speeds decrease significantly after reaching 5TB, which is not advantageous for my needs.\n\n# So, my questions are:\n\n1. Is sync.com the best option available, or are there other alternatives I should consider?\n2. Are there any third-party tools or programs that can enhance sync.com's functionality ?\n\n***I would greatly appreciate your insights and recommendations. Thank you in advance for your help!***\n\n# TL;DR:\n\nI'm searching for an inexpensive and cost-effective cloud storage system, not just a backup solution, with various sharing options and the ability to control folder access. I need unlimited storage for backing up at least 12TB of data. Since I'm based in Europe, fast speeds are essential. Amazon is not available for me. Currently, I'm considering sync.com, which offers unlimited storage at a slightly cheaper price than Dropbox. However, the upload speed seems slow, and the interface is a bit not so advanced like compared to Google Drive. Are there any better options, or are there third-party tools that can enhance sync.com's functionality?", "author_fullname": "t2_ewg4x2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an Affordable and Fast Cloud System (preferably with good speed in Europe)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144ca6g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686238497.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686238060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! Sorry for my bad English in advance, &lt;em&gt;TLDR down below.&lt;/em&gt;I&amp;#39;m on the lookout for a cloud storage system that  fits my needs without breaking the bank. I want something that is not just a backup solution, offers affordable pricing, and is kinda working in a normal cloud system way. Here are my main considerations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I require a cloud storage service that offers unlimited storage capacity. I need to back up at least 12TB of data, and I also plan to back up my family&amp;#39; data in the future. Also I know when I say cloud and not a backup system, because after what I had searched on the web, kinda all backup services have two problem, one with super low speed and another with hide warnings about deletion of data.&lt;/li&gt;\n&lt;li&gt;Since I&amp;#39;m located in Europe, it&amp;#39;s essential for the cloud storage service to provide &lt;strong&gt;fast upload speeds.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m searching for something inexpensive cloud storage, like not so expensive system that provides value for money.&lt;/li&gt;\n&lt;li&gt;Maybe a nice system to see your pictures, because most of my data are Raw/DNG photos&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t have amazon in my country, so no Amazon Photos  unfortunately&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After some research, I came across &lt;strong&gt;sync.com&lt;/strong&gt;, which seems promising. They offer an unlimited storage plan at a slightly lower price than Dropbox &lt;em&gt;($18 per month as of June 2023)&lt;/em&gt;. However, during my tests, I found the upload speed to be quite slow, but I think is quite good, tooking in to the consideration that their servers are based in Canada.It took around &lt;strong&gt;12 minutes to upload just 1GB&lt;/strong&gt; of data, which is a bit concerning, regarding that  I have quite a lot to upload to the cloud.  Also, the interface of sync.com is a bit not so advanced, with such a wide range of options, as Google Drive. I miss the ability to choose which folders to sync and control the synchronization settings. Or maybe I don&amp;#39;t know how??&lt;/p&gt;\n\n&lt;p&gt;Another services that I have found is JottaCloud, being an interesting option, since they are based in Europe and offer good speeds. However, their speeds decrease significantly after reaching 5TB, which is not advantageous for my needs.&lt;/p&gt;\n\n&lt;h1&gt;So, my questions are:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is sync.com the best option available, or are there other alternatives I should consider?&lt;/li&gt;\n&lt;li&gt;Are there any third-party tools or programs that can enhance sync.com&amp;#39;s functionality ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;I would greatly appreciate your insights and recommendations. Thank you in advance for your help!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;TL;DR:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;m searching for an inexpensive and cost-effective cloud storage system, not just a backup solution, with various sharing options and the ability to control folder access. I need unlimited storage for backing up at least 12TB of data. Since I&amp;#39;m based in Europe, fast speeds are essential. Amazon is not available for me. Currently, I&amp;#39;m considering sync.com, which offers unlimited storage at a slightly cheaper price than Dropbox. However, the upload speed seems slow, and the interface is a bit not so advanced like compared to Google Drive. Are there any better options, or are there third-party tools that can enhance sync.com&amp;#39;s functionality?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144ca6g", "is_robot_indexable": true, "report_reasons": null, "author": "stef7EAmis", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144ca6g/looking_for_an_affordable_and_fast_cloud_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144ca6g/looking_for_an_affordable_and_fast_cloud_system/", "subreddit_subscribers": 686633, "created_utc": 1686238060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all!  \nI've got a Supermicro X9SRE board that has only SATA ports.  \nSince I'd like to buy 6 of [these](https://www.piospartslap.de/HGST-6TB-72K-SAS-12Gbps-HDD-HUS726060AL5214-0F22881) 6TB SAS drives, I need a SAS controller with at least 6 ports to use them in a JBOD configuration (with SnapRAID) for backup purposes.  \nSearching  in this sub, I've read about SAS expander, can they be used with any controller? How do they work?\n\nThanks!", "author_fullname": "t2_zpww0n1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap SAS controller with at leats 6 ports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449m5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;br/&gt;\nI&amp;#39;ve got a Supermicro X9SRE board that has only SATA ports.&lt;br/&gt;\nSince I&amp;#39;d like to buy 6 of &lt;a href=\"https://www.piospartslap.de/HGST-6TB-72K-SAS-12Gbps-HDD-HUS726060AL5214-0F22881\"&gt;these&lt;/a&gt; 6TB SAS drives, I need a SAS controller with at least 6 ports to use them in a JBOD configuration (with SnapRAID) for backup purposes.&lt;br/&gt;\nSearching  in this sub, I&amp;#39;ve read about SAS expander, can they be used with any controller? How do they work?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?auto=webp&amp;v=enabled&amp;s=709e796f2397cab4f78aff183fbae8c7c192c9a4", "width": 580, "height": 580}, "resolutions": [{"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f74c0d220a8d6a693a73a7a72396123d7e1c37b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ec7efc1bb076f2eabd1856ade65a2847d31dff8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=029d698b7d5812c15c2a392243d46391b32ea0bc", "width": 320, "height": 320}], "variants": {}, "id": "5Dv7fhnNHVhNylPTdNyuGPYuV40oQMIiakKd7RH2P1k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1449m5a", "is_robot_indexable": true, "report_reasons": null, "author": "andreape_x", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1449m5a/cheap_sas_controller_with_at_leats_6_ports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1449m5a/cheap_sas_controller_with_at_leats_6_ports/", "subreddit_subscribers": 686633, "created_utc": 1686231629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "has anyone done a warranty claim with serverpartdeals?\n\ni want to buy a few WD 14TB drives.  the manufacture refurbs are out of stock; the seller refurbs are in stock.  both have a 2 year warranty.\n\nif the WD manufacturer refurb drive goes bad within 2 years, i assume i don't go thru WD but instead thru the seller -- serverpartdeals  -- for warranty claim, right?\n\nif so what would be the diff, from a warranty perspective, btw the 2 refurb types?\n\nany help appreciated.", "author_fullname": "t2_5vej5jn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "warranty on serverpartdeals' manufacturer vs seller refurbished drives ...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143qrus", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686176016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;has anyone done a warranty claim with serverpartdeals?&lt;/p&gt;\n\n&lt;p&gt;i want to buy a few WD 14TB drives.  the manufacture refurbs are out of stock; the seller refurbs are in stock.  both have a 2 year warranty.&lt;/p&gt;\n\n&lt;p&gt;if the WD manufacturer refurb drive goes bad within 2 years, i assume i don&amp;#39;t go thru WD but instead thru the seller -- serverpartdeals  -- for warranty claim, right?&lt;/p&gt;\n\n&lt;p&gt;if so what would be the diff, from a warranty perspective, btw the 2 refurb types?&lt;/p&gt;\n\n&lt;p&gt;any help appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143qrus", "is_robot_indexable": true, "report_reasons": null, "author": "redditmail9999", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143qrus/warranty_on_serverpartdeals_manufacturer_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143qrus/warranty_on_serverpartdeals_manufacturer_vs/", "subreddit_subscribers": 686633, "created_utc": 1686176016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had previously been using Grabber to back up artists who uploaded only to twitter, though it doesn't seeany posts containing nsfw content on most accounts anymore. Or even sfw content from some artists, for whatever reason.\n\nAre there any currently working tools to download all media uploaded by a given twitter account? I've looked into gallery-dl, though unless I'm missing something, it only seems to grab a handful of recent posts.", "author_fullname": "t2_7hojt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool that can backup all media posts from a Twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144foia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had previously been using Grabber to back up artists who uploaded only to twitter, though it doesn&amp;#39;t seeany posts containing nsfw content on most accounts anymore. Or even sfw content from some artists, for whatever reason.&lt;/p&gt;\n\n&lt;p&gt;Are there any currently working tools to download all media uploaded by a given twitter account? I&amp;#39;ve looked into gallery-dl, though unless I&amp;#39;m missing something, it only seems to grab a handful of recent posts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144foia", "is_robot_indexable": true, "report_reasons": null, "author": "Britefire", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144foia/is_there_a_tool_that_can_backup_all_media_posts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144foia/is_there_a_tool_that_can_backup_all_media_posts/", "subreddit_subscribers": 686633, "created_utc": 1686245872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got the new drive yesterday and it seems to be defective. Much noisier when loading tapes than LTO5 I'm trying to replace and ejects tapes after attempting a load. I also don't like the way the cartridge moves after initial insertion, LTO5 drive definitely doesn't do that. Here's a video - [https://www.youtube.com/watch?v=oiN2FCtYiSc](https://www.youtube.com/watch?v=oiN2FCtYiSc)\n\nI haven't used LTO9 drives before but assumed that it would operate the same way as LTO5.\n\nAm I missing something obvious or the drive is faulty?", "author_fullname": "t2_zw3xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO9 drive noisy and ejecting tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144c7z3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686237911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got the new drive yesterday and it seems to be defective. Much noisier when loading tapes than LTO5 I&amp;#39;m trying to replace and ejects tapes after attempting a load. I also don&amp;#39;t like the way the cartridge moves after initial insertion, LTO5 drive definitely doesn&amp;#39;t do that. Here&amp;#39;s a video - &lt;a href=\"https://www.youtube.com/watch?v=oiN2FCtYiSc\"&gt;https://www.youtube.com/watch?v=oiN2FCtYiSc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t used LTO9 drives before but assumed that it would operate the same way as LTO5.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something obvious or the drive is faulty?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?auto=webp&amp;v=enabled&amp;s=e331f7c7298866143ace7fd4f30affd36b8c26eb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebd9738cac6ef98fe92ede81a0f77b0fae3c0564", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca50bbe69b97e813bacdda6a25cc7ce78d1f1be8", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=419a848260fa92c83dc15af1e1089488ba99d910", "width": 320, "height": 240}], "variants": {}, "id": "qyMJauVfeMVCwkIHQshyVua_e1Vjec6HpzOWMkYUIfw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144c7z3", "is_robot_indexable": true, "report_reasons": null, "author": "K7API", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144c7z3/lto9_drive_noisy_and_ejecting_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144c7z3/lto9_drive_noisy_and_ejecting_tapes/", "subreddit_subscribers": 686633, "created_utc": 1686237911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Article in German](https://www.golem.de/news/update-fuer-google-maps-google-street-view-kehrt-nach-deutschland-zurueck-2306-174797.html)  \n\n\nApparently there is no great way to do a backup, as Google doesn't let you download the data directly. I sadly also don't have enough space for what is probably a *huge* amount of data.\n\n&amp;#x200B;\n\nHas anybody tried doing a backup of at least some of it?", "author_fullname": "t2_fj7d7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Street View returns to Germany, but old images will be removed. Did any of you do a backup yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144b1w3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686235139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.golem.de/news/update-fuer-google-maps-google-street-view-kehrt-nach-deutschland-zurueck-2306-174797.html\"&gt;Article in German&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Apparently there is no great way to do a backup, as Google doesn&amp;#39;t let you download the data directly. I sadly also don&amp;#39;t have enough space for what is probably a &lt;em&gt;huge&lt;/em&gt; amount of data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anybody tried doing a backup of at least some of it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?auto=webp&amp;v=enabled&amp;s=38650721293b1f165f76e8e4c6e5a1c99e9f36a1", "width": 832, "height": 468}, "resolutions": [{"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d55d9f8d805086ba7f731e0e73e9fdc9e76279e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc7fb37a0d84ceeb854d015b82751ff9e74dab78", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ac7f133e3d2f1581f65135189df556bbb96870", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6feb6fd2d1f84b6f271ec74f6c243c602131a0f0", "width": 640, "height": 360}], "variants": {}, "id": "jo6E3kwJl2GuMymce5-i70F7B8NealpDuxMNmr6sJK8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "144b1w3", "is_robot_indexable": true, "report_reasons": null, "author": "Keteo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144b1w3/google_street_view_returns_to_germany_but_old/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144b1w3/google_street_view_returns_to_germany_but_old/", "subreddit_subscribers": 686633, "created_utc": 1686235139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\n&amp;#x200B;\n\nI have in my possession a  HP QR490-63001 drive array and I am looking to wipe all drives using Blancco. I am aware that I need to connect this to another server through a fibre cable and some kind of card to help them connect to I can see the drives through Blancco. \n\nBasically I am looking for some kind of tutorial or tips to make this possible as normally where I work we mainly deal with Laptops and PC's this all new to me. Anyone with tips or a direction to help would be greatly apricated.", "author_fullname": "t2_wk6ta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wiping a Data array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449pr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have in my possession a  HP QR490-63001 drive array and I am looking to wipe all drives using Blancco. I am aware that I need to connect this to another server through a fibre cable and some kind of card to help them connect to I can see the drives through Blancco. &lt;/p&gt;\n\n&lt;p&gt;Basically I am looking for some kind of tutorial or tips to make this possible as normally where I work we mainly deal with Laptops and PC&amp;#39;s this all new to me. Anyone with tips or a direction to help would be greatly apricated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1449pr8", "is_robot_indexable": true, "report_reasons": null, "author": "lyndon777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1449pr8/wiping_a_data_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1449pr8/wiping_a_data_array/", "subreddit_subscribers": 686633, "created_utc": 1686231886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hello, if i download a archiveteam file of reddit for example. will i still be able to reade posts/comments from reddit and other sites in that file?", "author_fullname": "t2_x3j2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "seeing actual posts comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144bppl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686236696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello, if i download a archiveteam file of reddit for example. will i still be able to reade posts/comments from reddit and other sites in that file?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144bppl", "is_robot_indexable": true, "report_reasons": null, "author": "worthplayingfor25", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144bppl/seeing_actual_posts_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144bppl/seeing_actual_posts_comments/", "subreddit_subscribers": 686633, "created_utc": 1686236696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1004q6ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this ok? Scrutiny shows passed but there are a lot of errors. New drive. Also how f*cked I am? I ordered 2 drives on eBay and only arrived this one. I heard some people when you buy more than 1 unit of something they send you only one and if you don't have a video opening. You don't get refund", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "name": "t3_144auxu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Cnyscc1vCXk6SSM1sPhK8jueEF0B7uGLyn1VOSyHOmM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686234659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3vw8grpd1t4b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?auto=webp&amp;v=enabled&amp;s=ae3a6fe8333da46fbc6e16555a63da0ee0d3fb07", "width": 1080, "height": 486}, "resolutions": [{"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b2f20e63ccbc0648af5ec15b846919cae092910", "width": 108, "height": 48}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b85811b13de2d2010a1dbd03c5b793e33cc32c0e", "width": 216, "height": 97}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=332e216869ba7ed263e1dff2d1844aa329cbe3e9", "width": 320, "height": 144}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7814a59c4365b8dc158a3956a4b48bb1bb99b17", "width": 640, "height": 288}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93408e06eacd950ec8bf92cd55fb2002f2f73cac", "width": 960, "height": 432}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f973ba23a0b10b541244d40b36d66e1087217bd0", "width": 1080, "height": 486}], "variants": {}, "id": "pgEwF2vNTq2ra6fthzqVsDLdOwHJ2kpLGfHElarm7mU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144auxu", "is_robot_indexable": true, "report_reasons": null, "author": "gerardit04", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144auxu/is_this_ok_scrutiny_shows_passed_but_there_are_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3vw8grpd1t4b1.png", "subreddit_subscribers": 686633, "created_utc": 1686234659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How do I archive a mediawiki site with wget? I'm currently running `wget -mkxpKE --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0\" -e robots=off --reject=\"*Special:*\" --reject=\"*User:*\" [URL]` but I always end up with thousands of requests that look like this\n\n```\n2023-06-07 22:53:08 (9.92 MB/s) - \u2018diyhrt.cafe/index.php?title=Special:CreateAccount&amp;returnto=Main+Page&amp;returntoquery=oldid=843.tmp.html\u2019 saved [90731] \n\nRemoving diyhrt.cafe/index.php?title=Special:CreateAccount&amp;returnto=Main+Page&amp;returntoquery=oldid=843.tmp.html since it should be rejected.\n```\n\nIs it possible to skip all of these pages instead of downloading, then removing them? Is there a different tool I should be using?", "author_fullname": "t2_66jgtbcp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mediawiki with wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143rvwc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686179937.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686178718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do I archive a mediawiki site with wget? I&amp;#39;m currently running &lt;code&gt;wget -mkxpKE --user-agent=&amp;quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0&amp;quot; -e robots=off --reject=&amp;quot;*Special:*&amp;quot; --reject=&amp;quot;*User:*&amp;quot; [URL]&lt;/code&gt; but I always end up with thousands of requests that look like this&lt;/p&gt;\n\n&lt;p&gt;```\n2023-06-07 22:53:08 (9.92 MB/s) - \u2018diyhrt.cafe/index.php?title=Special:CreateAccount&amp;amp;returnto=Main+Page&amp;amp;returntoquery=oldid=843.tmp.html\u2019 saved [90731] &lt;/p&gt;\n\n&lt;p&gt;Removing diyhrt.cafe/index.php?title=Special:CreateAccount&amp;amp;returnto=Main+Page&amp;amp;returntoquery=oldid=843.tmp.html since it should be rejected.\n```&lt;/p&gt;\n\n&lt;p&gt;Is it possible to skip all of these pages instead of downloading, then removing them? Is there a different tool I should be using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143rvwc", "is_robot_indexable": true, "report_reasons": null, "author": "uGoldfish", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143rvwc/mediawiki_with_wget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143rvwc/mediawiki_with_wget/", "subreddit_subscribers": 686633, "created_utc": 1686178718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was thinking of grabbing one of these with a 6500 Intel and installing  a sata card for all my drive in JBOD. I have 5 drives so far in asst sizes (3 are 2.5 and 2 are 3.5 drives)\n\nWill this motherboard w/16gb of ram run a  server for 2 people?\n\nWhat would the projected wattage draw be? Can i put all drives to sleep and only wake when requested? Better yet, can i put whole computer to sleep and wake when needed?", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP ProDesk 600 G3 SFF motherboard", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143r2iv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686176713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking of grabbing one of these with a 6500 Intel and installing  a sata card for all my drive in JBOD. I have 5 drives so far in asst sizes (3 are 2.5 and 2 are 3.5 drives)&lt;/p&gt;\n\n&lt;p&gt;Will this motherboard w/16gb of ram run a  server for 2 people?&lt;/p&gt;\n\n&lt;p&gt;What would the projected wattage draw be? Can i put all drives to sleep and only wake when requested? Better yet, can i put whole computer to sleep and wake when needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143r2iv", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143r2iv/hp_prodesk_600_g3_sff_motherboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143r2iv/hp_prodesk_600_g3_sff_motherboard/", "subreddit_subscribers": 686633, "created_utc": 1686176713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello gang, I'm looking for some ideas for longterm storage of video projects. I'll try to explain as succinctly as possible.\n\nI plan to store the original media (currently mostly 4K/C4K BRAW) for about 3-5 years, after which I'd like to encode it to something smaller that still preserves decent quality. I'm not sure I'll even ever look at this archived media again, but I will rest easier knowing it's been well-stored.\n\nI'm aware of the risks involved, I don't mind if it takes a while, and I'd slightly favor quality over squeezing out every possible GB. But when a single short project from 4 years ago that went nowhere takes up 100+ GB, well, it's time to start trimming some fat.\n\nIn addition to the usual FOSS stuff, I also have DaVinci Resolve Studio and the full Adobe CC suite in case one or the other might be better for my purposes.\n\nI guess H.265 might be the best option (?) but I'm certainly not a codec expert. I would really appreciate any and all suggestions on codecs, settings, and most efficient program. Thanks so much in advance!\n\n---\n\nEdit: It seems I sacrificed quality in favor of economy of space in my post so I'll elaborate a bit.\n\nFor the longterm, I will continue to keep the original media of every project that is of importance in any way, whether that is of importance to me or potentially to a collaborator. I have a 3-2-1 backup solution and I'm overly careful, cautious, and conservative with my data. I mean, jeez, I recently had a client come to me and ask me to re-edit something I shot 7 years ago. I still had the original media and project file and could just jump right back in where I left off in 2016.\n\nThe media that I'm looking to consolidate is footage that has little to no importance (and where I will still keep the original media for 3-5 years anyway just in case). Videos that I was working on on my own for fun and decided to scrap, home movie kind of stuff that I accidentally shot at 200 MB/s, cut talking head footage that has no possible reusability value, etc. It's either junk media that 99.9% of people would just delete or stuff which simply doesn't need to be at the quality it's at (I don't need 400 clips of my cats in C4K 200 MB/s BRAW...). I want to have some kind of backup of this footage but no one will ever work with the material again and it's fine that there's quality loss.", "author_fullname": "t2_jcrj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Videographer/editor longterm storage codec suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144d4kc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686246789.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686240030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello gang, I&amp;#39;m looking for some ideas for longterm storage of video projects. I&amp;#39;ll try to explain as succinctly as possible.&lt;/p&gt;\n\n&lt;p&gt;I plan to store the original media (currently mostly 4K/C4K BRAW) for about 3-5 years, after which I&amp;#39;d like to encode it to something smaller that still preserves decent quality. I&amp;#39;m not sure I&amp;#39;ll even ever look at this archived media again, but I will rest easier knowing it&amp;#39;s been well-stored.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of the risks involved, I don&amp;#39;t mind if it takes a while, and I&amp;#39;d slightly favor quality over squeezing out every possible GB. But when a single short project from 4 years ago that went nowhere takes up 100+ GB, well, it&amp;#39;s time to start trimming some fat.&lt;/p&gt;\n\n&lt;p&gt;In addition to the usual FOSS stuff, I also have DaVinci Resolve Studio and the full Adobe CC suite in case one or the other might be better for my purposes.&lt;/p&gt;\n\n&lt;p&gt;I guess H.265 might be the best option (?) but I&amp;#39;m certainly not a codec expert. I would really appreciate any and all suggestions on codecs, settings, and most efficient program. Thanks so much in advance!&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Edit: It seems I sacrificed quality in favor of economy of space in my post so I&amp;#39;ll elaborate a bit.&lt;/p&gt;\n\n&lt;p&gt;For the longterm, I will continue to keep the original media of every project that is of importance in any way, whether that is of importance to me or potentially to a collaborator. I have a 3-2-1 backup solution and I&amp;#39;m overly careful, cautious, and conservative with my data. I mean, jeez, I recently had a client come to me and ask me to re-edit something I shot 7 years ago. I still had the original media and project file and could just jump right back in where I left off in 2016.&lt;/p&gt;\n\n&lt;p&gt;The media that I&amp;#39;m looking to consolidate is footage that has little to no importance (and where I will still keep the original media for 3-5 years anyway just in case). Videos that I was working on on my own for fun and decided to scrap, home movie kind of stuff that I accidentally shot at 200 MB/s, cut talking head footage that has no possible reusability value, etc. It&amp;#39;s either junk media that 99.9% of people would just delete or stuff which simply doesn&amp;#39;t need to be at the quality it&amp;#39;s at (I don&amp;#39;t need 400 clips of my cats in C4K 200 MB/s BRAW...). I want to have some kind of backup of this footage but no one will ever work with the material again and it&amp;#39;s fine that there&amp;#39;s quality loss.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "43 terrorbytes", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144d4kc", "is_robot_indexable": true, "report_reasons": null, "author": "yopoyo", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/144d4kc/videographereditor_longterm_storage_codec/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144d4kc/videographereditor_longterm_storage_codec/", "subreddit_subscribers": 686633, "created_utc": 1686240030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to collect and organize files of a niche hobby that are currently scattered over various forums, discords and FB groups.\nBecause I sure can't do it alone, it would be great if other people could send me those files _in an orderly fashion_, i.e. not random email attachments or massive dumps that _I_ have to sort through. I'd rather like something like Github pullrequests that I (or other co-collaborators) can accept or deny (eg if metadata is missing); this would also make it easier for people to keep track to avoid sending duplicates.\n\nI anticipate the binaries will quickly grow to dozens, if not hundreds of GB, so using git/github is probably out of question. (I know it is possible to handle giant git repos if you are Microsoft, but for everyone else the current state of git lfs + sparse checkout is atm just not usable enough)\n\nWhat are my options here? Anyone has some experience with this kind of thing?\n(I also wonder how those sharing \"Linux isos\" do that \u2013 is that always a single person sorting out duplicates?)", "author_fullname": "t2_1ru2s9bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tools to let others collaborate on my collection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144bkq0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686236395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to collect and organize files of a niche hobby that are currently scattered over various forums, discords and FB groups.\nBecause I sure can&amp;#39;t do it alone, it would be great if other people could send me those files &lt;em&gt;in an orderly fashion&lt;/em&gt;, i.e. not random email attachments or massive dumps that &lt;em&gt;I&lt;/em&gt; have to sort through. I&amp;#39;d rather like something like Github pullrequests that I (or other co-collaborators) can accept or deny (eg if metadata is missing); this would also make it easier for people to keep track to avoid sending duplicates.&lt;/p&gt;\n\n&lt;p&gt;I anticipate the binaries will quickly grow to dozens, if not hundreds of GB, so using git/github is probably out of question. (I know it is possible to handle giant git repos if you are Microsoft, but for everyone else the current state of git lfs + sparse checkout is atm just not usable enough)&lt;/p&gt;\n\n&lt;p&gt;What are my options here? Anyone has some experience with this kind of thing?\n(I also wonder how those sharing &amp;quot;Linux isos&amp;quot; do that \u2013 is that always a single person sorting out duplicates?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144bkq0", "is_robot_indexable": true, "report_reasons": null, "author": "plg94", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144bkq0/tools_to_let_others_collaborate_on_my_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144bkq0/tools_to_let_others_collaborate_on_my_collection/", "subreddit_subscribers": 686633, "created_utc": 1686236395.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried using google colab using some scripts that I've found online and I managed to transferred 80GB of files from my google drive to my mega account. But after that, it doesn't worked anymore, the error seems happening on Mega's end, or not sure if there are limitations or restrictions in using that?\n\nThere's another option I've found using rclone, but the problem with this is very slow and it using my internet bandwidth to transfer data (download and upload) compare to the scripts using google colab which transfer is happening from server to server (80GB took only 15min more or less).  \n\n\nAre there any other options out there? There are also paid services available but I'm not keen on availing that.", "author_fullname": "t2_8c5s3s58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud transfer Google Drive to Mega", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1443cuq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686212690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using google colab using some scripts that I&amp;#39;ve found online and I managed to transferred 80GB of files from my google drive to my mega account. But after that, it doesn&amp;#39;t worked anymore, the error seems happening on Mega&amp;#39;s end, or not sure if there are limitations or restrictions in using that?&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s another option I&amp;#39;ve found using rclone, but the problem with this is very slow and it using my internet bandwidth to transfer data (download and upload) compare to the scripts using google colab which transfer is happening from server to server (80GB took only 15min more or less).  &lt;/p&gt;\n\n&lt;p&gt;Are there any other options out there? There are also paid services available but I&amp;#39;m not keen on availing that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1443cuq", "is_robot_indexable": true, "report_reasons": null, "author": "elryoma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1443cuq/cloud_transfer_google_drive_to_mega/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1443cuq/cloud_transfer_google_drive_to_mega/", "subreddit_subscribers": 686633, "created_utc": 1686212690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title as it says. I found our old tapes recently, some we recorded and others we bought. I'd like to get all of them transfered, as some of the commercial tapes stick out in one way or another. I have a capture card and a RT5X, so I can rip them if I want, but I trust a service more since they probably have a cleaner way to rip them.", "author_fullname": "t2_1ghhtckn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do VHS Tranfer Companies Typically Transfer Commcercial Tapes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1444103", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686215129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title as it says. I found our old tapes recently, some we recorded and others we bought. I&amp;#39;d like to get all of them transfered, as some of the commercial tapes stick out in one way or another. I have a capture card and a RT5X, so I can rip them if I want, but I trust a service more since they probably have a cleaner way to rip them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1444103", "is_robot_indexable": true, "report_reasons": null, "author": "KevinPike87", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1444103/do_vhs_tranfer_companies_typically_transfer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1444103/do_vhs_tranfer_companies_typically_transfer/", "subreddit_subscribers": 686633, "created_utc": 1686215129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using the admirable RedditScrape and reddit-save projects to download my own upvotes/saves, as well as individual subs.  What I'd like to do, though, is download all my friends' posts.  The /r/friends \"sub\" has all of that, but it shows up as an invalid sub in the scrape/save tools.  Does anyone know of a tool that does work for /r/friends?", "author_fullname": "t2_e9xgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool for downloading all of friends' posts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143mg7o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686166109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using the admirable RedditScrape and reddit-save projects to download my own upvotes/saves, as well as individual subs.  What I&amp;#39;d like to do, though, is download all my friends&amp;#39; posts.  The &lt;a href=\"/r/friends\"&gt;/r/friends&lt;/a&gt; &amp;quot;sub&amp;quot; has all of that, but it shows up as an invalid sub in the scrape/save tools.  Does anyone know of a tool that does work for &lt;a href=\"/r/friends\"&gt;/r/friends&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143mg7o", "is_robot_indexable": true, "report_reasons": null, "author": "oozforashag", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143mg7o/is_there_a_tool_for_downloading_all_of_friends/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143mg7o/is_there_a_tool_for_downloading_all_of_friends/", "subreddit_subscribers": 686633, "created_utc": 1686166109.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}