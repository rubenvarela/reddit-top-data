{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Faster sorting algorithms discovered using deep reinforcement learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 51, "top_awarded_type": null, "hide_score": false, "name": "t3_1445idg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 103, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 103, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/_6CMidQNwnpryTTrkgZsDmQu4g4ZU1lwDpYQLRvTkR8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686220142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "nature.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.nature.com/articles/s41586-023-06004-9", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?auto=webp&amp;v=enabled&amp;s=0dbe22f40422f2d827efa1ddbdcd3bc253dcd72b", "width": 685, "height": 254}, "resolutions": [{"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10b04aa3d0856412bce5e98a15197bc73f8bb4a1", "width": 108, "height": 40}, {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=201b1153b6006c3ed2254e8ef4e6a2742193e750", "width": 216, "height": 80}, {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1394dc9af391fc433d0d2b19dcb458adca9ca4cb", "width": 320, "height": 118}, {"url": "https://external-preview.redd.it/E9lNJhtmUhpuMQkN80KDdHpKYucalso4jYZM2e_mEU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22558066595da63560a5e442dc49ac9fc99f18fa", "width": 640, "height": 237}], "variants": {}, "id": "5XnGLzdtcgD58mlZFry7KmbBERR_Bafyn5GyOj4D83k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1445idg", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1445idg/faster_sorting_algorithms_discovered_using_deep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.nature.com/articles/s41586-023-06004-9", "subreddit_subscribers": 920865, "created_utc": 1686220142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey! I work for a F50 company and they use dated job titles that doesn\u2019t exist outside of the company. I wanted to be on the lookout for another job given that my company has been laying people off left and right but not sure what title should I apply to. This is a gist of responsibilities: \n\nI only work on deployed models that are running in production. I make tools using Python for their monitoring such as model drift, data drift. Investigating if something went wrong with the model, data. It\u2019s a bit difficult to list out the things I do but model monitoring and maintenance is a good description.\n\nWhat are some of the job roles I could apply to given my responsibilities? I use Python and SQL.", "author_fullname": "t2_bv171ji2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What job roles should I apply to based on my current job responsibilities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143kqad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686162068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I work for a F50 company and they use dated job titles that doesn\u2019t exist outside of the company. I wanted to be on the lookout for another job given that my company has been laying people off left and right but not sure what title should I apply to. This is a gist of responsibilities: &lt;/p&gt;\n\n&lt;p&gt;I only work on deployed models that are running in production. I make tools using Python for their monitoring such as model drift, data drift. Investigating if something went wrong with the model, data. It\u2019s a bit difficult to list out the things I do but model monitoring and maintenance is a good description.&lt;/p&gt;\n\n&lt;p&gt;What are some of the job roles I could apply to given my responsibilities? I use Python and SQL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143kqad", "is_robot_indexable": true, "report_reasons": null, "author": "quite--average", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143kqad/what_job_roles_should_i_apply_to_based_on_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143kqad/what_job_roles_should_i_apply_to_based_on_my/", "subreddit_subscribers": 920865, "created_utc": 1686162068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Has anyone here tried Mojo yet? What are your initial views?", "author_fullname": "t2_3wr0pzmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mojo programming language", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143xglf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686193422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here tried Mojo yet? What are your initial views?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143xglf", "is_robot_indexable": true, "report_reasons": null, "author": "sARUcasm", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143xglf/mojo_programming_language/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143xglf/mojo_programming_language/", "subreddit_subscribers": 920865, "created_utc": 1686193422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, I have an upcoming business case interview, I have one week to prepare and provide the presentation.\n\nThe odd thing is that the 'business case' is two questions that I need to answer, however, the tricky part is, I have **not received** any data sets or anything to analyze.\n\nI suppose I'm expected to retrieve data online to answer the questions and provide a solution that is both strategic and logical.\n\nAnyone else that had experience with a similar situation and that can provide some tips on what to focus?", "author_fullname": "t2_8qgzhwuo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business case without data to analyze", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144f7fy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686244763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I have an upcoming business case interview, I have one week to prepare and provide the presentation.&lt;/p&gt;\n\n&lt;p&gt;The odd thing is that the &amp;#39;business case&amp;#39; is two questions that I need to answer, however, the tricky part is, I have &lt;strong&gt;not received&lt;/strong&gt; any data sets or anything to analyze.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m expected to retrieve data online to answer the questions and provide a solution that is both strategic and logical.&lt;/p&gt;\n\n&lt;p&gt;Anyone else that had experience with a similar situation and that can provide some tips on what to focus?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144f7fy", "is_robot_indexable": true, "report_reasons": null, "author": "PizzaStartup", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144f7fy/business_case_without_data_to_analyze/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144f7fy/business_case_without_data_to_analyze/", "subreddit_subscribers": 920865, "created_utc": 1686244763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I currently work as a supply chain analyst and I utilize python to automate tasks within SAP as well as analyze large sets of data with IF statements to help me make strategic decisions (inventory rebalancing, forecasting, purchasing). Does this fall more under a software engineer or data scientist skill set?", "author_fullname": "t2_317zg1g3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is using python for supply chain considered data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144068g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686201542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work as a supply chain analyst and I utilize python to automate tasks within SAP as well as analyze large sets of data with IF statements to help me make strategic decisions (inventory rebalancing, forecasting, purchasing). Does this fall more under a software engineer or data scientist skill set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144068g", "is_robot_indexable": true, "report_reasons": null, "author": "C17Wing", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144068g/is_using_python_for_supply_chain_considered_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144068g/is_using_python_for_supply_chain_considered_data/", "subreddit_subscribers": 920865, "created_utc": 1686201542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_oime0u9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_144ed4x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "author_name": "PSN Academy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OISLU6FWlgc/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@PSNAcademy"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/144ed4x", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JuajMx4-3I1GBwnMEdSZI1cT4RXjWw_0XVP4J0FkZ0U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686242848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=OISLU6FWlgc&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?auto=webp&amp;v=enabled&amp;s=8ac302293204c47e5da89bd2016d45daa4e919d8", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19071c06bdb8aa79fc627188f56d3e31af938307", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e330ea451a896099c9202e5c5ef32ac287b07a4b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GBC3cFsU6i1qORxZIuK850IYHN0JhYkBBWDtm3hUxMM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1da7e7a4875ef10610c171fd7988a8d00a4593d", "width": 320, "height": 240}], "variants": {}, "id": "r2MUKkg8ca1_s8pyFfcwN3ycxWdpaHhWVrKK98Fyq2o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "144ed4x", "is_robot_indexable": true, "report_reasons": null, "author": "profpsnayak", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144ed4x/\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25_\ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\ud835\udc2c_\ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d_\ud835\udc28\ud835\udc1f_\ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=OISLU6FWlgc&amp;feature=share", "subreddit_subscribers": 920865, "created_utc": 1686242848.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OISLU6FWlgc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"\ud835\udc0a\ud835\udc1a\ud835\udc2b\ud835\udc25 \ud835\udc0f\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc2c\ud835\udc28\ud835\udc27\u2019\ud835\udc2c \ud835\udc02\ud835\udc28\ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc2d \ud835\udc28\ud835\udc1f \ud835\udc12\ud835\udc24\ud835\udc1e\ud835\udc30\ud835\udc27\ud835\udc1e\ud835\udc2c\ud835\udc2c\"&gt;&lt;/iframe&gt;", "author_name": "PSN Academy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/OISLU6FWlgc/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@PSNAcademy"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all!\n\n&amp;#x200B;\n\nI am building a community of social/data/computer scientists at the University of Mannheim focused on applications and theory of Large Language Models.  The community will be working to make contributions along research, teaching, and enterpreneurial dimensions.  The positions can be at the graduate student, pre-doc (ABD), or post-doc level.  The details for the call are coming, but given the impending summer, I wanted to get an initial announcement out.  \n\n&amp;#x200B;\n\nI am looking for the right people at least as much as the right skills: people  open to providing and receiving feedback, supporting others, and who is committed and looking to grow intellectually.  The more experience with the relevant coding (Python, HuggingFace), mathematical statistics, algorithms, and applied work in the social sciences, the better.  We will have our own infrastructure, so experience setting up and running servers will be a big plus.  \n\n&amp;#x200B;\n\nA formal announcement will follow, but the position will be at the E13 level.   Teaching requirements will depend on the level and skills of the applicant, but will be tied directly to your research interests.  Salary is competitive, and will be at the standard E13 level as set by the state of Baden-W\u00fcrttemberg.\n\n&amp;#x200B;\n\nIf you have any questions, please send them to [MarcRatkovic@gmail.com](mailto:MarcRatkovic@gmail.com).  When you reach out, it would help if you would include a CV and any relevant working papers (at any stage of development). \n\n&amp;#x200B;\n\nAll the best,\n\nMarc Ratkovic\n\nW3 Chair of Social Data Science\n\nUniversity of Mannheim", "author_fullname": "t2_czuym6b7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLMs at the University of Mannheim", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144ecn5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686242817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am building a community of social/data/computer scientists at the University of Mannheim focused on applications and theory of Large Language Models.  The community will be working to make contributions along research, teaching, and enterpreneurial dimensions.  The positions can be at the graduate student, pre-doc (ABD), or post-doc level.  The details for the call are coming, but given the impending summer, I wanted to get an initial announcement out.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am looking for the right people at least as much as the right skills: people  open to providing and receiving feedback, supporting others, and who is committed and looking to grow intellectually.  The more experience with the relevant coding (Python, HuggingFace), mathematical statistics, algorithms, and applied work in the social sciences, the better.  We will have our own infrastructure, so experience setting up and running servers will be a big plus.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A formal announcement will follow, but the position will be at the E13 level.   Teaching requirements will depend on the level and skills of the applicant, but will be tied directly to your research interests.  Salary is competitive, and will be at the standard E13 level as set by the state of Baden-W\u00fcrttemberg.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions, please send them to [&lt;a href=\"mailto:MarcRatkovic@gmail.com\"&gt;MarcRatkovic@gmail.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:MarcRatkovic@gmail.com\"&gt;MarcRatkovic@gmail.com&lt;/a&gt;).  When you reach out, it would help if you would include a CV and any relevant working papers (at any stage of development). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;All the best,&lt;/p&gt;\n\n&lt;p&gt;Marc Ratkovic&lt;/p&gt;\n\n&lt;p&gt;W3 Chair of Social Data Science&lt;/p&gt;\n\n&lt;p&gt;University of Mannheim&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144ecn5", "is_robot_indexable": true, "report_reasons": null, "author": "MarcRatkovic", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144ecn5/llms_at_the_university_of_mannheim/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144ecn5/llms_at_the_university_of_mannheim/", "subreddit_subscribers": 920865, "created_utc": 1686242817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi!\n\nI'm currently working on connecting a React app with a Shiny app (Python), but I'm facing difficulties in doing so. I haven't been able to find any resources or guides that specifically address this integration. As a newcomer to web application development, I am not sure if there's any additional setup I may have missed.\n\nI'm curious if it's possible to connect Shiny (Python) and React. If anyone has experience or knowledge in this area, I would greatly appreciate any advice or insights!\n\nOn the other hand, I'm open to exploring alternative tech stacks that might simplify the whole process. For example, using React with Flask instead of Shiny (Python). If anyone has recommendations or suggestions, I'd love to hear them!\n\nThank you in advance!\n\n&amp;#x200B;\n\nDetails:\n\nCurrently, I have both a React app and a Shiny app (Python) running independently on different localhost ports. In my attempt to connect these two apps, I followed a tutorial available at [**https://github.com/filipakkad/react-shiny-template**](https://github.com/filipakkad/react-shiny-template), which demonstrates the use of websockets to establish communication between Shiny (R) and React.\n\nHowever, I encountered an error message that states \"Uncaught TypeError: Cannot read properties of undefined (reading 'addCustomMessageHandler')\" when using the \n\n    window.Shiny.addCustomMessageHandler\n\nfunction mentioned in the tutorial. \n\n I also tried another tutorial provided in this repository: [**https://github.com/BrandenKeck/shiny\\_react\\_template**](https://github.com/BrandenKeck/shiny_react_template). Unfortunately, the webpack config approach mentioned in this tutorial did not work for me either. I received an error message stating \"Module not found: Can't resolve 'shiny'\".\n\nSorry if the error messages are too vague to troubleshoot.", "author_fullname": "t2_95g1e07t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to connect Shiny for Python with React?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143wno4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on connecting a React app with a Shiny app (Python), but I&amp;#39;m facing difficulties in doing so. I haven&amp;#39;t been able to find any resources or guides that specifically address this integration. As a newcomer to web application development, I am not sure if there&amp;#39;s any additional setup I may have missed.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if it&amp;#39;s possible to connect Shiny (Python) and React. If anyone has experience or knowledge in this area, I would greatly appreciate any advice or insights!&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I&amp;#39;m open to exploring alternative tech stacks that might simplify the whole process. For example, using React with Flask instead of Shiny (Python). If anyone has recommendations or suggestions, I&amp;#39;d love to hear them!&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Details:&lt;/p&gt;\n\n&lt;p&gt;Currently, I have both a React app and a Shiny app (Python) running independently on different localhost ports. In my attempt to connect these two apps, I followed a tutorial available at &lt;a href=\"https://github.com/filipakkad/react-shiny-template\"&gt;&lt;strong&gt;https://github.com/filipakkad/react-shiny-template&lt;/strong&gt;&lt;/a&gt;, which demonstrates the use of websockets to establish communication between Shiny (R) and React.&lt;/p&gt;\n\n&lt;p&gt;However, I encountered an error message that states &amp;quot;Uncaught TypeError: Cannot read properties of undefined (reading &amp;#39;addCustomMessageHandler&amp;#39;)&amp;quot; when using the &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;window.Shiny.addCustomMessageHandler\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;function mentioned in the tutorial. &lt;/p&gt;\n\n&lt;p&gt;I also tried another tutorial provided in this repository: &lt;a href=\"https://github.com/BrandenKeck/shiny_react_template\"&gt;&lt;strong&gt;https://github.com/BrandenKeck/shiny_react_template&lt;/strong&gt;&lt;/a&gt;. Unfortunately, the webpack config approach mentioned in this tutorial did not work for me either. I received an error message stating &amp;quot;Module not found: Can&amp;#39;t resolve &amp;#39;shiny&amp;#39;&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Sorry if the error messages are too vague to troubleshoot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?auto=webp&amp;v=enabled&amp;s=df682b7818bf96a86f346c10684a73b607e7f31c", "width": 651, "height": 320}, "resolutions": [{"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9662c36ed5b3b66e858821f09a9e6003dbc5aa04", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4485b075ebc6a083cd92daa62a76be41bbf526a", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a1478ad2a984ef28466a22393f52d564331efb6", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/rq2GsfWN4Xi2O8UvoAPaJTA7yZohGFML5mS07N1m74c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3132c9ee0abdbf2aaa2ad69c7a89dfd6bdd8fe27", "width": 640, "height": 314}], "variants": {}, "id": "GBFg7Huw_zEKgghuGLuNBfvugskO8_GA2tV8ZiqndUI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143wno4", "is_robot_indexable": true, "report_reasons": null, "author": "Feisty-Temporary4403", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143wno4/is_it_possible_to_connect_shiny_for_python_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143wno4/is_it_possible_to_connect_shiny_for_python_with/", "subreddit_subscribers": 920865, "created_utc": 1686191145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m interested in learning more about signal processing. I\u2019m a chemical engineer by degree so I know a minimal amount about it from tuning control loops in my past life but definitely not enough to find it useful for data science et al. I\u2019m familiar with a decent amount of the terminology and math but not as much how to actually do it. Where would you recommend I start?", "author_fullname": "t2_14hblx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Signal Processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144fbjt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m interested in learning more about signal processing. I\u2019m a chemical engineer by degree so I know a minimal amount about it from tuning control loops in my past life but definitely not enough to find it useful for data science et al. I\u2019m familiar with a decent amount of the terminology and math but not as much how to actually do it. Where would you recommend I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144fbjt", "is_robot_indexable": true, "report_reasons": null, "author": "Ryush806", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144fbjt/signal_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144fbjt/signal_processing/", "subreddit_subscribers": 920865, "created_utc": 1686245026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know that set ups will vary based on all kinds of variables, but I am looking for a diagram that shows a typical set up. Any help is greatly appreciated!", "author_fullname": "t2_br8ttrfio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi folks! Can anyone help me understand the basic structure of a hyperscale/cloud data center?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144fbco", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245015.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that set ups will vary based on all kinds of variables, but I am looking for a diagram that shows a typical set up. Any help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144fbco", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Week1997", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144fbco/hi_folks_can_anyone_help_me_understand_the_basic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144fbco/hi_folks_can_anyone_help_me_understand_the_basic/", "subreddit_subscribers": 920865, "created_utc": 1686245015.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Unlike  normal reporting, A/B testing collects data of a different combination  of dimensions every time. It is also a complicated kind of analysis of  immense data. In our case, we have a real-time data volume of millions  of OPS (Operations Per Second), with each operation involving around 20  data tags and over a dozen dimensions.\n\nFor  effective A/B testing, as data engineers, we must ensure quick  computation as well as high data integrity (which means no duplication  and no data loss). I\u2019m sure I\u2019m not the only one to say this: it is  hard!\n\nLet me show you our long-term struggle with our previous Druid-based data platform.\n\n# Platform Architecture 1.0\n\n**Components**: Apache Storm + Apache Druid + MySQL\n\nThis  was our real-time datawarehouse, where Apache Storm was the real-time  data processing engine and Apache Druid pre-aggregated the data.  However, Druid did not support certain paging and join queries, so we  wrote data from Druid to MySQL regularly, making MySQL the \u201cmaterialized  view\u201d of Druid. But that was only a duct tape solution as it couldn\u2019t  support our ever enlarging real-time data size. So data timeliness was  unattainable.\n\nhttps://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2\n\n# Platform Architecture 2.0\n\n**Components**: Apache Flink + Apache Druid + TiDB\n\nThis  time, we replaced Storm with Flink, and MySQL with TiDB. Flink was more  powerful in terms of semantics and features, while TiDB, with its  distributed capability, was more maintainable than MySQL. But  architecture 2.0 was nowhere near our goal of end-to-end data  consistency, either, because when processing huge data, enabling TiDB  transactions largely slowed down data writing. Plus, Druid itself did  not support standard SQL, so there were some learning costs and  frictions in usage.\n\nhttps://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68756da95ae7d2221d85e419759103e068205ad8\n\n# Platform Architecture 3.0\n\n**Components**: Apache Flink + [Apache Doris](https://github.com/apache/doris)\n\nWe  replaced Apache Druid with Apache Doris as the OLAP engine, which could  also serve as a unified data serving gateway. So in Architecture 3.0,  we only need to maintain one set of query logic. And we layered our  real-time datawarehouse to increase reusability of real-time data.\n\nhttps://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409\n\nTurns  out the combination of Flink and Doris was the answer. We can exploit  their features to realize quick computation and data consistency. Keep  reading and see how we make it happen.\n\n# Quick Computation\n\nAs  one piece of operation data can be attached to 20 tags, in A/B testing,  we compare two groups of data centering only one tag each time. At  first, we thought about splitting one piece of operation data (with 20  tags) into 20 pieces of data of only one tag upon data ingestion, and  then importing them into Doris for analysis, but that could cause a data  explosion and thus huge pressure on our clusters.\n\nThen  we tried moving part of such workload to the computation engine. So we  tried and \u201cexploded\u201d the data in Flink, but soon regretted it, because  when we aggregated the data using the global hash windows in Flink jobs,  the network and CPU usage also \u201cexploded\u201d.\n\nOur  third shot was to aggregate data locally in Flink right after we split  it. As is shown below, we create a window in the memory of one operator  for local aggregation; then we further aggregate it using the global  hash windows. Since two operators chained together are in one thread,  transferring data between operators consumes much less network  resources. **The two-step aggregation method, combined with the** [**Aggregate model**](https://doris.apache.org/docs/dev/data-table/data-model) **of Apache Doris, can keep data explosion in a manageable range.**\n\nhttps://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152\n\nFor  convenience in A/B testing, we make the test tag ID the first sorted  field in Apache Doris, so we can quickly locate the target data using  sorted indexes. To further minimize data processing in queries, we  create materialized views with the frequently used dimensions. With  constant modification and updates, the materialized views are applicable  in 80% of our queries.\n\nTo  sum up, with the application of sorted index and materialized views, we  reduce our query response time to merely seconds in A/B testing.\n\n# Data Integrity Guarantee\n\nImagine  that your algorithm designers worked sweat and tears trying to improve  the business, only to find their solution unable to be validated by A/B  testing due to data loss. This is an unbearable situation, and we make  every effort to avoid it.\n\n# Develop a Sink-to-Doris Component\n\nTo  ensure end-to-end data integrity, we developed a Sink-to-Doris  component. It is built on our own Flink Stream API scaffolding and  realized by the idempotent writing of Apache Doris and the two-stage  commit mechanism of Apache Flink. On top of it, we have a data  protection mechanism against anomalies.\n\nIt  is the result of our long-term evolution. We used to ensure data  consistency by implementing \u201cone writing for one tag ID\u201d. Then we  realized we could make good use of the transactions in Apache Doris and  the two-stage commit of Apache Flink.\n\nhttps://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df\n\nAs is shown above, this is how two-stage commit works to guarantee data consistency:\n\n1. Write data into local files;\n2. Stage One: pre-commit data to Apache Doris. Save the Doris transaction ID into status;\n3. If checkpoint fails, manually abandon the transaction; if checkpoint succeeds, commit the transaction in Stage Two;\n4. If  the commit fails after multiple retries, the transaction ID and the  relevant data will be saved in HDFS, and we can restore the data via  Broker Load.\n\nWe  make it possible to split a single checkpoint into multiple  transactions, so that we can prevent one Stream Load from taking more  time than a Flink checkpoint in the event of large data volumes.\n\n# Application Display\n\nThis  is how we implement Sink-to-Doris. The component has blocked API calls  and topology assembly. With simple configuration, we can write data into  Apache Doris via Stream Load.\n\nhttps://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19\n\n# Cluster Monitoring\n\nFor  cluster and host monitoring, we adopted the metrics templates provided  by the Apache Doris community. For data monitoring, in addition to the  template metrics, we added Stream Load request numbers and loading  rates.\n\nhttps://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c64d709b3f894937e97519e94305df903125c5\n\nOther metrics of our concerns include data writing speed and task  processing time. In the case of anomalies, we will receive notifications  in the form of phone calls, messages, and emails.\n\nhttps://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9\n\n# Key Takeaways\n\nThe  recipe for successful A/B testing is quick computation and high data  integrity. For this purpose, we implement a two-step aggregation method  in Apache Flink, utilize the Aggregate model, materialized view, and  short indexes of Apache Doris. Then we develop a Sink-to-Doris  component, which is realized by the idempotent writing of Apache Doris  and the two-stage commit mechanism of Apache Flink.", "author_fullname": "t2_uu592ayo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A/B Testing was a handful", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z58h5wi0nt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c186c8d84ad698b1adffca6c9c64b6301108a92"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16b576ec77a9d852b4df6e1d63cf0ed5b6410b4d"}, {"y": 222, "x": 320, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ebe77c0cbe174b1236494a5ea257cbb442265ed"}, {"y": 444, "x": 640, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14e64d246ced5ab52476c332a91eaae763b0a9e5"}, {"y": 666, "x": 960, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8c12701cebe7f1c9c669bf93b73981ee89db6cc"}, {"y": 749, "x": 1080, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34792d4a2e64abdce938b6891a947f55ecf4253f"}], "s": {"y": 888, "x": 1280, "u": "https://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9"}, "id": "z58h5wi0nt4b1"}, "q9n64kxkmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 73, "x": 108, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0603e3d2c444bc4f87f72b192643ae18b7664b8e"}, {"y": 146, "x": 216, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa932ac3d49037d216936831e1caa3dbfc54c01c"}, {"y": 217, "x": 320, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26474b55a8ecc9c786e038a151026f9a1b096534"}, {"y": 435, "x": 640, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=109702225c81efa8993ba39e8c3aa82d79a427dd"}, {"y": 653, "x": 960, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a246f002a5420725c8c24200c7803d93b6fc3d8"}, {"y": 734, "x": 1080, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee640b626a81a2d48f8c9592235b25d2121d11b0"}], "s": {"y": 1083, "x": 1592, "u": "https://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=68756da95ae7d2221d85e419759103e068205ad8"}, "id": "q9n64kxkmt4b1"}, "tgbprtyqmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff36b6a06a4ac8f93ba7a36af9187ab32904d634"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9ca2b68989ff7725cb179e447663fd66315f780"}, {"y": 121, "x": 320, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d6a8f713d87af08fdfc70a02fce3fa0be5d7a6"}, {"y": 243, "x": 640, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13cfdbb1daad2ebc6f7c8553d8e5b66d5aacb0d6"}, {"y": 364, "x": 960, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd8202247a46501cceb7f6685d3965dac6777a1f"}, {"y": 410, "x": 1080, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46882ee3cc966e4484ea670a4da79b92ad103e0"}], "s": {"y": 624, "x": 1642, "u": "https://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152"}, "id": "tgbprtyqmt4b1"}, "ap45rzcimt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1060d30e8e8b426a4679486a310b1c68d9ae8f46"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5239e031369d5865370f0f3c393deaf013939b3"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ad226e08dddd3e079249fd77ad8ec767db7be82"}, {"y": 359, "x": 640, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acdd001188c2eba27245633b48727358b8db19f7"}, {"y": 539, "x": 960, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66e6fede5197c1f0f35ceac6ec16aa7d9172ffc4"}, {"y": 606, "x": 1080, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4bd30e72998308d3144af8bc4beb7b5a9a144183"}], "s": {"y": 960, "x": 1709, "u": "https://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2"}, "id": "ap45rzcimt4b1"}, "959gakxvmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4972a7fc2ee20f78f6382073037faee99a8fb86"}, {"y": 70, "x": 216, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec74e5cda741392c42a239be359afb8caf9f5ecf"}, {"y": 104, "x": 320, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4cc00e39254fd8aa94da00aa618bdea994a1991"}, {"y": 209, "x": 640, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f368c5ab850faaa8b9a83a38b754835890bb632"}, {"y": 314, "x": 960, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b0623e145a6ca61d5e41a17972c1d783c3edae4"}, {"y": 353, "x": 1080, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fba560b79ca348ada09825e52dc5ca642319a6b5"}], "s": {"y": 1077, "x": 3289, "u": "https://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19"}, "id": "959gakxvmt4b1"}, "xpbtl0qtmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861849f31aca428cea518ea44aa7efb8e4c5c86f"}, {"y": 213, "x": 216, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dcaeab2b61a82650a80bba6ea00165dab6085205"}, {"y": 315, "x": 320, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=932f5c0dbaebf378db7be90eb4fa9c50a8551580"}, {"y": 631, "x": 640, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1dcc3f2215da1ccacabd2ca66a57b034b18d42a"}, {"y": 946, "x": 960, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e10bd14283e50ab5013996cf166e2a42c2702cee"}, {"y": 1065, "x": 1080, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d03a7529df0c08b87d1c4bc36969986c66e88d2f"}], "s": {"y": 3334, "x": 3380, "u": "https://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df"}, "id": "xpbtl0qtmt4b1"}, "1nq2spknmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 88, "x": 108, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f956430ac71dfcc10b585283675ef6eb02baa2e7"}, {"y": 177, "x": 216, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0457ffa834e73dc4759aa14b92e446c1838f94a0"}, {"y": 262, "x": 320, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07232fe4cbb19c96079332219db2623f3f994214"}, {"y": 525, "x": 640, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06972e5fc878794c1947ab79ccf48c3ab20b4e2b"}, {"y": 788, "x": 960, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be277ff1b357dc92a0ab2e9fbd00cc154fa389a0"}, {"y": 887, "x": 1080, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70e9cc0cf7c63362ca7e732901ffc11a15687e6d"}], "s": {"y": 1101, "x": 1340, "u": "https://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409"}, "id": "1nq2spknmt4b1"}, "e9mu9nsxmt4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1902eafba7fcf630a6e11a4e3a502987d3f99c1"}, {"y": 89, "x": 216, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=156b22585646291a2ec49dfea975e244b000ac0a"}, {"y": 133, "x": 320, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f989b277baca4a11ea0275296f0224960cbb1894"}, {"y": 266, "x": 640, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35120ab9e0bc9d0ac7352c9ee152530b0aa70c49"}, {"y": 399, "x": 960, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4035886dc47e5f7acf1ec9e107f862850504292"}, {"y": 449, "x": 1080, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9c11f276f65f3d2b5a953f6241558b97cf659ba"}], "s": {"y": 832, "x": 2001, "u": "https://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89c64d709b3f894937e97519e94305df903125c5"}, "id": "e9mu9nsxmt4b1"}}, "name": "t3_144e0cp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kS9dMuDFFlEJP-gyR3r-H4sq1T3T440E8jWxQqiwrEk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1686242035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unlike  normal reporting, A/B testing collects data of a different combination  of dimensions every time. It is also a complicated kind of analysis of  immense data. In our case, we have a real-time data volume of millions  of OPS (Operations Per Second), with each operation involving around 20  data tags and over a dozen dimensions.&lt;/p&gt;\n\n&lt;p&gt;For  effective A/B testing, as data engineers, we must ensure quick  computation as well as high data integrity (which means no duplication  and no data loss). I\u2019m sure I\u2019m not the only one to say this: it is  hard!&lt;/p&gt;\n\n&lt;p&gt;Let me show you our long-term struggle with our previous Druid-based data platform.&lt;/p&gt;\n\n&lt;h1&gt;Platform Architecture 1.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;: Apache Storm + Apache Druid + MySQL&lt;/p&gt;\n\n&lt;p&gt;This  was our real-time datawarehouse, where Apache Storm was the real-time  data processing engine and Apache Druid pre-aggregated the data.  However, Druid did not support certain paging and join queries, so we  wrote data from Druid to MySQL regularly, making MySQL the \u201cmaterialized  view\u201d of Druid. But that was only a duct tape solution as it couldn\u2019t  support our ever enlarging real-time data size. So data timeliness was  unattainable.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2\"&gt;https://preview.redd.it/ap45rzcimt4b1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bdbb8480ec2a63b41d0ee40a8243b22fee51cce2&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Platform Architecture 2.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;: Apache Flink + Apache Druid + TiDB&lt;/p&gt;\n\n&lt;p&gt;This  time, we replaced Storm with Flink, and MySQL with TiDB. Flink was more  powerful in terms of semantics and features, while TiDB, with its  distributed capability, was more maintainable than MySQL. But  architecture 2.0 was nowhere near our goal of end-to-end data  consistency, either, because when processing huge data, enabling TiDB  transactions largely slowed down data writing. Plus, Druid itself did  not support standard SQL, so there were some learning costs and  frictions in usage.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=68756da95ae7d2221d85e419759103e068205ad8\"&gt;https://preview.redd.it/q9n64kxkmt4b1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=68756da95ae7d2221d85e419759103e068205ad8&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Platform Architecture 3.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;: Apache Flink + &lt;a href=\"https://github.com/apache/doris\"&gt;Apache Doris&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We  replaced Apache Druid with Apache Doris as the OLAP engine, which could  also serve as a unified data serving gateway. So in Architecture 3.0,  we only need to maintain one set of query logic. And we layered our  real-time datawarehouse to increase reusability of real-time data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409\"&gt;https://preview.redd.it/1nq2spknmt4b1.png?width=1340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6dc0125b44ac9de1bf76d0d8bd78fa9868f36409&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Turns  out the combination of Flink and Doris was the answer. We can exploit  their features to realize quick computation and data consistency. Keep  reading and see how we make it happen.&lt;/p&gt;\n\n&lt;h1&gt;Quick Computation&lt;/h1&gt;\n\n&lt;p&gt;As  one piece of operation data can be attached to 20 tags, in A/B testing,  we compare two groups of data centering only one tag each time. At  first, we thought about splitting one piece of operation data (with 20  tags) into 20 pieces of data of only one tag upon data ingestion, and  then importing them into Doris for analysis, but that could cause a data  explosion and thus huge pressure on our clusters.&lt;/p&gt;\n\n&lt;p&gt;Then  we tried moving part of such workload to the computation engine. So we  tried and \u201cexploded\u201d the data in Flink, but soon regretted it, because  when we aggregated the data using the global hash windows in Flink jobs,  the network and CPU usage also \u201cexploded\u201d.&lt;/p&gt;\n\n&lt;p&gt;Our  third shot was to aggregate data locally in Flink right after we split  it. As is shown below, we create a window in the memory of one operator  for local aggregation; then we further aggregate it using the global  hash windows. Since two operators chained together are in one thread,  transferring data between operators consumes much less network  resources. &lt;strong&gt;The two-step aggregation method, combined with the&lt;/strong&gt; &lt;a href=\"https://doris.apache.org/docs/dev/data-table/data-model\"&gt;&lt;strong&gt;Aggregate model&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;of Apache Doris, can keep data explosion in a manageable range.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152\"&gt;https://preview.redd.it/tgbprtyqmt4b1.png?width=1642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=36aa64e17328c85b3d3466c0196485abcc7eb152&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For  convenience in A/B testing, we make the test tag ID the first sorted  field in Apache Doris, so we can quickly locate the target data using  sorted indexes. To further minimize data processing in queries, we  create materialized views with the frequently used dimensions. With  constant modification and updates, the materialized views are applicable  in 80% of our queries.&lt;/p&gt;\n\n&lt;p&gt;To  sum up, with the application of sorted index and materialized views, we  reduce our query response time to merely seconds in A/B testing.&lt;/p&gt;\n\n&lt;h1&gt;Data Integrity Guarantee&lt;/h1&gt;\n\n&lt;p&gt;Imagine  that your algorithm designers worked sweat and tears trying to improve  the business, only to find their solution unable to be validated by A/B  testing due to data loss. This is an unbearable situation, and we make  every effort to avoid it.&lt;/p&gt;\n\n&lt;h1&gt;Develop a Sink-to-Doris Component&lt;/h1&gt;\n\n&lt;p&gt;To  ensure end-to-end data integrity, we developed a Sink-to-Doris  component. It is built on our own Flink Stream API scaffolding and  realized by the idempotent writing of Apache Doris and the two-stage  commit mechanism of Apache Flink. On top of it, we have a data  protection mechanism against anomalies.&lt;/p&gt;\n\n&lt;p&gt;It  is the result of our long-term evolution. We used to ensure data  consistency by implementing \u201cone writing for one tag ID\u201d. Then we  realized we could make good use of the transactions in Apache Doris and  the two-stage commit of Apache Flink.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df\"&gt;https://preview.redd.it/xpbtl0qtmt4b1.png?width=3380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c4751197e0c64bd4f3726784cf8b86686ce512df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As is shown above, this is how two-stage commit works to guarantee data consistency:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write data into local files;&lt;/li&gt;\n&lt;li&gt;Stage One: pre-commit data to Apache Doris. Save the Doris transaction ID into status;&lt;/li&gt;\n&lt;li&gt;If checkpoint fails, manually abandon the transaction; if checkpoint succeeds, commit the transaction in Stage Two;&lt;/li&gt;\n&lt;li&gt;If  the commit fails after multiple retries, the transaction ID and the  relevant data will be saved in HDFS, and we can restore the data via  Broker Load.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We  make it possible to split a single checkpoint into multiple  transactions, so that we can prevent one Stream Load from taking more  time than a Flink checkpoint in the event of large data volumes.&lt;/p&gt;\n\n&lt;h1&gt;Application Display&lt;/h1&gt;\n\n&lt;p&gt;This  is how we implement Sink-to-Doris. The component has blocked API calls  and topology assembly. With simple configuration, we can write data into  Apache Doris via Stream Load.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19\"&gt;https://preview.redd.it/959gakxvmt4b1.png?width=3289&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b0ed7ed72ebf22cd20aeab530dc86498e4219c19&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Cluster Monitoring&lt;/h1&gt;\n\n&lt;p&gt;For  cluster and host monitoring, we adopted the metrics templates provided  by the Apache Doris community. For data monitoring, in addition to the  template metrics, we added Stream Load request numbers and loading  rates.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c64d709b3f894937e97519e94305df903125c5\"&gt;https://preview.redd.it/e9mu9nsxmt4b1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89c64d709b3f894937e97519e94305df903125c5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Other metrics of our concerns include data writing speed and task  processing time. In the case of anomalies, we will receive notifications  in the form of phone calls, messages, and emails.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9\"&gt;https://preview.redd.it/z58h5wi0nt4b1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=cf08a1be084c22482973dc13eea6ecc9e2bc75f9&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Key Takeaways&lt;/h1&gt;\n\n&lt;p&gt;The  recipe for successful A/B testing is quick computation and high data  integrity. For this purpose, we implement a two-step aggregation method  in Apache Flink, utilize the Aggregate model, materialized view, and  short indexes of Apache Doris. Then we develop a Sink-to-Doris  component, which is realized by the idempotent writing of Apache Doris  and the two-stage commit mechanism of Apache Flink.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?auto=webp&amp;v=enabled&amp;s=ed9468f916100de164d0d4bde72466b599c4cd1f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b6e3934133a26ae55487c7bbcd89b5b26e72e07", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b8498732899bc298095c13caff27f9e29df5080", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51922ef3618bde282118ae560099eb52869c7485", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c24685b8cefde69b831f57afaa42c4da18f5168f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89f303ee2533ad6574df1ef6180d85d04705dc3a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/-ggh9eKQatmuKsahxCBWRVeMAt_c5DiVqNR_J_NiitI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=696f7137a7d46624c2b51624addbf2cb83b2fb89", "width": 1080, "height": 540}], "variants": {}, "id": "D3qhZW6Eh5PWuR0-NDgx2j7uZCBc6ZryuUEfgYXvDIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "144e0cp", "is_robot_indexable": true, "report_reasons": null, "author": "Any_Opportunity1234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144e0cp/ab_testing_was_a_handful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144e0cp/ab_testing_was_a_handful/", "subreddit_subscribers": 920865, "created_utc": 1686242035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TL;DR, is deep learning overrated when applied for multivariate TSAD/ multivariate predictive maintenance? Are there proven statistical methods applied in the field?  \n\n\nI would like to bring here a discussion that recently hit my thoughts. This short yt [video](https://www.youtube.com/watch?v=Vg1p3DouX8w) by [Eamonn Keogh](https://scholar.google.com/citations?user=slVcOQIAAAAJ&amp;hl=en) talks about UNIVARIATE TSAD, and critics the use of DL papers recently diffused in this field (he says that a lot of the ones we have are unprecised). Actually I share his ideas, but It does not talk about multivariate TSAD (extensible also for the reasoning of multivariate predictive maintenance for complex sensor systems). So I would like to know from your side if this reasoning is extensible also for multivariate case. For my personal study case I have found out a lot of LSTM Autoencoders applications when trying to model this problem.", "author_fullname": "t2_i6lyoywf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deep learning papers about Time Series Anomaly Detection (TSAD) are flawed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144a7kb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686233088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR, is deep learning overrated when applied for multivariate TSAD/ multivariate predictive maintenance? Are there proven statistical methods applied in the field?  &lt;/p&gt;\n\n&lt;p&gt;I would like to bring here a discussion that recently hit my thoughts. This short yt &lt;a href=\"https://www.youtube.com/watch?v=Vg1p3DouX8w\"&gt;video&lt;/a&gt; by &lt;a href=\"https://scholar.google.com/citations?user=slVcOQIAAAAJ&amp;amp;hl=en\"&gt;Eamonn Keogh&lt;/a&gt; talks about UNIVARIATE TSAD, and critics the use of DL papers recently diffused in this field (he says that a lot of the ones we have are unprecised). Actually I share his ideas, but It does not talk about multivariate TSAD (extensible also for the reasoning of multivariate predictive maintenance for complex sensor systems). So I would like to know from your side if this reasoning is extensible also for multivariate case. For my personal study case I have found out a lot of LSTM Autoencoders applications when trying to model this problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?auto=webp&amp;v=enabled&amp;s=c25788dc2f5e76b27aafda619fb6c2c4b2c74217", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9244d39957aa5938f36729d257f69064f42a2895", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b32b1acfd332537a6d5a14d46a031736b5464cf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/5kkl06Y7h5IUskbYcj-s-O2dOOjNC168fyaiboGo8Ng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=deb96c7d81f84612c4f1e9bd4ef97a0b5a6a438d", "width": 320, "height": 240}], "variants": {}, "id": "mn1T4iM0XKmCAI0A2QM7l--U5oqj5obH3Tq18NzGQ_Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144a7kb", "is_robot_indexable": true, "report_reasons": null, "author": "ginotherhino1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144a7kb/deep_learning_papers_about_time_series_anomaly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144a7kb/deep_learning_papers_about_time_series_anomaly/", "subreddit_subscribers": 920865, "created_utc": 1686233088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm taking a Data Science certificate and it only uses Python with Altair and Pandas. I would like to learn other programming languages like Java, C#, Tableau, VBA, SQL, Excel in the meantime.\n\nWhat are your recommendations for course platforms and why? I found Codecademy. Please explain why your recommendation is a good choice. Thank you.", "author_fullname": "t2_71bowl26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Best Place to Take Courses For New Programming Languages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ve7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686187728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m taking a Data Science certificate and it only uses Python with Altair and Pandas. I would like to learn other programming languages like Java, C#, Tableau, VBA, SQL, Excel in the meantime.&lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for course platforms and why? I found Codecademy. Please explain why your recommendation is a good choice. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143ve7a", "is_robot_indexable": true, "report_reasons": null, "author": "bioheal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143ve7a/the_best_place_to_take_courses_for_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143ve7a/the_best_place_to_take_courses_for_new/", "subreddit_subscribers": 920865, "created_utc": 1686187728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I build out a tool for work and want some of the other teams to use it.\n\nThe tool takes like 5 minutes to set up if skilled, but that skill is sql. Also I'd be pretty shocked if I could train the team to spin up a  vm, point the query to a new table and hit run.\n\nIt need to do 2 things, from a super ugly GUI I (or well we got interns today) will design:\n\n1. Pick the correct table to run from under GCP project\n\n2. Export the results to specified SharePoint directory (it already exports results to 2 different sql tables but back to they don't know sql)\n\nWhat front end frameworks have you had good experiences with?", "author_fullname": "t2_m3d4ku9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you guys have any suggestions on easy front end frameworks to implement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143t2xs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686181668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I build out a tool for work and want some of the other teams to use it.&lt;/p&gt;\n\n&lt;p&gt;The tool takes like 5 minutes to set up if skilled, but that skill is sql. Also I&amp;#39;d be pretty shocked if I could train the team to spin up a  vm, point the query to a new table and hit run.&lt;/p&gt;\n\n&lt;p&gt;It need to do 2 things, from a super ugly GUI I (or well we got interns today) will design:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Pick the correct table to run from under GCP project&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Export the results to specified SharePoint directory (it already exports results to 2 different sql tables but back to they don&amp;#39;t know sql)&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What front end frameworks have you had good experiences with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143t2xs", "is_robot_indexable": true, "report_reasons": null, "author": "DifficultyNext7666", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143t2xs/do_you_guys_have_any_suggestions_on_easy_front/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143t2xs/do_you_guys_have_any_suggestions_on_easy_front/", "subreddit_subscribers": 920865, "created_utc": 1686181668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just thought I'd share. Some of us have to jump around languages so it's nice when a library exists in things like C#, Python, R, Rust, etc.  \n\n\n[https://scisharp.github.io/SciSharp/](https://scisharp.github.io/SciSharp/)", "author_fullname": "t2_7ilnhzuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": ".NET version of Python Data Science Libraries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143nt05", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686169267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just thought I&amp;#39;d share. Some of us have to jump around languages so it&amp;#39;s nice when a library exists in things like C#, Python, R, Rust, etc.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://scisharp.github.io/SciSharp/\"&gt;https://scisharp.github.io/SciSharp/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143nt05", "is_robot_indexable": true, "report_reasons": null, "author": "DreJDavis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143nt05/net_version_of_python_data_science_libraries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143nt05/net_version_of_python_data_science_libraries/", "subreddit_subscribers": 920865, "created_utc": 1686169267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I've been investigating these ethical dilemmas lately, especially regarding algorithmic bias. What are your opinions on this topic? What do you think about this topic?", "author_fullname": "t2_6wl1sz1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ethical Dilemmas in Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144fouf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;ve been investigating these ethical dilemmas lately, especially regarding algorithmic bias. What are your opinions on this topic? What do you think about this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144fouf", "is_robot_indexable": true, "report_reasons": null, "author": "Diegoapaps", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144fouf/ethical_dilemmas_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144fouf/ethical_dilemmas_in_data_science/", "subreddit_subscribers": 920865, "created_utc": 1686245894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I asked in a [previous post](https://www.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/) for advice about how to find insight in unstructured text data. Almost everyone [recommended BERTopic](https://www.reddit.com/r/datascience/comments/13ltn9q/comment/jkrwrug/?utm_source=share&amp;utm_medium=web2x&amp;context=3), but I wasn't able to run BERTopic on my machine locally (segmentation fault). Fortunately, I found [Top2Vec](https://github.com/ddangelov/Top2Vec), which uses [HBDSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) and [UMAP](https://umap-learn.readthedocs.io/en/latest/) to quickly find good topics in uncleaned(!) text data.\n\nTo try to get the most out of Top2Vec, I wrote some code to select the best [HDBSCAN hyperparameters](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html), specifically the distance metric and the cluster selection method. My code runs Top2Vec with different distance and cluster selection method parameters and displays the [UMAP points plot](https://umap-learn.readthedocs.io/en/latest/plotting.html), and then I select the one with the clearest looking clusters in that plot (the business needs a good clustering visualization that really *pops* in a slide deck). My plan is to plot the network graph in Plotly when I'm finished.\n\nThe best I can do is get 5-10 topics that are *kind of good*, but not awesome. My plan today is to subset the data based on the topics, and run the documents that are assigned to each topic through Top2Vec again to try to get subtopics, and use the first topics as parent topics.\n\nDoes anyone have experience with doing something like this? Does this approach make sense? What else should I do to get the most out of Top2Vec?", "author_fullname": "t2_6gwxih9e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for best Top2Vec (HDBSCAN) usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1446lm8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686223549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I asked in a &lt;a href=\"https://www.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/\"&gt;previous post&lt;/a&gt; for advice about how to find insight in unstructured text data. Almost everyone &lt;a href=\"https://www.reddit.com/r/datascience/comments/13ltn9q/comment/jkrwrug/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;recommended BERTopic&lt;/a&gt;, but I wasn&amp;#39;t able to run BERTopic on my machine locally (segmentation fault). Fortunately, I found &lt;a href=\"https://github.com/ddangelov/Top2Vec\"&gt;Top2Vec&lt;/a&gt;, which uses &lt;a href=\"https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\"&gt;HBDSCAN&lt;/a&gt; and &lt;a href=\"https://umap-learn.readthedocs.io/en/latest/\"&gt;UMAP&lt;/a&gt; to quickly find good topics in uncleaned(!) text data.&lt;/p&gt;\n\n&lt;p&gt;To try to get the most out of Top2Vec, I wrote some code to select the best &lt;a href=\"https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\"&gt;HDBSCAN hyperparameters&lt;/a&gt;, specifically the distance metric and the cluster selection method. My code runs Top2Vec with different distance and cluster selection method parameters and displays the &lt;a href=\"https://umap-learn.readthedocs.io/en/latest/plotting.html\"&gt;UMAP points plot&lt;/a&gt;, and then I select the one with the clearest looking clusters in that plot (the business needs a good clustering visualization that really &lt;em&gt;pops&lt;/em&gt; in a slide deck). My plan is to plot the network graph in Plotly when I&amp;#39;m finished.&lt;/p&gt;\n\n&lt;p&gt;The best I can do is get 5-10 topics that are &lt;em&gt;kind of good&lt;/em&gt;, but not awesome. My plan today is to subset the data based on the topics, and run the documents that are assigned to each topic through Top2Vec again to try to get subtopics, and use the first topics as parent topics.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have experience with doing something like this? Does this approach make sense? What else should I do to get the most out of Top2Vec?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?auto=webp&amp;v=enabled&amp;s=b9dcf2856d881a082ba5b5abe3639f81dc42feb3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5519ba53fd55e1839842d46474d45a432d26d1ff", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c643da33a63d89089445be4dd3c7d6e835dac8e2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ea4612372122bd5404ac4adf308fd8f26099a4f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caf4d6224a869143bce4c0f665c1da57e66720cf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f12d33b764d4d25178b0596fba2120e5e3d3ad6e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/azhWJ5QDhw2-kNh9I294qzHWVztC20AoQPAMa84c4hg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce25173c8e5050e94fb690134c076fa95677d205", "width": 1080, "height": 540}], "variants": {}, "id": "aBSVvJdg6ekbq76-m9w74cXXRT4dCIQBjn0MUgj2e9c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1446lm8", "is_robot_indexable": true, "report_reasons": null, "author": "abelEngineer", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1446lm8/tips_for_best_top2vec_hdbscan_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1446lm8/tips_for_best_top2vec_hdbscan_usage/", "subreddit_subscribers": 920865, "created_utc": 1686223549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I just graduated maths/physics at the OU (in 37 and have worked in IT for 15 years). I've been taking some data analysis stuff on coursera and was considering what to do next, and came across this:\n\nhttps://www.coursera.org/degrees/msc-data-science-ul\n\nSince its brand new I'm not sure what to think, can anyone offer any insight?", "author_fullname": "t2_asgejs9xc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New DS Msc being offered at Leeds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143nphj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686169049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just graduated maths/physics at the OU (in 37 and have worked in IT for 15 years). I&amp;#39;ve been taking some data analysis stuff on coursera and was considering what to do next, and came across this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.coursera.org/degrees/msc-data-science-ul\"&gt;https://www.coursera.org/degrees/msc-data-science-ul&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Since its brand new I&amp;#39;m not sure what to think, can anyone offer any insight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?auto=webp&amp;v=enabled&amp;s=27d1fbfa41af08ab445125a5830ec1430f088fbe", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b3ed072c56ec3ae0f01b63b31f88e9dcfedd755", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aac48616e1f390d96fa9ec298d73476bb0c10a0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d3c1980b3aaf1c175d31609ebfcb55b32519959", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9b5b8270e6b546ed7251d47ac6e5acf7403cd1f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07b9aa4e8ce45c44a350f4ee3e281f47dbc30394", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/pLSR0Rpmlm0YOrI_U2sKzEeF87LsInGWnlb54-hObD0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=642d9f69a16a0baabc5c6d9c77a1a7de26ad4e49", "width": 1080, "height": 565}], "variants": {}, "id": "nnmvZitIdtl3UJSov7ZcIqMwnFG7OvGcVvkNPR4Vsrc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143nphj", "is_robot_indexable": true, "report_reasons": null, "author": "Embarrassed_Sky_yes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/143nphj/new_ds_msc_being_offered_at_leeds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/143nphj/new_ds_msc_being_offered_at_leeds/", "subreddit_subscribers": 920865, "created_utc": 1686169049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Before diving into the purpose i.e will\n\nMojo\ud83d\udd25 replace Python\ud83d\udc0d or not, first of all, let's see what is Mojo.  \n[https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521](https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521)", "author_fullname": "t2_icilc2wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review \ud83d\udd25 Mojo: Will Mojo\ud83d\udd25 Replace Python? \ud83e\udd14", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144euf9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686243925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before diving into the purpose i.e will&lt;/p&gt;\n\n&lt;p&gt;Mojo\ud83d\udd25 replace Python\ud83d\udc0d or not, first of all, let&amp;#39;s see what is Mojo.&lt;br/&gt;\n&lt;a href=\"https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521\"&gt;https://medium.com/python-in-plain-english/review-mojo-will-mojo-replace-python-36471c154521&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?auto=webp&amp;v=enabled&amp;s=1b6758ef9e33db8344d93a82c9b04aa20dfd3942", "width": 832, "height": 470}, "resolutions": [{"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2edf9f78592752569097c6101967c97f70cea9ae", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9194847bf52a6105e57a7e29e2024a5931b8374", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5354363d198876e0b43805cf2cee8e45cbedf530", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/8v-wfUZegJVNpnmRYQdX1U8Xbo0pgS6dtv0p3-fGoic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3e95214d4c3b91e4784b9e5a8068f4f0448f733", "width": 640, "height": 361}], "variants": {}, "id": "ufwQMjelZj16nhL0qkZMNIgiNqBpIcGV9sHJFCUVDTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144euf9", "is_robot_indexable": true, "report_reasons": null, "author": "gaodalie", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/144euf9/review_mojo_will_mojo_replace_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/144euf9/review_mojo_will_mojo_replace_python/", "subreddit_subscribers": 920865, "created_utc": 1686243925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "  Everyone talk about space as final front, but sea is the last one. Because can be a great source from valuable resource as oil, minerals and food. World largest corps actually explores and make money from it, being estimated as to be a big industry in next decades.\n  So I guess data professionals can be useful and make money helping to discover where interesting things can be found. So how work in this industry, and what might be useful?", "author_fullname": "t2_c12nt86a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How work in sea exploration as data profissional?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449ohj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone talk about space as final front, but sea is the last one. Because can be a great source from valuable resource as oil, minerals and food. World largest corps actually explores and make money from it, being estimated as to be a big industry in next decades.\n  So I guess data professionals can be useful and make money helping to discover where interesting things can be found. So how work in this industry, and what might be useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1449ohj", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Trifle-2735", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1449ohj/how_work_in_sea_exploration_as_data_profissional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1449ohj/how_work_in_sea_exploration_as_data_profissional/", "subreddit_subscribers": 920865, "created_utc": 1686231795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got 4 Data Science job offers with salaries between $100k - $150k in a single week, and I have a degree in English Literature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144bt1i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_9ntzngqbv", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have 3 years experience as a Data Analyst and a certificate (not a degree) an online Data Science program. Those are pretty weak credentials, and I'm sure I'm not the only person with that kind of background that starts the job search thinking there's no chance anyone would ever hire me.\n\nI wanted to share what worked for me, just in case it can work for anybody else.\n\nBasically, it's this:\n\n**Treat the job interview like you're selling a service**\n\nWhat worked for me was to stop thinking of it as a job interview.\n\nInstead, imagine that you're the sales rep for a Data company answering an RFP. A client has a problem and they need a solution. You're just there to demonstrate that you can implement it.\n\nTry to figure out what problem they're trying to solve with this role before the interview begins. That might be something like: \"We have data but we don't know how to get meaning out of it\" or \"We need to re-architect our data\" or even just: \"We have a guy who does a great job, but we need two of him.\"\n\nCenter everything you say around the key message of: \"I know what your problem is and I know how to solve it.\"\n\nWhen they ask you to tell them about yourself:\n\n1. Focus your answer on demonstrating that you have experience solving problems like theirs\n2. Wrap it up by saying you were interested in the job because you got the impression that they need that problem solved, and you have a lot of experience solving that problem\n3. Ask the interviewer if you're on the right about what problem they need solved\n\nIt's fine if you've totally misread the company. The point is that, when you ask that question, early in the interview, you force the interviewer to explain what they want the person who takes the role to be able to do.\n\nIt also switches the whole dynamic of the interview. Instead of them asking you questions, it's now about you troubleshooting that problem.\n\nRespond by:\n\n1. Asking clarifying questions about the problem they have\n2. Explaining how you would approach the problem\n3. Describing past similar projects you've worked on and how you solved them\n4. Highlighting the business impact of your solutions\n\nDoing this made a *massive* difference in my job search. I didn't hear back from any job I applied to until I tried this approach, but I heard back from everybody after I did.", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got 4 Data Science job offers with salaries between $100k - $150k in a single week, and I have a degree in English Literature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_umse6v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1811, "total_awards_received": 3, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1811, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1652215972.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 2}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1652215748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 years experience as a Data Analyst and a certificate (not a degree) an online Data Science program. Those are pretty weak credentials, and I&amp;#39;m sure I&amp;#39;m not the only person with that kind of background that starts the job search thinking there&amp;#39;s no chance anyone would ever hire me.&lt;/p&gt;\n\n&lt;p&gt;I wanted to share what worked for me, just in case it can work for anybody else.&lt;/p&gt;\n\n&lt;p&gt;Basically, it&amp;#39;s this:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Treat the job interview like you&amp;#39;re selling a service&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;What worked for me was to stop thinking of it as a job interview.&lt;/p&gt;\n\n&lt;p&gt;Instead, imagine that you&amp;#39;re the sales rep for a Data company answering an RFP. A client has a problem and they need a solution. You&amp;#39;re just there to demonstrate that you can implement it.&lt;/p&gt;\n\n&lt;p&gt;Try to figure out what problem they&amp;#39;re trying to solve with this role before the interview begins. That might be something like: &amp;quot;We have data but we don&amp;#39;t know how to get meaning out of it&amp;quot; or &amp;quot;We need to re-architect our data&amp;quot; or even just: &amp;quot;We have a guy who does a great job, but we need two of him.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Center everything you say around the key message of: &amp;quot;I know what your problem is and I know how to solve it.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;When they ask you to tell them about yourself:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Focus your answer on demonstrating that you have experience solving problems like theirs&lt;/li&gt;\n&lt;li&gt;Wrap it up by saying you were interested in the job because you got the impression that they need that problem solved, and you have a lot of experience solving that problem&lt;/li&gt;\n&lt;li&gt;Ask the interviewer if you&amp;#39;re on the right about what problem they need solved&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It&amp;#39;s fine if you&amp;#39;ve totally misread the company. The point is that, when you ask that question, early in the interview, you force the interviewer to explain what they want the person who takes the role to be able to do.&lt;/p&gt;\n\n&lt;p&gt;It also switches the whole dynamic of the interview. Instead of them asking you questions, it&amp;#39;s now about you troubleshooting that problem.&lt;/p&gt;\n\n&lt;p&gt;Respond by:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Asking clarifying questions about the problem they have&lt;/li&gt;\n&lt;li&gt;Explaining how you would approach the problem&lt;/li&gt;\n&lt;li&gt;Describing past similar projects you&amp;#39;ve worked on and how you solved them&lt;/li&gt;\n&lt;li&gt;Highlighting the business impact of your solutions&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Doing this made a &lt;em&gt;massive&lt;/em&gt; difference in my job search. I didn&amp;#39;t hear back from any job I applied to until I tried this approach, but I heard back from everybody after I did.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 2, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "umse6v", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 290, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/umse6v/i_got_4_data_science_job_offers_with_salaries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/umse6v/i_got_4_data_science_job_offers_with_salaries/", "subreddit_subscribers": 920865, "created_utc": 1652215748.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686236912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/datascience/comments/umse6v/i_got_4_data_science_job_offers_with_salaries/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144bt1i", "is_robot_indexable": true, "report_reasons": null, "author": "Itchy_Ad6103", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_umse6v", "author_flair_text_color": null, "permalink": "/r/datascience/comments/144bt1i/i_got_4_data_science_job_offers_with_salaries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/datascience/comments/umse6v/i_got_4_data_science_job_offers_with_salaries/", "subreddit_subscribers": 920865, "created_utc": 1686236912.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}