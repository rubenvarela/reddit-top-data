{"kind": "Listing", "data": {"after": "t3_143x8kz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our use cases for airflow will almost entirely involve it triggering other services that do the heavy lifting and have their own compute. I am really struggling to understand why I would need a full on cluster with separate workers, scheduler and webserver. Could I get away with deploying it on say a single ec2 instance or am I missing something obvious?", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow- am i missing something? Why does it need to be run on a large cluster with lots of workers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1438mkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686130858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our use cases for airflow will almost entirely involve it triggering other services that do the heavy lifting and have their own compute. I am really struggling to understand why I would need a full on cluster with separate workers, scheduler and webserver. Could I get away with deploying it on say a single ec2 instance or am I missing something obvious?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1438mkz", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1438mkz/airflow_am_i_missing_something_why_does_it_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1438mkz/airflow_am_i_missing_something_why_does_it_need/", "subreddit_subscribers": 109443, "created_utc": 1686130858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel dirty even asking this since whenever someone posts \"How do I learn about X?\", someone inevitably posts \"Go read 'The Big Book of X' by [very important person]\". It seems to be a very popular way to learn.\n\nI have never once read any book related to data. I've read some blogs, articles, websites, watched some videos, but largely learnt from the others around me or through tackling the challenges I encounter in my job and projects on an ad hoc basis. In some cases, I've learnt through a few courses. But never once have I read a technical knowledge book.", "author_fullname": "t2_d505p8is", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone learn without reading books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143f4n0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686149000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel dirty even asking this since whenever someone posts &amp;quot;How do I learn about X?&amp;quot;, someone inevitably posts &amp;quot;Go read &amp;#39;The Big Book of X&amp;#39; by [very important person]&amp;quot;. It seems to be a very popular way to learn.&lt;/p&gt;\n\n&lt;p&gt;I have never once read any book related to data. I&amp;#39;ve read some blogs, articles, websites, watched some videos, but largely learnt from the others around me or through tackling the challenges I encounter in my job and projects on an ad hoc basis. In some cases, I&amp;#39;ve learnt through a few courses. But never once have I read a technical knowledge book.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143f4n0", "is_robot_indexable": true, "report_reasons": null, "author": "Length-Working", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143f4n0/does_anyone_learn_without_reading_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143f4n0/does_anyone_learn_without_reading_books/", "subreddit_subscribers": 109443, "created_utc": 1686149000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our dashboards at work basically have powerbi doing all the merging and aggregation.\n\nI am not positive if it's done by query or after pointed to the sql tables.\n\nIs that normal, or best practices? I would have built a view and pointed to the view.", "author_fullname": "t2_m3d4ku9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dashboards best practices - how much transformation should PowerBI or Tableau be doing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143t8vc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686182089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our dashboards at work basically have powerbi doing all the merging and aggregation.&lt;/p&gt;\n\n&lt;p&gt;I am not positive if it&amp;#39;s done by query or after pointed to the sql tables.&lt;/p&gt;\n\n&lt;p&gt;Is that normal, or best practices? I would have built a view and pointed to the view.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143t8vc", "is_robot_indexable": true, "report_reasons": null, "author": "DifficultyNext7666", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143t8vc/dashboards_best_practices_how_much_transformation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143t8vc/dashboards_best_practices_how_much_transformation/", "subreddit_subscribers": 109443, "created_utc": 1686182089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nMy employer is offering 2k towards a class/certification.  I have extensive experience in SQL and have done data migrations the old school way of using stored procedures etc. I really want to get a better understanding of the tools and methods of running a data pipeline. I have limited experience with Python. Is there a recommendation for a good \u201call-in-one\u201d data engineering course I can take? I\u2019d really like to take up the offer of using the 2k towards this. \n\nThanks!", "author_fullname": "t2_7y30rgjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2k towards a course in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143gt1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686152909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;My employer is offering 2k towards a class/certification.  I have extensive experience in SQL and have done data migrations the old school way of using stored procedures etc. I really want to get a better understanding of the tools and methods of running a data pipeline. I have limited experience with Python. Is there a recommendation for a good \u201call-in-one\u201d data engineering course I can take? I\u2019d really like to take up the offer of using the 2k towards this. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143gt1s", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Fox940", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143gt1s/2k_towards_a_course_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143gt1s/2k_towards_a_course_in_data_engineering/", "subreddit_subscribers": 109443, "created_utc": 1686152909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nIf any of you are like me, I get so sucked into what I am working on that day that I end up sitting in my chair for hours at a time. I wanted to put together a project that would help me stop that trend, and I thought I would share because I am sure there are others in the community that struggle with the same issue. \n\n[https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/](https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/)\n\nIn this project, I go from end-to-end and create a daily alert to stretch if I haven't already. Here is a brief outline of the project:\n\n* Use Python code to call the Peloton API to get my personal workout data\n* Do some basic cleaning with Python on the data\n* Upload the data to Snowflake\n* Set up an alert with SQL that will send an email to me if I haven't stretched by 1PM. \n\nThis project was completed in Shipyard (which is where I work), however it could be completed using any orchestration tool or even just scripts. \n\nLet me know what you think. If there are any improvements that you could note, feel free to pass those along as well!", "author_fullname": "t2_pvgyqb8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End-to-End Data Project to Send an Automated Reminder to Stretch Throughout the Day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143dvll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686145910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;If any of you are like me, I get so sucked into what I am working on that day that I end up sitting in my chair for hours at a time. I wanted to put together a project that would help me stop that trend, and I thought I would share because I am sure there are others in the community that struggle with the same issue. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/\"&gt;https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this project, I go from end-to-end and create a daily alert to stretch if I haven&amp;#39;t already. Here is a brief outline of the project:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use Python code to call the Peloton API to get my personal workout data&lt;/li&gt;\n&lt;li&gt;Do some basic cleaning with Python on the data&lt;/li&gt;\n&lt;li&gt;Upload the data to Snowflake&lt;/li&gt;\n&lt;li&gt;Set up an alert with SQL that will send an email to me if I haven&amp;#39;t stretched by 1PM. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This project was completed in Shipyard (which is where I work), however it could be completed using any orchestration tool or even just scripts. &lt;/p&gt;\n\n&lt;p&gt;Let me know what you think. If there are any improvements that you could note, feel free to pass those along as well!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?auto=webp&amp;v=enabled&amp;s=53e4f42d4b968cf96281d54558f49e0e2dc06c0b", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e06cb2de3fe205c2525a8f16da78d8dec24e4ad", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b77b899e5ac04903779b0da694ddcb33097a2219", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3c08fe3ca38f4d220909733f36c17ff7300b4e3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f47e3cf99f5eb182dba016c2b06d28a0e943bca", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af3d408011a6ba1cea8c307a9d16c88af30e00c9", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33d69a5e958572130638050edf963eabb1448572", "width": 1080, "height": 607}], "variants": {}, "id": "-EBJ3hUdCCn6i7lpzQ3kGn1v-Ye1nHFdLirmF50JK4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "143dvll", "is_robot_indexable": true, "report_reasons": null, "author": "Steven_Johnson34", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143dvll/endtoend_data_project_to_send_an_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143dvll/endtoend_data_project_to_send_an_automated/", "subreddit_subscribers": 109443, "created_utc": 1686145910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zia33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Profiler 0.9.0 -- offering a massive improvement to memory usage during profiling of large datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_143dscv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sejrMas-9x1K5DtGLpcxdJ_h-N4dt9jYl13XnnKB-ss.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686145681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/capitalone/DataProfiler", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?auto=webp&amp;v=enabled&amp;s=e5536a80b6750ba08e3bf3856396b1dd19f3fbc3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc011712f24eebfe017ce880714982d8cdec6f42", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3afb78a8d1a5bb0c1baf7c3413391ae6e363e1b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff1f1a543249dd5270e2efb9385ef330e13dff79", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9097872352ace9c407ce48682cbe0acae1757d25", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93509909321fafcfde66039cafa368a0a1722127", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17b39558456f9099164009ff3235052559b3b9dd", "width": 1080, "height": 540}], "variants": {}, "id": "WBr7WyealiMEllP6S-Ob6zF8PTPg7XX4xon-M5lsTe0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143dscv", "is_robot_indexable": true, "report_reasons": null, "author": "fitz_n_fitz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143dscv/data_profiler_090_offering_a_massive_improvement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/capitalone/DataProfiler", "subreddit_subscribers": 109443, "created_utc": 1686145681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just finished a technical interview for a lead data engineer position. It is an hour long interview and spent the first half of it  going through SQL leetcode with complex window functions.\n\nAt around 40 mins mark I realised that they are just looking for a SQL guru and ignoring the facts that I have more to offers eg knowledge about AWS services, Terraforming infrastructure, data architecture, etc.\n\nIs this data engineering all about (being great with SQL) or did i make a good decision and asked to stop the interview at minute 45? What are your thoughts?", "author_fullname": "t2_8n43xmar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviewing for lead data engineer position.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143wn2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just finished a technical interview for a lead data engineer position. It is an hour long interview and spent the first half of it  going through SQL leetcode with complex window functions.&lt;/p&gt;\n\n&lt;p&gt;At around 40 mins mark I realised that they are just looking for a SQL guru and ignoring the facts that I have more to offers eg knowledge about AWS services, Terraforming infrastructure, data architecture, etc.&lt;/p&gt;\n\n&lt;p&gt;Is this data engineering all about (being great with SQL) or did i make a good decision and asked to stop the interview at minute 45? What are your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "143wn2y", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-River1467", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143wn2y/interviewing_for_lead_data_engineer_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143wn2y/interviewing_for_lead_data_engineer_position/", "subreddit_subscribers": 109443, "created_utc": 1686191104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "While there are some projects where you  can make a use case of their product and write a blog about it and you are a contributor. But are there any projects where you can contribute by building ETL pipelines for them?", "author_fullname": "t2_ens6kw85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it impossible to contribute to open source as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143us7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686186094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While there are some projects where you  can make a use case of their product and write a blog about it and you are a contributor. But are there any projects where you can contribute by building ETL pipelines for them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143us7i", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent-Tadpole-564", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143us7i/is_it_impossible_to_contribute_to_open_source_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143us7i/is_it_impossible_to_contribute_to_open_source_as/", "subreddit_subscribers": 109443, "created_utc": 1686186094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm learning about Presto and Trino, and this term \"Interactive querying\" comes up a lot. I have a hard time wrapping my head around it.   \n\n\nIf you are using Presto or Trino, why do you use it, compared to Simple hive query, for example?", "author_fullname": "t2_8r6amwln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What exactly is interactive querying? When do you need it? How is it different from running a couple of Hive SQL commands?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ts6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686183465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m learning about Presto and Trino, and this term &amp;quot;Interactive querying&amp;quot; comes up a lot. I have a hard time wrapping my head around it.   &lt;/p&gt;\n\n&lt;p&gt;If you are using Presto or Trino, why do you use it, compared to Simple hive query, for example?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143ts6n", "is_robot_indexable": true, "report_reasons": null, "author": "money_noob_007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143ts6n/what_exactly_is_interactive_querying_when_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143ts6n/what_exactly_is_interactive_querying_when_do_you/", "subreddit_subscribers": 109443, "created_utc": 1686183465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is transitioning all of their data to a new DBMS. They are trying to decide what software to host the database on. They are currently using Microsoft products and considering the following DBMS: Oracle, MySQL, MS SQL, Google BigQuery and PostgreSQL.   \n\n\nHow do all of these compare? What is best for a small company with only two data engineers? (We are very interested in layering a GUI over the database so everyone can interact with data and creating automated reporting / dashboards). \n\nThank you for your thoughts!\n\n[View Poll](https://www.reddit.com/poll/143cmhx)", "author_fullname": "t2_7ox5swxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best DBMS for a small business with very limited engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143cmhx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686142770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is transitioning all of their data to a new DBMS. They are trying to decide what software to host the database on. They are currently using Microsoft products and considering the following DBMS: Oracle, MySQL, MS SQL, Google BigQuery and PostgreSQL.   &lt;/p&gt;\n\n&lt;p&gt;How do all of these compare? What is best for a small company with only two data engineers? (We are very interested in layering a GUI over the database so everyone can interact with data and creating automated reporting / dashboards). &lt;/p&gt;\n\n&lt;p&gt;Thank you for your thoughts!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/143cmhx\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143cmhx", "is_robot_indexable": true, "report_reasons": null, "author": "Use_Clean", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1686401970439, "options": [{"text": "Oracle", "id": "23376726"}, {"text": "MySQL", "id": "23376727"}, {"text": "MS SQL", "id": "23376728"}, {"text": "Google BigQuery", "id": "23376729"}, {"text": "PostgreSQL", "id": "23376730"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 398, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143cmhx/best_dbms_for_a_small_business_with_very_limited/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/143cmhx/best_dbms_for_a_small_business_with_very_limited/", "subreddit_subscribers": 109443, "created_utc": 1686142770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is too dumb to ask this question, but I don't truly understand the importance of dimensional modelling. Can't is simply pull exact replicas of oltp data to a database/datawarehouse(as per my knowledge, I feel both are similar when we consider the storage part)  and build bi reports on top of it without building fact and dim tables. lets take the example of simple book sales.   \n\n\nIn OLTP we  have 3 tables books, authors and sales. even if we build fact and dim tables we will end up having same tables except that we will not have relation ships between dim tables(books and authors) that exists in oltp and aditionally we use date dim tables in dimensional modelling.  \n\n\nThese changes really make the process of building bi reports effective? because I can build the reports even on exact copy of oltp tables without spending time on building dim and fact tables.  \n\n\nAgain sorry if my question looks very basic and dumb. Thank you!", "author_fullname": "t2_v1vre9oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building Bi reports on OLTP model without dimensional modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1433zob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686114454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is too dumb to ask this question, but I don&amp;#39;t truly understand the importance of dimensional modelling. Can&amp;#39;t is simply pull exact replicas of oltp data to a database/datawarehouse(as per my knowledge, I feel both are similar when we consider the storage part)  and build bi reports on top of it without building fact and dim tables. lets take the example of simple book sales.   &lt;/p&gt;\n\n&lt;p&gt;In OLTP we  have 3 tables books, authors and sales. even if we build fact and dim tables we will end up having same tables except that we will not have relation ships between dim tables(books and authors) that exists in oltp and aditionally we use date dim tables in dimensional modelling.  &lt;/p&gt;\n\n&lt;p&gt;These changes really make the process of building bi reports effective? because I can build the reports even on exact copy of oltp tables without spending time on building dim and fact tables.  &lt;/p&gt;\n\n&lt;p&gt;Again sorry if my question looks very basic and dumb. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1433zob", "is_robot_indexable": true, "report_reasons": null, "author": "sach_mess10", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1433zob/building_bi_reports_on_oltp_model_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1433zob/building_bi_reports_on_oltp_model_without/", "subreddit_subscribers": 109443, "created_utc": 1686114454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My background is mostly in Microsoft and Azure products, but I have a potential contract opportunity working with GCP.  Not sure if I'll be successful or not, so it's potentially a moot point.  But the opportunity to get away from the Microsoft treadmill is appealing... I'd be more than happy if I never saw ADF/Synapse ever again.  \n\nCan anyone talk about the pros and cons of Google's stack... and particularly how it lines up against Azure?", "author_fullname": "t2_5txrt2ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ubuc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686184898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My background is mostly in Microsoft and Azure products, but I have a potential contract opportunity working with GCP.  Not sure if I&amp;#39;ll be successful or not, so it&amp;#39;s potentially a moot point.  But the opportunity to get away from the Microsoft treadmill is appealing... I&amp;#39;d be more than happy if I never saw ADF/Synapse ever again.  &lt;/p&gt;\n\n&lt;p&gt;Can anyone talk about the pros and cons of Google&amp;#39;s stack... and particularly how it lines up against Azure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "143ubuc", "is_robot_indexable": true, "report_reasons": null, "author": "tarzanboy76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143ubuc/azure_to_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143ubuc/azure_to_gcp/", "subreddit_subscribers": 109443, "created_utc": 1686184898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tech stack: \nAzure data lake gen 2 storage ;\nAzure data factory ;\nDatabricks premium ;\nMedallion Architecture ;\n\nTo preface: my organization has 20 different source systems across 15 different servers. Each server has a SQL DB with like data.  That data is extracted from the source sql db and stored in bronze layer as parquet files. \n\n\nI then have 1 silver databricks transformation notebook that the source system name is passed through from an ADF parameter to filter the source and target data.  \n\nAt the end of the procedure, I complete a merge into (update and insert) into a delta table that is partitioned by source system.  \n\nIn the merge into routine I am filtering the source AND target by the source system parameter. \n\nI have 1 pipeline that is triggered by the source system name (20 different triggers that all start at the same time, but with a different source system parameter). \n\nEven though my delta table is partitioned by source system and my notebook is being passed a source system, I still receive an error: files were added to partition source system = x , by a concurrent update. \n\n#my question\nIs there any way around this without creating a silver.deltatable_sourcesystem?\n\n\n\n\nThis is my redacted merge into code: \n\np_sourceSystemName = 'SourceSystem1'\n\ndeltaTable =  f\"delta.`abfss://pathtotable/Silver_mytable`\"\n\n-Get the list of column names for the source table\nsource_cols = spark.table(\"test\").columns\n\n-Create a temporary table with distinct records from the source table\nspark.table(\"test\").select(source_cols).distinct().createOrReplaceTempView(\"temp_source\")\n\n\n-Update existing records\nsql_update = f\"\"\"MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = '{p_sourceSystemName}'\n) AS source\n\nON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue != source.HashValue\nAND target.currentIndicator = 'Y'\nAND target.SourceSystem = '{p_sourceSystemName}'\n\nWHEN MATCHED THEN\n    UPDATE SET\n        currentIndicator = 'N',\n        ValidToTs = current_timestamp,\n        actionType = 'Update'\n\"\"\"\n- insert new records\nsql_insert = f\"\"\"MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = '{p_sourceSystemName}'\n) AS source\nON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue = source.HashValue\nAND target.currentIndicator = 'Y'\nAND target.SourceSystem = '{p_sourceSystemName}'\n\nWHEN NOT MATCHED THEN\n    INSERT ({\", \".join(source_cols)}) -- dynamically construct the column list\n    VALUES ({\", \".join([\"source.\" + col for col in source_cols])});  -- dynamically construct the value list\n\"\"\"\n\n- Execute the update and insert statements\nspark.sql(sql_update)\nspark.sql(sql_insert)", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Merge into", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143pi7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686173956.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686173087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tech stack: \nAzure data lake gen 2 storage ;\nAzure data factory ;\nDatabricks premium ;\nMedallion Architecture ;&lt;/p&gt;\n\n&lt;p&gt;To preface: my organization has 20 different source systems across 15 different servers. Each server has a SQL DB with like data.  That data is extracted from the source sql db and stored in bronze layer as parquet files. &lt;/p&gt;\n\n&lt;p&gt;I then have 1 silver databricks transformation notebook that the source system name is passed through from an ADF parameter to filter the source and target data.  &lt;/p&gt;\n\n&lt;p&gt;At the end of the procedure, I complete a merge into (update and insert) into a delta table that is partitioned by source system.  &lt;/p&gt;\n\n&lt;p&gt;In the merge into routine I am filtering the source AND target by the source system parameter. &lt;/p&gt;\n\n&lt;p&gt;I have 1 pipeline that is triggered by the source system name (20 different triggers that all start at the same time, but with a different source system parameter). &lt;/p&gt;\n\n&lt;p&gt;Even though my delta table is partitioned by source system and my notebook is being passed a source system, I still receive an error: files were added to partition source system = x , by a concurrent update. &lt;/p&gt;\n\n&lt;h1&gt;my question&lt;/h1&gt;\n\n&lt;p&gt;Is there any way around this without creating a silver.deltatable_sourcesystem?&lt;/p&gt;\n\n&lt;p&gt;This is my redacted merge into code: &lt;/p&gt;\n\n&lt;p&gt;p_sourceSystemName = &amp;#39;SourceSystem1&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;deltaTable =  f&amp;quot;delta.&lt;code&gt;abfss://pathtotable/Silver_mytable&lt;/code&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;-Get the list of column names for the source table\nsource_cols = spark.table(&amp;quot;test&amp;quot;).columns&lt;/p&gt;\n\n&lt;p&gt;-Create a temporary table with distinct records from the source table\nspark.table(&amp;quot;test&amp;quot;).select(source_cols).distinct().createOrReplaceTempView(&amp;quot;temp_source&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;-Update existing records\nsql_update = f&amp;quot;&amp;quot;&amp;quot;MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;\n) AS source&lt;/p&gt;\n\n&lt;p&gt;ON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue != source.HashValue\nAND target.currentIndicator = &amp;#39;Y&amp;#39;\nAND target.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;WHEN MATCHED THEN\n    UPDATE SET\n        currentIndicator = &amp;#39;N&amp;#39;,\n        ValidToTs = current_timestamp,\n        actionType = &amp;#39;Update&amp;#39;\n&amp;quot;&amp;quot;&amp;quot;\n- insert new records\nsql_insert = f&amp;quot;&amp;quot;&amp;quot;MERGE INTO {deltaTable} AS target\nUSING (\nSELECT\n*\nFROM temp_source S\nWHERE S.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;\n) AS source\nON target.CoIDPatNo = source.CoIDPatNo\nAND target.HashValue = source.HashValue\nAND target.currentIndicator = &amp;#39;Y&amp;#39;\nAND target.SourceSystem = &amp;#39;{p_sourceSystemName}&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;WHEN NOT MATCHED THEN\n    INSERT ({&amp;quot;, &amp;quot;.join(source_cols)}) -- dynamically construct the column list\n    VALUES ({&amp;quot;, &amp;quot;.join([&amp;quot;source.&amp;quot; + col for col in source_cols])});  -- dynamically construct the value list\n&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Execute the update and insert statements\nspark.sql(sql_update)\nspark.sql(sql_insert)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143pi7b", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143pi7b/databricks_merge_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143pi7b/databricks_merge_into/", "subreddit_subscribers": 109443, "created_utc": 1686173087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in search of a tool or a service that can be configured to keep two databases with slightly differing models in sync. \n\nI am looking to capture the source database changes in real time, transform the data as needed and propagate the changes to the destination database. \n\nWhat standalone tools OR AWS services can be used for this task?", "author_fullname": "t2_w1tdhbrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools or methods would work best for Postgres-&gt;Postgres CDC with transformations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143c63e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686141618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in search of a tool or a service that can be configured to keep two databases with slightly differing models in sync. &lt;/p&gt;\n\n&lt;p&gt;I am looking to capture the source database changes in real time, transform the data as needed and propagate the changes to the destination database. &lt;/p&gt;\n\n&lt;p&gt;What standalone tools OR AWS services can be used for this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143c63e", "is_robot_indexable": true, "report_reasons": null, "author": "data_pie3", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143c63e/what_tools_or_methods_would_work_best_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143c63e/what_tools_or_methods_would_work_best_for/", "subreddit_subscribers": 109443, "created_utc": 1686141618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for a solid stack to start with data-science.\n\n\\- Pandas\n\n\\- NumPy\n\n\\- scikit-learn\n\n\\- TensorFlow\n\n\\- PyTorch\n\n\\- Keras\n\n\\- Jupyter Notebook\n\n\\- ...\n\n&amp;#x200B;\n\ncan anyone refer to such a stack, which i can deploy on my docker-server?\n\n&amp;#x200B;\n\ntyvm!!", "author_fullname": "t2_9yw5ycsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a MLOps Stack for Docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1435020", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686117754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a solid stack to start with data-science.&lt;/p&gt;\n\n&lt;p&gt;- Pandas&lt;/p&gt;\n\n&lt;p&gt;- NumPy&lt;/p&gt;\n\n&lt;p&gt;- scikit-learn&lt;/p&gt;\n\n&lt;p&gt;- TensorFlow&lt;/p&gt;\n\n&lt;p&gt;- PyTorch&lt;/p&gt;\n\n&lt;p&gt;- Keras&lt;/p&gt;\n\n&lt;p&gt;- Jupyter Notebook&lt;/p&gt;\n\n&lt;p&gt;- ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;can anyone refer to such a stack, which i can deploy on my docker-server?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;tyvm!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1435020", "is_robot_indexable": true, "report_reasons": null, "author": "alber7777", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1435020/looking_for_a_mlops_stack_for_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1435020/looking_for_a_mlops_stack_for_docker/", "subreddit_subscribers": 109443, "created_utc": 1686117754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found this article describing an ELT pipeline.  It\u2019s a good read, but I\u2019m struggling to understand why you would store a data frame as parquet and store on Google Cloud Storage and then use a BQ Load Job to put it into BQ to transform and store in a BQ table.  Why not cut out the Cloud Storage step and put it directly into BQ?  Any ideas?", "author_fullname": "t2_5065w9mg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sky-Pipe: Prefect-GCP-dbt Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_143xvwc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TDhxRwWd7jcI-b7f-ZC7F5SXgnQBJuw5gYXtCXmDPWw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686194639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jackskylord.medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this article describing an ELT pipeline.  It\u2019s a good read, but I\u2019m struggling to understand why you would store a data frame as parquet and store on Google Cloud Storage and then use a BQ Load Job to put it into BQ to transform and store in a BQ table.  Why not cut out the Cloud Storage step and put it directly into BQ?  Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jackskylord.medium.com/sky-pipe-prefect-gcp-dbt-data-pipeline-80561107c38f", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?auto=webp&amp;v=enabled&amp;s=6a57dfcad039d0ec448dad3dbd92f7a5a5aa2c2a", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89a2f638491c9ddae01840225c197c4359dab5ce", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02237bea5a4ec78593ba28526a6ad716e1b65d7d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e16472eb10e9a958c2e3a0c07053b944d0a9e0df", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed293de1c2af0ca87d5a1cbc954125aad24a4d03", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a1e127fea200ccf0eab8fb901c422825c05ede4", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/vOIWIfrqqX9dM0GiVvgptRDC-7_AXqGmV1dpW7XnBjg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4ff12c625dc37c1a955168358b0644794f12c0b", "width": 1080, "height": 810}], "variants": {}, "id": "6V3XW5iiLUmun0rfq3Lz5QlX4QVzL7-Y8JQI4TRZuDI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143xvwc", "is_robot_indexable": true, "report_reasons": null, "author": "kvotheTHEinquisitor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143xvwc/skypipe_prefectgcpdbt_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jackskylord.medium.com/sky-pipe-prefect-gcp-dbt-data-pipeline-80561107c38f", "subreddit_subscribers": 109443, "created_utc": 1686194639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently learning elastic search..", "author_fullname": "t2_ur0ro3ju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good resource or blogs for Elastic Search ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ws10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686191476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently learning elastic search..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143ws10", "is_robot_indexable": true, "report_reasons": null, "author": "chaddlb0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143ws10/any_good_resource_or_blogs_for_elastic_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143ws10/any_good_resource_or_blogs_for_elastic_search/", "subreddit_subscribers": 109443, "created_utc": 1686191476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using this custom data validation library called Cerberus. Cerberus has a pre-built function, validate(), that validates data based on a table schema and set of validation rules that you give it. The function can either take in a row(in the form of a dictionary), or an entire pandas dataframe. \n\nThe function itself can take a really long time if the amount of data is large. Is there any way that I can partition the data and run the function against the partitions on multiple clusters in parallel?\n\nI've tried looking at many of Sparks features and couldn't figure out a solution ):", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help integrating Spark with an open source data validation library.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143meqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686166010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using this custom data validation library called Cerberus. Cerberus has a pre-built function, validate(), that validates data based on a table schema and set of validation rules that you give it. The function can either take in a row(in the form of a dictionary), or an entire pandas dataframe. &lt;/p&gt;\n\n&lt;p&gt;The function itself can take a really long time if the amount of data is large. Is there any way that I can partition the data and run the function against the partitions on multiple clusters in parallel?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried looking at many of Sparks features and couldn&amp;#39;t figure out a solution ):&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143meqi", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/143meqi/need_some_help_integrating_spark_with_an_open/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143meqi/need_some_help_integrating_spark_with_an_open/", "subreddit_subscribers": 109443, "created_utc": 1686166010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ayc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Daft: A High-Performance Distributed Dataframe Library for Multimodal Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "name": "t3_143g5la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jwGyLjzZbEIgBtBgbuKaWsGg1LxyH7-fX-cny7P6PQc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686151394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.getdaft.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.getdaft.io/p/introducing-daft-a-high-performance", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?auto=webp&amp;v=enabled&amp;s=af44267447f9459af1d070fc65d1957009020626", "width": 946, "height": 369}, "resolutions": [{"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a645bf20db63c3833dd03a6b7c5637d856189d42", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=266bc632d794474fd89bbdc267fdb0f9b6165094", "width": 216, "height": 84}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3da78b4472f60f2f820e4e7beb924ec94e3c53b", "width": 320, "height": 124}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8df22713f1d50180e70199fb801c3b8e2c0dd566", "width": 640, "height": 249}], "variants": {}, "id": "kEXEo96ULUyl0I7ys3DrbgOGio_zYSY3_GKj6DhNg2Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143g5la", "is_robot_indexable": true, "report_reasons": null, "author": "fridder", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143g5la/introducing_daft_a_highperformance_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.getdaft.io/p/introducing-daft-a-high-performance", "subreddit_subscribers": 109443, "created_utc": 1686151394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are building up our data warehouse/marts, but we\u2019ve got about 3k metabase queries which are complex and based on the raw tables.  Some queries hit snowflake and some hit Postgres directly.\n\nAre there any catalog/lineage/audit tools which can help identify \u201ccommon joins\u201d or \u201cCTE similarity\u201d?\n\nOr do we just need to:\n\n- elbow grease our way through the list, prioritized by usage/value, and extract common data model concepts \n\n- agile user development asking users what they want.", "author_fullname": "t2_1qwjj2kn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metabase/Dashboard Tech Debt: Tools to help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143e9ij", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686146873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are building up our data warehouse/marts, but we\u2019ve got about 3k metabase queries which are complex and based on the raw tables.  Some queries hit snowflake and some hit Postgres directly.&lt;/p&gt;\n\n&lt;p&gt;Are there any catalog/lineage/audit tools which can help identify \u201ccommon joins\u201d or \u201cCTE similarity\u201d?&lt;/p&gt;\n\n&lt;p&gt;Or do we just need to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;elbow grease our way through the list, prioritized by usage/value, and extract common data model concepts &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;agile user development asking users what they want.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143e9ij", "is_robot_indexable": true, "report_reasons": null, "author": "tomhallett", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143e9ij/metabasedashboard_tech_debt_tools_to_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143e9ij/metabasedashboard_tech_debt_tools_to_help/", "subreddit_subscribers": 109443, "created_utc": 1686146873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My objective is to create .parquet single file in Azure data lake. But when I do it with spark it always creates the additional nested folder and files and my name of the file starts with part. Is it even possible  to create a single .parquet file without using pandas or .to pandas() ? I am also restricted to use the file delete script. PS: I am using Azure Databricks.", "author_fullname": "t2_26t0xas9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": ".toParquet()", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143dxhm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686146048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My objective is to create .parquet single file in Azure data lake. But when I do it with spark it always creates the additional nested folder and files and my name of the file starts with part. Is it even possible  to create a single .parquet file without using pandas or .to pandas() ? I am also restricted to use the file delete script. PS: I am using Azure Databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143dxhm", "is_robot_indexable": true, "report_reasons": null, "author": "engg_garbage98", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143dxhm/toparquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143dxhm/toparquet/", "subreddit_subscribers": 109443, "created_utc": 1686146048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# [Jailer Database Tools.](https://wisser.github.io/Jailer/)\n\nJailer is a tool for database subsetting and relational data browsing.\n\nIt creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Subsetter creates small slices from your database (consistent and referentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.\n\n# Features\n\nExports consistent and referentially intact row-sets from your productive database and imports the data into your development and test environment.\n\nImproves database performance by removing and archiving obsolete data without violating integrity.\n\nGenerates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.\n\nData Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.", "author_fullname": "t2_5sa5b0ia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jailer Database Tools v15 released", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143aj03", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686137026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;&lt;a href=\"https://wisser.github.io/Jailer/\"&gt;Jailer Database Tools.&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;Jailer is a tool for database subsetting and relational data browsing.&lt;/p&gt;\n\n&lt;p&gt;It creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Subsetter creates small slices from your database (consistent and referentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.&lt;/p&gt;\n\n&lt;h1&gt;Features&lt;/h1&gt;\n\n&lt;p&gt;Exports consistent and referentially intact row-sets from your productive database and imports the data into your development and test environment.&lt;/p&gt;\n\n&lt;p&gt;Improves database performance by removing and archiving obsolete data without violating integrity.&lt;/p&gt;\n\n&lt;p&gt;Generates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.&lt;/p&gt;\n\n&lt;p&gt;Data Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143aj03", "is_robot_indexable": true, "report_reasons": null, "author": "Plane-Discussion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143aj03/jailer_database_tools_v15_released/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143aj03/jailer_database_tools_v15_released/", "subreddit_subscribers": 109443, "created_utc": 1686137026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I find it incredibly cheap to use spot with AWS Batch, but in the last week or so i've started seeing a lot more instances being terminated.\n\nI guess there's more demand at the moment, everyone training their LLMs!!!\n\nBut i guess this is your choice though right - basically don't use spot if you want reliability?  How do others manage this balance?", "author_fullname": "t2_30v538jt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using spot with AWS Batch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1439tzo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686134937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find it incredibly cheap to use spot with AWS Batch, but in the last week or so i&amp;#39;ve started seeing a lot more instances being terminated.&lt;/p&gt;\n\n&lt;p&gt;I guess there&amp;#39;s more demand at the moment, everyone training their LLMs!!!&lt;/p&gt;\n\n&lt;p&gt;But i guess this is your choice though right - basically don&amp;#39;t use spot if you want reliability?  How do others manage this balance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1439tzo", "is_robot_indexable": true, "report_reasons": null, "author": "codek1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1439tzo/using_spot_with_aws_batch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1439tzo/using_spot_with_aws_batch/", "subreddit_subscribers": 109443, "created_utc": 1686134937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI have an ADF question regarding Mapping data flow where CDC is ticked.\n\nSuppose I ticked the option that says \u201cFull on first run, then incremental.\u201d I tried running the dataflow within a pipeline for the first two runs and it worked fine. First load = load everything. Second load = didn\u2019t insert or do anything since I didn\u2019t change anything from source.\n\nNow, I know ADF manages its own checkpoint keys. I tried rerunning the dataflow, this time overriding the checkpoint key with a different value. What I\u2019m expecting here is that ADF will read again all rows from source, but won\u2019t insert whatever ID is already present in the sink. However, it looks like ADF still inserts leading to duplicate keys. \n\nI thought when I set a KEY column in the sink settings, it will know which rows already exist in the sink.\n\nHope it makes sense. TIA", "author_fullname": "t2_64zxitsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF Mapping Data Flow CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_143yg5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686196273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I have an ADF question regarding Mapping data flow where CDC is ticked.&lt;/p&gt;\n\n&lt;p&gt;Suppose I ticked the option that says \u201cFull on first run, then incremental.\u201d I tried running the dataflow within a pipeline for the first two runs and it worked fine. First load = load everything. Second load = didn\u2019t insert or do anything since I didn\u2019t change anything from source.&lt;/p&gt;\n\n&lt;p&gt;Now, I know ADF manages its own checkpoint keys. I tried rerunning the dataflow, this time overriding the checkpoint key with a different value. What I\u2019m expecting here is that ADF will read again all rows from source, but won\u2019t insert whatever ID is already present in the sink. However, it looks like ADF still inserts leading to duplicate keys. &lt;/p&gt;\n\n&lt;p&gt;I thought when I set a KEY column in the sink settings, it will know which rows already exist in the sink.&lt;/p&gt;\n\n&lt;p&gt;Hope it makes sense. TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143yg5s", "is_robot_indexable": true, "report_reasons": null, "author": "vinsanity1603", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143yg5s/adf_mapping_data_flow_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143yg5s/adf_mapping_data_flow_cdc/", "subreddit_subscribers": 109443, "created_utc": 1686196273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious - how big are the data lakes you're actively maintaining? Daily input/output? Mostly static or always being queried? What are you using for it? What works, what doesn't?\n\nForget the buzzwords - data lake, data lakehouse, data marsh, whatever you call it - the thing that isn't a data warehouse that people use to query", "author_fullname": "t2_40buwhod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time to Flex - How Big is your Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143x8kz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686196635.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686192800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious - how big are the data lakes you&amp;#39;re actively maintaining? Daily input/output? Mostly static or always being queried? What are you using for it? What works, what doesn&amp;#39;t?&lt;/p&gt;\n\n&lt;p&gt;Forget the buzzwords - data lake, data lakehouse, data marsh, whatever you call it - the thing that isn&amp;#39;t a data warehouse that people use to query&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143x8kz", "is_robot_indexable": true, "report_reasons": null, "author": "DrZachman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143x8kz/time_to_flex_how_big_is_your_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143x8kz/time_to_flex_how_big_is_your_data_lake/", "subreddit_subscribers": 109443, "created_utc": 1686192800.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}