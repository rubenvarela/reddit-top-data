{"kind": "Listing", "data": {"after": "t3_1443cuq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I came across these guys when working to save all of pikamee's youtube channel before it was deleted. They had already been saving everything as it was released. The site hosts over 200,000 videos. \n\nThe stated closure date is \"on or before July 24, 2023.\"\n\nAccording to the letter posted about the issue, they were using a Google Workspaces team drive with no backup. Apparently, Google is cracking down on their storage policies and a 1.38 PB teams drive is understandably pretty high up on their priority list.\n\nThis post is my warning to check for and save anything you care about that may be lost before the site is gone forever.\n\n[this is their website](https://archive.ragtag.moe/)", "author_fullname": "t2_tglchi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ragtag Archive is going offline - 1.38 PB of vtuber archives will be gone, many of which do not exist elsewhere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143zvuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 473, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 473, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686200596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across these guys when working to save all of pikamee&amp;#39;s youtube channel before it was deleted. They had already been saving everything as it was released. The site hosts over 200,000 videos. &lt;/p&gt;\n\n&lt;p&gt;The stated closure date is &amp;quot;on or before July 24, 2023.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;According to the letter posted about the issue, they were using a Google Workspaces team drive with no backup. Apparently, Google is cracking down on their storage policies and a 1.38 PB teams drive is understandably pretty high up on their priority list.&lt;/p&gt;\n\n&lt;p&gt;This post is my warning to check for and save anything you care about that may be lost before the site is gone forever.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.ragtag.moe/\"&gt;this is their website&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?auto=webp&amp;v=enabled&amp;s=6965c765e274449bdf42d3e1ee8713323b9e4172", "width": 1606, "height": 1625}, "resolutions": [{"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb55a6beb91f37fd66ed1f6d5a5832ad175f44ea", "width": 108, "height": 109}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8bd3ab6a5847f5a67c0df948a8539981014dc1f", "width": 216, "height": 218}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19caf8d55f164ee26441b77fc627cf22bb60f290", "width": 320, "height": 323}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7c64f85d5cf1290838552a8c1801d5138faa55b", "width": 640, "height": 647}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9df107ea0a4fa460ee2d124246813a1c9aa132fa", "width": 960, "height": 971}, {"url": "https://external-preview.redd.it/J3ulZhU6El7W3BDobcSf5krdmci61XUB1sYrTg3P4N4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b774fd9ccae178fe28e60ff981e6cf41ff694714", "width": 1080, "height": 1092}], "variants": {}, "id": "xjNVVn9Si3HT9w46y-2tNSDqP-BvogIS9gJZJgKG7XI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143zvuh", "is_robot_indexable": true, "report_reasons": null, "author": "avypath", "discussion_type": null, "num_comments": 160, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143zvuh/ragtag_archive_is_going_offline_138_pb_of_vtuber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143zvuh/ragtag_archive_is_going_offline_138_pb_of_vtuber/", "subreddit_subscribers": 686670, "created_utc": 1686200596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am an amateur programmer and I have been working on writing a downloader/content management system over the past few months for managing my own personal archive of NSFW content creators. The idea behind it is that with content creators branching out and advertising themselves on so many different websites, many times under different usernames, it becomes too hard for one to keep track of them based off of websites alone. Instead of tracking them via websites, you can track them in one centralized folder by storing their username(s) in a single file. The program is called `ripandtear` and uses a .rat file to keep track of the content creators names across different websites (don't worry, the .rat is just a .json file with a unique extension).\n\nWith the program you can create a folder and input all information for a user with one command (and a lot of flags). After that ripandtear can manage initially downloading all files, updating the user by downloading new previously undownloaded files, hashing the files to remove duplicates and sorting the files into content specific directories. \n\nHere is a quick example to make a folder, store usernames, download content, remove duplicates and sort files:\n\n    ripandtear -mk 'big-igloo' -r 'big-igloo' -R 'Big-Igloo' -o 'bigigloo' -t 'BiggyIgloo' -sa -H -S\n\n-mk - create a new directory with the given name and run the following flags from within it\n\n-r - adds Reddit usernames to the .rat file\n\n-R - adds Redgifs usernames to the .rat file\n\n-o - adds Onlyfans usernames to the .rat file\n\n-t - adds Twitter usernames to the .rat file\n\n-sa - have ripandtear automatically download and sync all content from supported sites (Reddit, Redgifs and Coomer.party ATM) and all saved urls to be downloaded later (as long as there is a supported extractor)\n\n-H - Hash and remove duplicate files in the current directory\n\n-S - sort the files into content specific folders (pics, vids, audio, text)\n\nIt is written in Python and I use pypi to manage and distribue ripandtear so it is just a `pip` away [if you are interested](https://pypi.org/project/ripandtear/). There is a much more intensive guide not only on pypi, but [the gitlab page](https://gitlab.com/johnny_barracuda/ripandtear) for the project if you want to take a look at the guide and the code. Again I am an amateur programmer and this is my first \"big\" project so please don't roast me too hard. Oh, I also use and developed ripandtear on Ubuntu so if you are a Windows user I don't know how many bugs you might come across. Let me know and I will try to help you out.  \n\nI mainly download a lot of content from Reddit and with the upcoming changes to the API and ban on NSFW links through the API, I thought I would share this project just in case someone else might find it useful.\n\nEdit - Forgot that I wanted to include what the .rat would look like for the example command I ran above\n\n    {\n      \"names\": {\n        \"reddit\": [\n          \"big-igloo\"\n        ],\n        \"redgifs\": [\n          \"Big-Igloo\"\n        ],\n        \"onlyfans\": [\n          \"bigigloo\"\n        ],\n        \"fansly\": [],\n        \"pornhub\": [],\n        \"twitter\": [\n          \"BiggyIgloo\"\n        ],\n        \"instagram\": [],\n        \"tiktits\": [],\n        \"youtube\": [],\n        \"tiktok\": [],\n        \"twitch\": [],\n        \"patreon\": [],\n        \"tumblr\": [],\n        \"myfreecams\": [],\n        \"chaturbate\": [],\n        \"generic\": []\n      },\n      \"links\": {\n        \"coomer\": [],\n        \"manyvids\": [],\n        \"simpcity\": []\n      },\n      \"urls_to_download\": [],\n      \"tags\": [],\n      \"urls_downloaded\": [],\n      \"file_hashes\": {},\n      \"error_dictionaries\": []\n    }", "author_fullname": "t2_4r2gulv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripandtear - A Reddit NSFW Downloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144hpgv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 119, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 119, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": 1686255796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686250585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an amateur programmer and I have been working on writing a downloader/content management system over the past few months for managing my own personal archive of NSFW content creators. The idea behind it is that with content creators branching out and advertising themselves on so many different websites, many times under different usernames, it becomes too hard for one to keep track of them based off of websites alone. Instead of tracking them via websites, you can track them in one centralized folder by storing their username(s) in a single file. The program is called &lt;code&gt;ripandtear&lt;/code&gt; and uses a .rat file to keep track of the content creators names across different websites (don&amp;#39;t worry, the .rat is just a .json file with a unique extension).&lt;/p&gt;\n\n&lt;p&gt;With the program you can create a folder and input all information for a user with one command (and a lot of flags). After that ripandtear can manage initially downloading all files, updating the user by downloading new previously undownloaded files, hashing the files to remove duplicates and sorting the files into content specific directories. &lt;/p&gt;\n\n&lt;p&gt;Here is a quick example to make a folder, store usernames, download content, remove duplicates and sort files:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ripandtear -mk &amp;#39;big-igloo&amp;#39; -r &amp;#39;big-igloo&amp;#39; -R &amp;#39;Big-Igloo&amp;#39; -o &amp;#39;bigigloo&amp;#39; -t &amp;#39;BiggyIgloo&amp;#39; -sa -H -S\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;-mk - create a new directory with the given name and run the following flags from within it&lt;/p&gt;\n\n&lt;p&gt;-r - adds Reddit usernames to the .rat file&lt;/p&gt;\n\n&lt;p&gt;-R - adds Redgifs usernames to the .rat file&lt;/p&gt;\n\n&lt;p&gt;-o - adds Onlyfans usernames to the .rat file&lt;/p&gt;\n\n&lt;p&gt;-t - adds Twitter usernames to the .rat file&lt;/p&gt;\n\n&lt;p&gt;-sa - have ripandtear automatically download and sync all content from supported sites (Reddit, Redgifs and Coomer.party ATM) and all saved urls to be downloaded later (as long as there is a supported extractor)&lt;/p&gt;\n\n&lt;p&gt;-H - Hash and remove duplicate files in the current directory&lt;/p&gt;\n\n&lt;p&gt;-S - sort the files into content specific folders (pics, vids, audio, text)&lt;/p&gt;\n\n&lt;p&gt;It is written in Python and I use pypi to manage and distribue ripandtear so it is just a &lt;code&gt;pip&lt;/code&gt; away &lt;a href=\"https://pypi.org/project/ripandtear/\"&gt;if you are interested&lt;/a&gt;. There is a much more intensive guide not only on pypi, but &lt;a href=\"https://gitlab.com/johnny_barracuda/ripandtear\"&gt;the gitlab page&lt;/a&gt; for the project if you want to take a look at the guide and the code. Again I am an amateur programmer and this is my first &amp;quot;big&amp;quot; project so please don&amp;#39;t roast me too hard. Oh, I also use and developed ripandtear on Ubuntu so if you are a Windows user I don&amp;#39;t know how many bugs you might come across. Let me know and I will try to help you out.  &lt;/p&gt;\n\n&lt;p&gt;I mainly download a lot of content from Reddit and with the upcoming changes to the API and ban on NSFW links through the API, I thought I would share this project just in case someone else might find it useful.&lt;/p&gt;\n\n&lt;p&gt;Edit - Forgot that I wanted to include what the .rat would look like for the example command I ran above&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;names&amp;quot;: {\n    &amp;quot;reddit&amp;quot;: [\n      &amp;quot;big-igloo&amp;quot;\n    ],\n    &amp;quot;redgifs&amp;quot;: [\n      &amp;quot;Big-Igloo&amp;quot;\n    ],\n    &amp;quot;onlyfans&amp;quot;: [\n      &amp;quot;bigigloo&amp;quot;\n    ],\n    &amp;quot;fansly&amp;quot;: [],\n    &amp;quot;pornhub&amp;quot;: [],\n    &amp;quot;twitter&amp;quot;: [\n      &amp;quot;BiggyIgloo&amp;quot;\n    ],\n    &amp;quot;instagram&amp;quot;: [],\n    &amp;quot;tiktits&amp;quot;: [],\n    &amp;quot;youtube&amp;quot;: [],\n    &amp;quot;tiktok&amp;quot;: [],\n    &amp;quot;twitch&amp;quot;: [],\n    &amp;quot;patreon&amp;quot;: [],\n    &amp;quot;tumblr&amp;quot;: [],\n    &amp;quot;myfreecams&amp;quot;: [],\n    &amp;quot;chaturbate&amp;quot;: [],\n    &amp;quot;generic&amp;quot;: []\n  },\n  &amp;quot;links&amp;quot;: {\n    &amp;quot;coomer&amp;quot;: [],\n    &amp;quot;manyvids&amp;quot;: [],\n    &amp;quot;simpcity&amp;quot;: []\n  },\n  &amp;quot;urls_to_download&amp;quot;: [],\n  &amp;quot;tags&amp;quot;: [],\n  &amp;quot;urls_downloaded&amp;quot;: [],\n  &amp;quot;file_hashes&amp;quot;: {},\n  &amp;quot;error_dictionaries&amp;quot;: []\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?auto=webp&amp;v=enabled&amp;s=f0cc8dce4c4d114433073f7ec64bf299623fcef9", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b8865fd719f17e774b2178948603d0c4bfb2673", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7f78455787f3622b85aa8394a3ee4b6f14e35c1", "width": 216, "height": 216}], "variants": {"obfuscated": {"source": {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=44daddb82daea84f7406a12ffa0b5a88a3217b29", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=9bc9bc5e0f224b021521ed4316023a8901f42b31", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0a9af77886496b90725fefc8589818e2341729c2", "width": 216, "height": 216}]}, "nsfw": {"source": {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=44daddb82daea84f7406a12ffa0b5a88a3217b29", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=9bc9bc5e0f224b021521ed4316023a8901f42b31", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/0kryPnq0tdgIBps0GUhMZoZ9rxHjvu2Jd-BPJ8vovPA.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0a9af77886496b90725fefc8589818e2341729c2", "width": 216, "height": 216}]}}, "id": "IUHM4ctLZQorzkPuYJ4IkGSag8BtaIqZoyqL1L53KuM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144hpgv", "is_robot_indexable": true, "report_reasons": null, "author": "big-igloo", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144hpgv/ripandtear_a_reddit_nsfw_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144hpgv/ripandtear_a_reddit_nsfw_downloader/", "subreddit_subscribers": 686670, "created_utc": 1686250585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8gagi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Reveals HAMR HDD Roadmap: 32TB First, 40TB Follows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_144bzwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5NwS5LWEc5DZovhPMX0Y_cm0yoyue-CcAQ8ir8jnL4Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686237377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tomshardware.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tomshardware.com/news/seagate-reveals-hamr-roadmap-32-tb-comes-first", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?auto=webp&amp;v=enabled&amp;s=c5102b8e1443e127b8716303781b9dbf2a76e617", "width": 970, "height": 545}, "resolutions": [{"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c80c107734a2fdcd19d264b6e270ee237e6dcd0", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6643066a44079afb3e75a19dcb8aa14c5798bb", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c8db52fe100928120d215d22f0a08c2ad76d641", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2f0972a0be6f367b3dd6a91fb8e8cd17e1c67c2", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/EnoSMyLdV0ctMvX--GkBLtp1LMU_XLqAbFoata918MI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a697a1fa85d558dbec8265b573c9b4cfb326b421", "width": 960, "height": 539}], "variants": {}, "id": "kRg5RyowODqqOJ7ePMaiyS-M5KEDFFcqtEoWi3ur4iI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144bzwt", "is_robot_indexable": true, "report_reasons": null, "author": "ben7337", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144bzwt/seagate_reveals_hamr_hdd_roadmap_32tb_first_40tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tomshardware.com/news/seagate-reveals-hamr-roadmap-32-tb-comes-first", "subreddit_subscribers": 686670, "created_utc": 1686237377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 8tb drive that is probably 5+ years old that appears to be corrupt. I have tons of files on it and this morning when I open a folder and click on a video file, I get a \"file missing\" message. However I can see the file and click on its properties as well. Another odd thing, I some files simply vanished, I clicked on a certain movie and got that error message, when I search for the file again its gone, as if i deleted it. My best guess is that this HD just crapped out. Luckily I have a back up, my questions are, is there a way to fix this damaged HD? can formatting it fix it? is is it time to buy a  new one? or is there another option ?\n\n&amp;#x200B;\n\nupdate: Crystal Disk info [https://ibb.co/VH9W7q4](https://ibb.co/VH9W7q4)", "author_fullname": "t2_8x1jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8tb hard drive just died on me.....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1442krz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686224586.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686209812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 8tb drive that is probably 5+ years old that appears to be corrupt. I have tons of files on it and this morning when I open a folder and click on a video file, I get a &amp;quot;file missing&amp;quot; message. However I can see the file and click on its properties as well. Another odd thing, I some files simply vanished, I clicked on a certain movie and got that error message, when I search for the file again its gone, as if i deleted it. My best guess is that this HD just crapped out. Luckily I have a back up, my questions are, is there a way to fix this damaged HD? can formatting it fix it? is is it time to buy a  new one? or is there another option ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;update: Crystal Disk info &lt;a href=\"https://ibb.co/VH9W7q4\"&gt;https://ibb.co/VH9W7q4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?auto=webp&amp;v=enabled&amp;s=2f8bdfe5473e821beafe8890b10931458ccb2f8b", "width": 1017, "height": 982}, "resolutions": [{"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d0e929568bcf474623d885d0cc9ad342bea504b", "width": 108, "height": 104}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f097589251a1479fae1088ee8022b226747c85b", "width": 216, "height": 208}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3516d4e879b225c078f593e080428101b4ea3788", "width": 320, "height": 308}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8a83255272b42b868c62bfc79990044f13088a6", "width": 640, "height": 617}, {"url": "https://external-preview.redd.it/lS047YPqvwi81Rd7g0oZgBmqQO5Iu9HYJZEpBkJj11Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=885d0a36484824a76843295711e9864cbc486c34", "width": 960, "height": 926}], "variants": {}, "id": "eVDXWjO_hVosDSjRMsDHbyUi2L7SWPUtnocTGivdmD8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1442krz", "is_robot_indexable": true, "report_reasons": null, "author": "FackJooBish", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1442krz/8tb_hard_drive_just_died_on_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1442krz/8tb_hard_drive_just_died_on_me/", "subreddit_subscribers": 686670, "created_utc": 1686209812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nWhat solution are you guys using to host/browse downloaded reddit content?\n\nI am working with the ZST files downloaded from Pushshift and sorted into subreddits by the lovely u/watchful1 [here](https://academictorrents.com/details/c398a571976c78d346c325bd75c47b82edf6124e). ZST is too compressed to browse on its own but using scripts like [this one](https://github.com/Watchful1/PushshiftDumps) you can process them into readable NDJSON files. From there im not sure what to do. I would like to have a self hosted reddit-clone that i can import these dumps into and browse freely.\n\nI'm thinking i will have to get a project like [redarc](https://github.com/Yakabuff/redarc) or [BDFR-to-HTML](https://github.com/BlipRanger/bdfr-html) or much more likely [Pushshift-Importer](https://github.com/Paul-E/Pushshift-Importer) which allows you to import pushshift downloads into a SQLite database. From there i would have to hook up the database to a reddit-like frontend.\n\nIs there a solution for this already?", "author_fullname": "t2_gn46ie9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you using to browse/self host downloaded reddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143vpsm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686188612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;What solution are you guys using to host/browse downloaded reddit content?&lt;/p&gt;\n\n&lt;p&gt;I am working with the ZST files downloaded from Pushshift and sorted into subreddits by the lovely &lt;a href=\"/u/watchful1\"&gt;u/watchful1&lt;/a&gt; &lt;a href=\"https://academictorrents.com/details/c398a571976c78d346c325bd75c47b82edf6124e\"&gt;here&lt;/a&gt;. ZST is too compressed to browse on its own but using scripts like &lt;a href=\"https://github.com/Watchful1/PushshiftDumps\"&gt;this one&lt;/a&gt; you can process them into readable NDJSON files. From there im not sure what to do. I would like to have a self hosted reddit-clone that i can import these dumps into and browse freely.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking i will have to get a project like &lt;a href=\"https://github.com/Yakabuff/redarc\"&gt;redarc&lt;/a&gt; or &lt;a href=\"https://github.com/BlipRanger/bdfr-html\"&gt;BDFR-to-HTML&lt;/a&gt; or much more likely &lt;a href=\"https://github.com/Paul-E/Pushshift-Importer\"&gt;Pushshift-Importer&lt;/a&gt; which allows you to import pushshift downloads into a SQLite database. From there i would have to hook up the database to a reddit-like frontend.&lt;/p&gt;\n\n&lt;p&gt;Is there a solution for this already?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?auto=webp&amp;v=enabled&amp;s=918b2eeb77d118fd8f4ba293fa5244eb3286a9c4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff569e6656835558df2eb771cd6c91a6cf219c17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b26d33631721450780f7ec8348b184ec52bb60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6563332be0778922ae7d781b4de71ae36c5970", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0a85df145bd3584118bc59eecc1952652f8e1e5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bb05ca3c8192d78215c635f3bff8a98f332659c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=427bc2290a0c6c1b2004e5dadd7d6573f8b74411", "width": 1080, "height": 567}], "variants": {}, "id": "9ZQzIXgHwWUMmDwxygJ-VDFa1eAMjAGg-cgqxWhg4Js"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143vpsm", "is_robot_indexable": true, "report_reasons": null, "author": "pm_me_xenomorphs", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143vpsm/what_are_you_using_to_browseself_host_downloaded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143vpsm/what_are_you_using_to_browseself_host_downloaded/", "subreddit_subscribers": 686670, "created_utc": 1686188612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Article in German](https://www.golem.de/news/update-fuer-google-maps-google-street-view-kehrt-nach-deutschland-zurueck-2306-174797.html)  \n\n\nApparently there is no great way to do a backup, as Google doesn't let you download the data directly. I sadly also don't have enough space for what is probably a *huge* amount of data.\n\n&amp;#x200B;\n\nHas anybody tried doing a backup of at least some of it?", "author_fullname": "t2_fj7d7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Street View returns to Germany, but old images will be removed. Did any of you do a backup yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144b1w3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686235139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.golem.de/news/update-fuer-google-maps-google-street-view-kehrt-nach-deutschland-zurueck-2306-174797.html\"&gt;Article in German&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Apparently there is no great way to do a backup, as Google doesn&amp;#39;t let you download the data directly. I sadly also don&amp;#39;t have enough space for what is probably a &lt;em&gt;huge&lt;/em&gt; amount of data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anybody tried doing a backup of at least some of it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?auto=webp&amp;v=enabled&amp;s=38650721293b1f165f76e8e4c6e5a1c99e9f36a1", "width": 832, "height": 468}, "resolutions": [{"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d55d9f8d805086ba7f731e0e73e9fdc9e76279e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc7fb37a0d84ceeb854d015b82751ff9e74dab78", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ac7f133e3d2f1581f65135189df556bbb96870", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/YHCvBs9ib-4Ct8c9ZefZMEA3Ra4mhfM2oUqrPp6buqU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6feb6fd2d1f84b6f271ec74f6c243c602131a0f0", "width": 640, "height": 360}], "variants": {}, "id": "jo6E3kwJl2GuMymce5-i70F7B8NealpDuxMNmr6sJK8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "144b1w3", "is_robot_indexable": true, "report_reasons": null, "author": "Keteo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144b1w3/google_street_view_returns_to_germany_but_old/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144b1w3/google_street_view_returns_to_germany_but_old/", "subreddit_subscribers": 686670, "created_utc": 1686235139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "completed rar-bg magnet backup (2.9mil certain)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144i16o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2w2u2132", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Piracy", "selftext": "https://github.com/2004content/rarbg\n\nJust making a new post to let people know that I finished my project below. They are not kidding about this new 'rarbg' spam filter. It's probably necessary.\n\nhttps://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/", "author_fullname": "t2_2w2u2132", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "completed rar-bg magnet backup (2.9mil certain)", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/Piracy", "hidden": false, "pwls": null, "link_flair_css_class": "News", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144i01l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "", "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "", "author_flair_richtext": [{"e": "text", "t": " "}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686251232.0, "link_flair_type": "richtext", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.Piracy", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/2004content/rarbg\"&gt;https://github.com/2004content/rarbg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just making a new post to let people know that I finished my project below. They are not kidding about this new &amp;#39;rarbg&amp;#39; spam filter. It&amp;#39;s probably necessary.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/\"&gt;https://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?auto=webp&amp;v=enabled&amp;s=8f05ef089d585c63a81ee138c16bde6ce1e7c2c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae450b8b9eee60540787a240417b966df334f504", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1312b0d7619a20abac09301d75b6774089b33e61", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c54391c2f5ff9119fd84f378039d78506529988d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3593b448fa5113c9a7f6d4665a208283d5907f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c8a01b9513324438dd518db42b6062438824989", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fabf118cc4f46b8266336be6ada6357939c62a84", "width": 1080, "height": 540}], "variants": {}, "id": "qHRd3QT6np-ya_pKWqGI4-flsOYF14Xul_VdgQulGv4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "593e89be-6cbb-11e7-8746-0e06f5cc7a06", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": " ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qmox", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d26913", "id": "144i01l", "is_robot_indexable": true, "report_reasons": null, "author": "trilionaire07", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/Piracy/comments/144i01l/completed_rarbg_magnet_backup_29mil_certain/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/Piracy/comments/144i01l/completed_rarbg_magnet_backup_29mil_certain/", "subreddit_subscribers": 1182547, "created_utc": 1686251232.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686251307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Piracy", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Piracy/comments/144i01l/completed_rarbg_magnet_backup_29mil_certain/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?auto=webp&amp;v=enabled&amp;s=8f05ef089d585c63a81ee138c16bde6ce1e7c2c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae450b8b9eee60540787a240417b966df334f504", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1312b0d7619a20abac09301d75b6774089b33e61", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c54391c2f5ff9119fd84f378039d78506529988d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3593b448fa5113c9a7f6d4665a208283d5907f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c8a01b9513324438dd518db42b6062438824989", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9Chtjv0Fm243pytiPvbv5ZjEz4wZUct0RqmeePjpM3g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fabf118cc4f46b8266336be6ada6357939c62a84", "width": 1080, "height": 540}], "variants": {}, "id": "qHRd3QT6np-ya_pKWqGI4-flsOYF14Xul_VdgQulGv4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144i16o", "is_robot_indexable": true, "report_reasons": null, "author": "trilionaire07", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_144i01l", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144i16o/completed_rarbg_magnet_backup_29mil_certain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Piracy/comments/144i01l/completed_rarbg_magnet_backup_29mil_certain/", "subreddit_subscribers": 686670, "created_utc": 1686251307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I started archiving my favorite shows a few months ago. Then I started recording live TV from a couple networks. *Now* Im curious if there's a list that well, *lists*, every company that produces television shows in America (for now, I'd love to do other countries later).\n\nI'm obviously not trying to archive every TV company's content. That would be insane. But having a spreadsheet with some basic info on them would be nice!\n\nI found [this](https://en.wikipedia.org/wiki/List_of_television_production_companies#United_States) page but was wondering if there's something more authoritative than Wikipedia. Anyone ever try to put something together like this?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a comprehensive list of every company that produces TV shows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144a6pm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686233034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started archiving my favorite shows a few months ago. Then I started recording live TV from a couple networks. &lt;em&gt;Now&lt;/em&gt; Im curious if there&amp;#39;s a list that well, &lt;em&gt;lists&lt;/em&gt;, every company that produces television shows in America (for now, I&amp;#39;d love to do other countries later).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m obviously not trying to archive every TV company&amp;#39;s content. That would be insane. But having a spreadsheet with some basic info on them would be nice!&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://en.wikipedia.org/wiki/List_of_television_production_companies#United_States\"&gt;this&lt;/a&gt; page but was wondering if there&amp;#39;s something more authoritative than Wikipedia. Anyone ever try to put something together like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144a6pm", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144a6pm/is_there_a_comprehensive_list_of_every_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144a6pm/is_there_a_comprehensive_list_of_every_company/", "subreddit_subscribers": 686670, "created_utc": 1686233034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My children's school regularly posts photos and videos of school life to their twitter account. I have been using gallery-dl to archive these images. I tend to donwload a month at a time, but recently it has stopped working, can anyone please help me?  \n\n\nHere is the command I usually issue (using google instead of real account):\n\n    PS C:\\Users\\me\\Downloads&gt; .\\gallery-dl --dest gallery-dl/twitter/2023-04 \"https://twitter.com/search?q=(from%3Agoogle)%20until%3A2023-05-01%20since%3A2023-04-01\"\n    [twitter][info] Requesting guest token\n    [twitter][error] 403 Forbidden (Forbidden.)\n\nAnd here is one without the date constraints:\n\n    PS C:\\Users\\me\\Downloads\\&gt; .\\gallery-dl --dest gallery-dl/twitter \"https://twitter.com/search?q=from:google\"\n    [twitter][info] Requesting guest token\n    [twitter][error] 403 Forbidden (Forbidden.)\n\nNeither command works anymore.\n\nHowever, if I run it without using the search, it still works, but then I can't filter a month or year at a time.\n\n    PS C:\\Users\\me\\Downloads&gt; .\\gallery-dl --dest gallery-dl/twitter \"http://twitter.com/google\"\n    * gallery-dl\\twitter\\2023-04\\twitter\\Google\\1666183917177536512_1.mp4\n\nIs this something to do with recent changes to the Twitter API?  \n\n\nThank you", "author_fullname": "t2_stw7a9lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gallery-dl Twitter no longer working", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1447t5w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686226996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My children&amp;#39;s school regularly posts photos and videos of school life to their twitter account. I have been using gallery-dl to archive these images. I tend to donwload a month at a time, but recently it has stopped working, can anyone please help me?  &lt;/p&gt;\n\n&lt;p&gt;Here is the command I usually issue (using google instead of real account):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PS C:\\Users\\me\\Downloads&amp;gt; .\\gallery-dl --dest gallery-dl/twitter/2023-04 &amp;quot;https://twitter.com/search?q=(from%3Agoogle)%20until%3A2023-05-01%20since%3A2023-04-01&amp;quot;\n[twitter][info] Requesting guest token\n[twitter][error] 403 Forbidden (Forbidden.)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And here is one without the date constraints:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PS C:\\Users\\me\\Downloads\\&amp;gt; .\\gallery-dl --dest gallery-dl/twitter &amp;quot;https://twitter.com/search?q=from:google&amp;quot;\n[twitter][info] Requesting guest token\n[twitter][error] 403 Forbidden (Forbidden.)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Neither command works anymore.&lt;/p&gt;\n\n&lt;p&gt;However, if I run it without using the search, it still works, but then I can&amp;#39;t filter a month or year at a time.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PS C:\\Users\\me\\Downloads&amp;gt; .\\gallery-dl --dest gallery-dl/twitter &amp;quot;http://twitter.com/google&amp;quot;\n* gallery-dl\\twitter\\2023-04\\twitter\\Google\\1666183917177536512_1.mp4\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is this something to do with recent changes to the Twitter API?  &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1447t5w", "is_robot_indexable": true, "report_reasons": null, "author": "InfamousLibrarian518", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1447t5w/gallerydl_twitter_no_longer_working/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1447t5w/gallerydl_twitter_no_longer_working/", "subreddit_subscribers": 686670, "created_utc": 1686226996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows (11/10) incremental backup solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143tz5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_a65nr", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "WindowsHelp", "selftext": "What backup solution do you use for windows and why? If it\u2019s still sold, how much does a lifetime license cost and does it get free upgrades?\n\nGot fed up with Windows Backup and looking for a replacement that doesn\u2019t cost too much.", "author_fullname": "t2_a65nr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows (11/10) incremental backup solution", "link_flair_richtext": [{"e": "text", "t": "Windows 11"}], "subreddit_name_prefixed": "r/WindowsHelp", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143m6cj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Windows 11", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686165481.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WindowsHelp", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What backup solution do you use for windows and why? If it\u2019s still sold, how much does a lifetime license cost and does it get free upgrades?&lt;/p&gt;\n\n&lt;p&gt;Got fed up with Windows Backup and looking for a replacement that doesn\u2019t cost too much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1a63a5a8-d28d-11eb-b580-0e2e73aaec5d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_38hjl", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00adef", "id": "143m6cj", "is_robot_indexable": true, "report_reasons": null, "author": "Gymnastboatman", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "subreddit_subscribers": 18321, "created_utc": 1686165481.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686183974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WindowsHelp", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "20TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143tz5f", "is_robot_indexable": true, "report_reasons": null, "author": "Gymnastboatman", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_143m6cj", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/143tz5f/windows_1110_incremental_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/WindowsHelp/comments/143m6cj/windows_1110_incremental_backup_solution/", "subreddit_subscribers": 686670, "created_utc": 1686183974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all!  \nI've got a Supermicro X9SRE board that has only SATA ports.  \nSince I'd like to buy 6 of [these](https://www.piospartslap.de/HGST-6TB-72K-SAS-12Gbps-HDD-HUS726060AL5214-0F22881) 6TB SAS drives, I need a SAS controller with at least 6 ports to use them in a JBOD configuration (with SnapRAID) for backup purposes.  \nSearching  in this sub, I've read about SAS expander, can they be used with any controller? How do they work?\n\nThanks!", "author_fullname": "t2_zpww0n1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap SAS controller with at leats 6 ports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449m5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;br/&gt;\nI&amp;#39;ve got a Supermicro X9SRE board that has only SATA ports.&lt;br/&gt;\nSince I&amp;#39;d like to buy 6 of &lt;a href=\"https://www.piospartslap.de/HGST-6TB-72K-SAS-12Gbps-HDD-HUS726060AL5214-0F22881\"&gt;these&lt;/a&gt; 6TB SAS drives, I need a SAS controller with at least 6 ports to use them in a JBOD configuration (with SnapRAID) for backup purposes.&lt;br/&gt;\nSearching  in this sub, I&amp;#39;ve read about SAS expander, can they be used with any controller? How do they work?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?auto=webp&amp;v=enabled&amp;s=709e796f2397cab4f78aff183fbae8c7c192c9a4", "width": 580, "height": 580}, "resolutions": [{"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f74c0d220a8d6a693a73a7a72396123d7e1c37b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ec7efc1bb076f2eabd1856ade65a2847d31dff8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/fQC19HntfUbwrw9YPrQeeQaVekDV2mrR8SPNVG_-tMM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=029d698b7d5812c15c2a392243d46391b32ea0bc", "width": 320, "height": 320}], "variants": {}, "id": "5Dv7fhnNHVhNylPTdNyuGPYuV40oQMIiakKd7RH2P1k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1449m5a", "is_robot_indexable": true, "report_reasons": null, "author": "andreape_x", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1449m5a/cheap_sas_controller_with_at_leats_6_ports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1449m5a/cheap_sas_controller_with_at_leats_6_ports/", "subreddit_subscribers": 686670, "created_utc": 1686231629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "has anyone done a warranty claim with serverpartdeals?\n\ni want to buy a few WD 14TB drives.  the manufacture refurbs are out of stock; the seller refurbs are in stock.  both have a 2 year warranty.\n\nif the WD manufacturer refurb drive goes bad within 2 years, i assume i don't go thru WD but instead thru the seller -- serverpartdeals  -- for warranty claim, right?\n\nif so what would be the diff, from a warranty perspective, btw the 2 refurb types?\n\nany help appreciated.", "author_fullname": "t2_5vej5jn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "warranty on serverpartdeals' manufacturer vs seller refurbished drives ...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143qrus", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686176016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;has anyone done a warranty claim with serverpartdeals?&lt;/p&gt;\n\n&lt;p&gt;i want to buy a few WD 14TB drives.  the manufacture refurbs are out of stock; the seller refurbs are in stock.  both have a 2 year warranty.&lt;/p&gt;\n\n&lt;p&gt;if the WD manufacturer refurb drive goes bad within 2 years, i assume i don&amp;#39;t go thru WD but instead thru the seller -- serverpartdeals  -- for warranty claim, right?&lt;/p&gt;\n\n&lt;p&gt;if so what would be the diff, from a warranty perspective, btw the 2 refurb types?&lt;/p&gt;\n\n&lt;p&gt;any help appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143qrus", "is_robot_indexable": true, "report_reasons": null, "author": "redditmail9999", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143qrus/warranty_on_serverpartdeals_manufacturer_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143qrus/warranty_on_serverpartdeals_manufacturer_vs/", "subreddit_subscribers": 686670, "created_utc": 1686176016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there; I just purchased a Scansnap SV600 scanner after reading posts here, as it seemed the best of the limited options for my use case based on the comments.  Unfortunately, I think I've gotten a defective unit, and the spotty support provided for the machine doesn't seem to list the issue I'm having.  Since this sub seems the most familiar with the device given its applications, I'm wondering if anyone's encountered this issue. \n\nWhen I follow the directions re: downloading &amp; installing the software, plugging in the device's power supply and USB, and hitting the Scan button to turn it on, I hear some scanner-esque whirring for a couple seconds within the head, then a brief \"chuck-chuck-chuck\" sound like a mechanism's stuck on something, and the light on the base turns from blue to orange and starts flashing.   (I've checked the unit, and I don't think there are any inserts I've failed to remove on which mechanisms could get stuck.) Hitting the Scan button again will turn the light back from flashing orange to solid blue, but attempting to scan something via the software, in either blue-light or orange-flashing-light state, will result in two error messages: \"Failed to communicate with the camera,\" then, if I try to continue, \"the scanning unit might not have returned to its initial position.\"\n\nMy keen technical senses tell me that the *chuck-chuck-chuck* indicates something's gone wrong on the mechanical level and I need to initiate a return, but in the unlikely event anyone's encountered this issue and managed to recover from it, I'd appreciate hearing from you.\n\nThanks for any info.", "author_fullname": "t2_kzog6zzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my Scansnap SV600 busted? (Won't scan; flashing orange light)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_144jesr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686254494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there; I just purchased a Scansnap SV600 scanner after reading posts here, as it seemed the best of the limited options for my use case based on the comments.  Unfortunately, I think I&amp;#39;ve gotten a defective unit, and the spotty support provided for the machine doesn&amp;#39;t seem to list the issue I&amp;#39;m having.  Since this sub seems the most familiar with the device given its applications, I&amp;#39;m wondering if anyone&amp;#39;s encountered this issue. &lt;/p&gt;\n\n&lt;p&gt;When I follow the directions re: downloading &amp;amp; installing the software, plugging in the device&amp;#39;s power supply and USB, and hitting the Scan button to turn it on, I hear some scanner-esque whirring for a couple seconds within the head, then a brief &amp;quot;chuck-chuck-chuck&amp;quot; sound like a mechanism&amp;#39;s stuck on something, and the light on the base turns from blue to orange and starts flashing.   (I&amp;#39;ve checked the unit, and I don&amp;#39;t think there are any inserts I&amp;#39;ve failed to remove on which mechanisms could get stuck.) Hitting the Scan button again will turn the light back from flashing orange to solid blue, but attempting to scan something via the software, in either blue-light or orange-flashing-light state, will result in two error messages: &amp;quot;Failed to communicate with the camera,&amp;quot; then, if I try to continue, &amp;quot;the scanning unit might not have returned to its initial position.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;My keen technical senses tell me that the &lt;em&gt;chuck-chuck-chuck&lt;/em&gt; indicates something&amp;#39;s gone wrong on the mechanical level and I need to initiate a return, but in the unlikely event anyone&amp;#39;s encountered this issue and managed to recover from it, I&amp;#39;d appreciate hearing from you.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any info.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144jesr", "is_robot_indexable": true, "report_reasons": null, "author": "pomodorosapwm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144jesr/is_my_scansnap_sv600_busted_wont_scan_flashing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144jesr/is_my_scansnap_sv600_busted_wont_scan_flashing/", "subreddit_subscribers": 686670, "created_utc": 1686254494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Every Seagate Exos X18 drive I have in my collection is a  **ST18000NM000J-2TV103** with firmware **SN04**.\n\nToday I received another shipment of mystery MDD 18TB drives and these two report **ST18000NM014J-2WS103** which I can't find on any datasheets (or Google search).  I'm guessing I got a custom enterprise build somehow, but the two of them have different firmware versions (**CM03** and **CM04**) as well.  The Seagate website doesn't give me any details for either serial number.\n\nAnyone else ever come across this model drive?  It won't take any firmware from [HDDGuru.com](https://HDDGuru.com), so it's unique enough to require it's own special firmware.\n\nThese are both on my Chia farmer: [http://atomicinternet.homeip.net/crypto/?system=gigahorse](http://atomicinternet.homeip.net/crypto/?system=gigahorse)", "author_fullname": "t2_5pk1lo3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mystery Seagate X18 drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144gxb9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686248760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every Seagate Exos X18 drive I have in my collection is a  &lt;strong&gt;ST18000NM000J-2TV103&lt;/strong&gt; with firmware &lt;strong&gt;SN04&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Today I received another shipment of mystery MDD 18TB drives and these two report &lt;strong&gt;ST18000NM014J-2WS103&lt;/strong&gt; which I can&amp;#39;t find on any datasheets (or Google search).  I&amp;#39;m guessing I got a custom enterprise build somehow, but the two of them have different firmware versions (&lt;strong&gt;CM03&lt;/strong&gt; and &lt;strong&gt;CM04&lt;/strong&gt;) as well.  The Seagate website doesn&amp;#39;t give me any details for either serial number.&lt;/p&gt;\n\n&lt;p&gt;Anyone else ever come across this model drive?  It won&amp;#39;t take any firmware from &lt;a href=\"https://HDDGuru.com\"&gt;HDDGuru.com&lt;/a&gt;, so it&amp;#39;s unique enough to require it&amp;#39;s own special firmware.&lt;/p&gt;\n\n&lt;p&gt;These are both on my Chia farmer: &lt;a href=\"http://atomicinternet.homeip.net/crypto/?system=gigahorse\"&gt;http://atomicinternet.homeip.net/crypto/?system=gigahorse&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144gxb9", "is_robot_indexable": true, "report_reasons": null, "author": "SupportExtra", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144gxb9/mystery_seagate_x18_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144gxb9/mystery_seagate_x18_drive/", "subreddit_subscribers": 686670, "created_utc": 1686248760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! I don't know if you'll count this is data hoarding, but I definitely do cause I think I store more information about people than I really need to.   \n\n\nI use Google Contacts for storing all of my contacts, but it has some limitations that I really can't stand. When exporting contacts, the birthdays are stored as yyyy-mm-dd, and when importing them, it has to be in the same format. However, if you go to the contact and make an edit, you'll have to re-enter the birthday, since from the UI it only accepts mm/dd/yyyy, which is a huge hassle, considering I have over 2000 contacts, and regularly import lots of new ones.   \n\n\nI'd also like to be able to have a Company, Job Title, Department, Tags, Notes, and several custom field options, apart from the obvious. Being able to give them profile pictures in Base64 JPG formats would be a huge plus, too.   \n\n\nI've tried hosting my own CardDAV server, but my ISP doesn't allow port forwarding so I wasn't able to do that. What do you guys use for your contacts? Thanks!", "author_fullname": "t2_rzyitgj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic but complete cloud contacts solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144g7lz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686247088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! I don&amp;#39;t know if you&amp;#39;ll count this is data hoarding, but I definitely do cause I think I store more information about people than I really need to.   &lt;/p&gt;\n\n&lt;p&gt;I use Google Contacts for storing all of my contacts, but it has some limitations that I really can&amp;#39;t stand. When exporting contacts, the birthdays are stored as yyyy-mm-dd, and when importing them, it has to be in the same format. However, if you go to the contact and make an edit, you&amp;#39;ll have to re-enter the birthday, since from the UI it only accepts mm/dd/yyyy, which is a huge hassle, considering I have over 2000 contacts, and regularly import lots of new ones.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also like to be able to have a Company, Job Title, Department, Tags, Notes, and several custom field options, apart from the obvious. Being able to give them profile pictures in Base64 JPG formats would be a huge plus, too.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried hosting my own CardDAV server, but my ISP doesn&amp;#39;t allow port forwarding so I wasn&amp;#39;t able to do that. What do you guys use for your contacts? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144g7lz", "is_robot_indexable": true, "report_reasons": null, "author": "6marvil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144g7lz/basic_but_complete_cloud_contacts_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144g7lz/basic_but_complete_cloud_contacts_solution/", "subreddit_subscribers": 686670, "created_utc": 1686247088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! Sorry for my bad English in advance, *TLDR down below.*I'm on the lookout for a cloud storage system that  fits my needs without breaking the bank. I want something that is not just a backup solution, offers affordable pricing, and is kinda working in a normal cloud system way. Here are my main considerations:\n\n* I require a cloud storage service that offers unlimited storage capacity. I need to back up at least 12TB of data, and I also plan to back up my family' data in the future. Also I know when I say cloud and not a backup system, because after what I had searched on the web, kinda all backup services have two problem, one with super low speed and another with hide warnings about deletion of data.\n* Since I'm located in Europe, it's essential for the cloud storage service to provide **fast upload speeds.**\n* I'm searching for something inexpensive cloud storage, like not so expensive system that provides value for money.\n* Maybe a nice system to see your pictures, because most of my data are Raw/DNG photos\n* I don't have amazon in my country, so no Amazon Photos  unfortunately\n\nAfter some research, I came across **sync.com**, which seems promising. They offer an unlimited storage plan at a slightly lower price than Dropbox *($18 per month as of June 2023)*. However, during my tests, I found the upload speed to be quite slow, but I think is quite good, tooking in to the consideration that their servers are based in Canada.It took around **12 minutes to upload just 1GB** of data, which is a bit concerning, regarding that  I have quite a lot to upload to the cloud.  Also, the interface of sync.com is a bit not so advanced, with such a wide range of options, as Google Drive. I miss the ability to choose which folders to sync and control the synchronization settings. Or maybe I don't know how??\n\nAnother services that I have found is JottaCloud, being an interesting option, since they are based in Europe and offer good speeds. However, their speeds decrease significantly after reaching 5TB, which is not advantageous for my needs.\n\n# So, my questions are:\n\n1. Is sync.com the best option available, or are there other alternatives I should consider?\n2. Are there any third-party tools or programs that can enhance sync.com's functionality ?\n\n***I would greatly appreciate your insights and recommendations. Thank you in advance for your help!***\n\n# TL;DR:\n\nI'm searching for an inexpensive and cost-effective cloud storage system, not just a backup solution, with various sharing options and the ability to control folder access. I need unlimited storage for backing up at least 12TB of data. Since I'm based in Europe, fast speeds are essential. Amazon is not available for me. Currently, I'm considering sync.com, which offers unlimited storage at a slightly cheaper price than Dropbox. However, the upload speed seems slow, and the interface is a bit not so advanced like compared to Google Drive. Are there any better options, or are there third-party tools that can enhance sync.com's functionality?", "author_fullname": "t2_ewg4x2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an Affordable and Fast Cloud System (preferably with good speed in Europe)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144ca6g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686238497.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686238060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! Sorry for my bad English in advance, &lt;em&gt;TLDR down below.&lt;/em&gt;I&amp;#39;m on the lookout for a cloud storage system that  fits my needs without breaking the bank. I want something that is not just a backup solution, offers affordable pricing, and is kinda working in a normal cloud system way. Here are my main considerations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I require a cloud storage service that offers unlimited storage capacity. I need to back up at least 12TB of data, and I also plan to back up my family&amp;#39; data in the future. Also I know when I say cloud and not a backup system, because after what I had searched on the web, kinda all backup services have two problem, one with super low speed and another with hide warnings about deletion of data.&lt;/li&gt;\n&lt;li&gt;Since I&amp;#39;m located in Europe, it&amp;#39;s essential for the cloud storage service to provide &lt;strong&gt;fast upload speeds.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m searching for something inexpensive cloud storage, like not so expensive system that provides value for money.&lt;/li&gt;\n&lt;li&gt;Maybe a nice system to see your pictures, because most of my data are Raw/DNG photos&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t have amazon in my country, so no Amazon Photos  unfortunately&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After some research, I came across &lt;strong&gt;sync.com&lt;/strong&gt;, which seems promising. They offer an unlimited storage plan at a slightly lower price than Dropbox &lt;em&gt;($18 per month as of June 2023)&lt;/em&gt;. However, during my tests, I found the upload speed to be quite slow, but I think is quite good, tooking in to the consideration that their servers are based in Canada.It took around &lt;strong&gt;12 minutes to upload just 1GB&lt;/strong&gt; of data, which is a bit concerning, regarding that  I have quite a lot to upload to the cloud.  Also, the interface of sync.com is a bit not so advanced, with such a wide range of options, as Google Drive. I miss the ability to choose which folders to sync and control the synchronization settings. Or maybe I don&amp;#39;t know how??&lt;/p&gt;\n\n&lt;p&gt;Another services that I have found is JottaCloud, being an interesting option, since they are based in Europe and offer good speeds. However, their speeds decrease significantly after reaching 5TB, which is not advantageous for my needs.&lt;/p&gt;\n\n&lt;h1&gt;So, my questions are:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is sync.com the best option available, or are there other alternatives I should consider?&lt;/li&gt;\n&lt;li&gt;Are there any third-party tools or programs that can enhance sync.com&amp;#39;s functionality ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;I would greatly appreciate your insights and recommendations. Thank you in advance for your help!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;TL;DR:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;m searching for an inexpensive and cost-effective cloud storage system, not just a backup solution, with various sharing options and the ability to control folder access. I need unlimited storage for backing up at least 12TB of data. Since I&amp;#39;m based in Europe, fast speeds are essential. Amazon is not available for me. Currently, I&amp;#39;m considering sync.com, which offers unlimited storage at a slightly cheaper price than Dropbox. However, the upload speed seems slow, and the interface is a bit not so advanced like compared to Google Drive. Are there any better options, or are there third-party tools that can enhance sync.com&amp;#39;s functionality?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144ca6g", "is_robot_indexable": true, "report_reasons": null, "author": "stef7EAmis", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144ca6g/looking_for_an_affordable_and_fast_cloud_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144ca6g/looking_for_an_affordable_and_fast_cloud_system/", "subreddit_subscribers": 686670, "created_utc": 1686238060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got the new drive yesterday and it seems to be defective. Much noisier when loading tapes than LTO5 I'm trying to replace and ejects tapes after attempting a load. I also don't like the way the cartridge moves after initial insertion, LTO5 drive definitely doesn't do that. Here's a video - [https://www.youtube.com/watch?v=oiN2FCtYiSc](https://www.youtube.com/watch?v=oiN2FCtYiSc)\n\nI haven't used LTO9 drives before but assumed that it would operate the same way as LTO5.\n\nAm I missing something obvious or the drive is faulty?", "author_fullname": "t2_zw3xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO9 drive noisy and ejecting tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144c7z3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686237911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got the new drive yesterday and it seems to be defective. Much noisier when loading tapes than LTO5 I&amp;#39;m trying to replace and ejects tapes after attempting a load. I also don&amp;#39;t like the way the cartridge moves after initial insertion, LTO5 drive definitely doesn&amp;#39;t do that. Here&amp;#39;s a video - &lt;a href=\"https://www.youtube.com/watch?v=oiN2FCtYiSc\"&gt;https://www.youtube.com/watch?v=oiN2FCtYiSc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t used LTO9 drives before but assumed that it would operate the same way as LTO5.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something obvious or the drive is faulty?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?auto=webp&amp;v=enabled&amp;s=e331f7c7298866143ace7fd4f30affd36b8c26eb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebd9738cac6ef98fe92ede81a0f77b0fae3c0564", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca50bbe69b97e813bacdda6a25cc7ce78d1f1be8", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/t-9sr2kimCUQAkLTXcawPtuxzJ1VLK-Asf1Sn7ODMlo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=419a848260fa92c83dc15af1e1089488ba99d910", "width": 320, "height": 240}], "variants": {}, "id": "qyMJauVfeMVCwkIHQshyVua_e1Vjec6HpzOWMkYUIfw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144c7z3", "is_robot_indexable": true, "report_reasons": null, "author": "K7API", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144c7z3/lto9_drive_noisy_and_ejecting_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144c7z3/lto9_drive_noisy_and_ejecting_tapes/", "subreddit_subscribers": 686670, "created_utc": 1686237911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\n&amp;#x200B;\n\nI have in my possession a  HP QR490-63001 drive array and I am looking to wipe all drives using Blancco. I am aware that I need to connect this to another server through a fibre cable and some kind of card to help them connect to I can see the drives through Blancco. \n\nBasically I am looking for some kind of tutorial or tips to make this possible as normally where I work we mainly deal with Laptops and PC's this all new to me. Anyone with tips or a direction to help would be greatly apricated.", "author_fullname": "t2_wk6ta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wiping a Data array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1449pr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686231886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have in my possession a  HP QR490-63001 drive array and I am looking to wipe all drives using Blancco. I am aware that I need to connect this to another server through a fibre cable and some kind of card to help them connect to I can see the drives through Blancco. &lt;/p&gt;\n\n&lt;p&gt;Basically I am looking for some kind of tutorial or tips to make this possible as normally where I work we mainly deal with Laptops and PC&amp;#39;s this all new to me. Anyone with tips or a direction to help would be greatly apricated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1449pr8", "is_robot_indexable": true, "report_reasons": null, "author": "lyndon777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1449pr8/wiping_a_data_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1449pr8/wiping_a_data_array/", "subreddit_subscribers": 686670, "created_utc": 1686231886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had previously been using Grabber to back up artists who uploaded only to twitter, though it doesn't seeany posts containing nsfw content on most accounts anymore. Or even sfw content from some artists, for whatever reason.\n\nAre there any currently working tools to download all media uploaded by a given twitter account? I've looked into gallery-dl, though unless I'm missing something, it only seems to grab a handful of recent posts.", "author_fullname": "t2_7hojt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool that can backup all media posts from a Twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144foia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686245872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had previously been using Grabber to back up artists who uploaded only to twitter, though it doesn&amp;#39;t seeany posts containing nsfw content on most accounts anymore. Or even sfw content from some artists, for whatever reason.&lt;/p&gt;\n\n&lt;p&gt;Are there any currently working tools to download all media uploaded by a given twitter account? I&amp;#39;ve looked into gallery-dl, though unless I&amp;#39;m missing something, it only seems to grab a handful of recent posts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144foia", "is_robot_indexable": true, "report_reasons": null, "author": "Britefire", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144foia/is_there_a_tool_that_can_backup_all_media_posts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144foia/is_there_a_tool_that_can_backup_all_media_posts/", "subreddit_subscribers": 686670, "created_utc": 1686245872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hello, if i download a archiveteam file of reddit for example. will i still be able to reade posts/comments from reddit and other sites in that file?", "author_fullname": "t2_x3j2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "seeing actual posts comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144bppl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686236696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello, if i download a archiveteam file of reddit for example. will i still be able to reade posts/comments from reddit and other sites in that file?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144bppl", "is_robot_indexable": true, "report_reasons": null, "author": "worthplayingfor25", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144bppl/seeing_actual_posts_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144bppl/seeing_actual_posts_comments/", "subreddit_subscribers": 686670, "created_utc": 1686236696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1004q6ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this ok? Scrutiny shows passed but there are a lot of errors. New drive. Also how f*cked I am? I ordered 2 drives on eBay and only arrived this one. I heard some people when you buy more than 1 unit of something they send you only one and if you don't have a video opening. You don't get refund", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "name": "t3_144auxu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Cnyscc1vCXk6SSM1sPhK8jueEF0B7uGLyn1VOSyHOmM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686234659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3vw8grpd1t4b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?auto=webp&amp;v=enabled&amp;s=ae3a6fe8333da46fbc6e16555a63da0ee0d3fb07", "width": 1080, "height": 486}, "resolutions": [{"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b2f20e63ccbc0648af5ec15b846919cae092910", "width": 108, "height": 48}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b85811b13de2d2010a1dbd03c5b793e33cc32c0e", "width": 216, "height": 97}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=332e216869ba7ed263e1dff2d1844aa329cbe3e9", "width": 320, "height": 144}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7814a59c4365b8dc158a3956a4b48bb1bb99b17", "width": 640, "height": 288}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93408e06eacd950ec8bf92cd55fb2002f2f73cac", "width": 960, "height": 432}, {"url": "https://preview.redd.it/3vw8grpd1t4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f973ba23a0b10b541244d40b36d66e1087217bd0", "width": 1080, "height": 486}], "variants": {}, "id": "pgEwF2vNTq2ra6fthzqVsDLdOwHJ2kpLGfHElarm7mU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144auxu", "is_robot_indexable": true, "report_reasons": null, "author": "gerardit04", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144auxu/is_this_ok_scrutiny_shows_passed_but_there_are_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3vw8grpd1t4b1.png", "subreddit_subscribers": 686670, "created_utc": 1686234659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How do I archive a mediawiki site with wget? I'm currently running `wget -mkxpKE --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0\" -e robots=off --reject=\"*Special:*\" --reject=\"*User:*\" [URL]` but I always end up with thousands of requests that look like this\n\n```\n2023-06-07 22:53:08 (9.92 MB/s) - \u2018diyhrt.cafe/index.php?title=Special:CreateAccount&amp;returnto=Main+Page&amp;returntoquery=oldid=843.tmp.html\u2019 saved [90731] \n\nRemoving diyhrt.cafe/index.php?title=Special:CreateAccount&amp;returnto=Main+Page&amp;returntoquery=oldid=843.tmp.html since it should be rejected.\n```\n\nIs it possible to skip all of these pages instead of downloading, then removing them? Is there a different tool I should be using?", "author_fullname": "t2_66jgtbcp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mediawiki with wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143rvwc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686179937.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686178718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do I archive a mediawiki site with wget? I&amp;#39;m currently running &lt;code&gt;wget -mkxpKE --user-agent=&amp;quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0&amp;quot; -e robots=off --reject=&amp;quot;*Special:*&amp;quot; --reject=&amp;quot;*User:*&amp;quot; [URL]&lt;/code&gt; but I always end up with thousands of requests that look like this&lt;/p&gt;\n\n&lt;p&gt;```\n2023-06-07 22:53:08 (9.92 MB/s) - \u2018diyhrt.cafe/index.php?title=Special:CreateAccount&amp;amp;returnto=Main+Page&amp;amp;returntoquery=oldid=843.tmp.html\u2019 saved [90731] &lt;/p&gt;\n\n&lt;p&gt;Removing diyhrt.cafe/index.php?title=Special:CreateAccount&amp;amp;returnto=Main+Page&amp;amp;returntoquery=oldid=843.tmp.html since it should be rejected.\n```&lt;/p&gt;\n\n&lt;p&gt;Is it possible to skip all of these pages instead of downloading, then removing them? Is there a different tool I should be using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143rvwc", "is_robot_indexable": true, "report_reasons": null, "author": "uGoldfish", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143rvwc/mediawiki_with_wget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143rvwc/mediawiki_with_wget/", "subreddit_subscribers": 686670, "created_utc": 1686178718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was thinking of grabbing one of these with a 6500 Intel and installing  a sata card for all my drive in JBOD. I have 5 drives so far in asst sizes (3 are 2.5 and 2 are 3.5 drives)\n\nWill this motherboard w/16gb of ram run a  server for 2 people?\n\nWhat would the projected wattage draw be? Can i put all drives to sleep and only wake when requested? Better yet, can i put whole computer to sleep and wake when needed?", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP ProDesk 600 G3 SFF motherboard", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143r2iv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686176713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking of grabbing one of these with a 6500 Intel and installing  a sata card for all my drive in JBOD. I have 5 drives so far in asst sizes (3 are 2.5 and 2 are 3.5 drives)&lt;/p&gt;\n\n&lt;p&gt;Will this motherboard w/16gb of ram run a  server for 2 people?&lt;/p&gt;\n\n&lt;p&gt;What would the projected wattage draw be? Can i put all drives to sleep and only wake when requested? Better yet, can i put whole computer to sleep and wake when needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143r2iv", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143r2iv/hp_prodesk_600_g3_sff_motherboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143r2iv/hp_prodesk_600_g3_sff_motherboard/", "subreddit_subscribers": 686670, "created_utc": 1686176713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to collect and organize files of a niche hobby that are currently scattered over various forums, discords and FB groups.\nBecause I sure can't do it alone, it would be great if other people could send me those files _in an orderly fashion_, i.e. not random email attachments or massive dumps that _I_ have to sort through. I'd rather like something like Github pullrequests that I (or other co-collaborators) can accept or deny (eg if metadata is missing); this would also make it easier for people to keep track to avoid sending duplicates.\n\nI anticipate the binaries will quickly grow to dozens, if not hundreds of GB, so using git/github is probably out of question. (I know it is possible to handle giant git repos if you are Microsoft, but for everyone else the current state of git lfs + sparse checkout is atm just not usable enough)\n\nWhat are my options here? Anyone has some experience with this kind of thing?\n(I also wonder how those sharing \"Linux isos\" do that \u2013 is that always a single person sorting out duplicates?)", "author_fullname": "t2_1ru2s9bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tools to let others collaborate on my collection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144bkq0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686236395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to collect and organize files of a niche hobby that are currently scattered over various forums, discords and FB groups.\nBecause I sure can&amp;#39;t do it alone, it would be great if other people could send me those files &lt;em&gt;in an orderly fashion&lt;/em&gt;, i.e. not random email attachments or massive dumps that &lt;em&gt;I&lt;/em&gt; have to sort through. I&amp;#39;d rather like something like Github pullrequests that I (or other co-collaborators) can accept or deny (eg if metadata is missing); this would also make it easier for people to keep track to avoid sending duplicates.&lt;/p&gt;\n\n&lt;p&gt;I anticipate the binaries will quickly grow to dozens, if not hundreds of GB, so using git/github is probably out of question. (I know it is possible to handle giant git repos if you are Microsoft, but for everyone else the current state of git lfs + sparse checkout is atm just not usable enough)&lt;/p&gt;\n\n&lt;p&gt;What are my options here? Anyone has some experience with this kind of thing?\n(I also wonder how those sharing &amp;quot;Linux isos&amp;quot; do that \u2013 is that always a single person sorting out duplicates?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "144bkq0", "is_robot_indexable": true, "report_reasons": null, "author": "plg94", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/144bkq0/tools_to_let_others_collaborate_on_my_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/144bkq0/tools_to_let_others_collaborate_on_my_collection/", "subreddit_subscribers": 686670, "created_utc": 1686236395.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried using google colab using some scripts that I've found online and I managed to transferred 80GB of files from my google drive to my mega account. But after that, it doesn't worked anymore, the error seems happening on Mega's end, or not sure if there are limitations or restrictions in using that?\n\nThere's another option I've found using rclone, but the problem with this is very slow and it using my internet bandwidth to transfer data (download and upload) compare to the scripts using google colab which transfer is happening from server to server (80GB took only 15min more or less).  \n\n\nAre there any other options out there? There are also paid services available but I'm not keen on availing that.", "author_fullname": "t2_8c5s3s58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud transfer Google Drive to Mega", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1443cuq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686212690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using google colab using some scripts that I&amp;#39;ve found online and I managed to transferred 80GB of files from my google drive to my mega account. But after that, it doesn&amp;#39;t worked anymore, the error seems happening on Mega&amp;#39;s end, or not sure if there are limitations or restrictions in using that?&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s another option I&amp;#39;ve found using rclone, but the problem with this is very slow and it using my internet bandwidth to transfer data (download and upload) compare to the scripts using google colab which transfer is happening from server to server (80GB took only 15min more or less).  &lt;/p&gt;\n\n&lt;p&gt;Are there any other options out there? There are also paid services available but I&amp;#39;m not keen on availing that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1443cuq", "is_robot_indexable": true, "report_reasons": null, "author": "elryoma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1443cuq/cloud_transfer_google_drive_to_mega/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1443cuq/cloud_transfer_google_drive_to_mega/", "subreddit_subscribers": 686670, "created_utc": 1686212690.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}