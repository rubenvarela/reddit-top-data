{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Really excited for this opportunity! Of all the programs I got accepted to, this felt like the best one for me :)\n\nThis really feels good especially considering I flunked out of grad school the first time I tried going from 2015-2017.", "author_fullname": "t2_1bd1y4s0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accepted into UCSD\u2019s Masters of Data Science!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14e3bra", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687284176.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687243183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Really excited for this opportunity! Of all the programs I got accepted to, this felt like the best one for me :)&lt;/p&gt;\n\n&lt;p&gt;This really feels good especially considering I flunked out of grad school the first time I tried going from 2015-2017.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14e3bra", "is_robot_indexable": true, "report_reasons": null, "author": "Hour-Adeptness-5954", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14e3bra/accepted_into_ucsds_masters_of_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14e3bra/accepted_into_ucsds_masters_of_data_science/", "subreddit_subscribers": 928207, "created_utc": 1687243183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work as a beginner data scientist in a startup where I don't have any expert in the field to rely on.  \n\n\nAt the moment, I am working on project for a big customer and they are asking how each of their operational metrics affect one important metric, let's call it \"y\".  \n\n\nThey would like to get the individual contribution of each feature  in terms of % increase or decrease of \"y\". For instance, the best way would be to have a linear regression where each coefficient has a weight with it's corresponding variation YoY and when you multiply them together and sum the percentage contribution you get the variation of \"y\" YoY.  \n\n\nI have tried so many things:  \n\\- tried linear regression (fitted many models and chose the best using the R squared adjusted, but the model was predicting an increase of \"y\" instead of a decrease)  \n\\- tried to fit Random Forest and SVR then tried to use SHAP and LIME to interpret the predictions. LIME is not stable and doesn't give a coherent story, while SHAP doesn't give a convincing story. For instance, it says that one of the variable positively affects \"y\" while physically it doesn't make sense.  \n\\- I would like to take the causal route and try to understand how each variable are intertwined but I don't have the subject matter knowledge required to do it.\n\n  \nCan you please guide me or route me to some potential solutions? Thanks a lot", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q] - What is the best way to find the direction and contribution of each feature when doing a regression using Random Forest?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14eedsl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687275428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a beginner data scientist in a startup where I don&amp;#39;t have any expert in the field to rely on.  &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am working on project for a big customer and they are asking how each of their operational metrics affect one important metric, let&amp;#39;s call it &amp;quot;y&amp;quot;.  &lt;/p&gt;\n\n&lt;p&gt;They would like to get the individual contribution of each feature  in terms of % increase or decrease of &amp;quot;y&amp;quot;. For instance, the best way would be to have a linear regression where each coefficient has a weight with it&amp;#39;s corresponding variation YoY and when you multiply them together and sum the percentage contribution you get the variation of &amp;quot;y&amp;quot; YoY.  &lt;/p&gt;\n\n&lt;p&gt;I have tried so many things:&lt;br/&gt;\n- tried linear regression (fitted many models and chose the best using the R squared adjusted, but the model was predicting an increase of &amp;quot;y&amp;quot; instead of a decrease)&lt;br/&gt;\n- tried to fit Random Forest and SVR then tried to use SHAP and LIME to interpret the predictions. LIME is not stable and doesn&amp;#39;t give a coherent story, while SHAP doesn&amp;#39;t give a convincing story. For instance, it says that one of the variable positively affects &amp;quot;y&amp;quot; while physically it doesn&amp;#39;t make sense.&lt;br/&gt;\n- I would like to take the causal route and try to understand how each variable are intertwined but I don&amp;#39;t have the subject matter knowledge required to do it.&lt;/p&gt;\n\n&lt;p&gt;Can you please guide me or route me to some potential solutions? Thanks a lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14eedsl", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14eedsl/q_what_is_the_best_way_to_find_the_direction_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14eedsl/q_what_is_the_best_way_to_find_the_direction_and/", "subreddit_subscribers": 928207, "created_utc": 1687275428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I don't know if it's Imposter's Syndrome or that I am really bad at remembering things, but every time I do something I have to check my notes/yt videos to make sure I'm doing things the right way. Is this normal? Or should I start studying more on my free time?", "author_fullname": "t2_4ifgtsq9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much time of your job is spent reviewing theory that you've forgotten?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ek11n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687288145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s Imposter&amp;#39;s Syndrome or that I am really bad at remembering things, but every time I do something I have to check my notes/yt videos to make sure I&amp;#39;m doing things the right way. Is this normal? Or should I start studying more on my free time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ek11n", "is_robot_indexable": true, "report_reasons": null, "author": "Laerance", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14ek11n/how_much_time_of_your_job_is_spent_reviewing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14ek11n/how_much_time_of_your_job_is_spent_reviewing/", "subreddit_subscribers": 928207, "created_utc": 1687288145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There was a \u201chow much ML do you use in your daily workflow\u201d post recently. Most answers were \u201clittle to none\u201d since most businesses don\u2019t have need for a bunch of ML. That doesn\u2019t mean a seasoned and tenured DS shouldn\u2019t be senior at that company. So if you\u2019re senior and still learning, and things like LLM\u2019s or deep learning aren\u2019t applicable, what do you spend your time learning to take yourself to the next level?\n\nContext - I\u2019m a sr. DS at a company that doesn\u2019t need ML, but I feel pressured to learn more advanced ML techniques to move up or be considered a \u201creal Sr. DS\u201d.", "author_fullname": "t2_bcbozi6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does learning look like as senior?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14em1me", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687292586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a \u201chow much ML do you use in your daily workflow\u201d post recently. Most answers were \u201clittle to none\u201d since most businesses don\u2019t have need for a bunch of ML. That doesn\u2019t mean a seasoned and tenured DS shouldn\u2019t be senior at that company. So if you\u2019re senior and still learning, and things like LLM\u2019s or deep learning aren\u2019t applicable, what do you spend your time learning to take yourself to the next level?&lt;/p&gt;\n\n&lt;p&gt;Context - I\u2019m a sr. DS at a company that doesn\u2019t need ML, but I feel pressured to learn more advanced ML techniques to move up or be considered a \u201creal Sr. DS\u201d.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14em1me", "is_robot_indexable": true, "report_reasons": null, "author": "sizable_data", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14em1me/what_does_learning_look_like_as_senior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14em1me/what_does_learning_look_like_as_senior/", "subreddit_subscribers": 928207, "created_utc": 1687292586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Any suggestions for accounts to follow on Blue Sky that post good data science or machine learning content?\n\nI recently stopped using Twitter because my feed had become full of garbage, and I use to be one of the primary places I would get ML news. Looking for replacements. \n\n\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f", "author_fullname": "t2_63qxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blue Sky accounts to follow? (Social media)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14e41iu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687245582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions for accounts to follow on Blue Sky that post good data science or machine learning content?&lt;/p&gt;\n\n&lt;p&gt;I recently stopped using Twitter because my feed had become full of garbage, and I use to be one of the primary places I would get ML news. Looking for replacements. &lt;/p&gt;\n\n&lt;p&gt;\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14e41iu", "is_robot_indexable": true, "report_reasons": null, "author": "NeuralNomd", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14e41iu/blue_sky_accounts_to_follow_social_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14e41iu/blue_sky_accounts_to_follow_social_media/", "subreddit_subscribers": 928207, "created_utc": 1687245582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm not a data scientist, I'm totally willing to hire somebody for this project if necessary because I'm way out of my depth here.  \n\n\nHere is the problem, we have multiple clients, and sizing the market is complicated and is taking us 3 to 4 weeks, the info we get most of the time is how many sales the client and X amount of competitors have over time, how many sales per month, sales speed, pricing variance,  etc. basically we have a lot of good data on pricing, sales volume, marketing spend and all of that for customers and competitors, we have also noticed that CCI plays an important role in B2B SaaS so we use that too.  \n\n\nNow I stumbled upon an article explaining that Market size can be calculated with a Montecarlo simulation but it seems broken? I tried following the example in jupyter and it is broken even in the article itself.  \n\n\nSo I come here for enlightenment, would Montecarlo be an appropriate solution and if so, is there a way a noob like me can do it or should I find a data scientist in Upwork maybe?", "author_fullname": "t2_o3hg65o0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Market size calculation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14du7nk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687217013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a data scientist, I&amp;#39;m totally willing to hire somebody for this project if necessary because I&amp;#39;m way out of my depth here.  &lt;/p&gt;\n\n&lt;p&gt;Here is the problem, we have multiple clients, and sizing the market is complicated and is taking us 3 to 4 weeks, the info we get most of the time is how many sales the client and X amount of competitors have over time, how many sales per month, sales speed, pricing variance,  etc. basically we have a lot of good data on pricing, sales volume, marketing spend and all of that for customers and competitors, we have also noticed that CCI plays an important role in B2B SaaS so we use that too.  &lt;/p&gt;\n\n&lt;p&gt;Now I stumbled upon an article explaining that Market size can be calculated with a Montecarlo simulation but it seems broken? I tried following the example in jupyter and it is broken even in the article itself.  &lt;/p&gt;\n\n&lt;p&gt;So I come here for enlightenment, would Montecarlo be an appropriate solution and if so, is there a way a noob like me can do it or should I find a data scientist in Upwork maybe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14du7nk", "is_robot_indexable": true, "report_reasons": null, "author": "Pristine_Swimming_16", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14du7nk/market_size_calculation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14du7nk/market_size_calculation/", "subreddit_subscribers": 928207, "created_utc": 1687217013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone. First post here.\n\nReacently, i've been working on a sales forecast for a fast food company stores (and just the in-house attention).\n\nFor this forecast, I made two things:\n\n**1) Sales**\n\nFor the sales, i just take the transactions, because the value of the ticket could make my forecast more noisy... So, the first thing using the transactions is separating by **days of the week.**\n\nThe thing is, this fast food company has a seasonality by-week, being friday and saturday the peak of the week and mondays the valleys. Knowing this, i separate the daily transaction data by weekday and the forecast i made calculate the following days by week day (i just use mondays to calculate the future mondays, thursday to future thursday, and so on...)\n\nKnowing this, to have a **representative estimate of every series of weekday data, i calculate a log-weighted average, having more reacently dta with more weight than past data.** The reason of this weighted average is that the stores hardly going to have the same level of sales than months or years ago **(important to considerate that there is no montly seasonality)**. So, its most probably that the store could have a level of transactions very similar to what they do currently than what they do in the past. So, for this weighted avergare, i use a Logarithmic distribution (p=0.95) to give the weights of every day.\n\nFinaly, i have 7 weighted average for each weekday for my stores. Now, the next step is...\n\n**2) Variation**\n\nThe WA i calculate by itself its the current status of the stores, but, for the future, the store its not going to make that exact value, because every store has a trend and cicles of sales.\n\nHaving again this separation by weekdays, what i make is a first diference calculation for every weekday. So, the data i have now its the diference of \"i\" monday vs \"i-1\" monday (in percent), and the same for every weekday.\n\nWhat i take to know the trend of the store its taking th **median fo diference** of every weekday. I don't use the mean, because the series of diferences its asymetrical (because there are a large amount of days that, for promotional or other reasons, some first diferences are like 100% up or more. For Fast Food, this is total understandable. so, for that reason, i use the median, yo have a more representative statistic of the first variation series. This variations, aplicated to the WA calculated before, its going to be my prediction for the next days.\n\nAnd not just that. For now on, i also taking some confidence interval, where the first variations could oscilate up or down. In this case, for every day i taking the 10 and 90 percentile to have a down and up boundaries of confidence for my forecast. So the prediction before could move up or down what i have in percentile 10 and 90 of the first diference series.\n\n**Finally, tidyng up all the calculation, i have a forecast for the next week.**\n\n&amp;#x200B;\n\nThe thing is that my testing gives me a MAPE of 12%m which means for me that im near with my forecast, but i want to make it better. **BUT MOST IMPORTANTLY, im having a problem in justifing my work.**\n\nSo, my question is (for everyone who take the time to read all my text, many many many thank you, really), what do you think about the method i made for forecasting sales? what other thing you would consider to aggregate to have a better forecast? Its there things that are not very clear? It's a valid forecast method or maybe i could give a check to other methods that implies statisticals models like ARIMA?.\n\nThanks again you all", "author_fullname": "t2_6ihag2sc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a forecast for my job, any tip for a better prediction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14eo2xi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687297034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. First post here.&lt;/p&gt;\n\n&lt;p&gt;Reacently, i&amp;#39;ve been working on a sales forecast for a fast food company stores (and just the in-house attention).&lt;/p&gt;\n\n&lt;p&gt;For this forecast, I made two things:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1) Sales&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the sales, i just take the transactions, because the value of the ticket could make my forecast more noisy... So, the first thing using the transactions is separating by &lt;strong&gt;days of the week.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The thing is, this fast food company has a seasonality by-week, being friday and saturday the peak of the week and mondays the valleys. Knowing this, i separate the daily transaction data by weekday and the forecast i made calculate the following days by week day (i just use mondays to calculate the future mondays, thursday to future thursday, and so on...)&lt;/p&gt;\n\n&lt;p&gt;Knowing this, to have a &lt;strong&gt;representative estimate of every series of weekday data, i calculate a log-weighted average, having more reacently dta with more weight than past data.&lt;/strong&gt; The reason of this weighted average is that the stores hardly going to have the same level of sales than months or years ago &lt;strong&gt;(important to considerate that there is no montly seasonality)&lt;/strong&gt;. So, its most probably that the store could have a level of transactions very similar to what they do currently than what they do in the past. So, for this weighted avergare, i use a Logarithmic distribution (p=0.95) to give the weights of every day.&lt;/p&gt;\n\n&lt;p&gt;Finaly, i have 7 weighted average for each weekday for my stores. Now, the next step is...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2) Variation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The WA i calculate by itself its the current status of the stores, but, for the future, the store its not going to make that exact value, because every store has a trend and cicles of sales.&lt;/p&gt;\n\n&lt;p&gt;Having again this separation by weekdays, what i make is a first diference calculation for every weekday. So, the data i have now its the diference of &amp;quot;i&amp;quot; monday vs &amp;quot;i-1&amp;quot; monday (in percent), and the same for every weekday.&lt;/p&gt;\n\n&lt;p&gt;What i take to know the trend of the store its taking th &lt;strong&gt;median fo diference&lt;/strong&gt; of every weekday. I don&amp;#39;t use the mean, because the series of diferences its asymetrical (because there are a large amount of days that, for promotional or other reasons, some first diferences are like 100% up or more. For Fast Food, this is total understandable. so, for that reason, i use the median, yo have a more representative statistic of the first variation series. This variations, aplicated to the WA calculated before, its going to be my prediction for the next days.&lt;/p&gt;\n\n&lt;p&gt;And not just that. For now on, i also taking some confidence interval, where the first variations could oscilate up or down. In this case, for every day i taking the 10 and 90 percentile to have a down and up boundaries of confidence for my forecast. So the prediction before could move up or down what i have in percentile 10 and 90 of the first diference series.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Finally, tidyng up all the calculation, i have a forecast for the next week.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The thing is that my testing gives me a MAPE of 12%m which means for me that im near with my forecast, but i want to make it better. &lt;strong&gt;BUT MOST IMPORTANTLY, im having a problem in justifing my work.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So, my question is (for everyone who take the time to read all my text, many many many thank you, really), what do you think about the method i made for forecasting sales? what other thing you would consider to aggregate to have a better forecast? Its there things that are not very clear? It&amp;#39;s a valid forecast method or maybe i could give a check to other methods that implies statisticals models like ARIMA?.&lt;/p&gt;\n\n&lt;p&gt;Thanks again you all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14eo2xi", "is_robot_indexable": true, "report_reasons": null, "author": "bbmr__95", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14eo2xi/i_made_a_forecast_for_my_job_any_tip_for_a_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14eo2xi/i_made_a_forecast_for_my_job_any_tip_for_a_better/", "subreddit_subscribers": 928207, "created_utc": 1687297034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I currently work in a supermarket as a Data Scientist. I want to create a code to predict age using a camera and store it in a dataframe with the time the prediction was taken. If you guys can please recommend me a code or  a blog it would be really helpful!", "author_fullname": "t2_cn744n7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Age recognition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ekm8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687289474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work in a supermarket as a Data Scientist. I want to create a code to predict age using a camera and store it in a dataframe with the time the prediction was taken. If you guys can please recommend me a code or  a blog it would be really helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ekm8q", "is_robot_indexable": true, "report_reasons": null, "author": "neospartan12", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14ekm8q/age_recognition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14ekm8q/age_recognition/", "subreddit_subscribers": 928207, "created_utc": 1687289474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lrllrd1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which startup is hot in data field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14egfj4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687280109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14egfj4", "is_robot_indexable": true, "report_reasons": null, "author": "vishal-vora", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14egfj4/which_startup_is_hot_in_data_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14egfj4/which_startup_is_hot_in_data_field/", "subreddit_subscribers": 928207, "created_utc": 1687280109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/f6sud1fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d2659effb111ea9219fced0a51085db4a72425b3\n\nhttps://preview.redd.it/qprrw0fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=dc4fe5d2306379100df6b2cd677c30b452c19540\n\nhttps://preview.redd.it/fgemf1fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=59d60cf1e583698344a2544b9368cdb0d2797783\n\nhttps://preview.redd.it/ob5sbzeev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=16a09bf10eeb69610938e1cd9be25fd8e2e39f6b\n\nhttps://preview.redd.it/6runa7fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fcba0d79830eda036be27b24e21f8f291221a963\n\nhttps://preview.redd.it/xhopiqgev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=21a9d5b941eb918ba8a881bf69e6465e3c6004ad\n\nhttps://preview.redd.it/dz0qkxeev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=10c3a21a8090801bfcf3180cef95c5bb2e445c7c\n\nhttps://preview.redd.it/2s4qb0fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fd404b9542cbd6422a85e7cc151a0b4eb0ed2b6a", "author_fullname": "t2_9hzwgnznk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\udd0d Curious about how Netflix outshines the competition? \ud83d\udca1 Discover the 5 ingenious ways they utilize data analytics to dominate the streaming landscape!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"xhopiqgev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bfa0357d2f6a82de24db897fe62227b63c8d89aa"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7fcc22f3830fb94bdf206030bc762310677d429f"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbe7370ee19181f4911efd23633f4fb7cf0dc855"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e785888c440877d6115a7d1644e60f11ee83d92e"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2150cbdef9340ee49e469f8bffaa4c28e910c3a1"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f11677600f7f006626ce58938cc8258f6fb23e0"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/xhopiqgev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=21a9d5b941eb918ba8a881bf69e6465e3c6004ad"}, "id": "xhopiqgev77b1"}, "f6sud1fev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a31169ac12c5f3ae8d90efc3485ae0c9928bbe94"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97b4248805fc5ce845abd3373a771506f7983871"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed8d0820a1070b42371866b11340076ee0832766"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3074cc0e4434fd60f9985a65557bf06dacebf59d"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f171da5f47688a00c96002d4320bd5aaa68a17ea"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=071b62b10c25f02f5c49997291c025e0cb12459a"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/f6sud1fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=d2659effb111ea9219fced0a51085db4a72425b3"}, "id": "f6sud1fev77b1"}, "qprrw0fev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5d9f0b016fce4952a00c517951bb1400545d4bf"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a9316524870b6c8bcb740b604ce689c7ab77cdb"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cce9a5ec35df48190f619b599a937b4ece9b3f4"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=345ed67d60da120f922b3eb38d674b7b2129db3b"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9664758a3eb657c877b5539cef84e0c4feeb89a5"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=767ca0dd7315b977f5cc91158b0b54400bdd0e51"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/qprrw0fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=dc4fe5d2306379100df6b2cd677c30b452c19540"}, "id": "qprrw0fev77b1"}, "dz0qkxeev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=637ba26cabd48031ac4509c0f3d74cd43d4da5d0"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56c087b92c819405c6a054548851e892bed2e980"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=787a64d34a43ee53d2f2803ccc8949445623ab59"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32322884f9bf696e152d5e045f2f821377a0d0be"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee25626b44a78d049b2ca6ca5c0041a83cec7a48"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2fcde100ccfc82fabc0fd1c2a593df857369c5e"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/dz0qkxeev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=10c3a21a8090801bfcf3180cef95c5bb2e445c7c"}, "id": "dz0qkxeev77b1"}, "fgemf1fev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=693fd39aaac0447479e66d146e5bc3d90abe5d20"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b43b8e2c217eed66323b5ae4004ca4d7c13adb82"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a84fe0cc4a0896149c1ba54f5519e2dad7d48faf"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f9a7e31e5ee1e300fe73f4d9a29d9dde345e08f"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba8db1d1c486e076ce4f0f0762fdcf57a19e9428"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=830d4ef4121d3e282b5fc97cab892ddd7b57edef"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/fgemf1fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=59d60cf1e583698344a2544b9368cdb0d2797783"}, "id": "fgemf1fev77b1"}, "6runa7fev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dff2c9f4aca3e269572bea5019ce61ef5774692"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=584d0ec74fea56868ea4387dcfed0ec3bf7cc71d"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d30b4c977dddd839a42913b08af954af351f64f4"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b27c56535f1ef5577a2f17cdedfdaa33011ec7a5"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=63fea9a109f9cacb65bf967e4cf65bcfd233235b"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cd8c878e5d6e74803f754e4838cedd8f0f3766d"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/6runa7fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fcba0d79830eda036be27b24e21f8f291221a963"}, "id": "6runa7fev77b1"}, "ob5sbzeev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=799205c09aa6e35de99f75dc607f6d45b145aa7a"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7abbfad58c681e9c9942d5a43c250e5d9582dee"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f2e238ac6d84bc42e1646130a4d6ba65decd374"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=595a7bf3ee2fc48d9ee3f69aaed42d8b2142ec3d"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5da0c0a389379bc1703d2bc91817d08929bdf2eb"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94698b7dee6db3549e74d79417265509cf537781"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/ob5sbzeev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=16a09bf10eeb69610938e1cd9be25fd8e2e39f6b"}, "id": "ob5sbzeev77b1"}, "2s4qb0fev77b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 135, "x": 108, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6507626256ce51ea9c5e2f520765808c2f879af4"}, {"y": 270, "x": 216, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2447f698841ca0a805e3dbff39f79a45e6a27423"}, {"y": 400, "x": 320, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb4955cc6ae68dfcd10e5201a9886de6cd37db14"}, {"y": 800, "x": 640, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29b2821b13b22bd3b8e93f21e2552d1bfda4176b"}, {"y": 1200, "x": 960, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5c95b6e7b1ae1260b1fb3d70772a22b62a72533"}, {"y": 1350, "x": 1080, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b7fa2521825048a5dce138890ff85fa38877c46"}], "s": {"y": 2063, "x": 1650, "u": "https://preview.redd.it/2s4qb0fev77b1.jpg?width=1650&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fd404b9542cbd6422a85e7cc151a0b4eb0ed2b6a"}, "id": "2s4qb0fev77b1"}}, "name": "t3_14ej2wb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cZz2AwBfSTcXmSROadDQ1y2uwOm1Oxlzw6KpTtaBBCY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687286020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f6sud1fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d2659effb111ea9219fced0a51085db4a72425b3\"&gt;https://preview.redd.it/f6sud1fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d2659effb111ea9219fced0a51085db4a72425b3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qprrw0fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=dc4fe5d2306379100df6b2cd677c30b452c19540\"&gt;https://preview.redd.it/qprrw0fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=dc4fe5d2306379100df6b2cd677c30b452c19540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fgemf1fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=59d60cf1e583698344a2544b9368cdb0d2797783\"&gt;https://preview.redd.it/fgemf1fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=59d60cf1e583698344a2544b9368cdb0d2797783&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ob5sbzeev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=16a09bf10eeb69610938e1cd9be25fd8e2e39f6b\"&gt;https://preview.redd.it/ob5sbzeev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=16a09bf10eeb69610938e1cd9be25fd8e2e39f6b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6runa7fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fcba0d79830eda036be27b24e21f8f291221a963\"&gt;https://preview.redd.it/6runa7fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fcba0d79830eda036be27b24e21f8f291221a963&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xhopiqgev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=21a9d5b941eb918ba8a881bf69e6465e3c6004ad\"&gt;https://preview.redd.it/xhopiqgev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=21a9d5b941eb918ba8a881bf69e6465e3c6004ad&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dz0qkxeev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=10c3a21a8090801bfcf3180cef95c5bb2e445c7c\"&gt;https://preview.redd.it/dz0qkxeev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=10c3a21a8090801bfcf3180cef95c5bb2e445c7c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2s4qb0fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fd404b9542cbd6422a85e7cc151a0b4eb0ed2b6a\"&gt;https://preview.redd.it/2s4qb0fev77b1.jpg?width=1650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fd404b9542cbd6422a85e7cc151a0b4eb0ed2b6a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ej2wb", "is_robot_indexable": true, "report_reasons": null, "author": "abhilashjoseofficial", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14ej2wb/curious_about_how_netflix_outshines_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14ej2wb/curious_about_how_netflix_outshines_the/", "subreddit_subscribers": 928207, "created_utc": 1687286020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Title", "author_fullname": "t2_rzsjy5l8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the current job market in data science? (June 2023)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14e8gfh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687260127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14e8gfh", "is_robot_indexable": true, "report_reasons": null, "author": "Zealousideal_Pen3026", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14e8gfh/what_is_the_current_job_market_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14e8gfh/what_is_the_current_job_market_in_data_science/", "subreddit_subscribers": 928207, "created_utc": 1687260127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently 27. Civil engineer. Making $85k. Go to office in person twice a week. Have the potential to top out making $130k+ in the next few years if i job hop intellgiently. Overall comfortable position at my age. Unlike alot of data scientests posting on here, i get job offers out the wazoo. Literally every day I have a recruiter reach out to me and with my level of skills and experience as a civil engineer ill never be out of a job for long if I choose to.\n\n\nHowever, i feel a little bit unfulfilled and as if I didn't reach my full potential in my current career. I love my field but its just underpaid for the amount of work and liability we take on as well as the difriculty of going remote. Its a safe and stable but low paying career where your decisions have major life altering consequences. I hear these stories of 20 soemthings in DS making $250k working remotely and with the potential to make even more or start there own startups or business and it does make me somewhat envious as I feel like I've stagnated.\n\n\n\nI dislike programming but I love finance and math and i want to make bigger money. Unfortunately I can't escape programming as everything is moving towards that and that's where the money is. So data science I am considering as my best bet as it combines all these aspects and probably opens up alot of doors.\n\n\nBut on the other side and given what i read on here, is it even worth it and is my view of bagging a $200k+ remote job even realistic? \n\nThis subreddit is even more depressing then the civilengineering subreddit. Most people are struggling to find jobs and theres 1000 applicatitons per opening, the jobs they do find are unstable. I also think there is a push from the higher ups to drive wages down with fierce competition from people abroad who are willing to work at one fifth the wages. AI/ChatGPT is compounding all these issues and squeezing tehc workers even more. \n\n\nSo with all that if my goal is more money and more flexibility, is DS really the way?\n\n\n\n\n\n.", "author_fullname": "t2_tyh7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I being greedy for considering a career switch into data science for money/flexibility?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ebe5o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.32, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687269397.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687268272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently 27. Civil engineer. Making $85k. Go to office in person twice a week. Have the potential to top out making $130k+ in the next few years if i job hop intellgiently. Overall comfortable position at my age. Unlike alot of data scientests posting on here, i get job offers out the wazoo. Literally every day I have a recruiter reach out to me and with my level of skills and experience as a civil engineer ill never be out of a job for long if I choose to.&lt;/p&gt;\n\n&lt;p&gt;However, i feel a little bit unfulfilled and as if I didn&amp;#39;t reach my full potential in my current career. I love my field but its just underpaid for the amount of work and liability we take on as well as the difriculty of going remote. Its a safe and stable but low paying career where your decisions have major life altering consequences. I hear these stories of 20 soemthings in DS making $250k working remotely and with the potential to make even more or start there own startups or business and it does make me somewhat envious as I feel like I&amp;#39;ve stagnated.&lt;/p&gt;\n\n&lt;p&gt;I dislike programming but I love finance and math and i want to make bigger money. Unfortunately I can&amp;#39;t escape programming as everything is moving towards that and that&amp;#39;s where the money is. So data science I am considering as my best bet as it combines all these aspects and probably opens up alot of doors.&lt;/p&gt;\n\n&lt;p&gt;But on the other side and given what i read on here, is it even worth it and is my view of bagging a $200k+ remote job even realistic? &lt;/p&gt;\n\n&lt;p&gt;This subreddit is even more depressing then the civilengineering subreddit. Most people are struggling to find jobs and theres 1000 applicatitons per opening, the jobs they do find are unstable. I also think there is a push from the higher ups to drive wages down with fierce competition from people abroad who are willing to work at one fifth the wages. AI/ChatGPT is compounding all these issues and squeezing tehc workers even more. &lt;/p&gt;\n\n&lt;p&gt;So with all that if my goal is more money and more flexibility, is DS really the way?&lt;/p&gt;\n\n&lt;p&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14ebe5o", "is_robot_indexable": true, "report_reasons": null, "author": "aldjfh", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14ebe5o/am_i_being_greedy_for_considering_a_career_switch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14ebe5o/am_i_being_greedy_for_considering_a_career_switch/", "subreddit_subscribers": 928207, "created_utc": 1687268272.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}