{"kind": "Listing", "data": {"after": "t3_14diaq7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Data Engineers,\n\nThere was a great [discussion yesterday](https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&amp;utm_medium=web2x&amp;context=3) about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.\n\nSummary of potential community:\n\n* Verified data engineers (i.e. no bots, spammers)\n* Networking and in-person events\n* Advanced technical topics and industry news\n* Familiar Reddit-style feed\n\nIt's not meant to replace this community but it could in theory act as a backup. We don't have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.\n\nIf you're interested, please [**join the waitlist here**](https://tally.so/r/nGK7pe). If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.\n\n\\---\n\nWe are looking for 1-2 new mods to join our team!\n\nr/dataengineering has grown tremendously over the past few years and could use an extra set of hands and ideas. It's a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it's a great way to give back to the community. If you're interested, [please apply here](https://tally.so/r/3xj669).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New DE Community &amp; Looking for Mods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dgupv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687186533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;There was a great &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;discussion yesterday&lt;/a&gt; about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.&lt;/p&gt;\n\n&lt;p&gt;Summary of potential community:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Verified data engineers (i.e. no bots, spammers)&lt;/li&gt;\n&lt;li&gt;Networking and in-person events&lt;/li&gt;\n&lt;li&gt;Advanced technical topics and industry news&lt;/li&gt;\n&lt;li&gt;Familiar Reddit-style feed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s not meant to replace this community but it could in theory act as a backup. We don&amp;#39;t have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested, please &lt;a href=\"https://tally.so/r/nGK7pe\"&gt;&lt;strong&gt;join the waitlist here&lt;/strong&gt;&lt;/a&gt;. If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;We are looking for 1-2 new mods to join our team!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; has grown tremendously over the past few years and could use an extra set of hands and ideas. It&amp;#39;s a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it&amp;#39;s a great way to give back to the community. If you&amp;#39;re interested, &lt;a href=\"https://tally.so/r/3xj669\"&gt;please apply here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?auto=webp&amp;v=enabled&amp;s=959740544d8390056f25237218ceb90ba637127a", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48ecdd359f78a24667ad4816581834a9a92ccb16", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d95e85536dc5b64b82d2b4b91889b3b5541d82bf", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b0af19c61cfc7012ec5db942890f51a3543276", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2e4c6be4163d6167fcfb44cb1248af11df51667", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7be3948c6b699c70515b1b403a4688f2647f5ba7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75c55874f00d7e849f4f0b2e46ccb96488c843a3", "width": 1080, "height": 567}], "variants": {}, "id": "vXOF8G9GBUU_-_vM38jf2S1-5UiTZqBcFWecpk4eHS4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "14dgupv", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/", "subreddit_subscribers": 111291, "created_utc": 1687186533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's the job market like right now? (June 2023)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dg280", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687184628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dg280", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dg280/hows_the_job_market_like_right_now_june_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dg280/hows_the_job_market_like_right_now_june_2023/", "subreddit_subscribers": 111291, "created_utc": 1687184628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am investigating building tools to help data engineers build data pipelines for machine learning.   \n\n\nI was wondering what are the three biggest problems you encounter on a day-to-day basis.   \n\n\nFor example, is it extracting unstructured data, merging data streams, meeting throughput or latency requirements, keeping upstream and downstream schemas in sync, managing a large number of components in the pipeline, etc. or something else that gives you headaches? Curious to hear!", "author_fullname": "t2_9grwmp0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discussion: What are your biggest problems when building data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dlx8v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687198034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am investigating building tools to help data engineers build data pipelines for machine learning.   &lt;/p&gt;\n\n&lt;p&gt;I was wondering what are the three biggest problems you encounter on a day-to-day basis.   &lt;/p&gt;\n\n&lt;p&gt;For example, is it extracting unstructured data, merging data streams, meeting throughput or latency requirements, keeping upstream and downstream schemas in sync, managing a large number of components in the pipeline, etc. or something else that gives you headaches? Curious to hear!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14dlx8v", "is_robot_indexable": true, "report_reasons": null, "author": "Tricky_Drawer_2917", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dlx8v/discussion_what_are_your_biggest_problems_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dlx8v/discussion_what_are_your_biggest_problems_when/", "subreddit_subscribers": 111291, "created_utc": 1687198034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Started at a new company a couple of months ago, was tasked with terraforming our databricks environments. I\u2019ve never had experience aws, terraform, and databricks. The deployment is going well I have dev and prod workspaces setup but we cannot get authentication through to external tables. I\u2019ve read all the docs and watched all the videos with no luck. Have the metastore created, the metastore access role, the external table with pass role. I have unity catalog enabled with grants and privs defined. \n\nTerraforming all of this and I\u2019m fucking lost.", "author_fullname": "t2_18qay50v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I can\u2019t terraform my company\u2019s Databricks environment and I\u2019m going insane.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dxeco", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687225411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Started at a new company a couple of months ago, was tasked with terraforming our databricks environments. I\u2019ve never had experience aws, terraform, and databricks. The deployment is going well I have dev and prod workspaces setup but we cannot get authentication through to external tables. I\u2019ve read all the docs and watched all the videos with no luck. Have the metastore created, the metastore access role, the external table with pass role. I have unity catalog enabled with grants and privs defined. &lt;/p&gt;\n\n&lt;p&gt;Terraforming all of this and I\u2019m fucking lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dxeco", "is_robot_indexable": true, "report_reasons": null, "author": "Doyale_royale", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dxeco/i_cant_terraform_my_companys_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dxeco/i_cant_terraform_my_companys_databricks/", "subreddit_subscribers": 111291, "created_utc": 1687225411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we\u2019re looking to move from a very 20th century esq oracle based warehouse that is serving both transactional and analytical workloads into a lake house architecture in AWS. Thinking about the stack in layers, the ingestion layer is probably going to be mulesoft, because we\u2019re a sizable salesforce shop and they are good to work with, with the possibility of adding appstream or one of the other ingestion technologies later on. Storage were obviously looking at redshift+s3, we already have a contract with collate for open metadata for our catalog, for processing we\u2019re likely looking at alteryx for line of business customers, and possibly EMR or Glue, we aren\u2019t sure yet, for the more central IT code based jobs. Consumption/visualization obviously we\u2019d have the native querying ability from redshift, and Athena for querying from s3 directly. Vis we have Tableau on prem now, and WebFOCUS is engrained in our ecosystem so much at the moment it will have to stick around for at least a while. We\u2019re possibly also going to look at test driving looker in the near future to get a feel for it. \n\nWe\u2019re in the early stages of planning this project out. We\u2019ve had some changes that are opening up the ability for us to completely redefine our data strategy and stack so we\u2019re looking at a building a new solid baseline right now with possibility to grow each layer as needed. Since we\u2019ve come up with much of this plan in our own little echo chamber I thought it would be useful to ask for opinions and ask about tools we might not be thinking about etc. \n\nI should probably also mention that as of now, and for the foreseeable future we will be dealing primarily with pull based batch jobs from various sources into to warehouse with some possibility for transformation. At some point in the future we may move to more pushed based systems but as a baseline consider everything to be pull based. \n\nWhich also brings up the question. Storing in s3, are we best served by transforming all of our data stored there (to the extent that is reasonable, structure wise and such) as parquet files? Since both redshift and parquet are column based does this make it easier to move data between the two systems? \n\nI\u2019m thinking for the s3 side of things we\u2019d have a few layers. A landing zone layer that would be the raw native format data coming in. A raw storage layer of raw otherwise untransformed parquet files, and a curated layer of cleaned/curated data sets for specific purposes, such as specific ML jobs etc. \n\nFinally the transactional workloads I mentioned earlier (system to system transactional workloads) will most likely be moving into a separate mulesoft cluster that will handle only source to destination transactional data/apis.", "author_fullname": "t2_11vsm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting to rebuild our data stack, opinions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dw022", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687221612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we\u2019re looking to move from a very 20th century esq oracle based warehouse that is serving both transactional and analytical workloads into a lake house architecture in AWS. Thinking about the stack in layers, the ingestion layer is probably going to be mulesoft, because we\u2019re a sizable salesforce shop and they are good to work with, with the possibility of adding appstream or one of the other ingestion technologies later on. Storage were obviously looking at redshift+s3, we already have a contract with collate for open metadata for our catalog, for processing we\u2019re likely looking at alteryx for line of business customers, and possibly EMR or Glue, we aren\u2019t sure yet, for the more central IT code based jobs. Consumption/visualization obviously we\u2019d have the native querying ability from redshift, and Athena for querying from s3 directly. Vis we have Tableau on prem now, and WebFOCUS is engrained in our ecosystem so much at the moment it will have to stick around for at least a while. We\u2019re possibly also going to look at test driving looker in the near future to get a feel for it. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re in the early stages of planning this project out. We\u2019ve had some changes that are opening up the ability for us to completely redefine our data strategy and stack so we\u2019re looking at a building a new solid baseline right now with possibility to grow each layer as needed. Since we\u2019ve come up with much of this plan in our own little echo chamber I thought it would be useful to ask for opinions and ask about tools we might not be thinking about etc. &lt;/p&gt;\n\n&lt;p&gt;I should probably also mention that as of now, and for the foreseeable future we will be dealing primarily with pull based batch jobs from various sources into to warehouse with some possibility for transformation. At some point in the future we may move to more pushed based systems but as a baseline consider everything to be pull based. &lt;/p&gt;\n\n&lt;p&gt;Which also brings up the question. Storing in s3, are we best served by transforming all of our data stored there (to the extent that is reasonable, structure wise and such) as parquet files? Since both redshift and parquet are column based does this make it easier to move data between the two systems? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking for the s3 side of things we\u2019d have a few layers. A landing zone layer that would be the raw native format data coming in. A raw storage layer of raw otherwise untransformed parquet files, and a curated layer of cleaned/curated data sets for specific purposes, such as specific ML jobs etc. &lt;/p&gt;\n\n&lt;p&gt;Finally the transactional workloads I mentioned earlier (system to system transactional workloads) will most likely be moving into a separate mulesoft cluster that will handle only source to destination transactional data/apis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14dw022", "is_robot_indexable": true, "report_reasons": null, "author": "dale3887", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dw022/starting_to_rebuild_our_data_stack_opinions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dw022/starting_to_rebuild_our_data_stack_opinions/", "subreddit_subscribers": 111291, "created_utc": 1687221612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I want to practice my Kafka, Spark Streaming, Docker knowledge on a real project and also learn Kubernetes (k8s). So, I asked ChatGPT for a project, and it provided me with the following project description:   \n\" This project is a data pipeline that combines Kafka, Spark Streaming, Docker, AWS S3, and EKS (Elastic Kubernetes Service) for processing and analyzing real-time data. It uses a self-managed Kafka cluster for data ingestion and distribution, and a Spark cluster for real-time analytics. Docker ensures consistent deployment, while AWS S3 serves as a data repository. EKS manages the Spark cluster, optimizing resource utilization for efficient handling of high data volumes. \"\n\n Do you think this is a good project to undertake and include on my resume if I want to pursue a job in a major tech company like IBM, Oracle, or Amazon? Additionally, if you have any ideas to improve the project or suggestions on elements that need to be removed, please let me know.   \n", "author_fullname": "t2_feara4tb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Data Pipeline with Kafka, Spark Streaming, Docker, AWS S3, and EKS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dn7tk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687200962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to practice my Kafka, Spark Streaming, Docker knowledge on a real project and also learn Kubernetes (k8s). So, I asked ChatGPT for a project, and it provided me with the following project description:&lt;br/&gt;\n&amp;quot; This project is a data pipeline that combines Kafka, Spark Streaming, Docker, AWS S3, and EKS (Elastic Kubernetes Service) for processing and analyzing real-time data. It uses a self-managed Kafka cluster for data ingestion and distribution, and a Spark cluster for real-time analytics. Docker ensures consistent deployment, while AWS S3 serves as a data repository. EKS manages the Spark cluster, optimizing resource utilization for efficient handling of high data volumes. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Do you think this is a good project to undertake and include on my resume if I want to pursue a job in a major tech company like IBM, Oracle, or Amazon? Additionally, if you have any ideas to improve the project or suggestions on elements that need to be removed, please let me know.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dn7tk", "is_robot_indexable": true, "report_reasons": null, "author": "Kratos_1412", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dn7tk/building_a_data_pipeline_with_kafka_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dn7tk/building_a_data_pipeline_with_kafka_spark/", "subreddit_subscribers": 111291, "created_utc": 1687200962.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering, I am a FinOps analyst for an aerospace cloud technologies team, and part of my job is to ensure things in our cloud environment are as optimized for cost. Our company has several ERP systems and there is a big push this year to get all of that data centralized in AWS for consumption into Quicksight. \n\n&amp;#x200B;\n\nRight now we have a working production environment with a single ERP database currently being replicated into S3 and plans to include more soon. The database is running on an old version of IBM's DB2, so we use a Qlikreplicate job to extract changes from the tables and then upload them to S3 for processing in an AWS Glue job that leverages pyspark and hudi to load the files into an S3 datalake with all the metadata being stored in DynamoDB and the data itself queried with Athena and Quicksight.\n\n  \nThis Glue job is the main offender of cost, as the data is pushed once every hour so we have a Glue job running for 3-4 minutes for every table (about 140) and that cost starts to adds up. At first the business wanted these jobs to be ran every 15 minutes, I told them it would probably cost around $150k annually but I'm not sure they believed me because they pushed through with that frequency but then quickly backed the frequency to 1 hour when I showed them the AWS projections of the first month's bill. Right now we pay about $40k annually for just this one database to be replicated into AWS and I'm wondering is that really the most cost effective way to do things? Again, I'm not a data engineer myself (although its a field I have recently started learning in and would love to move that direction one day), and have only been working in Cloud for the past 2.5 years (I do have 7 AWS certifications though so I have a strong grasp on the basics how each service works), so my experience is might be limited, AWS themselves helped with consulting on this project so it very may well be this is the best way to do things, I just wanted to get some outside perspective and see if the collective internet had any thoughts.", "author_fullname": "t2_kaic0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this pipeline financially optimized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dj4l0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687191733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;, I am a FinOps analyst for an aerospace cloud technologies team, and part of my job is to ensure things in our cloud environment are as optimized for cost. Our company has several ERP systems and there is a big push this year to get all of that data centralized in AWS for consumption into Quicksight. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Right now we have a working production environment with a single ERP database currently being replicated into S3 and plans to include more soon. The database is running on an old version of IBM&amp;#39;s DB2, so we use a Qlikreplicate job to extract changes from the tables and then upload them to S3 for processing in an AWS Glue job that leverages pyspark and hudi to load the files into an S3 datalake with all the metadata being stored in DynamoDB and the data itself queried with Athena and Quicksight.&lt;/p&gt;\n\n&lt;p&gt;This Glue job is the main offender of cost, as the data is pushed once every hour so we have a Glue job running for 3-4 minutes for every table (about 140) and that cost starts to adds up. At first the business wanted these jobs to be ran every 15 minutes, I told them it would probably cost around $150k annually but I&amp;#39;m not sure they believed me because they pushed through with that frequency but then quickly backed the frequency to 1 hour when I showed them the AWS projections of the first month&amp;#39;s bill. Right now we pay about $40k annually for just this one database to be replicated into AWS and I&amp;#39;m wondering is that really the most cost effective way to do things? Again, I&amp;#39;m not a data engineer myself (although its a field I have recently started learning in and would love to move that direction one day), and have only been working in Cloud for the past 2.5 years (I do have 7 AWS certifications though so I have a strong grasp on the basics how each service works), so my experience is might be limited, AWS themselves helped with consulting on this project so it very may well be this is the best way to do things, I just wanted to get some outside perspective and see if the collective internet had any thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14dj4l0", "is_robot_indexable": true, "report_reasons": null, "author": "Jaggedfel2142", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dj4l0/is_this_pipeline_financially_optimized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dj4l0/is_this_pipeline_financially_optimized/", "subreddit_subscribers": 111291, "created_utc": 1687191733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to dataengineering, almost done with the ibm course on coursera, looking to get my hands dirty with the zoomcamp, is it better to dual boot my windows laptop and install a Linux partition, or should i stick with ubuntu wsl.\n\nThanks", "author_fullname": "t2_5466c8m7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Install Linux or stick with Ubuntu wsl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dxr7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687226398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to dataengineering, almost done with the ibm course on coursera, looking to get my hands dirty with the zoomcamp, is it better to dual boot my windows laptop and install a Linux partition, or should i stick with ubuntu wsl.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dxr7g", "is_robot_indexable": true, "report_reasons": null, "author": "EmotionalResolve9", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dxr7g/install_linux_or_stick_with_ubuntu_wsl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dxr7g/install_linux_or_stick_with_ubuntu_wsl/", "subreddit_subscribers": 111291, "created_utc": 1687226398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm writing a series of articles for begginers about manipulating apache Spark using docker.  \nThis describes multiple ways of creating and deploying spark images in local.  \nThis will help understanding the local development environment, by using several local deploy techniques such as 'docker-compose' , 'Kubernetes cluster' and using Helm charts.  \nWhich will cover in 3 parts, all the needed steps to deploy a cluster and runing a job in local.  \nI wish this could help  \n[https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222](https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222)", "author_fullname": "t2_ae4tw1pnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark images, docker-compose and kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dogw9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687203717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m writing a series of articles for begginers about manipulating apache Spark using docker.&lt;br/&gt;\nThis describes multiple ways of creating and deploying spark images in local.&lt;br/&gt;\nThis will help understanding the local development environment, by using several local deploy techniques such as &amp;#39;docker-compose&amp;#39; , &amp;#39;Kubernetes cluster&amp;#39; and using Helm charts.&lt;br/&gt;\nWhich will cover in 3 parts, all the needed steps to deploy a cluster and runing a job in local.&lt;br/&gt;\nI wish this could help&lt;br/&gt;\n&lt;a href=\"https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222\"&gt;https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14dogw9", "is_robot_indexable": true, "report_reasons": null, "author": "PhysicalTomorrow2098", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dogw9/spark_images_dockercompose_and_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dogw9/spark_images_dockercompose_and_kubernetes/", "subreddit_subscribers": 111291, "created_utc": 1687203717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey /r/dataengineering \n\nWe are building a platform called [ShareData](https://sharedata.in) that helps multiple organisations collaborate on large datasets without moving data to a warehouse or datalake. Think of it more like source agnostic snowflake share or delta share. \n\nThe platform is installed on an instance or k8s cluster, and any data in your data stores can be queries by an external 3rd party, without creating pipelines or networking interfaces. \n\nif multiple parties are involved, the platform creates a mesh that can run a federated query such as this \n\n`SELECT entityA.postgres.userTable.name,\nentityB.mysql.customerTable.prodName,...`\n`FROM entityA.postgres.userTable`\n`INNER JOIN entityB.mysql.customerTable`\n`ON entityA.postgres.userTable.name =entityB.mysql.customerTable.customerName;`\n\nWe have been building for the past few months are looking for early adopters and feedback.\\\n\nEdit: Adding an image showing a possible use-case, one entity consuming data from 3 different entities without pipelines. \n\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502                             \u2502\n                           \u2502        Data consumer org    \u2502\n                           \u2502                             \u2502\n                           \u2502            API/JDBC         \u2502\n                           \u2502            \u250c\u2500\u2500\u2500\u2510            \u2502\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u253c\u253c\u253c\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502             \u2502            \u2502\u253c\u253c\u253c\u2502            \u2502                  \u2502\n             \u2502             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u253c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n             \u2502                            \u2502                                 \u2502\n             \u2502                            \u2502                                 \u2502\n             \u2502                            \u2502                                 \u2502\n             \u2502                            \u2502                                 \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                  \u2502       \u2502                     \u2502           \u2502                    \u2502\n    \u2502                  \u2502       \u2502                     \u2502           \u2502                    \u2502\n    \u2502                  \u2502       \u2502                     \u2502           \u2502                    \u2502\n    \u2502       Org 1      \u2502       \u2502        Org 2        \u2502           \u2502       Org 3        \u2502\n    \u2502 \u2500\u2500\u2510 \u2500\u2500\u2510 \u2500\u2500\u2510 \u2500\u2500\u2510  \u2502       \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u2502           \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2510  \u2502\n    \u2502 DB\u2502 DB\u2502 DB\u2502 DB\u2502  \u2502       \u2502  \u2502     \u2502  \u2502     \u2502   \u2502           \u2502  \u2502     \u2502   \u2502    \u2502  \u2502\n    \u2502 \u250c\u2500\u2524 \u250c\u2500\u2524 \u250c\u2500\u2524 \u250c\u2500\u2524  \u2502       \u2502  \u2502 hive\u2502  \u2502 s3  \u2502   \u2502           \u2502  \u2502ADSL \u2502   \u2502Mongo  \u2502\n    \u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2500\u2518       \u2514\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "author_fullname": "t2_v15080di", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback request: Multi-cloud, source agnostic, zero-copy data sharing platform.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dznoy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687241449.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687231697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;We are building a platform called &lt;a href=\"https://sharedata.in\"&gt;ShareData&lt;/a&gt; that helps multiple organisations collaborate on large datasets without moving data to a warehouse or datalake. Think of it more like source agnostic snowflake share or delta share. &lt;/p&gt;\n\n&lt;p&gt;The platform is installed on an instance or k8s cluster, and any data in your data stores can be queries by an external 3rd party, without creating pipelines or networking interfaces. &lt;/p&gt;\n\n&lt;p&gt;if multiple parties are involved, the platform creates a mesh that can run a federated query such as this &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT entityA.postgres.userTable.name,\nentityB.mysql.customerTable.prodName,...&lt;/code&gt;\n&lt;code&gt;FROM entityA.postgres.userTable&lt;/code&gt;\n&lt;code&gt;INNER JOIN entityB.mysql.customerTable&lt;/code&gt;\n&lt;code&gt;ON entityA.postgres.userTable.name =entityB.mysql.customerTable.customerName;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;We have been building for the past few months are looking for early adopters and feedback.\\&lt;/p&gt;\n\n&lt;p&gt;Edit: Adding an image showing a possible use-case, one entity consuming data from 3 different entities without pipelines. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                       \u2502                             \u2502\n                       \u2502        Data consumer org    \u2502\n                       \u2502                             \u2502\n                       \u2502            API/JDBC         \u2502\n                       \u2502            \u250c\u2500\u2500\u2500\u2510            \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u253c\u253c\u253c\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502             \u2502            \u2502\u253c\u253c\u253c\u2502            \u2502                  \u2502\n         \u2502             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u253c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n         \u2502                            \u2502                                 \u2502\n         \u2502                            \u2502                                 \u2502\n         \u2502                            \u2502                                 \u2502\n         \u2502                            \u2502                                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  \u2502       \u2502                     \u2502           \u2502                    \u2502\n\u2502                  \u2502       \u2502                     \u2502           \u2502                    \u2502\n\u2502                  \u2502       \u2502                     \u2502           \u2502                    \u2502\n\u2502       Org 1      \u2502       \u2502        Org 2        \u2502           \u2502       Org 3        \u2502\n\u2502 \u2500\u2500\u2510 \u2500\u2500\u2510 \u2500\u2500\u2510 \u2500\u2500\u2510  \u2502       \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u2502           \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 DB\u2502 DB\u2502 DB\u2502 DB\u2502  \u2502       \u2502  \u2502     \u2502  \u2502     \u2502   \u2502           \u2502  \u2502     \u2502   \u2502    \u2502  \u2502\n\u2502 \u250c\u2500\u2524 \u250c\u2500\u2524 \u250c\u2500\u2524 \u250c\u2500\u2524  \u2502       \u2502  \u2502 hive\u2502  \u2502 s3  \u2502   \u2502           \u2502  \u2502ADSL \u2502   \u2502Mongo  \u2502\n\u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2500\u2518       \u2514\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14dznoy", "is_robot_indexable": true, "report_reasons": null, "author": "I_eat_dosa", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dznoy/feedback_request_multicloud_source_agnostic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dznoy/feedback_request_multicloud_source_agnostic/", "subreddit_subscribers": 111291, "created_utc": 1687231697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been seeing a lot of VCs looking for Data Engineers lately, curious if the work is all that interesting or how different is it from working at a typical startup that builds products.", "author_fullname": "t2_5w2yqiczk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have experience working for VCs as Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dou8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687204548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been seeing a lot of VCs looking for Data Engineers lately, curious if the work is all that interesting or how different is it from working at a typical startup that builds products.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dou8c", "is_robot_indexable": true, "report_reasons": null, "author": "ReliefCreepy8397", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dou8c/anyone_have_experience_working_for_vcs_as_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dou8c/anyone_have_experience_working_for_vcs_as_data/", "subreddit_subscribers": 111291, "created_utc": 1687204548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_gpjiq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a functional, effectful Distributed System from scratch in Scala 3, just to avoid Leetcode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14do59q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1687203006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chollinger.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://chollinger.com/blog/2023/06/building-a-functional-effectful-distributed-system-from-scratch-in-scala-3-just-to-avoid-leetcode-part-1/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "14do59q", "is_robot_indexable": true, "report_reasons": null, "author": "otter-in-a-suit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14do59q/building_a_functional_effectful_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://chollinger.com/blog/2023/06/building-a-functional-effectful-distributed-system-from-scratch-in-scala-3-just-to-avoid-leetcode-part-1/", "subreddit_subscribers": 111291, "created_utc": 1687203006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does transactions in Spark work? \n\nI'd like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn't seem to follow how transactions work in normal RDBMSs. \n\nIt would be great if someone could shed some light on this.", "author_fullname": "t2_vcdigx7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transactions in Spark / Delta lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d9tfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687167266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does transactions in Spark work? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn&amp;#39;t seem to follow how transactions work in normal RDBMSs. &lt;/p&gt;\n\n&lt;p&gt;It would be great if someone could shed some light on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d9tfc", "is_robot_indexable": true, "report_reasons": null, "author": "loudandclear11", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "subreddit_subscribers": 111291, "created_utc": 1687167266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks! We have several ML pipelines based on PyTorch and Spark but the folks who worked on them aren't with us anymore. Our data scientists have been facing a bunch of issue refactoring/updating the algorithm, on top of that the code seems to be lacking some optimization specific to Spark.\n\nWe are a small shop and nobody's expert in Spark infra, we were considering a complete rewrite with Ray for ML as the compute engine as it seems to support just Python code. Interestingly we couldn't find that many materials online regarding it and the documentation is not enough. For example, we couldn't find a proper guideline to setup a test environment with Docker/Docker Compose.\n\nDid you use Ray and what was your experience, in comparison to the alternatives?", "author_fullname": "t2_12lkky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ray for ML projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14e2obe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687241081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks! We have several ML pipelines based on PyTorch and Spark but the folks who worked on them aren&amp;#39;t with us anymore. Our data scientists have been facing a bunch of issue refactoring/updating the algorithm, on top of that the code seems to be lacking some optimization specific to Spark.&lt;/p&gt;\n\n&lt;p&gt;We are a small shop and nobody&amp;#39;s expert in Spark infra, we were considering a complete rewrite with Ray for ML as the compute engine as it seems to support just Python code. Interestingly we couldn&amp;#39;t find that many materials online regarding it and the documentation is not enough. For example, we couldn&amp;#39;t find a proper guideline to setup a test environment with Docker/Docker Compose.&lt;/p&gt;\n\n&lt;p&gt;Did you use Ray and what was your experience, in comparison to the alternatives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14e2obe", "is_robot_indexable": true, "report_reasons": null, "author": "ratulotron", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14e2obe/ray_for_ml_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14e2obe/ray_for_ml_projects/", "subreddit_subscribers": 111291, "created_utc": 1687241081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jcps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lessons learned building on Snowpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 135, "top_awarded_type": null, "hide_score": false, "name": "t3_14dpem9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4qYXfKh3_D3Wl-bRfoCMSFNWe1QMO2nHpAyaBK0dVR4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687205828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "learningfromdata.zingg.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.learningfromdata.zingg.ai/p/building-identity-resolution-on-snowflake", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?auto=webp&amp;v=enabled&amp;s=6e38b5b421a3789acd233ff97777a777ca9f77aa", "width": 1080, "height": 1049}, "resolutions": [{"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d0e172ac0d5797c63702045e4ef8b787abcfea3", "width": 108, "height": 104}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=491f0bc79555df73f58e098b002b947ef9fd9900", "width": 216, "height": 209}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f29ba29c5a021fcee815ccfea5ccb9106fefea8e", "width": 320, "height": 310}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4c8acbcf15768d003cc90f6ed881c7540c556c8", "width": 640, "height": 621}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b04658cff1bd7378f14965a8285a649a3944a2a", "width": 960, "height": 932}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ac48361806694da49c91f4b9578affa4d4bfa6d", "width": 1080, "height": 1049}], "variants": {}, "id": "nGS0oBLgu-qRM_n4RrQEgFL_itbpg1CW56SmE9hYCeQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14dpem9", "is_robot_indexable": true, "report_reasons": null, "author": "sonalg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dpem9/lessons_learned_building_on_snowpark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.learningfromdata.zingg.ai/p/building-identity-resolution-on-snowflake", "subreddit_subscribers": 111291, "created_utc": 1687205828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been Big data engineer ( spark, hadoop, python, pipeline buidling on cloud as well on premise) with 7 plus years of experience and have been java developer for 2 years before that. I have worked independently with out any supervision for some part of my career and even lead one major project for my team where co-ordination needed across 7 teams but never as an official lead in the project.\n\nI do agree i never got to work as an architect but i built pipelines independently all by my myself using diff tech. \n\nI have been putting away taking next step in my career so far but rite now i wanna pursue it and really wanna take it seriously. Any suggestions on the process and key areas i need to focus and ways to improve would be nice. Thank you!", "author_fullname": "t2_17b2qof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do one become Staff engineer in bigdata ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dmvxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687200230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been Big data engineer ( spark, hadoop, python, pipeline buidling on cloud as well on premise) with 7 plus years of experience and have been java developer for 2 years before that. I have worked independently with out any supervision for some part of my career and even lead one major project for my team where co-ordination needed across 7 teams but never as an official lead in the project.&lt;/p&gt;\n\n&lt;p&gt;I do agree i never got to work as an architect but i built pipelines independently all by my myself using diff tech. &lt;/p&gt;\n\n&lt;p&gt;I have been putting away taking next step in my career so far but rite now i wanna pursue it and really wanna take it seriously. Any suggestions on the process and key areas i need to focus and ways to improve would be nice. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dmvxf", "is_robot_indexable": true, "report_reasons": null, "author": "I_like_salads", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dmvxf/how_do_one_become_staff_engineer_in_bigdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dmvxf/how_do_one_become_staff_engineer_in_bigdata/", "subreddit_subscribers": 111291, "created_utc": 1687200230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a software engineer, working on implementing the following.\n- Create an API that can be triggered when necessary.\n- When triggered, fetch some records from a read-only datastore.\n- Current estimate is that 1-2 triggers will be received in a week.\n- Each trigger will result in fetching [50 - 100] million records from the datastore for processing.\n- Apply some simple business logic on each record.\n- Based on the output of the previous step, make API calls to other internal systems.\n- Reports need to be generated after the processing is done. The way this will be consumed is not yet decided.\n- Speed of completing the processing is a factor.\n\nThe high level solution that I have for this now:\n- A dockerized web application(referred to as ZZ from here on) which authenticates user and exposes the required API.\n- The internal APIs that are being called, can handle 10K TPS from ZZ\n- ZZ will fetch the records for a job from the datastore, and write it to a Kafka topic.\n- ZZ has a background service which is polling the kafka topic and processing records.\n- Initial testing shows one ZZ container is able to handle 500 TPS of record processing.\n- ZZ will be scaled to 20, so that it can fully utilize the internal API capacity of 10K TPS.\n- ZZ will have some internal controls to only send 500 TPS at a time.\n- If a record processing fails it is retried once.\n- Reports are generated and written to Postgres for retrieval/query.\n- The reports from a job are expected to be available for 1 week after its run.\n- Before a job is initiated, ZZ will be scaled up. And if needed the other internal APIs.\n\nWorking through this, I feel this might be a problem that is common in the data engineering world.\nAny solution/tool that already solves this?\nAppreciate any advice you can provide.", "author_fullname": "t2_1l8inv31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I reinventing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dilon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687190542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a software engineer, working on implementing the following.\n- Create an API that can be triggered when necessary.\n- When triggered, fetch some records from a read-only datastore.\n- Current estimate is that 1-2 triggers will be received in a week.\n- Each trigger will result in fetching [50 - 100] million records from the datastore for processing.\n- Apply some simple business logic on each record.\n- Based on the output of the previous step, make API calls to other internal systems.\n- Reports need to be generated after the processing is done. The way this will be consumed is not yet decided.\n- Speed of completing the processing is a factor.&lt;/p&gt;\n\n&lt;p&gt;The high level solution that I have for this now:\n- A dockerized web application(referred to as ZZ from here on) which authenticates user and exposes the required API.\n- The internal APIs that are being called, can handle 10K TPS from ZZ\n- ZZ will fetch the records for a job from the datastore, and write it to a Kafka topic.\n- ZZ has a background service which is polling the kafka topic and processing records.\n- Initial testing shows one ZZ container is able to handle 500 TPS of record processing.\n- ZZ will be scaled to 20, so that it can fully utilize the internal API capacity of 10K TPS.\n- ZZ will have some internal controls to only send 500 TPS at a time.\n- If a record processing fails it is retried once.\n- Reports are generated and written to Postgres for retrieval/query.\n- The reports from a job are expected to be available for 1 week after its run.\n- Before a job is initiated, ZZ will be scaled up. And if needed the other internal APIs.&lt;/p&gt;\n\n&lt;p&gt;Working through this, I feel this might be a problem that is common in the data engineering world.\nAny solution/tool that already solves this?\nAppreciate any advice you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dilon", "is_robot_indexable": true, "report_reasons": null, "author": "find_my_path01", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dilon/am_i_reinventing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dilon/am_i_reinventing/", "subreddit_subscribers": 111291, "created_utc": 1687190542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Couldn\u2019t find on LinkedIn Learning and only 1-2 blog posts. I read this recently and thought it was more clear than many others so trying to find more that are possibly less sales-e .  \n\nhttps://estuary.dev/chatgpt-custom-data/", "author_fullname": "t2_a6gn9tzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any could tutorials on building knowledgeable/ vector store data pipeline to enrich llm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dh3iv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687187083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn\u2019t find on LinkedIn Learning and only 1-2 blog posts. I read this recently and thought it was more clear than many others so trying to find more that are possibly less sales-e .  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://estuary.dev/chatgpt-custom-data/\"&gt;https://estuary.dev/chatgpt-custom-data/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dh3iv", "is_robot_indexable": true, "report_reasons": null, "author": "HovercraftGold980", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dh3iv/any_could_tutorials_on_building_knowledgeable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dh3iv/any_could_tutorials_on_building_knowledgeable/", "subreddit_subscribers": 111291, "created_utc": 1687187083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have recently taken the GCP Professional Data Engineer certificate and have any advice on question topics you remember or websites with great practice questions please leave a comment, TIA!", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good practice question banks for CP Professional Data Engineer Cert? question topics from anyone who has taken it recently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14df68i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687182476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have recently taken the GCP Professional Data Engineer certificate and have any advice on question topics you remember or websites with great practice questions please leave a comment, TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14df68i", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14df68i/good_practice_question_banks_for_cp_professional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14df68i/good_practice_question_banks_for_cp_professional/", "subreddit_subscribers": 111291, "created_utc": 1687182476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! My team is giving a try to the Azure Managed Airflow. I wonder if anyone has successfully implemented the Azure Managed Airflow?  How did you implement CI/CD, are there any tips to know before implementing it? Is it better to deploy it on AKS?\n\nI'm using Astro CLI for local dev and hoping to establish a CI/CD pipeline to deploy it on Azure.", "author_fullname": "t2_7zv6bhar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying on Azure Managed Airflow vs. AKS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dpg2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687205922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! My team is giving a try to the Azure Managed Airflow. I wonder if anyone has successfully implemented the Azure Managed Airflow?  How did you implement CI/CD, are there any tips to know before implementing it? Is it better to deploy it on AKS?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Astro CLI for local dev and hoping to establish a CI/CD pipeline to deploy it on Azure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dpg2o", "is_robot_indexable": true, "report_reasons": null, "author": "Resident-Income-5222", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dpg2o/deploying_on_azure_managed_airflow_vs_aks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dpg2o/deploying_on_azure_managed_airflow_vs_aks/", "subreddit_subscribers": 111291, "created_utc": 1687205922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was reading a [blog post](https://simmering.dev/blog/dataframes/) from 2021 comparing some python and R dataframe api libraries and I thought it was a truly valuable analysis worth revisiting in the present given some of the innovations I've heard of in this area lately. I'm hoping to spark (literally no pun intended) a thought-provoking discussion around the Python dataframe landscape and the tools and paradigms that are shaping our field today. As someone who works in SQL and Python and has worked across projects large and small using regular old pandas to frameworks like hadoop/hive, Spark, and many other big data tools under the sun, I consistently long to equip teams of engineers with a scalable dataframe API that can handle a variety of workloads and backends. I'm very curious about the theoretical and realized frameworks or emerging trends that are exciting and are pushing our field forward in terms of better, faster, and stronger dataframe programming APIs.\n\nImagine a project's priorities include both read and write efficiency with an emphasis on minimizing external dependencies and reducing the need for extensive infrastructure management. Endpoints for this hypothetical project could range from various SQL data warehouses (like PostgreSQL, MySQL, traditional RDBMS, etc.) to cloud native/file based rdbms systems (Snowflake, Duckdb) to different file systems (such as local and S3 blob storage, and possibly Azure Data Lake Storage - ADLS). For the sake of this discussion I was hoping to exclude Spark as an API due to its somewhat hefty infrastructure requirements (I've used it extensively across several platforms but in my experience non data developer/analyst adoption is too steep), but I can be convinced if someone makes a compelling argument that doesn't involve SaaS in the middle.\n\nThere are several common libraries like Pandas, Dask, and Polars that come to mind for dataframe manipulations, and then others like SQLAlchemy and Ibis for SQL interactions. But the questions I'm most interested in are:\n\n1. **Overlap &amp; Uniqueness:** Where do libraries and tools overlap, and where do they offer unique advantages? What are the abstract principles that would guide you when choosing one over the other, particularly regarding read and write operations? \n\n2. **Emerging Trends:** Are there any new or emerging libraries or tools that are exciting you? Maybe there's something that's not yet mainstream but could potentially be a game-changer? What do you hear on the streets of slack with fellow devs?\n\n3. **Theoretical Best Practices:** What theoretical considerations or best practices guide your decision-making when it comes to Python dataframe operations and SQL data warehouse interactions? Where should transformations take place?\n\n3. **Usability:** I hinted at this with my hot take on Spark already but I tend to believe that the best solution is one that is simple to use for the greatest number of people. Is there a case to be made for any new or old libraries for usability, extensibility, or other considerations around implementation and adoption by large numbers of developers and end users of different skill levels?\n\nUltimately the audience of this discussion is your fellow data engineers. Let's discuss!", "author_fullname": "t2_uqrd0850", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Dataframe APIs: If you had to start from scratch, which would you build with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14e06dy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687233278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was reading a &lt;a href=\"https://simmering.dev/blog/dataframes/\"&gt;blog post&lt;/a&gt; from 2021 comparing some python and R dataframe api libraries and I thought it was a truly valuable analysis worth revisiting in the present given some of the innovations I&amp;#39;ve heard of in this area lately. I&amp;#39;m hoping to spark (literally no pun intended) a thought-provoking discussion around the Python dataframe landscape and the tools and paradigms that are shaping our field today. As someone who works in SQL and Python and has worked across projects large and small using regular old pandas to frameworks like hadoop/hive, Spark, and many other big data tools under the sun, I consistently long to equip teams of engineers with a scalable dataframe API that can handle a variety of workloads and backends. I&amp;#39;m very curious about the theoretical and realized frameworks or emerging trends that are exciting and are pushing our field forward in terms of better, faster, and stronger dataframe programming APIs.&lt;/p&gt;\n\n&lt;p&gt;Imagine a project&amp;#39;s priorities include both read and write efficiency with an emphasis on minimizing external dependencies and reducing the need for extensive infrastructure management. Endpoints for this hypothetical project could range from various SQL data warehouses (like PostgreSQL, MySQL, traditional RDBMS, etc.) to cloud native/file based rdbms systems (Snowflake, Duckdb) to different file systems (such as local and S3 blob storage, and possibly Azure Data Lake Storage - ADLS). For the sake of this discussion I was hoping to exclude Spark as an API due to its somewhat hefty infrastructure requirements (I&amp;#39;ve used it extensively across several platforms but in my experience non data developer/analyst adoption is too steep), but I can be convinced if someone makes a compelling argument that doesn&amp;#39;t involve SaaS in the middle.&lt;/p&gt;\n\n&lt;p&gt;There are several common libraries like Pandas, Dask, and Polars that come to mind for dataframe manipulations, and then others like SQLAlchemy and Ibis for SQL interactions. But the questions I&amp;#39;m most interested in are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Overlap &amp;amp; Uniqueness:&lt;/strong&gt; Where do libraries and tools overlap, and where do they offer unique advantages? What are the abstract principles that would guide you when choosing one over the other, particularly regarding read and write operations? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Emerging Trends:&lt;/strong&gt; Are there any new or emerging libraries or tools that are exciting you? Maybe there&amp;#39;s something that&amp;#39;s not yet mainstream but could potentially be a game-changer? What do you hear on the streets of slack with fellow devs?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Theoretical Best Practices:&lt;/strong&gt; What theoretical considerations or best practices guide your decision-making when it comes to Python dataframe operations and SQL data warehouse interactions? Where should transformations take place?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Usability:&lt;/strong&gt; I hinted at this with my hot take on Spark already but I tend to believe that the best solution is one that is simple to use for the greatest number of people. Is there a case to be made for any new or old libraries for usability, extensibility, or other considerations around implementation and adoption by large numbers of developers and end users of different skill levels?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ultimately the audience of this discussion is your fellow data engineers. Let&amp;#39;s discuss!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14e06dy", "is_robot_indexable": true, "report_reasons": null, "author": "IncognitoEmployee", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14e06dy/python_dataframe_apis_if_you_had_to_start_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14e06dy/python_dataframe_apis_if_you_had_to_start_from/", "subreddit_subscribers": 111291, "created_utc": 1687233278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies if this is a noobie question, but I've been tasked to create a Power BI dashboard which always presents the latest data from our financial accounting system (MYOB). At present we call the data over the MYOB REST API manually via a python script - normally once a week - store the result in an Azure SQL Database and connect to that database from Power BI.\n\nMy boss has painted the picture that whenever we log into SharePoint to view our published dashboard, the data should be representative of every transaction that has been entered into our accounting system, even the ones we entered a few minutes ago.\n\nI have looked into Webhooks, however, MYOB doesn't have them, which makes me think that I'd need to set up some sort of event-based trigger which checks whether a transaction has been modified in the last few minutes, if yes, then it will go ahead and call all the data from the API. Is anyone able to point me the right direction for this sort of reporting? It's a bit out of my wheelhouse at present, but looks like a good learning opportunity.", "author_fullname": "t2_60zzpr2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Live Dashboard Reporting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dxznl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687227019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this is a noobie question, but I&amp;#39;ve been tasked to create a Power BI dashboard which always presents the latest data from our financial accounting system (MYOB). At present we call the data over the MYOB REST API manually via a python script - normally once a week - store the result in an Azure SQL Database and connect to that database from Power BI.&lt;/p&gt;\n\n&lt;p&gt;My boss has painted the picture that whenever we log into SharePoint to view our published dashboard, the data should be representative of every transaction that has been entered into our accounting system, even the ones we entered a few minutes ago.&lt;/p&gt;\n\n&lt;p&gt;I have looked into Webhooks, however, MYOB doesn&amp;#39;t have them, which makes me think that I&amp;#39;d need to set up some sort of event-based trigger which checks whether a transaction has been modified in the last few minutes, if yes, then it will go ahead and call all the data from the API. Is anyone able to point me the right direction for this sort of reporting? It&amp;#39;s a bit out of my wheelhouse at present, but looks like a good learning opportunity.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dxznl", "is_robot_indexable": true, "report_reasons": null, "author": "Jehhred", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14dxznl/live_dashboard_reporting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dxznl/live_dashboard_reporting/", "subreddit_subscribers": 111291, "created_utc": 1687227019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data lake v2 in Azure and a Synapse Serverless SQL pool on top of it.  I set up Synapse pipelines (ADF) that run every 10 minutes and move data dynamically from the data lake to Azure SQL.  It scans for new tables, schema drifts and upserts the data.\n\nThe tables are wide (approx. 600 columns per table) and sparse, the schemas drift frequently, and the data volume is low, maybe 20 records per hour per table.\n\nFor various reasons, I'd like to look for a new platform, preferably open source, where I can specify the source and the sink, do some minimal configuration like tell it the PK, the sink modified on a column in the source, and it can run upserts on the fly without breaking if the schema drifts in the source.  Bonus if the platform can do schema drifting; if not I have a custom stored procedure that schema drifts based on metadata tables in SQL Server (sys. tables, sys. columns, etc).\n\nI'm very open.  Thanks in advance.", "author_fullname": "t2_5yj82gi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Platform for really wide tables? Synapse / ADF isn't cutting it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dvr4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687224229.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687220960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data lake v2 in Azure and a Synapse Serverless SQL pool on top of it.  I set up Synapse pipelines (ADF) that run every 10 minutes and move data dynamically from the data lake to Azure SQL.  It scans for new tables, schema drifts and upserts the data.&lt;/p&gt;\n\n&lt;p&gt;The tables are wide (approx. 600 columns per table) and sparse, the schemas drift frequently, and the data volume is low, maybe 20 records per hour per table.&lt;/p&gt;\n\n&lt;p&gt;For various reasons, I&amp;#39;d like to look for a new platform, preferably open source, where I can specify the source and the sink, do some minimal configuration like tell it the PK, the sink modified on a column in the source, and it can run upserts on the fly without breaking if the schema drifts in the source.  Bonus if the platform can do schema drifting; if not I have a custom stored procedure that schema drifts based on metadata tables in SQL Server (sys. tables, sys. columns, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very open.  Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dvr4a", "is_robot_indexable": true, "report_reasons": null, "author": "Swimming_Cry_6841", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dvr4a/best_platform_for_really_wide_tables_synapse_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dvr4a/best_platform_for_really_wide_tables_synapse_adf/", "subreddit_subscribers": 111291, "created_utc": 1687220960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "people, I am data engineer with moderate background in ML. Working on ML/NLP tasks.\n\nMy question is, can we add more context or enrich text to have better embeddings...?\n\nContext:\n\nI am dealing with bunch of PDF files, which are in various structure but all of them are very high level updates, and mostly in bullet or numbered lists with little text.\n\nSo created embeddings out of it, and connected with LLM. Problem is, (imagine we have \"heading blah blah\" and some statements following...) when I ask what are the points from heading blah blah... (due to data is chunked... and embedded) I only get first few items (that are in same chunk as the heading) and rest of them are lost... I think it has to do with the context, rest of the embeddings do not have similarity with the heading so vector database is missing them (basically first vector db pulls the doc and share it with LLM...).\n\nSo I was thinking about either to enrich the data or adding more context (like, identifying heading and then adding something like \"below are points related to this topic...\") or creating nested dict (something like {\"heading 1\": {\"contents\": \\[\"aaaa\", \"bbbb\", \\], \"childs\"; {}}}\n\nI dont know whether these stupid ideas or not... need some help here... or please let me know if I am missing something?\n\n(btw, I really hope my question make sense...)", "author_fullname": "t2_4v8di1j8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with preprocessing unstructured data for better embeddings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dmcws", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687199047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;people, I am data engineer with moderate background in ML. Working on ML/NLP tasks.&lt;/p&gt;\n\n&lt;p&gt;My question is, can we add more context or enrich text to have better embeddings...?&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;I am dealing with bunch of PDF files, which are in various structure but all of them are very high level updates, and mostly in bullet or numbered lists with little text.&lt;/p&gt;\n\n&lt;p&gt;So created embeddings out of it, and connected with LLM. Problem is, (imagine we have &amp;quot;heading blah blah&amp;quot; and some statements following...) when I ask what are the points from heading blah blah... (due to data is chunked... and embedded) I only get first few items (that are in same chunk as the heading) and rest of them are lost... I think it has to do with the context, rest of the embeddings do not have similarity with the heading so vector database is missing them (basically first vector db pulls the doc and share it with LLM...).&lt;/p&gt;\n\n&lt;p&gt;So I was thinking about either to enrich the data or adding more context (like, identifying heading and then adding something like &amp;quot;below are points related to this topic...&amp;quot;) or creating nested dict (something like {&amp;quot;heading 1&amp;quot;: {&amp;quot;contents&amp;quot;: [&amp;quot;aaaa&amp;quot;, &amp;quot;bbbb&amp;quot;, ], &amp;quot;childs&amp;quot;; {}}}&lt;/p&gt;\n\n&lt;p&gt;I dont know whether these stupid ideas or not... need some help here... or please let me know if I am missing something?&lt;/p&gt;\n\n&lt;p&gt;(btw, I really hope my question make sense...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dmcws", "is_robot_indexable": true, "report_reasons": null, "author": "Tumbleweed-Afraid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dmcws/need_help_with_preprocessing_unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dmcws/need_help_with_preprocessing_unstructured_data/", "subreddit_subscribers": 111291, "created_utc": 1687199047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have implemented a Python script that validates the load of 3 critical tables every morning. What it does is a simple row count check.\n\nThe script is **very rudimentary**. I have one method per table (3). If the check fails a pre-determined criteria I send a slack message to a dedicated channel with a message containing the table name. In short:\n\n    def send_slack_message()\n    def check_table1()\n    def check_table2()\n    try:\n    check_table1()\n    check_table2()\n    else:\n    finally:\n\nI want to improve it though.  I am working on adding a check to compare today's date to a \"last update\" attribute in each of the 3 tables.\n\nWhat other kind of checks would you suggest I implement?", "author_fullname": "t2_8hos5mrg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic Load Check", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14diaq7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687189864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have implemented a Python script that validates the load of 3 critical tables every morning. What it does is a simple row count check.&lt;/p&gt;\n\n&lt;p&gt;The script is &lt;strong&gt;very rudimentary&lt;/strong&gt;. I have one method per table (3). If the check fails a pre-determined criteria I send a slack message to a dedicated channel with a message containing the table name. In short:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def send_slack_message()\ndef check_table1()\ndef check_table2()\ntry:\ncheck_table1()\ncheck_table2()\nelse:\nfinally:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I want to improve it though.  I am working on adding a check to compare today&amp;#39;s date to a &amp;quot;last update&amp;quot; attribute in each of the 3 tables.&lt;/p&gt;\n\n&lt;p&gt;What other kind of checks would you suggest I implement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14diaq7", "is_robot_indexable": true, "report_reasons": null, "author": "BatCommercial7523", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14diaq7/basic_load_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14diaq7/basic_load_check/", "subreddit_subscribers": 111291, "created_utc": 1687189864.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}