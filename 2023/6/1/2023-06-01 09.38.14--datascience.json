{"kind": "Listing", "data": {"after": "t3_13wudqk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_97r2dx3cw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is the best editor for Python in your opinion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wkkdv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 111, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 111, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685535246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wkkdv", "is_robot_indexable": true, "report_reasons": null, "author": "Bitter-Tell-8088", "discussion_type": null, "num_comments": 184, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wkkdv/which_is_the_best_editor_for_python_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wkkdv/which_is_the_best_editor_for_python_in_your/", "subreddit_subscribers": 914824, "created_utc": 1685535246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6cptvbsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "AI Basketball Referee Trained on 3000+ Images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nz0j7lzu3b3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=945ee62cfa6a179ced0057ffc57ee0482bc14153"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b5e3944f481c1f6302ce18a152a52f2ad93c459"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=620736d6a019f43d3b94717f4abed614e4083ff4"}, {"y": 361, "x": 640, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17a89b4cde3c4d647cbb1a21795dc6d0829da0a8"}, {"y": 541, "x": 960, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1361c2c0a26a6dc501db264468a3e82fec5cd7e8"}, {"y": 609, "x": 1080, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59d7a86406f8c7d9e3bbe0339e158755aff441fb"}], "s": {"y": 1706, "x": 3024, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=84fd7e17df924ad05b4f0de5a6b7fcd9d6bc039f"}, "id": "nz0j7lzu3b3b1"}, "elpboovn3b3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89e9ec3dc945416ed8775bc5ebd89a6aad5f34ea"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e6cd2098e59bf6536e8343850876a92358d450d"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbfa6d6b28e433e98c4601ae44c1af5b67595460"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa25dbd2cf00476567803ef9fe48dc540699f380"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d221f1e3946153c84bd505a5831030cadb3e302"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5a41760560a12834757c908760ac499f24c6083"}], "s": {"y": 1080, "x": 1920, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=058d2421525edc384537e03c91241bc6a64a9523"}, "id": "elpboovn3b3b1"}, "f8ejubs54b3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03e27469304d60cca4cb8acac45217694f9b3c25"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a20389ae407ec50f7198f9b984a0bf8198cd81d"}, {"y": 177, "x": 320, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05a7e53f83f3f49b8e1b9612642ecd025d905c20"}, {"y": 354, "x": 640, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6bff864483656cddb9524f3f7ad16f7cafc836f"}, {"y": 532, "x": 960, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64e57d9c79b78fa047bf33f07405b523513deb1b"}, {"y": 598, "x": 1080, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40362142505a6944c376ffa10de27a5e294342bc"}], "s": {"y": 1676, "x": 3024, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e184775674445579809504bb6c8f0e8e78599d3e"}, "id": "f8ejubs54b3b1"}}, "name": "t3_13x44iq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 67, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "I created version 2.0 of my AI Basketball Referee. I trained a custom machine-learning model with over 3000 images. ", "outbound_url": "https://youtu.be/VZgXUBi_wkM", "media_id": "f8ejubs54b3b1", "id": 282219390}, {"caption": "Tracks When Ball Is Held", "outbound_url": "https://youtu.be/VZgXUBi_wkM", "media_id": "nz0j7lzu3b3b1", "id": 282219391}, {"caption": "I created version 2.0 of my AI Basketball Referee. I trained a custom machine-learning model with over 3000 images. ", "outbound_url": "https://youtu.be/VZgXUBi_wkM", "media_id": "elpboovn3b3b1", "id": 282219392}]}, "link_flair_text": "Projects", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/djRVsBQGHpAs3VkV9F7MqU7NnaoqUYrP3isSXuJ_TLY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685581878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/13x44iq", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "13x44iq", "is_robot_indexable": true, "report_reasons": null, "author": "_ayushp_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x44iq/ai_basketball_referee_trained_on_3000_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/VZgXUBi_wkM", "subreddit_subscribers": 914824, "created_utc": 1685581878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In your jobs what data do you find most challenging to collect and wrange? Particularly interested in already structured data, but would be glad to hear any thoughts.\n\n\nThanks \ud83d\ude0a\n\nEdit: thanks for all these responses everyone, way more than I was expecting and things I hadn't even crossed my mind!", "author_fullname": "t2_aakdy8le7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most difficult data to collect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wns57", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685609559.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685543267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In your jobs what data do you find most challenging to collect and wrange? Particularly interested in already structured data, but would be glad to hear any thoughts.&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83d\ude0a&lt;/p&gt;\n\n&lt;p&gt;Edit: thanks for all these responses everyone, way more than I was expecting and things I hadn&amp;#39;t even crossed my mind!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wns57", "is_robot_indexable": true, "report_reasons": null, "author": "TipAccomplished1946", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wns57/most_difficult_data_to_collect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wns57/most_difficult_data_to_collect/", "subreddit_subscribers": 914824, "created_utc": 1685543267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I understand before one can apply Arima or Sarima for a time series, one needs to make the non stationary time series into stationary.\n\nBut making it stationary also means removing the trends and seasonality. Then how can these techniques fully capture the time series' properties? Would they be more predictive if there are components in their model that capture the trend and seasonality?\n\nSecond question, Sarima has a seasonal component, is it still necessary to make a job stationary time series stationary before running Sarima?", "author_fullname": "t2_17d42esw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused why we need to make non stationary time series stationary before applying Arima or Sarima", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x08tq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685572125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand before one can apply Arima or Sarima for a time series, one needs to make the non stationary time series into stationary.&lt;/p&gt;\n\n&lt;p&gt;But making it stationary also means removing the trends and seasonality. Then how can these techniques fully capture the time series&amp;#39; properties? Would they be more predictive if there are components in their model that capture the trend and seasonality?&lt;/p&gt;\n\n&lt;p&gt;Second question, Sarima has a seasonal component, is it still necessary to make a job stationary time series stationary before running Sarima?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x08tq", "is_robot_indexable": true, "report_reasons": null, "author": "Guyserbun007", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x08tq/confused_why_we_need_to_make_non_stationary_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x08tq/confused_why_we_need_to_make_non_stationary_time/", "subreddit_subscribers": 914824, "created_utc": 1685572125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let\u2019s say I have a variables such as transaction day part (time of day customer makes purchases) and product groups (products they buy). The silhouette score is low when I perform the initial cluster, but greatly improves when I add weights to the variables (post standardizing) putting more emphasis on day part than products. \n\nThe results look pretty good. For example, we have two afternoon clusters (our busiest time). One cluster, the customers purchase more grocery products, the other cluster purchase more clothing.\n\nThis is over simplified, but you get the point.\n\nIs it ok to add weights to variables? I googled and saw a few others ask this questions, and so far everyone has said it\u2019s ok. But I didn\u2019t see any papers about it, so I wanted to check here to get everyone\u2019s thoughts.\n\nThanks.", "author_fullname": "t2_fjll57b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ok to add weights to variables in a kmeans?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x1x1n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685576165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let\u2019s say I have a variables such as transaction day part (time of day customer makes purchases) and product groups (products they buy). The silhouette score is low when I perform the initial cluster, but greatly improves when I add weights to the variables (post standardizing) putting more emphasis on day part than products. &lt;/p&gt;\n\n&lt;p&gt;The results look pretty good. For example, we have two afternoon clusters (our busiest time). One cluster, the customers purchase more grocery products, the other cluster purchase more clothing.&lt;/p&gt;\n\n&lt;p&gt;This is over simplified, but you get the point.&lt;/p&gt;\n\n&lt;p&gt;Is it ok to add weights to variables? I googled and saw a few others ask this questions, and so far everyone has said it\u2019s ok. But I didn\u2019t see any papers about it, so I wanted to check here to get everyone\u2019s thoughts.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x1x1n", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Resort-4196", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x1x1n/is_it_ok_to_add_weights_to_variables_in_a_kmeans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x1x1n/is_it_ok_to_add_weights_to_variables_in_a_kmeans/", "subreddit_subscribers": 914824, "created_utc": 1685576165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Long story short, I\u2019m a data scientist at a company and I have yet to do any real data science. Our company has a long history of hiring outside agencies to complete DS work (and all types of work, to be frank). Since I came to the company, there is a cycle where I think of a project and propose it, leadership loves it, and then they have me start the work before abruptly hiring an agency to complete the project. They do this about a week or two after I start the work, before I\u2019ve presented any results, so it\u2019s not like I\u2019m moving \u201ctoo slow\u201d (and for reference, these agencies usually take months to deliver results).\n\nIt\u2019s very frustrating. I\u2019ve not gotten a chance to deliver on any of my ideas. I\u2019m basically an overpaid data analyst who comes up with good ideas for someone else to execute.\n\nAnyway, it\u2019s just happened again on a project that I was excited about and really got going on. My boss wants me to hand over the analysis I\u2019ve already done to an agency to complete. Usually the agency just starts from scratch, so this request is new. I personally feel it\u2019s wrong; if you want the agency to handle the project, then they should do the work, not using my hard work that I\u2019ll get no credit for.\n\nI\u2019m going to hand over my work because I don\u2019t really have a choice, my work is company property, but I\u2019m beyond irritated.", "author_fullname": "t2_7ci7himt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boss wants me to pass along my work for project completion by someone else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wsull", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685554944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, I\u2019m a data scientist at a company and I have yet to do any real data science. Our company has a long history of hiring outside agencies to complete DS work (and all types of work, to be frank). Since I came to the company, there is a cycle where I think of a project and propose it, leadership loves it, and then they have me start the work before abruptly hiring an agency to complete the project. They do this about a week or two after I start the work, before I\u2019ve presented any results, so it\u2019s not like I\u2019m moving \u201ctoo slow\u201d (and for reference, these agencies usually take months to deliver results).&lt;/p&gt;\n\n&lt;p&gt;It\u2019s very frustrating. I\u2019ve not gotten a chance to deliver on any of my ideas. I\u2019m basically an overpaid data analyst who comes up with good ideas for someone else to execute.&lt;/p&gt;\n\n&lt;p&gt;Anyway, it\u2019s just happened again on a project that I was excited about and really got going on. My boss wants me to hand over the analysis I\u2019ve already done to an agency to complete. Usually the agency just starts from scratch, so this request is new. I personally feel it\u2019s wrong; if you want the agency to handle the project, then they should do the work, not using my hard work that I\u2019ll get no credit for.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m going to hand over my work because I don\u2019t really have a choice, my work is company property, but I\u2019m beyond irritated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wsull", "is_robot_indexable": true, "report_reasons": null, "author": "njtw-1122", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wsull/boss_wants_me_to_pass_along_my_work_for_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wsull/boss_wants_me_to_pass_along_my_work_for_project/", "subreddit_subscribers": 914824, "created_utc": 1685554944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8glql2df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is the most well-known American data scientist and what field he or she is expert in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x684c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685587822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x684c", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Nerd1979", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x684c/who_is_the_most_wellknown_american_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x684c/who_is_the_most_wellknown_american_data_scientist/", "subreddit_subscribers": 914824, "created_utc": 1685587822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically just the title \\^ I'm an undergrad Applied Math + Stats student, been using R a lot for a research job I'm working. I've used Python for my econometrics course and some other courses/projects but I'll always choose R if I have a choice, even though all my friends hate on it. The syntax is structured exactly how my mind wants to transform data, and I'm also in love with the pipe operator %&gt;% or whatever it's called.\n\nAlso, my friends all hate on Matlab too but I'm in love with it...\n\nIf any of you have some favorite little-known R tricks or quirks, send them my way.", "author_fullname": "t2_jabpp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Declaring my love for R", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x8925", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685594265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically just the title ^ I&amp;#39;m an undergrad Applied Math + Stats student, been using R a lot for a research job I&amp;#39;m working. I&amp;#39;ve used Python for my econometrics course and some other courses/projects but I&amp;#39;ll always choose R if I have a choice, even though all my friends hate on it. The syntax is structured exactly how my mind wants to transform data, and I&amp;#39;m also in love with the pipe operator %&amp;gt;% or whatever it&amp;#39;s called.&lt;/p&gt;\n\n&lt;p&gt;Also, my friends all hate on Matlab too but I&amp;#39;m in love with it...&lt;/p&gt;\n\n&lt;p&gt;If any of you have some favorite little-known R tricks or quirks, send them my way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x8925", "is_robot_indexable": true, "report_reasons": null, "author": "jsh_", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x8925/declaring_my_love_for_r/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x8925/declaring_my_love_for_r/", "subreddit_subscribers": 914824, "created_utc": 1685594265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m just finished my junior year and am now in my final summer before graduation- with no internship. \n\ni feel like I\u2019m doing nothing\u2026because I am and so I don\u2019t want to waste this summer just because I didn\u2019t get an internship. \n\nWhat should I do? I know some people do projects, but how do I do that? Where do I go and how do I start? \n\nI know there are also a lot of data science boot camps- which are the most helpful and which do employers like to see? (And preferably very cheap or free) \n\nAny other suggestions?", "author_fullname": "t2_vojufwj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No internship- what should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wtfis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685556331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m just finished my junior year and am now in my final summer before graduation- with no internship. &lt;/p&gt;\n\n&lt;p&gt;i feel like I\u2019m doing nothing\u2026because I am and so I don\u2019t want to waste this summer just because I didn\u2019t get an internship. &lt;/p&gt;\n\n&lt;p&gt;What should I do? I know some people do projects, but how do I do that? Where do I go and how do I start? &lt;/p&gt;\n\n&lt;p&gt;I know there are also a lot of data science boot camps- which are the most helpful and which do employers like to see? (And preferably very cheap or free) &lt;/p&gt;\n\n&lt;p&gt;Any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wtfis", "is_robot_indexable": true, "report_reasons": null, "author": "comfy_cozy_35", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wtfis/no_internship_what_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wtfis/no_internship_what_should_i_do/", "subreddit_subscribers": 914824, "created_utc": 1685556331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was scrolling through this sub yesterday and saw a couple of posts related to what i recently went through - lay offs and freelancing. so thought i'd create a separate detailed post on how i got started freelancing and why not get back into FTE (if interested, see related post back in Jan just after getting laid off- [https://www.reddit.com/r/datascience/comments/10ivzyj/my\\_ds\\_experience\\_at\\_amazon/](https://www.reddit.com/r/datascience/comments/10ivzyj/my_ds_experience_at_amazon/)).\n\nAnyway, after my last FT job at Amazon, and having the luxury of not needing to jump back into the job market immediately, I had to choose between getting back to stable employment or exploring few things i had wanted to try out for a while now- freelancing and learning web development.\n\nEven with FTE, I had tried working on side projects on evenings/weekends after regular work hours, but with Amazon, \"regular work hours\" is a blurry line. and my projects never really got anywhere. January was also the time when chatGPT was getting popular and I had been playing around with chat-with-your-data apps using the openai LLMs. So on a whim, even before deciding anything, I created an Upwork profile. Mostly, I wanted to prove to myself that i could make money if i needed to without selling away all my time to a company.\n\nI reached out to a couple of folks on Upwork who needed help with AI chatbots, and to my surprise, within a couple of weeks, i had my first freelancing gig and soon after (thanks to the upwork algorithm), started getting jobs in my inbox. So much so that I could select what jobs i wanted to work on. Just last week, I declined a $10k contract offer just for the first phase as the client had been dragging their feet since March with \"approvals\" and the scope was unclear. I had to remind myself that i didn't want to get into another long contract which would effectively be full time employment.\n\nSince Feb, I have made $15k on Upwork without really 'applying' for any of them (all inbound requests), while being highly selective on how much time i freelance and what kind of work i want to do. I know $15k is nothing to brag about compared to a salary, but I share this as someone who 4 months back, thought freelancing was a non-option for me. To the person who asked about this yesterday, it's very doable.\n\nSo what have I been doing with the rest of my time? Spending time with my now 6-month son, traveling, and learning web development. Work-wise, I wanted to reconnect with fellow data scientists, so tried running a data science mentoring/mock interview group here but that never really took off ([https://www.reddit.com/r/datascience/comments/10uy2qi/starting\\_a\\_personalized\\_mentorship\\_group\\_for\\_data/](https://www.reddit.com/r/datascience/comments/10uy2qi/starting_a_personalized_mentorship_group_for_data/)).  So instead, I combined it with my other interest to learn web development and have built a data science mock interview app - [rightjoin.co](https://rightjoin.co). here is a short blurb-\n\n* get mock interviewed with an AI hiring manager for any company/job post/topic of choice. Warm up for your upcoming interview with questions tailored to the job posting/topic.\n* you can upload your resume and have your AI persona answer questions for you. so you get to observe how your chatGPT-version would answer questions.\n* get feedback on each response to the interviewer questions\n\nIts currently a \"phone-screen\" level interview, but i plan adding other types of interview rounds if people find this useful. And its free. please try out and let me know what you think, especially if you have an upcoming interview. here's a screenshot:\n\n[tailored mock interview by an imaginary hiring manager \\(https:\\/\\/rightjoin.co\\/\\)](https://preview.redd.it/hzycr666rc3b1.png?width=2158&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d56a4bb08b55a124e0fe447017fdc1eade3d39c0)\n\nFor me, it's been fun dabbling with web development (django, htmx), keeping up with AI wave, having the freedom to spend time with family when i feel like, and learn things that interest me. I know this may not be sustainable, but I do plan to ramp up freelancing later this year if needed to stress test that option, and only in the worst case, jump back into the job market.\n\nI understand, these options may not be viable for others, but if you do find yourself having to make additional income with freelancing or want to try out solopreneurship, it's possible. Obviously there is risk involved but also ways to manage that risk until your build your portfolio a little. the AI wave has made this easier than otherwise, but as a data scientist, why wouldn't you jump on this wave?\n\nThats it, hope this was useful for a few here and please connect if interested :)", "author_fullname": "t2_4m1ivn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My experience transitioning from full-time to freelancing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hzycr666rc3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86067ac822ba3ce9bd87d56948bd45fca248ed64"}, {"y": 126, "x": 216, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d6ba3aec51e2bb0d58c5b6b422cf078b1272465"}, {"y": 187, "x": 320, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=925e9a000c77b48d0c0799546563c2e787079053"}, {"y": 374, "x": 640, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9490aa59cd6133f7cfc70d3fb0ee62bc564ab01b"}, {"y": 561, "x": 960, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d28a38172ea630c63a6e4fe9ac1f2efe205957ee"}, {"y": 631, "x": 1080, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adb7e00c4106ede5507137585d58564017e8b8a3"}], "s": {"y": 1262, "x": 2158, "u": "https://preview.redd.it/hzycr666rc3b1.png?width=2158&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d56a4bb08b55a124e0fe447017fdc1eade3d39c0"}, "id": "hzycr666rc3b1"}}, "name": "t3_13xabtx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IMNZeyq0v7OIQhZDiV2QvV7WHqI_zoBgluNtITjYVqc.jpg", "edited": 1685606974.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685601703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was scrolling through this sub yesterday and saw a couple of posts related to what i recently went through - lay offs and freelancing. so thought i&amp;#39;d create a separate detailed post on how i got started freelancing and why not get back into FTE (if interested, see related post back in Jan just after getting laid off- &lt;a href=\"https://www.reddit.com/r/datascience/comments/10ivzyj/my_ds_experience_at_amazon/\"&gt;https://www.reddit.com/r/datascience/comments/10ivzyj/my_ds_experience_at_amazon/&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;Anyway, after my last FT job at Amazon, and having the luxury of not needing to jump back into the job market immediately, I had to choose between getting back to stable employment or exploring few things i had wanted to try out for a while now- freelancing and learning web development.&lt;/p&gt;\n\n&lt;p&gt;Even with FTE, I had tried working on side projects on evenings/weekends after regular work hours, but with Amazon, &amp;quot;regular work hours&amp;quot; is a blurry line. and my projects never really got anywhere. January was also the time when chatGPT was getting popular and I had been playing around with chat-with-your-data apps using the openai LLMs. So on a whim, even before deciding anything, I created an Upwork profile. Mostly, I wanted to prove to myself that i could make money if i needed to without selling away all my time to a company.&lt;/p&gt;\n\n&lt;p&gt;I reached out to a couple of folks on Upwork who needed help with AI chatbots, and to my surprise, within a couple of weeks, i had my first freelancing gig and soon after (thanks to the upwork algorithm), started getting jobs in my inbox. So much so that I could select what jobs i wanted to work on. Just last week, I declined a $10k contract offer just for the first phase as the client had been dragging their feet since March with &amp;quot;approvals&amp;quot; and the scope was unclear. I had to remind myself that i didn&amp;#39;t want to get into another long contract which would effectively be full time employment.&lt;/p&gt;\n\n&lt;p&gt;Since Feb, I have made $15k on Upwork without really &amp;#39;applying&amp;#39; for any of them (all inbound requests), while being highly selective on how much time i freelance and what kind of work i want to do. I know $15k is nothing to brag about compared to a salary, but I share this as someone who 4 months back, thought freelancing was a non-option for me. To the person who asked about this yesterday, it&amp;#39;s very doable.&lt;/p&gt;\n\n&lt;p&gt;So what have I been doing with the rest of my time? Spending time with my now 6-month son, traveling, and learning web development. Work-wise, I wanted to reconnect with fellow data scientists, so tried running a data science mentoring/mock interview group here but that never really took off (&lt;a href=\"https://www.reddit.com/r/datascience/comments/10uy2qi/starting_a_personalized_mentorship_group_for_data/\"&gt;https://www.reddit.com/r/datascience/comments/10uy2qi/starting_a_personalized_mentorship_group_for_data/&lt;/a&gt;).  So instead, I combined it with my other interest to learn web development and have built a data science mock interview app - &lt;a href=\"https://rightjoin.co\"&gt;rightjoin.co&lt;/a&gt;. here is a short blurb-&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;get mock interviewed with an AI hiring manager for any company/job post/topic of choice. Warm up for your upcoming interview with questions tailored to the job posting/topic.&lt;/li&gt;\n&lt;li&gt;you can upload your resume and have your AI persona answer questions for you. so you get to observe how your chatGPT-version would answer questions.&lt;/li&gt;\n&lt;li&gt;get feedback on each response to the interviewer questions&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Its currently a &amp;quot;phone-screen&amp;quot; level interview, but i plan adding other types of interview rounds if people find this useful. And its free. please try out and let me know what you think, especially if you have an upcoming interview. here&amp;#39;s a screenshot:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hzycr666rc3b1.png?width=2158&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d56a4bb08b55a124e0fe447017fdc1eade3d39c0\"&gt;tailored mock interview by an imaginary hiring manager (https://rightjoin.co/)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For me, it&amp;#39;s been fun dabbling with web development (django, htmx), keeping up with AI wave, having the freedom to spend time with family when i feel like, and learn things that interest me. I know this may not be sustainable, but I do plan to ramp up freelancing later this year if needed to stress test that option, and only in the worst case, jump back into the job market.&lt;/p&gt;\n\n&lt;p&gt;I understand, these options may not be viable for others, but if you do find yourself having to make additional income with freelancing or want to try out solopreneurship, it&amp;#39;s possible. Obviously there is risk involved but also ways to manage that risk until your build your portfolio a little. the AI wave has made this easier than otherwise, but as a data scientist, why wouldn&amp;#39;t you jump on this wave?&lt;/p&gt;\n\n&lt;p&gt;Thats it, hope this was useful for a few here and please connect if interested :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13xabtx", "is_robot_indexable": true, "report_reasons": null, "author": "sang89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13xabtx/my_experience_transitioning_from_fulltime_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13xabtx/my_experience_transitioning_from_fulltime_to/", "subreddit_subscribers": 914824, "created_utc": 1685601703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "perhaps the term outliers isnt appropriate here\n\nwhat i did is calculated the shap value for each feature, then :\n\n\\- i took the feature with highest importance\n\n\\- i took the index of each value with negative shap in this feature\n\n\\- i recreated dataset with removing these indexes\n\nsince a negative shap means a datapoint is contributing negatively to the model, i removed it\n\nmy f1score increased from 70% to 90 % ( the data was imbalanced )\n\nis this a good implementation of shap?\n\nbecause its mostly used for model interpretation but i used it for different purpose\n\nthanks for advice", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "using shap values for removing outliers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ww38i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685562625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;perhaps the term outliers isnt appropriate here&lt;/p&gt;\n\n&lt;p&gt;what i did is calculated the shap value for each feature, then :&lt;/p&gt;\n\n&lt;p&gt;- i took the feature with highest importance&lt;/p&gt;\n\n&lt;p&gt;- i took the index of each value with negative shap in this feature&lt;/p&gt;\n\n&lt;p&gt;- i recreated dataset with removing these indexes&lt;/p&gt;\n\n&lt;p&gt;since a negative shap means a datapoint is contributing negatively to the model, i removed it&lt;/p&gt;\n\n&lt;p&gt;my f1score increased from 70% to 90 % ( the data was imbalanced )&lt;/p&gt;\n\n&lt;p&gt;is this a good implementation of shap?&lt;/p&gt;\n\n&lt;p&gt;because its mostly used for model interpretation but i used it for different purpose&lt;/p&gt;\n\n&lt;p&gt;thanks for advice&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ww38i", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ww38i/using_shap_values_for_removing_outliers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ww38i/using_shap_values_for_removing_outliers/", "subreddit_subscribers": 914824, "created_utc": 1685562625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all\n\nI'm a researcher at a university and I have a project planned on urban movement and accessibility.\n\nCurrently, I'm in the planning stage of the work and need to figure out a data collection method. I am thinking of an app that allows participants of the study to eaisly share their daily movement (something like Google Maps Timeline) with us to analyse. I couldn't find any useful solutions, so hoping someone here can possibly help out with a recommendation.\n\nIdeally, the app will be able to log location, the route of movement, and time of day data of events.\n\nThanks in advance!", "author_fullname": "t2_nc9e2zv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "App Recommendations for Movement Tracking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x9lim", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685598949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a researcher at a university and I have a project planned on urban movement and accessibility.&lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m in the planning stage of the work and need to figure out a data collection method. I am thinking of an app that allows participants of the study to eaisly share their daily movement (something like Google Maps Timeline) with us to analyse. I couldn&amp;#39;t find any useful solutions, so hoping someone here can possibly help out with a recommendation.&lt;/p&gt;\n\n&lt;p&gt;Ideally, the app will be able to log location, the route of movement, and time of day data of events.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x9lim", "is_robot_indexable": true, "report_reasons": null, "author": "ruinartsocialist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x9lim/app_recommendations_for_movement_tracking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x9lim/app_recommendations_for_movement_tracking/", "subreddit_subscribers": 914824, "created_utc": 1685598949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\"If you want to be a good data scientist, you should spend \\~49% of your time developing your statistical intuition (i.e. how to ask good questions of the data), and \\~49% of your time on domain knowledge (improving overall understanding of your field). Only \\~2% on methods per se.\"\n\nNate said this back in 2019, but has repeated it in various ways since. What do you think?", "author_fullname": "t2_oa3ngk2o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you agree with this Nate Silver quote?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13xc0zt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685608277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;If you want to be a good data scientist, you should spend ~49% of your time developing your statistical intuition (i.e. how to ask good questions of the data), and ~49% of your time on domain knowledge (improving overall understanding of your field). Only ~2% on methods per se.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Nate said this back in 2019, but has repeated it in various ways since. What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13xc0zt", "is_robot_indexable": true, "report_reasons": null, "author": "chickenparmo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13xc0zt/do_you_agree_with_this_nate_silver_quote/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13xc0zt/do_you_agree_with_this_nate_silver_quote/", "subreddit_subscribers": 914824, "created_utc": 1685608277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_m6kzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenAI\u2019s Sam Altman: No GPT-5 In Training As Of Yet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13wnbkv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EASoidy_f1eAZLpaoG57zE-E4gyHxLRe6E5NiUNIPSs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685542095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/inkwater-atlas/openais-sam-altman-no-gpt-5-in-training-as-of-yet-8ddf95b9b3d6", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?auto=webp&amp;v=enabled&amp;s=d28963286cd3255df665cc8a57858a580e38352b", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a94bfb0a6757261531a6334bf53bc0a87ca6344", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45eca6e548d9d225f9943040e9f9316776d55ecc", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=222f3602a9520f7ac43e4f23be543a6851d84378", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=493adbae3aa1e88c003c0015c4034e789ed1d846", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5264174ef00a072c71ef0e309d6a8b89dbe43762", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3055ebe01fb6e531882f2e580bc54c0ae43876b", "width": 1080, "height": 720}], "variants": {}, "id": "N5lb3eq-yiwMylcDE1BYnr_U6EGw281aw2FNnsBweuc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wnbkv", "is_robot_indexable": true, "report_reasons": null, "author": "liquidocelotYT", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wnbkv/openais_sam_altman_no_gpt5_in_training_as_of_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/inkwater-atlas/openais-sam-altman-no-gpt-5-in-training-as-of-yet-8ddf95b9b3d6", "subreddit_subscribers": 914824, "created_utc": 1685542095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, does anyone know of a visualization platform that does a better job than power bi or tableau? There are typical calculations, metrics, and graphs that I use such as: seasonality graphs (x axis: months, legend: days), year on year, month-on-month, rolling averages, year-to-date, etc. would be nice to be able to do such things easily rather than having to add things to the base data or creating new fields / columns. Thank you", "author_fullname": "t2_3auev6d0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Something better than power bi or tableau", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x8t0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685596099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, does anyone know of a visualization platform that does a better job than power bi or tableau? There are typical calculations, metrics, and graphs that I use such as: seasonality graphs (x axis: months, legend: days), year on year, month-on-month, rolling averages, year-to-date, etc. would be nice to be able to do such things easily rather than having to add things to the base data or creating new fields / columns. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x8t0g", "is_robot_indexable": true, "report_reasons": null, "author": "alphamangocat", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x8t0g/something_better_than_power_bi_or_tableau/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x8t0g/something_better_than_power_bi_or_tableau/", "subreddit_subscribers": 914824, "created_utc": 1685596099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been informed that projects include, Markeing Mix Modeling and using machine learning. How should I prepare for this?", "author_fullname": "t2_47f0qg1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I be expected to know / expect to do in a data science internship for a marketing company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wvpz0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685561753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been informed that projects include, Markeing Mix Modeling and using machine learning. How should I prepare for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wvpz0", "is_robot_indexable": true, "report_reasons": null, "author": "IcyTitle1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wvpz0/what_should_i_be_expected_to_know_expect_to_do_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wvpz0/what_should_i_be_expected_to_know_expect_to_do_in/", "subreddit_subscribers": 914824, "created_utc": 1685561753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How are you supposed to randomize your groups in scenarios when certain groups can't be split up for a test? For example we may want to test conversion on local TV ad campaigns but can't randomly assign variations to different individual households within the same local network. Do we just randomize the locations? What if we only have access to a small number of locations and certain locations are already known to convert less often?", "author_fullname": "t2_dayu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AB Test Randomization with Fixed Groups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wossp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685545671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you supposed to randomize your groups in scenarios when certain groups can&amp;#39;t be split up for a test? For example we may want to test conversion on local TV ad campaigns but can&amp;#39;t randomly assign variations to different individual households within the same local network. Do we just randomize the locations? What if we only have access to a small number of locations and certain locations are already known to convert less often?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wossp", "is_robot_indexable": true, "report_reasons": null, "author": "TryWforWumbo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wossp/ab_test_randomization_with_fixed_groups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wossp/ab_test_randomization_with_fixed_groups/", "subreddit_subscribers": 914824, "created_utc": 1685545671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For example I want to use the time series of the temperature in Orlando to predict the temperature in Miami.  What are the ways I can use the Orlando daily high temperature time series to predict the current day's high temperature or forecast tomorrow's high temperature in Miami?", "author_fullname": "t2_2uajbxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the easiest ways to use one time series to predict or forecast a different time series?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13woa0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685545090.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685544489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example I want to use the time series of the temperature in Orlando to predict the temperature in Miami.  What are the ways I can use the Orlando daily high temperature time series to predict the current day&amp;#39;s high temperature or forecast tomorrow&amp;#39;s high temperature in Miami?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13woa0w", "is_robot_indexable": true, "report_reasons": null, "author": "penpapermouse", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13woa0w/what_are_the_easiest_ways_to_use_one_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13woa0w/what_are_the_easiest_ways_to_use_one_time_series/", "subreddit_subscribers": 914824, "created_utc": 1685544489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_9de03cxq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leading Data Science Events/ Summits in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 31, "top_awarded_type": null, "hide_score": false, "name": "t3_13wjt3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wJOXx2UnsMMvPT1hxgv-_sFjMgZqeXFljkLU6koQzYE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685533204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datasciencecertifications.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.datasciencecertifications.com/events", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?auto=webp&amp;v=enabled&amp;s=f61767c37724cc0b4240c0d2731559e32fd91c88", "width": 395, "height": 90}, "resolutions": [{"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4132cd92c5a841253bf488d30f12ef0be690f6fd", "width": 108, "height": 24}, {"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=912e4185fc9fec1551687db3399679432cab2bdf", "width": 216, "height": 49}, {"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a90aea8c5e70483bcf4f06c16924f861c42065fc", "width": 320, "height": 72}], "variants": {}, "id": "PKTwTdcu4YRmcmCdPjLwc-2a2R9Hk9V9yprRQ05Lrnk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wjt3v", "is_robot_indexable": true, "report_reasons": null, "author": "Palaksharma22", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wjt3v/leading_data_science_events_summits_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datasciencecertifications.com/events", "subreddit_subscribers": 914824, "created_utc": 1685533204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Other than \"Will ChatGPT took muh jerb aaahhh\" posts, I am actually wondering how will data analytics(Data Science, Data Analysis etc.) will evolve now.\n\nMy guess would be that since Large  Language Models will create code scripts so easily, industry will evolve into more domain knowledge and more maths. \n\nBasically you'll be the king if you really know maths and your domain. Coding is, even now, a secondary thing but its importance will reduce dramatically.\n\n&amp;#x200B;\n\nThat's my prediction, what is yours?", "author_fullname": "t2_9watfvxi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How will Data Analytics evolve in upcoming years? What are your predictions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13xcdze", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685609675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Other than &amp;quot;Will ChatGPT took muh jerb aaahhh&amp;quot; posts, I am actually wondering how will data analytics(Data Science, Data Analysis etc.) will evolve now.&lt;/p&gt;\n\n&lt;p&gt;My guess would be that since Large  Language Models will create code scripts so easily, industry will evolve into more domain knowledge and more maths. &lt;/p&gt;\n\n&lt;p&gt;Basically you&amp;#39;ll be the king if you really know maths and your domain. Coding is, even now, a secondary thing but its importance will reduce dramatically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s my prediction, what is yours?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13xcdze", "is_robot_indexable": true, "report_reasons": null, "author": "RadicalGuy__", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13xcdze/how_will_data_analytics_evolve_in_upcoming_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13xcdze/how_will_data_analytics_evolve_in_upcoming_years/", "subreddit_subscribers": 914824, "created_utc": 1685609675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "hello! as the title says, i will be entering premed but i have plans of shifting to statistics because i eventually want to work in data science instead of medicine. i'd like to hear your opinions about this (like if you think i'm making the right move in terms of program choice for data science or not).\n\n&amp;#x200B;\n\nbtw i've looked into the school's statistics program and they have subjects like python and i can take a minor in data science if i apply during second year", "author_fullname": "t2_16xuktd8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "incoming biology major who wants to shift to statistics because of data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13xc6qn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685608899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello! as the title says, i will be entering premed but i have plans of shifting to statistics because i eventually want to work in data science instead of medicine. i&amp;#39;d like to hear your opinions about this (like if you think i&amp;#39;m making the right move in terms of program choice for data science or not).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;btw i&amp;#39;ve looked into the school&amp;#39;s statistics program and they have subjects like python and i can take a minor in data science if i apply during second year&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13xc6qn", "is_robot_indexable": true, "report_reasons": null, "author": "Bottle_101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13xc6qn/incoming_biology_major_who_wants_to_shift_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13xc6qn/incoming_biology_major_who_wants_to_shift_to/", "subreddit_subscribers": 914824, "created_utc": 1685608899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been admitted to these two programs for a long time, but I have always been affected by various factors. So now I am still confused about how to choose these two projects.\n\nI am a student from China. My first choice is to stay and work in the US. But I know that the economic slump is hammering the job market these years. It is difficult to stay in the US as a person without a green card. So my second choice is to go back to China for employment. If I go back to China, I have some resources in the state-owned banks.\n\nI majored in environmental engineering as an undergraduate. Therefore my coding skills are not particularly wonderful. At the same time, I minored in Accounting and I had several internships related to financial quantitative investment. I am considering working in science+ds/stats OR simple quantitative finance in the future.\n\nIn my opinion, the pros of Umich: 1.rank the top (if i go back to China for job, the HRs in China value this point) 2. Applied Stats Masters Program is mature, it may offer a more refined curriculum 3.many chances to do research. The cons of Umich: 1.The location is not good, it may affect the chances for internship 2.HIGH COST:The total cost is about 170,000 US dollars, tuition is about 110,000 US dollars. 3.It may be more difficult for statistics to find a job than data science.\n\nThe pros of UCSD: [1. CHEAP](https://1.CHEAP): the total cost is about 110,000 US dollars, the tuition is about 66,000 US dollars. 2.You can choose to write a thesis, which will be more helpful for applying for a Ph.D. 3. It may be easier for data science to find a job than stats. 4. better location, either for life or for finding a job. The cons of UCSD: 1. rank slightly lower 2. Worried about my coding [skills.](https://skills.it) (it may be difficult for me to do research about Ai for science.)\n\nSincerely hope that everyone can put forward your valuable opinions on how to choose the program. Thank you so much!!!!!", "author_fullname": "t2_6oi6s3n71", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Umich Applied Stats VS UCSD MSDS (DS75)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xaxqn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685604007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been admitted to these two programs for a long time, but I have always been affected by various factors. So now I am still confused about how to choose these two projects.&lt;/p&gt;\n\n&lt;p&gt;I am a student from China. My first choice is to stay and work in the US. But I know that the economic slump is hammering the job market these years. It is difficult to stay in the US as a person without a green card. So my second choice is to go back to China for employment. If I go back to China, I have some resources in the state-owned banks.&lt;/p&gt;\n\n&lt;p&gt;I majored in environmental engineering as an undergraduate. Therefore my coding skills are not particularly wonderful. At the same time, I minored in Accounting and I had several internships related to financial quantitative investment. I am considering working in science+ds/stats OR simple quantitative finance in the future.&lt;/p&gt;\n\n&lt;p&gt;In my opinion, the pros of Umich: 1.rank the top (if i go back to China for job, the HRs in China value this point) 2. Applied Stats Masters Program is mature, it may offer a more refined curriculum 3.many chances to do research. The cons of Umich: 1.The location is not good, it may affect the chances for internship 2.HIGH COST:The total cost is about 170,000 US dollars, tuition is about 110,000 US dollars. 3.It may be more difficult for statistics to find a job than data science.&lt;/p&gt;\n\n&lt;p&gt;The pros of UCSD: &lt;a href=\"https://1.CHEAP\"&gt;1. CHEAP&lt;/a&gt;: the total cost is about 110,000 US dollars, the tuition is about 66,000 US dollars. 2.You can choose to write a thesis, which will be more helpful for applying for a Ph.D. 3. It may be easier for data science to find a job than stats. 4. better location, either for life or for finding a job. The cons of UCSD: 1. rank slightly lower 2. Worried about my coding &lt;a href=\"https://skills.it\"&gt;skills.&lt;/a&gt; (it may be difficult for me to do research about Ai for science.)&lt;/p&gt;\n\n&lt;p&gt;Sincerely hope that everyone can put forward your valuable opinions on how to choose the program. Thank you so much!!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13xaxqn", "is_robot_indexable": true, "report_reasons": null, "author": "Alinazhou_039", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13xaxqn/umich_applied_stats_vs_ucsd_msds_ds75/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13xaxqn/umich_applied_stats_vs_ucsd_msds_ds75/", "subreddit_subscribers": 914824, "created_utc": 1685604007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I have an engineering degree in computer science and data science plus a specialised master in computer science. I live in Paris (France) so things might be different than in the US. I did 2 internships as a data scientist but after graduating I was hired by a consulting company to work as a\u2026 technical support analyst.\n\nMy job is basically to enter data in a tool, verify if accounts should be merged, etc by taking requests from the users through an ITSM tool. This is honestly boring but the client company is using me as a free data analyst and making me do many others things (without getting paid for it of course) that are much more interesting. I had the opportunity to participate to an innovation contest and my data science solution got attention from the top management of the company (CEO and VP or a large group). \n\nI feel like I am currently wasting my time but I cannot find any data science position. Recently the client offered to give me a data project manager position that has nothing to do with data science. Should I leave my consulting company for them ? I fear this will not help me land a data science position one day but it is still better than my current job. However they told me I will be in charge of making projects like the one I did for the innovation contest. So aside from my data project manager activities I might be able to make data science using the data of the platform I will be responsible of. The problem is my consulting company does  not seem to try to find a suitable mission for me so I might stay forever on this boring position. What should I do ?", "author_fullname": "t2_cdfyaxay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I quit my job for a new one I am offered ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x2vux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685578927.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685578626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have an engineering degree in computer science and data science plus a specialised master in computer science. I live in Paris (France) so things might be different than in the US. I did 2 internships as a data scientist but after graduating I was hired by a consulting company to work as a\u2026 technical support analyst.&lt;/p&gt;\n\n&lt;p&gt;My job is basically to enter data in a tool, verify if accounts should be merged, etc by taking requests from the users through an ITSM tool. This is honestly boring but the client company is using me as a free data analyst and making me do many others things (without getting paid for it of course) that are much more interesting. I had the opportunity to participate to an innovation contest and my data science solution got attention from the top management of the company (CEO and VP or a large group). &lt;/p&gt;\n\n&lt;p&gt;I feel like I am currently wasting my time but I cannot find any data science position. Recently the client offered to give me a data project manager position that has nothing to do with data science. Should I leave my consulting company for them ? I fear this will not help me land a data science position one day but it is still better than my current job. However they told me I will be in charge of making projects like the one I did for the innovation contest. So aside from my data project manager activities I might be able to make data science using the data of the platform I will be responsible of. The problem is my consulting company does  not seem to try to find a suitable mission for me so I might stay forever on this boring position. What should I do ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x2vux", "is_robot_indexable": true, "report_reasons": null, "author": "SuccessfulWeb992", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x2vux/should_i_quit_my_job_for_a_new_one_i_am_offered/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x2vux/should_i_quit_my_job_for_a_new_one_i_am_offered/", "subreddit_subscribers": 914824, "created_utc": 1685578626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say you'd like to analyze behavior of users of an e-commerce shop. There are millions of daily active users. Given the compute power at hand, you have two options (or may be more?). First is to  take a small time period, say a week, and base your analysis on this one week for all users. Second is to take a much longer period, say a year, but choosing a smaller number of users. I understand that there are pros and cons to each method. But I would like to know what are the things that you take into account when making a choice.", "author_fullname": "t2_4oockqg5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subsampling: small time period or sparse sampling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wvlzr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685561492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say you&amp;#39;d like to analyze behavior of users of an e-commerce shop. There are millions of daily active users. Given the compute power at hand, you have two options (or may be more?). First is to  take a small time period, say a week, and base your analysis on this one week for all users. Second is to take a much longer period, say a year, but choosing a smaller number of users. I understand that there are pros and cons to each method. But I would like to know what are the things that you take into account when making a choice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wvlzr", "is_robot_indexable": true, "report_reasons": null, "author": "furioncruz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wvlzr/subsampling_small_time_period_or_sparse_sampling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wvlzr/subsampling_small_time_period_or_sparse_sampling/", "subreddit_subscribers": 914824, "created_utc": 1685561492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello!\n\nSo I understand that in RD, there are several tables that contain data about an object (Customers, Orders).\n\nWithin those tables, in order to easily identify say a customer or order, we use keys to uniquely identify a customer or order. Without using keys, if we used something as in DOB/Order Status to identify an object, we come across duplicates but they are different customers/orders.\n\nI am getting lost in the connection between the different tables in an RD.\n\n1.\u00a0Are connections between tables made through adding a foreign key in an existing table [(Example)](https://imgur.com/a/0nJa4L9). Or do you take at least 2 primary keys and add them to a brand new table [(Example)](https://imgur.com/a/br3iP1Q)? Or both?\n\n2. When you add a foreign key to an existing table, is the other corresponding information about that key also added or just the key. For example, if I add a Customer ID key as a foreign key to a Order table, does it only bring over the Customer ID, or its corresponding data such as Name, Billing Address, ETC?\n\nThank you!", "author_fullname": "t2_5l285b5x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relational Database - Am I misunderstanding connections", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wudqk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685558639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;So I understand that in RD, there are several tables that contain data about an object (Customers, Orders).&lt;/p&gt;\n\n&lt;p&gt;Within those tables, in order to easily identify say a customer or order, we use keys to uniquely identify a customer or order. Without using keys, if we used something as in DOB/Order Status to identify an object, we come across duplicates but they are different customers/orders.&lt;/p&gt;\n\n&lt;p&gt;I am getting lost in the connection between the different tables in an RD.&lt;/p&gt;\n\n&lt;p&gt;1.\u00a0Are connections between tables made through adding a foreign key in an existing table &lt;a href=\"https://imgur.com/a/0nJa4L9\"&gt;(Example)&lt;/a&gt;. Or do you take at least 2 primary keys and add them to a brand new table &lt;a href=\"https://imgur.com/a/br3iP1Q\"&gt;(Example)&lt;/a&gt;? Or both?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;When you add a foreign key to an existing table, is the other corresponding information about that key also added or just the key. For example, if I add a Customer ID key as a foreign key to a Order table, does it only bring over the Customer ID, or its corresponding data such as Name, Billing Address, ETC?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?auto=webp&amp;v=enabled&amp;s=40340782d497b529e2bc3ae3bcb120423d763dad", "width": 904, "height": 810}, "resolutions": [{"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5181154da4481752821a99ba300ce7fddd9bb9a", "width": 108, "height": 96}, {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1360339d8665e96394b95ac51249d3cf68f852d6", "width": 216, "height": 193}, {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbcc917c025fe4cb2147300e5384a29e69adee90", "width": 320, "height": 286}, {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=211e1d017c9462889ebfffba7c7d527dbfbc6808", "width": 640, "height": 573}], "variants": {}, "id": "C2AXxjtJcinDVuQMDGo18WD5hvEHB2xW_714EcdnkHE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wudqk", "is_robot_indexable": true, "report_reasons": null, "author": "htxastrowrld", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wudqk/relational_database_am_i_misunderstanding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wudqk/relational_database_am_i_misunderstanding/", "subreddit_subscribers": 914824, "created_utc": 1685558639.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}