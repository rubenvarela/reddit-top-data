{"kind": "Listing", "data": {"after": "t3_13wqqtv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_97r2dx3cw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is the best editor for Python in your opinion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wkkdv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 99, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 99, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685535246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wkkdv", "is_robot_indexable": true, "report_reasons": null, "author": "Bitter-Tell-8088", "discussion_type": null, "num_comments": 179, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wkkdv/which_is_the_best_editor_for_python_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wkkdv/which_is_the_best_editor_for_python_in_your/", "subreddit_subscribers": 914688, "created_utc": 1685535246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6cptvbsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "AI Basketball Referee Trained on 3000+ Images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nz0j7lzu3b3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=945ee62cfa6a179ced0057ffc57ee0482bc14153"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b5e3944f481c1f6302ce18a152a52f2ad93c459"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=620736d6a019f43d3b94717f4abed614e4083ff4"}, {"y": 361, "x": 640, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17a89b4cde3c4d647cbb1a21795dc6d0829da0a8"}, {"y": 541, "x": 960, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1361c2c0a26a6dc501db264468a3e82fec5cd7e8"}, {"y": 609, "x": 1080, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59d7a86406f8c7d9e3bbe0339e158755aff441fb"}], "s": {"y": 1706, "x": 3024, "u": "https://preview.redd.it/nz0j7lzu3b3b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=84fd7e17df924ad05b4f0de5a6b7fcd9d6bc039f"}, "id": "nz0j7lzu3b3b1"}, "elpboovn3b3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89e9ec3dc945416ed8775bc5ebd89a6aad5f34ea"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e6cd2098e59bf6536e8343850876a92358d450d"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbfa6d6b28e433e98c4601ae44c1af5b67595460"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa25dbd2cf00476567803ef9fe48dc540699f380"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d221f1e3946153c84bd505a5831030cadb3e302"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5a41760560a12834757c908760ac499f24c6083"}], "s": {"y": 1080, "x": 1920, "u": "https://preview.redd.it/elpboovn3b3b1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=058d2421525edc384537e03c91241bc6a64a9523"}, "id": "elpboovn3b3b1"}, "f8ejubs54b3b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03e27469304d60cca4cb8acac45217694f9b3c25"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a20389ae407ec50f7198f9b984a0bf8198cd81d"}, {"y": 177, "x": 320, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05a7e53f83f3f49b8e1b9612642ecd025d905c20"}, {"y": 354, "x": 640, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6bff864483656cddb9524f3f7ad16f7cafc836f"}, {"y": 532, "x": 960, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64e57d9c79b78fa047bf33f07405b523513deb1b"}, {"y": 598, "x": 1080, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40362142505a6944c376ffa10de27a5e294342bc"}], "s": {"y": 1676, "x": 3024, "u": "https://preview.redd.it/f8ejubs54b3b1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e184775674445579809504bb6c8f0e8e78599d3e"}, "id": "f8ejubs54b3b1"}}, "name": "t3_13x44iq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 45, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "I created version 2.0 of my AI Basketball Referee. I trained a custom machine-learning model with over 3000 images. ", "outbound_url": "https://youtu.be/VZgXUBi_wkM", "media_id": "f8ejubs54b3b1", "id": 282219390}, {"caption": "Tracks When Ball Is Held", "outbound_url": "https://youtu.be/VZgXUBi_wkM", "media_id": "nz0j7lzu3b3b1", "id": 282219391}, {"caption": "I created version 2.0 of my AI Basketball Referee. I trained a custom machine-learning model with over 3000 images. ", "outbound_url": "https://youtu.be/VZgXUBi_wkM", "media_id": "elpboovn3b3b1", "id": 282219392}]}, "link_flair_text": "Projects", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/djRVsBQGHpAs3VkV9F7MqU7NnaoqUYrP3isSXuJ_TLY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685581878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/13x44iq", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "13x44iq", "is_robot_indexable": true, "report_reasons": null, "author": "_ayushp_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x44iq/ai_basketball_referee_trained_on_3000_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/VZgXUBi_wkM", "subreddit_subscribers": 914688, "created_utc": 1685581878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In your jobs what data do you find most challenging to collect and wrange? Particularly interested in already structured data, but would be glad to hear any thoughts.\n\nThanks \ud83d\ude0a", "author_fullname": "t2_aakdy8le7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most difficult data to collect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wns57", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685543267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In your jobs what data do you find most challenging to collect and wrange? Particularly interested in already structured data, but would be glad to hear any thoughts.&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wns57", "is_robot_indexable": true, "report_reasons": null, "author": "TipAccomplished1946", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wns57/most_difficult_data_to_collect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wns57/most_difficult_data_to_collect/", "subreddit_subscribers": 914688, "created_utc": 1685543267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "little background: I am currently a data scientist at a small market research company. I have been a data analyst for a couple of years and then transitioned into a data scientist. My current role includes building pipelines, extracting data, cleaning, analyzing (using pandas, numpy, matplotlib) and building small models. It's not focused on ML. The work is impactful for the org (I know exactly how much revenue the product brings etc.).   \n\n\nNow I am thinking to apply for new roles. Reason - less salary and also feel like upskilling. But I'm confused if I should target product manager or core data science (ML heavy) roles. \n\n  \nI am good with people, I manage tasks well, I am also good with generating insights, and can persevere to learn new skills. I want a career that is more futuristic (given AI threats), something that has more visibility and good promotions, and also that can help me move around the world. (I'm a German, currently working in the UAE.) \n\nI feel data careers provide more hard skills that allow you to get more opportunities around the world. Can someone please give me some perspectives? Thank you", "author_fullname": "t2_o08dc9il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product Manager vs Data Scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wdyhl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685512939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;little background: I am currently a data scientist at a small market research company. I have been a data analyst for a couple of years and then transitioned into a data scientist. My current role includes building pipelines, extracting data, cleaning, analyzing (using pandas, numpy, matplotlib) and building small models. It&amp;#39;s not focused on ML. The work is impactful for the org (I know exactly how much revenue the product brings etc.).   &lt;/p&gt;\n\n&lt;p&gt;Now I am thinking to apply for new roles. Reason - less salary and also feel like upskilling. But I&amp;#39;m confused if I should target product manager or core data science (ML heavy) roles. &lt;/p&gt;\n\n&lt;p&gt;I am good with people, I manage tasks well, I am also good with generating insights, and can persevere to learn new skills. I want a career that is more futuristic (given AI threats), something that has more visibility and good promotions, and also that can help me move around the world. (I&amp;#39;m a German, currently working in the UAE.) &lt;/p&gt;\n\n&lt;p&gt;I feel data careers provide more hard skills that allow you to get more opportunities around the world. Can someone please give me some perspectives? Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wdyhl", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious-Wonder-342", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wdyhl/product_manager_vs_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wdyhl/product_manager_vs_data_scientist/", "subreddit_subscribers": 914688, "created_utc": 1685512939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I understand before one can apply Arima or Sarima for a time series, one needs to make the non stationary time series into stationary.\n\nBut making it stationary also means removing the trends and seasonality. Then how can these techniques fully capture the time series' properties? Would they be more predictive if there are components in their model that capture the trend and seasonality?\n\nSecond question, Sarima has a seasonal component, is it still necessary to make a job stationary time series stationary before running Sarima?", "author_fullname": "t2_17d42esw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused why we need to make non stationary time series stationary before applying Arima or Sarima", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x08tq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685572125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand before one can apply Arima or Sarima for a time series, one needs to make the non stationary time series into stationary.&lt;/p&gt;\n\n&lt;p&gt;But making it stationary also means removing the trends and seasonality. Then how can these techniques fully capture the time series&amp;#39; properties? Would they be more predictive if there are components in their model that capture the trend and seasonality?&lt;/p&gt;\n\n&lt;p&gt;Second question, Sarima has a seasonal component, is it still necessary to make a job stationary time series stationary before running Sarima?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x08tq", "is_robot_indexable": true, "report_reasons": null, "author": "Guyserbun007", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x08tq/confused_why_we_need_to_make_non_stationary_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x08tq/confused_why_we_need_to_make_non_stationary_time/", "subreddit_subscribers": 914688, "created_utc": 1685572125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Long story short, I\u2019m a data scientist at a company and I have yet to do any real data science. Our company has a long history of hiring outside agencies to complete DS work (and all types of work, to be frank). Since I came to the company, there is a cycle where I think of a project and propose it, leadership loves it, and then they have me start the work before abruptly hiring an agency to complete the project. They do this about a week or two after I start the work, before I\u2019ve presented any results, so it\u2019s not like I\u2019m moving \u201ctoo slow\u201d (and for reference, these agencies usually take months to deliver results).\n\nIt\u2019s very frustrating. I\u2019ve not gotten a chance to deliver on any of my ideas. I\u2019m basically an overpaid data analyst who comes up with good ideas for someone else to execute.\n\nAnyway, it\u2019s just happened again on a project that I was excited about and really got going on. My boss wants me to hand over the analysis I\u2019ve already done to an agency to complete. Usually the agency just starts from scratch, so this request is new. I personally feel it\u2019s wrong; if you want the agency to handle the project, then they should do the work, not using my hard work that I\u2019ll get no credit for.\n\nI\u2019m going to hand over my work because I don\u2019t really have a choice, my work is company property, but I\u2019m beyond irritated.", "author_fullname": "t2_7ci7himt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boss wants me to pass along my work for project completion by someone else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wsull", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685554944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, I\u2019m a data scientist at a company and I have yet to do any real data science. Our company has a long history of hiring outside agencies to complete DS work (and all types of work, to be frank). Since I came to the company, there is a cycle where I think of a project and propose it, leadership loves it, and then they have me start the work before abruptly hiring an agency to complete the project. They do this about a week or two after I start the work, before I\u2019ve presented any results, so it\u2019s not like I\u2019m moving \u201ctoo slow\u201d (and for reference, these agencies usually take months to deliver results).&lt;/p&gt;\n\n&lt;p&gt;It\u2019s very frustrating. I\u2019ve not gotten a chance to deliver on any of my ideas. I\u2019m basically an overpaid data analyst who comes up with good ideas for someone else to execute.&lt;/p&gt;\n\n&lt;p&gt;Anyway, it\u2019s just happened again on a project that I was excited about and really got going on. My boss wants me to hand over the analysis I\u2019ve already done to an agency to complete. Usually the agency just starts from scratch, so this request is new. I personally feel it\u2019s wrong; if you want the agency to handle the project, then they should do the work, not using my hard work that I\u2019ll get no credit for.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m going to hand over my work because I don\u2019t really have a choice, my work is company property, but I\u2019m beyond irritated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wsull", "is_robot_indexable": true, "report_reasons": null, "author": "njtw-1122", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wsull/boss_wants_me_to_pass_along_my_work_for_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wsull/boss_wants_me_to_pass_along_my_work_for_project/", "subreddit_subscribers": 914688, "created_utc": 1685554944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let\u2019s say I have a variables such as transaction day part (time of day customer makes purchases) and product groups (products they buy). The silhouette score is low when I perform the initial cluster, but greatly improves when I add weights to the variables (post standardizing) putting more emphasis on day part than products. \n\nThe results look pretty good. For example, we have two afternoon clusters (our busiest time). One cluster, the customers purchase more grocery products, the other cluster purchase more clothing.\n\nThis is over simplified, but you get the point.\n\nIs it ok to add weights to variables? I googled and saw a few others ask this questions, and so far everyone has said it\u2019s ok. But I didn\u2019t see any papers about it, so I wanted to check here to get everyone\u2019s thoughts.\n\nThanks.", "author_fullname": "t2_fjll57b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ok to add weights to variables in a kmeans?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x1x1n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685576165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let\u2019s say I have a variables such as transaction day part (time of day customer makes purchases) and product groups (products they buy). The silhouette score is low when I perform the initial cluster, but greatly improves when I add weights to the variables (post standardizing) putting more emphasis on day part than products. &lt;/p&gt;\n\n&lt;p&gt;The results look pretty good. For example, we have two afternoon clusters (our busiest time). One cluster, the customers purchase more grocery products, the other cluster purchase more clothing.&lt;/p&gt;\n\n&lt;p&gt;This is over simplified, but you get the point.&lt;/p&gt;\n\n&lt;p&gt;Is it ok to add weights to variables? I googled and saw a few others ask this questions, and so far everyone has said it\u2019s ok. But I didn\u2019t see any papers about it, so I wanted to check here to get everyone\u2019s thoughts.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x1x1n", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Resort-4196", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x1x1n/is_it_ok_to_add_weights_to_variables_in_a_kmeans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x1x1n/is_it_ok_to_add_weights_to_variables_in_a_kmeans/", "subreddit_subscribers": 914688, "created_utc": 1685576165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m just finished my junior year and am now in my final summer before graduation- with no internship. \n\ni feel like I\u2019m doing nothing\u2026because I am and so I don\u2019t want to waste this summer just because I didn\u2019t get an internship. \n\nWhat should I do? I know some people do projects, but how do I do that? Where do I go and how do I start? \n\nI know there are also a lot of data science boot camps- which are the most helpful and which do employers like to see? (And preferably very cheap or free) \n\nAny other suggestions?", "author_fullname": "t2_vojufwj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No internship- what should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wtfis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685556331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m just finished my junior year and am now in my final summer before graduation- with no internship. &lt;/p&gt;\n\n&lt;p&gt;i feel like I\u2019m doing nothing\u2026because I am and so I don\u2019t want to waste this summer just because I didn\u2019t get an internship. &lt;/p&gt;\n\n&lt;p&gt;What should I do? I know some people do projects, but how do I do that? Where do I go and how do I start? &lt;/p&gt;\n\n&lt;p&gt;I know there are also a lot of data science boot camps- which are the most helpful and which do employers like to see? (And preferably very cheap or free) &lt;/p&gt;\n\n&lt;p&gt;Any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wtfis", "is_robot_indexable": true, "report_reasons": null, "author": "comfy_cozy_35", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wtfis/no_internship_what_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wtfis/no_internship_what_should_i_do/", "subreddit_subscribers": 914688, "created_utc": 1685556331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "perhaps the term outliers isnt appropriate here\n\nwhat i did is calculated the shap value for each feature, then :\n\n\\- i took the feature with highest importance\n\n\\- i took the index of each value with negative shap in this feature\n\n\\- i recreated dataset with removing these indexes\n\nsince a negative shap means a datapoint is contributing negatively to the model, i removed it\n\nmy f1score increased from 70% to 90 % ( the data was imbalanced )\n\nis this a good implementation of shap?\n\nbecause its mostly used for model interpretation but i used it for different purpose\n\nthanks for advice", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "using shap values for removing outliers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ww38i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685562625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;perhaps the term outliers isnt appropriate here&lt;/p&gt;\n\n&lt;p&gt;what i did is calculated the shap value for each feature, then :&lt;/p&gt;\n\n&lt;p&gt;- i took the feature with highest importance&lt;/p&gt;\n\n&lt;p&gt;- i took the index of each value with negative shap in this feature&lt;/p&gt;\n\n&lt;p&gt;- i recreated dataset with removing these indexes&lt;/p&gt;\n\n&lt;p&gt;since a negative shap means a datapoint is contributing negatively to the model, i removed it&lt;/p&gt;\n\n&lt;p&gt;my f1score increased from 70% to 90 % ( the data was imbalanced )&lt;/p&gt;\n\n&lt;p&gt;is this a good implementation of shap?&lt;/p&gt;\n\n&lt;p&gt;because its mostly used for model interpretation but i used it for different purpose&lt;/p&gt;\n\n&lt;p&gt;thanks for advice&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ww38i", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ww38i/using_shap_values_for_removing_outliers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ww38i/using_shap_values_for_removing_outliers/", "subreddit_subscribers": 914688, "created_utc": 1685562625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been working as an ETL developer for a while, using Cloud Data Integration for building ETL pipelines and Cloud Data Warehouse for SQL querying. But now, I'm really keen on making a switch to a data science role. I know there's a bunch of hard skills I need to pick up before I can land a job in this field. So, I wanted to jump in here and ask for some advice on the best way to go about it.\n\nWhat's the recommended order or sequence of things I should learn and practice to build a solid profile that will impress potential employers? Any specific resources, courses, or projects you'd suggest? I'm all ears for your insights and personal experiences!\n\nThanks in advance", "author_fullname": "t2_oumsxias", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from ETL Developer to Data Science: Seeking Advice on Skill Development and Learning Path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13weulc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685516229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as an ETL developer for a while, using Cloud Data Integration for building ETL pipelines and Cloud Data Warehouse for SQL querying. But now, I&amp;#39;m really keen on making a switch to a data science role. I know there&amp;#39;s a bunch of hard skills I need to pick up before I can land a job in this field. So, I wanted to jump in here and ask for some advice on the best way to go about it.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the recommended order or sequence of things I should learn and practice to build a solid profile that will impress potential employers? Any specific resources, courses, or projects you&amp;#39;d suggest? I&amp;#39;m all ears for your insights and personal experiences!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13weulc", "is_robot_indexable": true, "report_reasons": null, "author": "vidit_108", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13weulc/transitioning_from_etl_developer_to_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13weulc/transitioning_from_etl_developer_to_data_science/", "subreddit_subscribers": 914688, "created_utc": 1685516229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_m6kzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenAI\u2019s Sam Altman: No GPT-5 In Training As Of Yet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13wnbkv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EASoidy_f1eAZLpaoG57zE-E4gyHxLRe6E5NiUNIPSs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685542095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/inkwater-atlas/openais-sam-altman-no-gpt-5-in-training-as-of-yet-8ddf95b9b3d6", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?auto=webp&amp;v=enabled&amp;s=d28963286cd3255df665cc8a57858a580e38352b", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a94bfb0a6757261531a6334bf53bc0a87ca6344", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45eca6e548d9d225f9943040e9f9316776d55ecc", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=222f3602a9520f7ac43e4f23be543a6851d84378", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=493adbae3aa1e88c003c0015c4034e789ed1d846", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5264174ef00a072c71ef0e309d6a8b89dbe43762", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/8zCuhNYuZ4e6m_2NbxAywt77jG8JLrLN4hZv8SYByjI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3055ebe01fb6e531882f2e580bc54c0ae43876b", "width": 1080, "height": 720}], "variants": {}, "id": "N5lb3eq-yiwMylcDE1BYnr_U6EGw281aw2FNnsBweuc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wnbkv", "is_robot_indexable": true, "report_reasons": null, "author": "liquidocelotYT", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wnbkv/openais_sam_altman_no_gpt5_in_training_as_of_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/inkwater-atlas/openais-sam-altman-no-gpt-5-in-training-as-of-yet-8ddf95b9b3d6", "subreddit_subscribers": 914688, "created_utc": 1685542095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically just the title \\^ I'm an undergrad Applied Math + Stats student, been using R a lot for a research job I'm working. I've used Python for my econometrics course and some other courses/projects but I'll always choose R if I have a choice, even though all my friends hate on it. The syntax is structured exactly how my mind wants to transform data, and I'm also in love with the pipe operator %&gt;% or whatever it's called.\n\nAlso, my friends all hate on Matlab too but I'm in love with it...\n\nIf any of you have some favorite little-known R tricks or quirks, send them my way.", "author_fullname": "t2_jabpp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Declaring my love for R", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13x8925", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685594265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically just the title ^ I&amp;#39;m an undergrad Applied Math + Stats student, been using R a lot for a research job I&amp;#39;m working. I&amp;#39;ve used Python for my econometrics course and some other courses/projects but I&amp;#39;ll always choose R if I have a choice, even though all my friends hate on it. The syntax is structured exactly how my mind wants to transform data, and I&amp;#39;m also in love with the pipe operator %&amp;gt;% or whatever it&amp;#39;s called.&lt;/p&gt;\n\n&lt;p&gt;Also, my friends all hate on Matlab too but I&amp;#39;m in love with it...&lt;/p&gt;\n\n&lt;p&gt;If any of you have some favorite little-known R tricks or quirks, send them my way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x8925", "is_robot_indexable": true, "report_reasons": null, "author": "jsh_", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x8925/declaring_my_love_for_r/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x8925/declaring_my_love_for_r/", "subreddit_subscribers": 914688, "created_utc": 1685594265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8glql2df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is the most well-known American data scientist and what field he or she is expert in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x684c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685587822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x684c", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Nerd1979", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x684c/who_is_the_most_wellknown_american_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x684c/who_is_the_most_wellknown_american_data_scientist/", "subreddit_subscribers": 914688, "created_utc": 1685587822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I have an engineering degree in computer science and data science plus a specialised master in computer science. I live in Paris (France) so things might be different than in the US. I did 2 internships as a data scientist but after graduating I was hired by a consulting company to work as a\u2026 technical support analyst.\n\nMy job is basically to enter data in a tool, verify if accounts should be merged, etc by taking requests from the users through an ITSM tool. This is honestly boring but the client company is using me as a free data analyst and making me do many others things (without getting paid for it of course) that are much more interesting. I had the opportunity to participate to an innovation contest and my data science solution got attention from the top management of the company (CEO and VP or a large group). \n\nI feel like I am currently wasting my time but I cannot find any data science position. Recently the client offered to give me a data project manager position that has nothing to do with data science. Should I leave my consulting company for them ? I fear this will not help me land a data science position one day but it is still better than my current job. However they told me I will be in charge of making projects like the one I did for the innovation contest. So aside from my data project manager activities I might be able to make data science using the data of the platform I will be responsible of. The problem is my consulting company does  not seem to try to find a suitable mission for me so I might stay forever on this boring position. What should I do ?", "author_fullname": "t2_cdfyaxay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I quit my job for a new one I am offered ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x2vux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685578927.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685578626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have an engineering degree in computer science and data science plus a specialised master in computer science. I live in Paris (France) so things might be different than in the US. I did 2 internships as a data scientist but after graduating I was hired by a consulting company to work as a\u2026 technical support analyst.&lt;/p&gt;\n\n&lt;p&gt;My job is basically to enter data in a tool, verify if accounts should be merged, etc by taking requests from the users through an ITSM tool. This is honestly boring but the client company is using me as a free data analyst and making me do many others things (without getting paid for it of course) that are much more interesting. I had the opportunity to participate to an innovation contest and my data science solution got attention from the top management of the company (CEO and VP or a large group). &lt;/p&gt;\n\n&lt;p&gt;I feel like I am currently wasting my time but I cannot find any data science position. Recently the client offered to give me a data project manager position that has nothing to do with data science. Should I leave my consulting company for them ? I fear this will not help me land a data science position one day but it is still better than my current job. However they told me I will be in charge of making projects like the one I did for the innovation contest. So aside from my data project manager activities I might be able to make data science using the data of the platform I will be responsible of. The problem is my consulting company does  not seem to try to find a suitable mission for me so I might stay forever on this boring position. What should I do ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13x2vux", "is_robot_indexable": true, "report_reasons": null, "author": "SuccessfulWeb992", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13x2vux/should_i_quit_my_job_for_a_new_one_i_am_offered/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13x2vux/should_i_quit_my_job_for_a_new_one_i_am_offered/", "subreddit_subscribers": 914688, "created_utc": 1685578626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been informed that projects include, Markeing Mix Modeling and using machine learning. How should I prepare for this?", "author_fullname": "t2_47f0qg1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I be expected to know / expect to do in a data science internship for a marketing company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wvpz0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685561753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been informed that projects include, Markeing Mix Modeling and using machine learning. How should I prepare for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wvpz0", "is_robot_indexable": true, "report_reasons": null, "author": "IcyTitle1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wvpz0/what_should_i_be_expected_to_know_expect_to_do_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wvpz0/what_should_i_be_expected_to_know_expect_to_do_in/", "subreddit_subscribers": 914688, "created_utc": 1685561753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How are you supposed to randomize your groups in scenarios when certain groups can't be split up for a test? For example we may want to test conversion on local TV ad campaigns but can't randomly assign variations to different individual households within the same local network. Do we just randomize the locations? What if we only have access to a small number of locations and certain locations are already known to convert less often?", "author_fullname": "t2_dayu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AB Test Randomization with Fixed Groups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wossp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685545671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you supposed to randomize your groups in scenarios when certain groups can&amp;#39;t be split up for a test? For example we may want to test conversion on local TV ad campaigns but can&amp;#39;t randomly assign variations to different individual households within the same local network. Do we just randomize the locations? What if we only have access to a small number of locations and certain locations are already known to convert less often?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wossp", "is_robot_indexable": true, "report_reasons": null, "author": "TryWforWumbo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wossp/ab_test_randomization_with_fixed_groups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wossp/ab_test_randomization_with_fixed_groups/", "subreddit_subscribers": 914688, "created_utc": 1685545671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For example I want to use the time series of the temperature in Orlando to predict the temperature in Miami.  What are the ways I can use the Orlando daily high temperature time series to predict the current day's high temperature or forecast tomorrow's high temperature in Miami?", "author_fullname": "t2_2uajbxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the easiest ways to use one time series to predict or forecast a different time series?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13woa0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685545090.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685544489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example I want to use the time series of the temperature in Orlando to predict the temperature in Miami.  What are the ways I can use the Orlando daily high temperature time series to predict the current day&amp;#39;s high temperature or forecast tomorrow&amp;#39;s high temperature in Miami?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13woa0w", "is_robot_indexable": true, "report_reasons": null, "author": "penpapermouse", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13woa0w/what_are_the_easiest_ways_to_use_one_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13woa0w/what_are_the_easiest_ways_to_use_one_time_series/", "subreddit_subscribers": 914688, "created_utc": 1685544489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_9de03cxq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leading Data Science Events/ Summits in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 31, "top_awarded_type": null, "hide_score": false, "name": "t3_13wjt3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wJOXx2UnsMMvPT1hxgv-_sFjMgZqeXFljkLU6koQzYE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685533204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datasciencecertifications.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.datasciencecertifications.com/events", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?auto=webp&amp;v=enabled&amp;s=f61767c37724cc0b4240c0d2731559e32fd91c88", "width": 395, "height": 90}, "resolutions": [{"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4132cd92c5a841253bf488d30f12ef0be690f6fd", "width": 108, "height": 24}, {"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=912e4185fc9fec1551687db3399679432cab2bdf", "width": 216, "height": 49}, {"url": "https://external-preview.redd.it/ouIguqrAcqAYq8xPs6WL6zwHTJx3wQz2XnEEMjPx33o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a90aea8c5e70483bcf4f06c16924f861c42065fc", "width": 320, "height": 72}], "variants": {}, "id": "PKTwTdcu4YRmcmCdPjLwc-2a2R9Hk9V9yprRQ05Lrnk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wjt3v", "is_robot_indexable": true, "report_reasons": null, "author": "Palaksharma22", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wjt3v/leading_data_science_events_summits_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datasciencecertifications.com/events", "subreddit_subscribers": 914688, "created_utc": 1685533204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know this may not be the appropriate sub for this kind of question, but I am lost and discouraged and could really use your help. For such a landmark paper on the field(OOD or out of distribution), I didn't see much supplementary materials or articles on the internet explaining it.\n\nMaybe it's that simple and easy and I should probably leave this field, but that's for another time. I'll leave after understanding this paper. \n\nhere's the link if anyone is interested https://arxiv.org/pdf/1610.02136.pdf\n\n\nI understand PR Curves and ROC curves and softmax, but I just can't seem to follow what they are doing.\n\n-THe whole convoluted set up of why they have separate metrics for correctly classifying whether the classifier that gets the answer correct and another two separate metrics of distinguishing in distribution datasets and out-of-distribution datasets. \n \n-For example, what does the value/score in even mean? I'm guessing value is the Area under the Curve, but what's the score? The base rate of the classes or something? https://d3i71xaburhd42.cloudfront.net/6ff2a434578ff2746b9283e45abf296887f48a2d/4-Table2-1.png\n\n-And I can't even seem to understand how they classify the out of distribution samples with just using the softmax without some sort of thresholding. Since the metric is Area under the PR curve and Area under the ROC curve across all imaginary thresholding so I'm guessing there is no need for thresholding? \n\n-and why do they take the negative scores of the out-of-distribution(OOD) test samples softmax output to determine if it is OOD or not?\n\nWould really appreciate the help if possible. I think it's supposed to be a very easy paper which is discouraging but that's for another time....\n\nAnd sorry about the long winded rant, you will not understand my rambling unless you read the paper. But it's a short read and not math or tech heavy so It shouldnt take too much time. Would really appreciate the input of someone smarter than me. \nthanks in advance.", "author_fullname": "t2_163jio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone read an old paper called \"Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13webkb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685514278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this may not be the appropriate sub for this kind of question, but I am lost and discouraged and could really use your help. For such a landmark paper on the field(OOD or out of distribution), I didn&amp;#39;t see much supplementary materials or articles on the internet explaining it.&lt;/p&gt;\n\n&lt;p&gt;Maybe it&amp;#39;s that simple and easy and I should probably leave this field, but that&amp;#39;s for another time. I&amp;#39;ll leave after understanding this paper. &lt;/p&gt;\n\n&lt;p&gt;here&amp;#39;s the link if anyone is interested &lt;a href=\"https://arxiv.org/pdf/1610.02136.pdf\"&gt;https://arxiv.org/pdf/1610.02136.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I understand PR Curves and ROC curves and softmax, but I just can&amp;#39;t seem to follow what they are doing.&lt;/p&gt;\n\n&lt;p&gt;-THe whole convoluted set up of why they have separate metrics for correctly classifying whether the classifier that gets the answer correct and another two separate metrics of distinguishing in distribution datasets and out-of-distribution datasets. &lt;/p&gt;\n\n&lt;p&gt;-For example, what does the value/score in even mean? I&amp;#39;m guessing value is the Area under the Curve, but what&amp;#39;s the score? The base rate of the classes or something? &lt;a href=\"https://d3i71xaburhd42.cloudfront.net/6ff2a434578ff2746b9283e45abf296887f48a2d/4-Table2-1.png\"&gt;https://d3i71xaburhd42.cloudfront.net/6ff2a434578ff2746b9283e45abf296887f48a2d/4-Table2-1.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;-And I can&amp;#39;t even seem to understand how they classify the out of distribution samples with just using the softmax without some sort of thresholding. Since the metric is Area under the PR curve and Area under the ROC curve across all imaginary thresholding so I&amp;#39;m guessing there is no need for thresholding? &lt;/p&gt;\n\n&lt;p&gt;-and why do they take the negative scores of the out-of-distribution(OOD) test samples softmax output to determine if it is OOD or not?&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate the help if possible. I think it&amp;#39;s supposed to be a very easy paper which is discouraging but that&amp;#39;s for another time....&lt;/p&gt;\n\n&lt;p&gt;And sorry about the long winded rant, you will not understand my rambling unless you read the paper. But it&amp;#39;s a short read and not math or tech heavy so It shouldnt take too much time. Would really appreciate the input of someone smarter than me. \nthanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13webkb", "is_robot_indexable": true, "report_reasons": null, "author": "THE_REAL_ODB", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13webkb/has_anyone_read_an_old_paper_called_baseline_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13webkb/has_anyone_read_an_old_paper_called_baseline_for/", "subreddit_subscribers": 914688, "created_utc": 1685514278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say you'd like to analyze behavior of users of an e-commerce shop. There are millions of daily active users. Given the compute power at hand, you have two options (or may be more?). First is to  take a small time period, say a week, and base your analysis on this one week for all users. Second is to take a much longer period, say a year, but choosing a smaller number of users. I understand that there are pros and cons to each method. But I would like to know what are the things that you take into account when making a choice.", "author_fullname": "t2_4oockqg5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subsampling: small time period or sparse sampling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wvlzr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685561492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say you&amp;#39;d like to analyze behavior of users of an e-commerce shop. There are millions of daily active users. Given the compute power at hand, you have two options (or may be more?). First is to  take a small time period, say a week, and base your analysis on this one week for all users. Second is to take a much longer period, say a year, but choosing a smaller number of users. I understand that there are pros and cons to each method. But I would like to know what are the things that you take into account when making a choice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wvlzr", "is_robot_indexable": true, "report_reasons": null, "author": "furioncruz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wvlzr/subsampling_small_time_period_or_sparse_sampling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wvlzr/subsampling_small_time_period_or_sparse_sampling/", "subreddit_subscribers": 914688, "created_utc": 1685561492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello!\n\nSo I understand that in RD, there are several tables that contain data about an object (Customers, Orders).\n\nWithin those tables, in order to easily identify say a customer or order, we use keys to uniquely identify a customer or order. Without using keys, if we used something as in DOB/Order Status to identify an object, we come across duplicates but they are different customers/orders.\n\nI am getting lost in the connection between the different tables in an RD.\n\n1.\u00a0Are connections between tables made through adding a foreign key in an existing table [(Example)](https://imgur.com/a/0nJa4L9). Or do you take at least 2 primary keys and add them to a brand new table [(Example)](https://imgur.com/a/br3iP1Q)? Or both?\n\n2. When you add a foreign key to an existing table, is the other corresponding information about that key also added or just the key. For example, if I add a Customer ID key as a foreign key to a Order table, does it only bring over the Customer ID, or its corresponding data such as Name, Billing Address, ETC?\n\nThank you!", "author_fullname": "t2_5l285b5x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relational Database - Am I misunderstanding connections", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wudqk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685558639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;So I understand that in RD, there are several tables that contain data about an object (Customers, Orders).&lt;/p&gt;\n\n&lt;p&gt;Within those tables, in order to easily identify say a customer or order, we use keys to uniquely identify a customer or order. Without using keys, if we used something as in DOB/Order Status to identify an object, we come across duplicates but they are different customers/orders.&lt;/p&gt;\n\n&lt;p&gt;I am getting lost in the connection between the different tables in an RD.&lt;/p&gt;\n\n&lt;p&gt;1.\u00a0Are connections between tables made through adding a foreign key in an existing table &lt;a href=\"https://imgur.com/a/0nJa4L9\"&gt;(Example)&lt;/a&gt;. Or do you take at least 2 primary keys and add them to a brand new table &lt;a href=\"https://imgur.com/a/br3iP1Q\"&gt;(Example)&lt;/a&gt;? Or both?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;When you add a foreign key to an existing table, is the other corresponding information about that key also added or just the key. For example, if I add a Customer ID key as a foreign key to a Order table, does it only bring over the Customer ID, or its corresponding data such as Name, Billing Address, ETC?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?auto=webp&amp;v=enabled&amp;s=40340782d497b529e2bc3ae3bcb120423d763dad", "width": 904, "height": 810}, "resolutions": [{"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5181154da4481752821a99ba300ce7fddd9bb9a", "width": 108, "height": 96}, {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1360339d8665e96394b95ac51249d3cf68f852d6", "width": 216, "height": 193}, {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbcc917c025fe4cb2147300e5384a29e69adee90", "width": 320, "height": 286}, {"url": "https://external-preview.redd.it/79oly1-JX2mHcnc7i8ofImEx_0Aim0Mow55u3iEX4ZA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=211e1d017c9462889ebfffba7c7d527dbfbc6808", "width": 640, "height": 573}], "variants": {}, "id": "C2AXxjtJcinDVuQMDGo18WD5hvEHB2xW_714EcdnkHE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wudqk", "is_robot_indexable": true, "report_reasons": null, "author": "htxastrowrld", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wudqk/relational_database_am_i_misunderstanding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wudqk/relational_database_am_i_misunderstanding/", "subreddit_subscribers": 914688, "created_utc": 1685558639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI have been doing data science for a little while now with no particular area of focus. I have worked on projects like record linkage, predicting sales volume, service ticket volume, and manufacturing defects but never causal inference. I recently got asked at work if I could take on a causal inference project and I want to check with all of you if the idea is feasible assuming we have the data to do it. The ask is to use causal inference to try and determine how many new clients we obtained because of our new product or how important it was to the acquisition of those new clients even if we cannot definitively say how many signed up because of it. Is this the correct application of causal inference?", "author_fullname": "t2_zivdx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First Causal Inference Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wsll5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685554334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have been doing data science for a little while now with no particular area of focus. I have worked on projects like record linkage, predicting sales volume, service ticket volume, and manufacturing defects but never causal inference. I recently got asked at work if I could take on a causal inference project and I want to check with all of you if the idea is feasible assuming we have the data to do it. The ask is to use causal inference to try and determine how many new clients we obtained because of our new product or how important it was to the acquisition of those new clients even if we cannot definitively say how many signed up because of it. Is this the correct application of causal inference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wsll5", "is_robot_indexable": true, "report_reasons": null, "author": "Meclimax", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wsll5/first_causal_inference_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wsll5/first_causal_inference_project/", "subreddit_subscribers": 914688, "created_utc": 1685554334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All. I\u2019m wondering what is the possible way to feed huge text to LLM so I could query it with custom questions and it will use the context of the entire document for each question or query?\n\nUnfortunately, all articles that I\u2019ve recently come across described the workflow on enhancing ChatGPT with custom text via the reduced summary of the document and prompting it for each query to keep the context. It does not look an option for me since the text is huge and I want to keep the details of the original document so the LLM can use it while building the answer.\n\nSo, I support the solution is to take some general-purpose LLM and fine-tune it (retrain) with my text to enhance the model? \nAny other solutions for the problem? Which LLM to use for that (any lightweight one that does not require huge resources to host it)?\n\nThanks!", "author_fullname": "t2_1z5jdh5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Augmenting LLM with my data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wrgiv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685551707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All. I\u2019m wondering what is the possible way to feed huge text to LLM so I could query it with custom questions and it will use the context of the entire document for each question or query?&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, all articles that I\u2019ve recently come across described the workflow on enhancing ChatGPT with custom text via the reduced summary of the document and prompting it for each query to keep the context. It does not look an option for me since the text is huge and I want to keep the details of the original document so the LLM can use it while building the answer.&lt;/p&gt;\n\n&lt;p&gt;So, I support the solution is to take some general-purpose LLM and fine-tune it (retrain) with my text to enhance the model? \nAny other solutions for the problem? Which LLM to use for that (any lightweight one that does not require huge resources to host it)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wrgiv", "is_robot_indexable": true, "report_reasons": null, "author": "Greg_Z_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wrgiv/augmenting_llm_with_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wrgiv/augmenting_llm_with_my_data/", "subreddit_subscribers": 914688, "created_utc": 1685551707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work on a data engineering team and love what I do and just graduated with a masters in data analytics which included several machine learning courses (done part time while I was still working full time). \nWe have a weekly data team training where someone in the department will present on a topic of their choice and the person coordinating has been asking me to present on a data science topic now that I finished my degree. \nWhat are a couple suggestions for quick but still interesting concepts that are easy to explain in a half hour session?\nThanks!", "author_fullname": "t2_bfa6tlx6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to Give Lessons On", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wrfym", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685551673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a data engineering team and love what I do and just graduated with a masters in data analytics which included several machine learning courses (done part time while I was still working full time). \nWe have a weekly data team training where someone in the department will present on a topic of their choice and the person coordinating has been asking me to present on a data science topic now that I finished my degree. \nWhat are a couple suggestions for quick but still interesting concepts that are easy to explain in a half hour session?\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wrfym", "is_robot_indexable": true, "report_reasons": null, "author": "Lost_Source824", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wrfym/what_to_give_lessons_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wrfym/what_to_give_lessons_on/", "subreddit_subscribers": 914688, "created_utc": 1685551673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What do you do if you have 1-4 observations on a hundred separate occasions (it is basically event starts and has 7 steps with an exponentialish increase and then it ends and there are a few other features that influence it at each time period), what type of models can you use? \n\nMy instinct is to just line it up as a regression problem and put the observations as features, but I've been wondering if there is a better way to do it (I know there is a lot of research into arma/arima, can you use it for separate instances and combine it with other factors at each time step).\n\nI also feel like there might be some manufacturing optimisation model, but I haven't seen a practical suggestion", "author_fullname": "t2_23bi1gsr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do with a bunch of small time series data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wqqtv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685550083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you do if you have 1-4 observations on a hundred separate occasions (it is basically event starts and has 7 steps with an exponentialish increase and then it ends and there are a few other features that influence it at each time period), what type of models can you use? &lt;/p&gt;\n\n&lt;p&gt;My instinct is to just line it up as a regression problem and put the observations as features, but I&amp;#39;ve been wondering if there is a better way to do it (I know there is a lot of research into arma/arima, can you use it for separate instances and combine it with other factors at each time step).&lt;/p&gt;\n\n&lt;p&gt;I also feel like there might be some manufacturing optimisation model, but I haven&amp;#39;t seen a practical suggestion&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wqqtv", "is_robot_indexable": true, "report_reasons": null, "author": "Alienbushman", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wqqtv/what_do_you_do_with_a_bunch_of_small_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wqqtv/what_do_you_do_with_a_bunch_of_small_time_series/", "subreddit_subscribers": 914688, "created_utc": 1685550083.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}