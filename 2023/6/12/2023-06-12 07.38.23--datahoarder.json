{"kind": "Listing", "data": {"after": "t3_1470mj5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, we'll keep this short, you already know what's going on.\n\nAs you've almost certainly heard by now Reddit is locking down their API starting July 1st with the introduction of paid usage. These changes are what killed pushshift.io (full reddit archives and searchable api used by mods and many research/academic papers) and what will kill most (if not all) third-party reddit clients. This is obviously a detriment to everyone, and while Reddit will almost certainly go through with these changes regardless, thousands of subreddits are going to be participating in a 2-day (or longer) blackout. You can read more about the blackouts at r/ModCoord. At the very least, the planned blackout seems to have convinced Reddit to give free API access to accessibility clients. Hopefully it can change their minds further.\n\nr/DataHoarder will be locked for an undetermined amount of time, see [this thread](https://old.reddit.com/r/DataHoarder/comments/1479c7b/historic_reddit_archives_ongoing_archival_effort) for reddit data archives, tools, etc. we will also be using this time to update our sidebar links and do some general maintenance in the hopes that this mess doesn't mean the end for us and the many communities that see this as a killing of the Reddit we have loved over the years. \n\n**Note; during this time no new posts can be made and all comments are black-holed.** \n--------\n\n~ The Mod Team, ciao for now.\n\n--------------\n\nTrack the blackout here: https://reddark.untone.uk", "author_fullname": "t2_f3l6o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API Clusterfuck! ~ We're locked, read this.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1479mua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1561, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 1561, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686528835.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686528261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, we&amp;#39;ll keep this short, you already know what&amp;#39;s going on.&lt;/p&gt;\n\n&lt;p&gt;As you&amp;#39;ve almost certainly heard by now Reddit is locking down their API starting July 1st with the introduction of paid usage. These changes are what killed pushshift.io (full reddit archives and searchable api used by mods and many research/academic papers) and what will kill most (if not all) third-party reddit clients. This is obviously a detriment to everyone, and while Reddit will almost certainly go through with these changes regardless, thousands of subreddits are going to be participating in a 2-day (or longer) blackout. You can read more about the blackouts at &lt;a href=\"/r/ModCoord\"&gt;r/ModCoord&lt;/a&gt;. At the very least, the planned blackout seems to have convinced Reddit to give free API access to accessibility clients. Hopefully it can change their minds further.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt; will be locked for an undetermined amount of time, see &lt;a href=\"https://old.reddit.com/r/DataHoarder/comments/1479c7b/historic_reddit_archives_ongoing_archival_effort\"&gt;this thread&lt;/a&gt; for reddit data archives, tools, etc. we will also be using this time to update our sidebar links and do some general maintenance in the hopes that this mess doesn&amp;#39;t mean the end for us and the many communities that see this as a killing of the Reddit we have loved over the years. &lt;/p&gt;\n\n&lt;h2&gt;&lt;strong&gt;Note; during this time no new posts can be made and all comments are black-holed.&lt;/strong&gt; &lt;/h2&gt;\n\n&lt;p&gt;~ The Mod Team, ciao for now.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Track the blackout here: &lt;a href=\"https://reddark.untone.uk\"&gt;https://reddark.untone.uk&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": "Not As Retired", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "1479mua", "is_robot_indexable": true, "report_reasons": null, "author": "-Archivist", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1479mua/api_clusterfuck_were_locked_read_this/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/1479mua/api_clusterfuck_were_locked_read_this/", "subreddit_subscribers": 687589, "created_utc": 1686528261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_63o77fvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Czkawka 6.0 - File cleaner, now finds similar audio files by content, files by size and name and fix and speedup similar images search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_146nmeh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 744, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/jqa60612ec5b1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jqa60612ec5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/jqa60612ec5b1/DASHPlaylist.mpd?a=1689147503%2CNTY0ZTMwZWViOTdjYTE0NDYxMzNjYjBjOWY1MDZiNmM5NzA0ZTAxZjBmOTMyMjUyZjM5OTA4NDkxZGQyZmM1Mw%3D%3D&amp;v=1&amp;f=sd", "duration": 158, "hls_url": "https://v.redd.it/jqa60612ec5b1/HLSPlaylist.m3u8?a=1689147503%2CMzMxMTdiZmM3NzNiMDU1ODcxNjU0NWUxYjAyM2I0MTAwYWRiYzQ1MWFhMWZmYWU2MjE1Y2RjNzMzZmJlZWNhNA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 744, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YffpmEmamU3bVQhoCxDNiQoWL69enwqyhjR7iugRr1A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "restricted", "created": 1686468974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/jqa60612ec5b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ff9f20bb6c013191dd6f44ee5792537685fe6f4b", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0c744882dc338b5c5fbcb402edcebdf67e4a670f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=180c425dfb145e0f6457f6ae610f360b86158e83", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=84d9c8eaa1077d43e24fbdf61c28d1f03d1a8ddd", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=362d6c8c4197b79e42f26cb076b614d15f9666e5", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b095a49eb39b899a9ddbbbff698ec0b3255f4c3a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ba9808773e1886acbc0446dc9bdf4ee65703f1b8", "width": 1080, "height": 607}], "variants": {}, "id": "__Kb4xeAQKPSI4h5sy9Y2K2G339SeeDekM62Clq58cU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146nmeh", "is_robot_indexable": true, "report_reasons": null, "author": "krutkrutrar", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146nmeh/czkawka_60_file_cleaner_now_finds_similar_audio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/jqa60612ec5b1", "subreddit_subscribers": 687589, "created_utc": 1686468974.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/jqa60612ec5b1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jqa60612ec5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/jqa60612ec5b1/DASHPlaylist.mpd?a=1689147503%2CNTY0ZTMwZWViOTdjYTE0NDYxMzNjYjBjOWY1MDZiNmM5NzA0ZTAxZjBmOTMyMjUyZjM5OTA4NDkxZGQyZmM1Mw%3D%3D&amp;v=1&amp;f=sd", "duration": 158, "hls_url": "https://v.redd.it/jqa60612ec5b1/HLSPlaylist.m3u8?a=1689147503%2CMzMxMTdiZmM3NzNiMDU1ODcxNjU0NWUxYjAyM2I0MTAwYWRiYzQ1MWFhMWZmYWU2MjE1Y2RjNzMzZmJlZWNhNA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's always been a concern of mine that stuff that exists on the Internet may one day just disappear. This is a problem, because I use the Internet as my extended memory: instead of remembering stuff I just remember how to find stuff. Imagine how horrible having an amnesia would be. Well, deleting stuff off the Internet (or gatekeeping access to public domain info) is like causing our collective super-intelligence to have an amnesia.\n\nKeep hoarding, and thank you for your service! You're doing a service to humanity!", "author_fullname": "t2_a3a159dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just found this sub, I am thankful that people like you exist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146v425", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 113, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Thankyou", "can_mod_post": false, "score": 113, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686493139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s always been a concern of mine that stuff that exists on the Internet may one day just disappear. This is a problem, because I use the Internet as my extended memory: instead of remembering stuff I just remember how to find stuff. Imagine how horrible having an amnesia would be. Well, deleting stuff off the Internet (or gatekeeping access to public domain info) is like causing our collective super-intelligence to have an amnesia.&lt;/p&gt;\n\n&lt;p&gt;Keep hoarding, and thank you for your service! You&amp;#39;re doing a service to humanity!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146v425", "is_robot_indexable": true, "report_reasons": null, "author": "bitcoincashautist", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146v425/just_found_this_sub_i_am_thankful_that_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146v425/just_found_this_sub_i_am_thankful_that_people/", "subreddit_subscribers": 687589, "created_utc": 1686493139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This thread will serve as a master list of Reddit data dumps, projects, downloaders and other related information and will be updated over the coming week. \n\n-------------------\n\n\nHelp ArchiveTeam put Reddit into the [Wayback Machine!](https://web.archive.org/web/20230000000000*/reddit.com)\n------\n\n* [Get started!](https://web.archive.org/web/20230611225629/https://old.reddit.com/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/)\n* [See progress!](https://tracker.archiveteam.org/reddit/)\n\nThis project aims to archive reddit as it's seen from a browser and includes media for viewing on the Wayback Machine. \n\n-------------------\n\nPushshift Archive ~ 2005-06 to 2023-03\n------\n\n[Pushshift](https://pushshift.io/) was a social media data collection, analysis, and archiving platform that since 2015 collected Reddit data and made it available to everyone. Pushshifts Reddit dataset was updated in real-time upto 2023-03 before Reddit killed it and includes historical data back to Reddit's inception.\n\nReddit cut off pushshift and had them remove direct http downloads of the bulk data (because it includes items removed at request), which makes this data more important than ever as we come to the end of the golden age of freely accessible Reddit data.\n\nThese archives are purely text and include no media. They are json items scraped from the Reddit API and include all information for every post and comment 2005-06 to 2023-03. \n\nReddit CEO [suggests](https://web.archive.org/web/20230608214509/https://old.reddit.com/r/ModCoord/comments/143rk5p/reddit_held_a_call_today_with_some_developers/jnbjtsc/) PS will come back if they can reach an agreement... little more information about what this means or how bastardized the PS service will be is available. \n\n&gt; June 7th: Pushshift will come back online for , but will stop doing the things we had an issue with, like reselling user data to other folks. The agreement will take another week or two, and we\u2019re in the process of finalizing. \n\n* Downloads: [here](https://the-eye.eu/redarcs/) or [here.](https://academictorrents.com/userdetails.php?id=9863) ~ 2TB* compressed. \n* Extract data [using...](https://github.com/Watchful1/PushshiftDumps)\n* Host browseable/searchable subs [using...](https://github.com/yakabuff/redarc)\n* View demo [here.](http://redarc.basedbin.org/) / [search.](http://redarc.basedbin.org/search) (note demo is select subs hosted by u/Yekab0f)\n\n-----------\n\nDownloaders!\n--------\n\nNote that Reddit limits using their API to get more than the last 1000 items, no download tool bypasses this. See pushshift archives if you wish to extract more posts.\n\n* [Bulk-Downloader-For-Reddit](https://github.com/aliparlakci/bulk-downloader-for-reddit) / [BDFRX](https://github.com/OMEGARAZER/bulk-downloader-for-reddit-x#differences-from-bdfr)\n\nBDFR(x) is targeted so gets quite granular and will download both media and post/comment bodies, it also supports post IDs.\n\n* [Gallery-DL](https://github.com/mikf/gallery-dl)\n\nGallery-DL will grab images and video from a given subreddit. \n\n* [RipMe](https://github.com/RipMeApp2/ripme)\n\nRipMe is similar to gallery-dl but has been around considerably longer, cross-platform and portable via java jar.", "author_fullname": "t2_f3l6o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Historic Reddit Archives, Ongoing Archival Effort &amp; Download Tools, Etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1479c7b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 90, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 90, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686527460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread will serve as a master list of Reddit data dumps, projects, downloaders and other related information and will be updated over the coming week. &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Help ArchiveTeam put Reddit into the &lt;a href=\"https://web.archive.org/web/20230000000000*/reddit.com\"&gt;Wayback Machine!&lt;/a&gt;&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://web.archive.org/web/20230611225629/https://old.reddit.com/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/\"&gt;Get started!&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://tracker.archiveteam.org/reddit/\"&gt;See progress!&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This project aims to archive reddit as it&amp;#39;s seen from a browser and includes media for viewing on the Wayback Machine. &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Pushshift Archive ~ 2005-06 to 2023-03&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://pushshift.io/\"&gt;Pushshift&lt;/a&gt; was a social media data collection, analysis, and archiving platform that since 2015 collected Reddit data and made it available to everyone. Pushshifts Reddit dataset was updated in real-time upto 2023-03 before Reddit killed it and includes historical data back to Reddit&amp;#39;s inception.&lt;/p&gt;\n\n&lt;p&gt;Reddit cut off pushshift and had them remove direct http downloads of the bulk data (because it includes items removed at request), which makes this data more important than ever as we come to the end of the golden age of freely accessible Reddit data.&lt;/p&gt;\n\n&lt;p&gt;These archives are purely text and include no media. They are json items scraped from the Reddit API and include all information for every post and comment 2005-06 to 2023-03. &lt;/p&gt;\n\n&lt;p&gt;Reddit CEO &lt;a href=\"https://web.archive.org/web/20230608214509/https://old.reddit.com/r/ModCoord/comments/143rk5p/reddit_held_a_call_today_with_some_developers/jnbjtsc/\"&gt;suggests&lt;/a&gt; PS will come back if they can reach an agreement... little more information about what this means or how bastardized the PS service will be is available. &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;June 7th: Pushshift will come back online for , but will stop doing the things we had an issue with, like reselling user data to other folks. The agreement will take another week or two, and we\u2019re in the process of finalizing. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Downloads: &lt;a href=\"https://the-eye.eu/redarcs/\"&gt;here&lt;/a&gt; or &lt;a href=\"https://academictorrents.com/userdetails.php?id=9863\"&gt;here.&lt;/a&gt; ~ 2TB* compressed. &lt;/li&gt;\n&lt;li&gt;Extract data &lt;a href=\"https://github.com/Watchful1/PushshiftDumps\"&gt;using...&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Host browseable/searchable subs &lt;a href=\"https://github.com/yakabuff/redarc\"&gt;using...&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;View demo &lt;a href=\"http://redarc.basedbin.org/\"&gt;here.&lt;/a&gt; / &lt;a href=\"http://redarc.basedbin.org/search\"&gt;search.&lt;/a&gt; (note demo is select subs hosted by &lt;a href=\"/u/Yekab0f\"&gt;u/Yekab0f&lt;/a&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Downloaders!&lt;/h2&gt;\n\n&lt;p&gt;Note that Reddit limits using their API to get more than the last 1000 items, no download tool bypasses this. See pushshift archives if you wish to extract more posts.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/aliparlakci/bulk-downloader-for-reddit\"&gt;Bulk-Downloader-For-Reddit&lt;/a&gt; / &lt;a href=\"https://github.com/OMEGARAZER/bulk-downloader-for-reddit-x#differences-from-bdfr\"&gt;BDFRX&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;BDFR(x) is targeted so gets quite granular and will download both media and post/comment bodies, it also supports post IDs.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mikf/gallery-dl\"&gt;Gallery-DL&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Gallery-DL will grab images and video from a given subreddit. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/RipMeApp2/ripme\"&gt;RipMe&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;RipMe is similar to gallery-dl but has been around considerably longer, cross-platform and portable via java jar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": "Not As Retired", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "1479c7b", "is_robot_indexable": true, "report_reasons": null, "author": "-Archivist", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1479c7b/historic_reddit_archives_ongoing_archival_effort/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/1479c7b/historic_reddit_archives_ongoing_archival_effort/", "subreddit_subscribers": 687589, "created_utc": 1686527460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nMany  redditors have already saved me from buying products I didn't know  because here it was possible to find real reviews from real people and  not a paid article on some strange website that only speaks good of the  product in addition to having 200 advertisements on the screen. To the  point that lately I've been searching for \"Product name site:reddit.com\"  to get filtered content and I've always had a good result. I lost count  how many times [r/selfhosted](https://www.reddit.com/r/selfhosted/) and [r/DataHoarder](https://www.reddit.com/r/DataHoarder/) saved me. With many communities going private and people stopping  posting I feel like I'm missing out on a great source of unbiased truth.\n\nThat's it, just saying something from my chest.\n\nRIP!", "author_fullname": "t2_v2fbqav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One of the ways that a lot of people will be affected by killing 3rd party apps is source of unbiased truth.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146yebg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686501251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many  redditors have already saved me from buying products I didn&amp;#39;t know  because here it was possible to find real reviews from real people and  not a paid article on some strange website that only speaks good of the  product in addition to having 200 advertisements on the screen. To the  point that lately I&amp;#39;ve been searching for &amp;quot;Product name site:reddit.com&amp;quot;  to get filtered content and I&amp;#39;ve always had a good result. I lost count  how many times &lt;a href=\"https://www.reddit.com/r/selfhosted/\"&gt;r/selfhosted&lt;/a&gt; and &lt;a href=\"https://www.reddit.com/r/DataHoarder/\"&gt;r/DataHoarder&lt;/a&gt; saved me. With many communities going private and people stopping  posting I feel like I&amp;#39;m missing out on a great source of unbiased truth.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s it, just saying something from my chest.&lt;/p&gt;\n\n&lt;p&gt;RIP!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146yebg", "is_robot_indexable": true, "report_reasons": null, "author": "lucasjose501", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146yebg/one_of_the_ways_that_a_lot_of_people_will_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146yebg/one_of_the_ways_that_a_lot_of_people_will_be/", "subreddit_subscribers": 687589, "created_utc": 1686501251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I am not a very techie person, but in light of recent Reddit events, i want to save wikis of the subbredits I am subscribed to. How would you recommend going about this for someone who is absolutely not a tech guru? I am looking for something that won't require me learning python etc. and can be done in a day. Thanks", "author_fullname": "t2_nkzqvk4f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to easily and quickly save all my subbreddit's wikis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146t26z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686487548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I am not a very techie person, but in light of recent Reddit events, i want to save wikis of the subbredits I am subscribed to. How would you recommend going about this for someone who is absolutely not a tech guru? I am looking for something that won&amp;#39;t require me learning python etc. and can be done in a day. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146t26z", "is_robot_indexable": true, "report_reasons": null, "author": "kitsuneterminator400", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146t26z/how_to_easily_and_quickly_save_all_my_subbreddits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146t26z/how_to_easily_and_quickly_save_all_my_subbreddits/", "subreddit_subscribers": 687589, "created_utc": 1686487548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a ton of physical copies of old Game Informer magazines dating back from 2004 to 2013. I have tried to find an archive of them online and have found the coverage to be spotty at best. What would be the best way to get them scanned?", "author_fullname": "t2_78vxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Game Informer Magazine Collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1473l0q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686513438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a ton of physical copies of old Game Informer magazines dating back from 2004 to 2013. I have tried to find an archive of them online and have found the coverage to be spotty at best. What would be the best way to get them scanned?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1473l0q", "is_robot_indexable": true, "report_reasons": null, "author": "Dioxide20", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1473l0q/game_informer_magazine_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1473l0q/game_informer_magazine_collection/", "subreddit_subscribers": 687589, "created_utc": 1686513438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We'll boys and girls, as we head into a two day break, now is a great time for backing up, checking back ups, cataloging and wish-listing.  This is an awesome little community, and I've always been told absence makes the heart grow fonder.   \nBe good to yourselves, and remember- 3,2,1....", "author_fullname": "t2_xnynomp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hit the lights on your way out", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_14784w0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-uZWPwJe4-ozmVEx3NV2ayoaZler5cHGTGRHrmJvQQo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "restricted", "created": 1686524246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ll boys and girls, as we head into a two day break, now is a great time for backing up, checking back ups, cataloging and wish-listing.  This is an awesome little community, and I&amp;#39;ve always been told absence makes the heart grow fonder.&lt;br/&gt;\nBe good to yourselves, and remember- 3,2,1....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/e2g63e6hyg5b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/e2g63e6hyg5b1.jpg?auto=webp&amp;v=enabled&amp;s=b8d4574d2198c203ee7f32b29ccddba2f6334bef", "width": 296, "height": 170}, "resolutions": [{"url": "https://preview.redd.it/e2g63e6hyg5b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e544fc9480642a51e9074e06187a5b230c9df6e", "width": 108, "height": 62}, {"url": "https://preview.redd.it/e2g63e6hyg5b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abe480e8a2242c87b4967b46220a5bf77e6dacb7", "width": 216, "height": 124}], "variants": {}, "id": "XNKjhpmJSnuYeib-xGG4tC1k-gzm-vZ9fZ-Lc1CnKvM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14784w0", "is_robot_indexable": true, "report_reasons": null, "author": "K1rkl4nd", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14784w0/hit_the_lights_on_your_way_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/e2g63e6hyg5b1.jpg", "subreddit_subscribers": 687589, "created_utc": 1686524246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://jff.jpf.go.jp/watch/independent-cinema/\n\nThe JAPANESE FILM FESTIVAL ONLINE's program, a new event part of the Japanese government's Japan Foundation arts and cultural promotion, will all be taken down in a couple days. I'm hoping they'll be able to be archived/preserved somehow \u2013 and I fear these films might be too niche for usual piracy release groups to do WEB-DLs of the content.\n\nThey're free to watch online once you register with an email/password, and it looks like the content is using Microsoft PlayReady DRM, with the .mp4s and .mpd (MPEG DASH streaming setup) viewable in the browser. You just need to get a key (probably using Widevine L3 Decryptor or the like &amp; the PSSH keys from the .mpd) and then Bento4's [mp4decrypt](https://www.bento4.com/documentation/mp4decrypt/) to decode the mp4 files, and ffmpeg to mux together video and audio.\n\nI've never done this before and am running into some issues but hopefully someone else is familiar with this all, or the above info helps people (there's a good amount of similar information in forum posts on videohelp.com as well).", "author_fullname": "t2_dyqly0md", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Japanese Film Festival's free-to-view films will go offline on Wednesday", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146vcbc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "PSA/Request", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686493730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://jff.jpf.go.jp/watch/independent-cinema/\"&gt;https://jff.jpf.go.jp/watch/independent-cinema/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The JAPANESE FILM FESTIVAL ONLINE&amp;#39;s program, a new event part of the Japanese government&amp;#39;s Japan Foundation arts and cultural promotion, will all be taken down in a couple days. I&amp;#39;m hoping they&amp;#39;ll be able to be archived/preserved somehow \u2013 and I fear these films might be too niche for usual piracy release groups to do WEB-DLs of the content.&lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re free to watch online once you register with an email/password, and it looks like the content is using Microsoft PlayReady DRM, with the .mp4s and .mpd (MPEG DASH streaming setup) viewable in the browser. You just need to get a key (probably using Widevine L3 Decryptor or the like &amp;amp; the PSSH keys from the .mpd) and then Bento4&amp;#39;s &lt;a href=\"https://www.bento4.com/documentation/mp4decrypt/\"&gt;mp4decrypt&lt;/a&gt; to decode the mp4 files, and ffmpeg to mux together video and audio.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never done this before and am running into some issues but hopefully someone else is familiar with this all, or the above info helps people (there&amp;#39;s a good amount of similar information in forum posts on videohelp.com as well).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?auto=webp&amp;v=enabled&amp;s=e1e388d2e637e5d108fd213ddebe1eae6cbba128", "width": 1920, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ae3a7ca8c003fff0f752d6be0b4e418301e31a7", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=706dbab0e15a60ce49b024f0981efbe34aa1d7a6", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=293fd240912c8e302e91ba92221668eb0cc32160", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d631f82d051ea4e1e9ae7e6bfd560a5fdb0577b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ca301e6c23e5279332de7556d8551d3fc8df62c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3694df1b3976505ff0aa3bab3b05d7584a31c2b7", "width": 1080, "height": 720}], "variants": {}, "id": "8iZAZJFbhCewb5MZAStsY8NC8PwXm4N1I8uSJnnWvtI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146vcbc", "is_robot_indexable": true, "report_reasons": null, "author": "throwra8138", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146vcbc/japanese_film_festivals_freetoview_films_will_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146vcbc/japanese_film_festivals_freetoview_films_will_go/", "subreddit_subscribers": 687589, "created_utc": 1686493730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/white-paper/white-paper-ssd-endurance-and-hdd-workloads.pdf\n\nThis white paper explains the origin of hard drive workload ratings: head fly height lowers when reading or writing. It's not a \"wear out\" kind of thing, but rather, more reads and writes increases the probability of failure due to head-platter collisions.\n\nThe paper also mentions that claimed failure rates are based on unspecified \"typical\" usage, not the workload rating. Well, the claimed failure rates are lies anyway.", "author_fullname": "t2_n9hqlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard drive workload ratings explained", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14750as", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686516746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/white-paper/white-paper-ssd-endurance-and-hdd-workloads.pdf\"&gt;https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/white-paper/white-paper-ssd-endurance-and-hdd-workloads.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This white paper explains the origin of hard drive workload ratings: head fly height lowers when reading or writing. It&amp;#39;s not a &amp;quot;wear out&amp;quot; kind of thing, but rather, more reads and writes increases the probability of failure due to head-platter collisions.&lt;/p&gt;\n\n&lt;p&gt;The paper also mentions that claimed failure rates are based on unspecified &amp;quot;typical&amp;quot; usage, not the workload rating. Well, the claimed failure rates are lies anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14750as", "is_robot_indexable": true, "report_reasons": null, "author": "cbm80", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14750as/hard_drive_workload_ratings_explained/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14750as/hard_drive_workload_ratings_explained/", "subreddit_subscribers": 687589, "created_utc": 1686516746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dell password reset cable and a little script, wow, what an improvement!", "author_fullname": "t2_ze2t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally quieted my Dell MD1200", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1471f2b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/WlB2WUqxeMiM3Sia2-vtlgTIx4rg2STP3lblEu78S_U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "restricted", "created": 1686508387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dell password reset cable and a little script, wow, what an improvement!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fs2to59bnf5b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fs2to59bnf5b1.jpg?auto=webp&amp;v=enabled&amp;s=fd816d2ed9f399dd6e9bdc0a0ea00b1252d55995", "width": 720, "height": 961}, "resolutions": [{"url": "https://preview.redd.it/fs2to59bnf5b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd9ae7a1521f23fa09ca966caf3accd35cfd00a", "width": 108, "height": 144}, {"url": "https://preview.redd.it/fs2to59bnf5b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67f2c5a64dad08d1b0e82491820358f2102327de", "width": 216, "height": 288}, {"url": "https://preview.redd.it/fs2to59bnf5b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecb0cabfa1cf0b47c2a877752453bd9d2a64d967", "width": 320, "height": 427}, {"url": "https://preview.redd.it/fs2to59bnf5b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cd35c9f692b9aa4f5649a3bfb88bf1c4929a7dc", "width": 640, "height": 854}], "variants": {}, "id": "Ux1X6vcgVkGqbw2A6S8uiCAo2A40CXtmIR8BHrluoD8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1471f2b", "is_robot_indexable": true, "report_reasons": null, "author": "PolicyPaul", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1471f2b/finally_quieted_my_dell_md1200/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fs2to59bnf5b1.jpg", "subreddit_subscribers": 687589, "created_utc": 1686508387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Follow up to [this](https://www.reddit.com/r/DataHoarder/comments/10fg8f3/issuu_making_changes_that_will_make_a_lot_of/) (I'm not OP).\n\nStarting from June 19, another limitation is going to be applied to free Basic plan:\n\n&gt;**Publishing limit:** Basic users will be limited to 5 published documents.  \n&gt;  \n&gt;Starting on June 19, 2023, we will begin a phased rollout to all Basic accounts limiting the number of published documents. Any existing uploads on a Basic account that exceed the above limitation will continue to be hosted on Issuu but not accessible to readers.\u00a0\n\nThey're basically killing the free plan, and making a lot of content unavaliable forever (the premium plan is at least $19 per month, IMHO too expensive for most publishers, without counting the \"zoombie publishers\" that are not even going to see what's happening)", "author_fullname": "t2_81fvh23e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issuu making changes that will make a lot of content unavaliable (yes, again)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146pmrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686476305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Follow up to &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10fg8f3/issuu_making_changes_that_will_make_a_lot_of/\"&gt;this&lt;/a&gt; (I&amp;#39;m not OP).&lt;/p&gt;\n\n&lt;p&gt;Starting from June 19, another limitation is going to be applied to free Basic plan:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Publishing limit:&lt;/strong&gt; Basic users will be limited to 5 published documents.  &lt;/p&gt;\n\n&lt;p&gt;Starting on June 19, 2023, we will begin a phased rollout to all Basic accounts limiting the number of published documents. Any existing uploads on a Basic account that exceed the above limitation will continue to be hosted on Issuu but not accessible to readers.\u00a0&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;They&amp;#39;re basically killing the free plan, and making a lot of content unavaliable forever (the premium plan is at least $19 per month, IMHO too expensive for most publishers, without counting the &amp;quot;zoombie publishers&amp;quot; that are not even going to see what&amp;#39;s happening)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146pmrp", "is_robot_indexable": true, "report_reasons": null, "author": "CheesecakeNo7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146pmrp/issuu_making_changes_that_will_make_a_lot_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146pmrp/issuu_making_changes_that_will_make_a_lot_of/", "subreddit_subscribers": 687589, "created_utc": 1686476305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like when i want to buy something, firstly i will search it on reddit like\"(product name) reddit/site:reddit.com\". But tomorrow a large major subreddit will go offline, and someone already backup all the possible data on reddit, i just want to know if there is a way that i can search for something in this backup data ?", "author_fullname": "t2_67n0v91d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IDK if is there is a way to search for information on reddit via archive ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1470x7t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686508492.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686507230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like when i want to buy something, firstly i will search it on reddit like&amp;quot;(product name) reddit/site:reddit.com&amp;quot;. But tomorrow a large major subreddit will go offline, and someone already backup all the possible data on reddit, i just want to know if there is a way that i can search for something in this backup data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1470x7t", "is_robot_indexable": true, "report_reasons": null, "author": "Dua_Leo_9564", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1470x7t/idk_if_is_there_is_a_way_to_search_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1470x7t/idk_if_is_there_is_a_way_to_search_for/", "subreddit_subscribers": 687589, "created_utc": 1686507230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to watch a seminar that I can\u2019t attend live today.  Is there a way to record that seminar and have it go directly to a cloud service?  From either my phone or laptop?\n\n(So that it won\u2019t be stored in my device)\n\nThank you.", "author_fullname": "t2_vfugp2oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to store data directly from screen recording on a lap top?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146wms3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686496901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to watch a seminar that I can\u2019t attend live today.  Is there a way to record that seminar and have it go directly to a cloud service?  From either my phone or laptop?&lt;/p&gt;\n\n&lt;p&gt;(So that it won\u2019t be stored in my device)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146wms3", "is_robot_indexable": true, "report_reasons": null, "author": "Afraid_Bus_5756", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146wms3/is_there_a_way_to_store_data_directly_from_screen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146wms3/is_there_a_way_to_store_data_directly_from_screen/", "subreddit_subscribers": 687589, "created_utc": 1686496901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There is this old Megaman fan website that has some production image files I want but the page with the images is not functioning. The images are likely still in the site though, it just hasnt been updated since 2012 so something probably broke in an update or other.\n\nI tried Cyotek Webcopy but it only grabs the HTML files and a few MP3, nothing else.\n\nEDIT: Just learned the site has \" Anti-Hotlinker script \" which is likely why I cant grab anything.  Offline Explorer grabbed quite a bit but none of the images or media\n\nIs there a way to bypass  Anti-Hotlinker script ?", "author_fullname": "t2_dd6cqc46", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good program to download a full website?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14776q8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686528966.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686521864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is this old Megaman fan website that has some production image files I want but the page with the images is not functioning. The images are likely still in the site though, it just hasnt been updated since 2012 so something probably broke in an update or other.&lt;/p&gt;\n\n&lt;p&gt;I tried Cyotek Webcopy but it only grabs the HTML files and a few MP3, nothing else.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Just learned the site has &amp;quot; Anti-Hotlinker script &amp;quot; which is likely why I cant grab anything.  Offline Explorer grabbed quite a bit but none of the images or media&lt;/p&gt;\n\n&lt;p&gt;Is there a way to bypass  Anti-Hotlinker script ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14776q8", "is_robot_indexable": true, "report_reasons": null, "author": "Standard-Historian79", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14776q8/what_is_a_good_program_to_download_a_full_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14776q8/what_is_a_good_program_to_download_a_full_website/", "subreddit_subscribers": 687589, "created_utc": 1686521864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I only found scripts and programms that save public stuff", "author_fullname": "t2_17wqptyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any easy way to save all posts I made on a private subreddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1476pue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686520751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I only found scripts and programms that save public stuff&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1476pue", "is_robot_indexable": true, "report_reasons": null, "author": "never__seen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1476pue/is_there_any_easy_way_to_save_all_posts_i_made_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1476pue/is_there_any_easy_way_to_save_all_posts_i_made_on/", "subreddit_subscribers": 687589, "created_utc": 1686520751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download all the tweets from a particular twitter account but the tools I used only seem to download the latest 3000 tweets or so. I'm trying to archive all the tweets and there are roughly 20k tweets from this account. How do I save them?", "author_fullname": "t2_qkzy7qz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I download ALL the tweets from an account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1472dh3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686510597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download all the tweets from a particular twitter account but the tools I used only seem to download the latest 3000 tweets or so. I&amp;#39;m trying to archive all the tweets and there are roughly 20k tweets from this account. How do I save them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1472dh3", "is_robot_indexable": true, "report_reasons": null, "author": "heheboi1110", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1472dh3/how_do_i_download_all_the_tweets_from_an_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1472dh3/how_do_i_download_all_the_tweets_from_an_account/", "subreddit_subscribers": 687589, "created_utc": 1686510597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have amassed a large amount of artbook scans, but the problem is that they're all in different file formats with varying levels of quality (even within one artbook). As I'm trying to grow this collection and hopefully find higher quality scans, I'm not sure how I should go about with a folder system. Should I go Series -&gt; Art Book Name -&gt; File Formats -&gt; Quality? I feel like that's too many folders for me though.  \n\n\nEdit: I use Windows.  \n", "author_fullname": "t2_c855jp2w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to organize scans of art books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14729l9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686511885.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686510341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have amassed a large amount of artbook scans, but the problem is that they&amp;#39;re all in different file formats with varying levels of quality (even within one artbook). As I&amp;#39;m trying to grow this collection and hopefully find higher quality scans, I&amp;#39;m not sure how I should go about with a folder system. Should I go Series -&amp;gt; Art Book Name -&amp;gt; File Formats -&amp;gt; Quality? I feel like that&amp;#39;s too many folders for me though.  &lt;/p&gt;\n\n&lt;p&gt;Edit: I use Windows.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14729l9", "is_robot_indexable": true, "report_reasons": null, "author": "FullAd419", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14729l9/how_to_organize_scans_of_art_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14729l9/how_to_organize_scans_of_art_books/", "subreddit_subscribers": 687589, "created_utc": 1686510341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**TL;DR:** I recently purchased nine 20TB Seagate Exos X20 drives. 5 came with the newer SN03 firmware, 4 have the older SN01 firmware. I'm seeking opinions on whether I should upgrade the firmware on the SN01 drives to SN03. Seagate flags this firmware as 'IMPORTANT' without further information.\n\n---\n\nOver the past two weeks, I purchased nine 20TB Seagate Exos X20 drives. Five of them (which arrived last week) have Seagate\u2019s SN03 firmware and four (which arrived yesterday) have the older SN01 firmware.\n\nHistorically, I\u2019ve never upgraded drive firmware. I figure, if the drives work, I should leave well-enough alone. However, after having been bitten (hard) by [the Samsung 990 Pro firmware fiasco](https://www.tomshardware.com/news/samsung-990-pro-firmware-update-released-ssd-health), I\u2019m thinking keeping drive firmware up to date may not be the worst idea.\n\nYou may recall that [Seagate used to strongly recommend against updating drive firmware](https://www.seagate.com/support/kb/does-my-drive-need-a-firmware-update-206091en/), saying:\n\n&gt; External, Serial ATA, and ATA drives are not designed for field firmware updates by end users.\n\nHowever, this advice [has now changed](https://www.seagate.com/support/kb/firmware-updates-for-seagate-products-207931en/) and they now say:\n\n&gt; Until recently, firmware updates for typical desktop and laptop computers were difficult and somewhat risky. This situation, in part, was based on a lack of friendly firmware download tools and operating system limitations. This situation has improved and Seagate now offers firmware updates as a routine matter for the general support of your Seagate drive.\n\nWhich means upgrading the firmware on the four SN01 drives to SN03 isn\u2019t out of the question.\n\nEntering the serial numbers for my SN01 drives on their website, Seagate provides a link to their installer that upgrades from SN01 to SN03. Seagate does not provide release notes for firmware updates, so there\u2019s no way to know what has changed. However, on their download page for the drive, they list the firmware as \u201cIMPORTANT\u201d, versus some other downloads that are merely \u201cRECOMMENDED.\u201d\n\nShould I upgrade this firmware? The drives live in an ARM-based QNAP NAS. To upgrade them in the safest manner possible, my plan would be to shut down the NAS, remove the drives from the NAS one at a time, and install them individually in an Intel desktop system. On that system, I would boot from the special USB flash drive that the Seagate utility creates, upgrade the firmware, reboot to make sure that the firmware upgrade took, and then shut down and remove the drive and return it to the NAS. I\u2019d repeat this four times.", "author_fullname": "t2_3sk1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To Upgrade or Not To Upgrade: Best Practices for Updating Hard Drive Firmware", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146yn7v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686520625.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686501851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I recently purchased nine 20TB Seagate Exos X20 drives. 5 came with the newer SN03 firmware, 4 have the older SN01 firmware. I&amp;#39;m seeking opinions on whether I should upgrade the firmware on the SN01 drives to SN03. Seagate flags this firmware as &amp;#39;IMPORTANT&amp;#39; without further information.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Over the past two weeks, I purchased nine 20TB Seagate Exos X20 drives. Five of them (which arrived last week) have Seagate\u2019s SN03 firmware and four (which arrived yesterday) have the older SN01 firmware.&lt;/p&gt;\n\n&lt;p&gt;Historically, I\u2019ve never upgraded drive firmware. I figure, if the drives work, I should leave well-enough alone. However, after having been bitten (hard) by &lt;a href=\"https://www.tomshardware.com/news/samsung-990-pro-firmware-update-released-ssd-health\"&gt;the Samsung 990 Pro firmware fiasco&lt;/a&gt;, I\u2019m thinking keeping drive firmware up to date may not be the worst idea.&lt;/p&gt;\n\n&lt;p&gt;You may recall that &lt;a href=\"https://www.seagate.com/support/kb/does-my-drive-need-a-firmware-update-206091en/\"&gt;Seagate used to strongly recommend against updating drive firmware&lt;/a&gt;, saying:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;External, Serial ATA, and ATA drives are not designed for field firmware updates by end users.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;However, this advice &lt;a href=\"https://www.seagate.com/support/kb/firmware-updates-for-seagate-products-207931en/\"&gt;has now changed&lt;/a&gt; and they now say:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Until recently, firmware updates for typical desktop and laptop computers were difficult and somewhat risky. This situation, in part, was based on a lack of friendly firmware download tools and operating system limitations. This situation has improved and Seagate now offers firmware updates as a routine matter for the general support of your Seagate drive.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Which means upgrading the firmware on the four SN01 drives to SN03 isn\u2019t out of the question.&lt;/p&gt;\n\n&lt;p&gt;Entering the serial numbers for my SN01 drives on their website, Seagate provides a link to their installer that upgrades from SN01 to SN03. Seagate does not provide release notes for firmware updates, so there\u2019s no way to know what has changed. However, on their download page for the drive, they list the firmware as \u201cIMPORTANT\u201d, versus some other downloads that are merely \u201cRECOMMENDED.\u201d&lt;/p&gt;\n\n&lt;p&gt;Should I upgrade this firmware? The drives live in an ARM-based QNAP NAS. To upgrade them in the safest manner possible, my plan would be to shut down the NAS, remove the drives from the NAS one at a time, and install them individually in an Intel desktop system. On that system, I would boot from the special USB flash drive that the Seagate utility creates, upgrade the firmware, reboot to make sure that the firmware upgrade took, and then shut down and remove the drive and return it to the NAS. I\u2019d repeat this four times.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SJEqNTIyY9mmEUgBSQy7Igw-rWb4xCOdVS-zzuIqT5k.jpg?auto=webp&amp;v=enabled&amp;s=15248fb0d1128ca0fb04a80eedd7b73edf24ec2e", "width": 970, "height": 546}, "resolutions": [{"url": "https://external-preview.redd.it/SJEqNTIyY9mmEUgBSQy7Igw-rWb4xCOdVS-zzuIqT5k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=978aaa96c4b2e72bb9ac746a608f5065abe5752a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/SJEqNTIyY9mmEUgBSQy7Igw-rWb4xCOdVS-zzuIqT5k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89cf1e1849da34ba0b1ef7b4e830b66aac2cb510", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/SJEqNTIyY9mmEUgBSQy7Igw-rWb4xCOdVS-zzuIqT5k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e78cf329cbe72771392cb9947c9cbe41398925c9", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/SJEqNTIyY9mmEUgBSQy7Igw-rWb4xCOdVS-zzuIqT5k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea4f512d3f2fdb0f97a971f7daf836e766957415", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/SJEqNTIyY9mmEUgBSQy7Igw-rWb4xCOdVS-zzuIqT5k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe0d2e2c5279e491dbf6c9853a9676f5df9db1bd", "width": 960, "height": 540}], "variants": {}, "id": "Ed2-8OmB012KxPn1KFu5X6Qf6nz-PbQ7ziWWw-Yh5NI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146yn7v", "is_robot_indexable": true, "report_reasons": null, "author": "vff", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146yn7v/to_upgrade_or_not_to_upgrade_best_practices_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146yn7v/to_upgrade_or_not_to_upgrade_best_practices_for/", "subreddit_subscribers": 687589, "created_utc": 1686501851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DIY DAS vs used enterprise device", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146xar8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_5cq4a", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [], "created": 1686498549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/146xa5u/diy_das_vs_used_enterprise_device/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146xar8", "is_robot_indexable": true, "report_reasons": null, "author": "beaverfingers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_146xa5u", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146xar8/diy_das_vs_used_enterprise_device/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/146xa5u/diy_das_vs_used_enterprise_device/", "subreddit_subscribers": 687589, "created_utc": 1686498549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to search for a YouTube video based on whether the video contains a portion of a song? Essentially I'm trying to find a video whose title or creator I can't remember because it's been so long, but I remember vividly a song that was used in it. When I try using search terms for actions or \"catch phrases\" from what I remember in the video, it is too general, is totally misleading, and I can't get any good results. Honestly, I'm not even sure if this video exists on youtube anymore, but I know it contained a pretty unique song so I figured that could be a good indicator, so I figured this was worth a shot. Anyone know how to do this search or is it too convoluted? If anyone knew how to do such a thing, I'd imagine someone in this community can help. Thanks in advance!!", "author_fullname": "t2_74i2uwce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you search a youtube video based on whether it contains a particular song?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146wex0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686496360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to search for a YouTube video based on whether the video contains a portion of a song? Essentially I&amp;#39;m trying to find a video whose title or creator I can&amp;#39;t remember because it&amp;#39;s been so long, but I remember vividly a song that was used in it. When I try using search terms for actions or &amp;quot;catch phrases&amp;quot; from what I remember in the video, it is too general, is totally misleading, and I can&amp;#39;t get any good results. Honestly, I&amp;#39;m not even sure if this video exists on youtube anymore, but I know it contained a pretty unique song so I figured that could be a good indicator, so I figured this was worth a shot. Anyone know how to do this search or is it too convoluted? If anyone knew how to do such a thing, I&amp;#39;d imagine someone in this community can help. Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146wex0", "is_robot_indexable": true, "report_reasons": null, "author": "humelectra2000", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146wex0/can_you_search_a_youtube_video_based_on_whether/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146wex0/can_you_search_a_youtube_video_based_on_whether/", "subreddit_subscribers": 687589, "created_utc": 1686496360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI'd like to know if there's a tool that can automatically download all media and all tweets from a GDPR data request, like \"saving\" everything offline: tweets, images, videos that are linked to my account (that I liked, retweeted, bookmarked and posted).", "author_fullname": "t2_sufg5mng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Retrieve all media from Twitter GDPR data request?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146sxe0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686487155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if there&amp;#39;s a tool that can automatically download all media and all tweets from a GDPR data request, like &amp;quot;saving&amp;quot; everything offline: tweets, images, videos that are linked to my account (that I liked, retweeted, bookmarked and posted).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146sxe0", "is_robot_indexable": true, "report_reasons": null, "author": "calmingcroco", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146sxe0/retrieve_all_media_from_twitter_gdpr_data_request/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146sxe0/retrieve_all_media_from_twitter_gdpr_data_request/", "subreddit_subscribers": 687589, "created_utc": 1686487155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, I have ADD, schizophrenia, and severe depression which is why I'm asking for help. My condition makes it a bit.. tough to keep clear headed and focused. Writing this post has taken me roughly 5 hours to write so far..\n\nI just want to migrate 9Tb of local data on a raid 5 to two drives on raid 1. I think I looked around newegg and found 14Tb drives to be the most cost effective. I don't know if that's right.. something like 139.99 a piece?\n\nI just know that I need to migrate my data and my pc does not have any more sata ports available.\n\nI have like 6 more Tb of data on additional HDD's. I'd like it all in one place but I'm not made of money so around $300-400 is my limit unless someone can offer a better solution.\n\nI just hate having that stack of 20 500Gb HDD's just sitting there.(all backed up 1-1, so it's more like 10) I feel like they're waiting to be knocked over or for a static touch to end them.\n\nI was thinking two 14Tb drives in raid 1 for the majority. Not sure how to do so since I lack any more sata ports currently\n\nThank you my dudes! If you have any better ideas, please let me know! I'd appreciate it greatly.", "author_fullname": "t2_fw7be", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help finding best solution or cost per TB HDDs to migrate 9-15Tb of data from a Raid 5 to a Raid 1..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1478lng", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686525437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have ADD, schizophrenia, and severe depression which is why I&amp;#39;m asking for help. My condition makes it a bit.. tough to keep clear headed and focused. Writing this post has taken me roughly 5 hours to write so far..&lt;/p&gt;\n\n&lt;p&gt;I just want to migrate 9Tb of local data on a raid 5 to two drives on raid 1. I think I looked around newegg and found 14Tb drives to be the most cost effective. I don&amp;#39;t know if that&amp;#39;s right.. something like 139.99 a piece?&lt;/p&gt;\n\n&lt;p&gt;I just know that I need to migrate my data and my pc does not have any more sata ports available.&lt;/p&gt;\n\n&lt;p&gt;I have like 6 more Tb of data on additional HDD&amp;#39;s. I&amp;#39;d like it all in one place but I&amp;#39;m not made of money so around $300-400 is my limit unless someone can offer a better solution.&lt;/p&gt;\n\n&lt;p&gt;I just hate having that stack of 20 500Gb HDD&amp;#39;s just sitting there.(all backed up 1-1, so it&amp;#39;s more like 10) I feel like they&amp;#39;re waiting to be knocked over or for a static touch to end them.&lt;/p&gt;\n\n&lt;p&gt;I was thinking two 14Tb drives in raid 1 for the majority. Not sure how to do so since I lack any more sata ports currently&lt;/p&gt;\n\n&lt;p&gt;Thank you my dudes! If you have any better ideas, please let me know! I&amp;#39;d appreciate it greatly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1478lng", "is_robot_indexable": true, "report_reasons": null, "author": "robert812003", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1478lng/need_help_finding_best_solution_or_cost_per_tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1478lng/need_help_finding_best_solution_or_cost_per_tb/", "subreddit_subscribers": 687589, "created_utc": 1686525437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have the posters from the Hero Up minigame from the now defunct Marvel HQ website?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1474mym", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_d97xoif2", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [], "created": 1686515878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Marvel", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Marvel/comments/13ycy3k/does_anyone_have_the_posters_from_the_hero_up/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1474mym", "is_robot_indexable": true, "report_reasons": null, "author": "Other_Vanilla_5100", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13ycy3k", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1474mym/does_anyone_have_the_posters_from_the_hero_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Marvel/comments/13ycy3k/does_anyone_have_the_posters_from_the_hero_up/", "subreddit_subscribers": 687589, "created_utc": 1686515878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm working on my first regular system backup. I have Macrium Reflect setup to do various backup jobs throughout the day/week/month. Those backups make their way into folders on an external hard drive plugged into the same computer that's been backed up. I then have a second computer, a homelab if you will, that houses my long term backup z-pool. I don't have this machine powered up unless I'm using it, but I'd like to find a piece of software that would allow me to get all the configuration out of the way (where to send the files and authorizations) so that once a week I can ensure everything's powered up and click one button to have the files/folders sent/synced to my long term storage on the other machine.\n\nAnything out there that fits that bill? I was looking at Seafile and others like it but I didn't want to crawl down that rabbit hole if that's not the right tool for me.\n\nThanks folks!", "author_fullname": "t2_h6ue5wyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a file syncing solution that I can use, once configured, to one-click do-that-job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1470mj5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "restricted", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686506535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on my first regular system backup. I have Macrium Reflect setup to do various backup jobs throughout the day/week/month. Those backups make their way into folders on an external hard drive plugged into the same computer that&amp;#39;s been backed up. I then have a second computer, a homelab if you will, that houses my long term backup z-pool. I don&amp;#39;t have this machine powered up unless I&amp;#39;m using it, but I&amp;#39;d like to find a piece of software that would allow me to get all the configuration out of the way (where to send the files and authorizations) so that once a week I can ensure everything&amp;#39;s powered up and click one button to have the files/folders sent/synced to my long term storage on the other machine.&lt;/p&gt;\n\n&lt;p&gt;Anything out there that fits that bill? I was looking at Seafile and others like it but I didn&amp;#39;t want to crawl down that rabbit hole if that&amp;#39;s not the right tool for me.&lt;/p&gt;\n\n&lt;p&gt;Thanks folks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1470mj5", "is_robot_indexable": true, "report_reasons": null, "author": "elzorroazul777", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1470mj5/is_there_a_file_syncing_solution_that_i_can_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1470mj5/is_there_a_file_syncing_solution_that_i_can_use/", "subreddit_subscribers": 687589, "created_utc": 1686506535.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}