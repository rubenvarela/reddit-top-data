{"kind": "Listing", "data": {"after": "t3_14fp7f7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you guys orchestrating your pipelines? \n\nCurrently our environment is : \nADF for orchestration\n,databricks for transformations\n,AZDL Gen 2 storage\n,synapse for reporting (which is slowly going away as we transition to Databricks). \n\nMy company is doing away with Azure (don\u2019t ask me why) and transitioning to GCP. We will keep databricks for transformations and most likely reporting, but now we lose our orchestration tool. \n\nCurrently we use ADF for connections to our source systems where we have 100s of sql ingestion queries (that are stored in a table and looped through) to copy data out of 40 different source system sql db\u2019s. Data is ingested into AZDL Gen2 storage as parquet files then picked up in databricks notebooks. \n\nWhat tools are you using that can replace ADF?", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fjdfm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687381445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you guys orchestrating your pipelines? &lt;/p&gt;\n\n&lt;p&gt;Currently our environment is : \nADF for orchestration\n,databricks for transformations\n,AZDL Gen 2 storage\n,synapse for reporting (which is slowly going away as we transition to Databricks). &lt;/p&gt;\n\n&lt;p&gt;My company is doing away with Azure (don\u2019t ask me why) and transitioning to GCP. We will keep databricks for transformations and most likely reporting, but now we lose our orchestration tool. &lt;/p&gt;\n\n&lt;p&gt;Currently we use ADF for connections to our source systems where we have 100s of sql ingestion queries (that are stored in a table and looped through) to copy data out of 40 different source system sql db\u2019s. Data is ingested into AZDL Gen2 storage as parquet files then picked up in databricks notebooks. &lt;/p&gt;\n\n&lt;p&gt;What tools are you using that can replace ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fjdfm", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fjdfm/orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fjdfm/orchestration/", "subreddit_subscribers": 111707, "created_utc": 1687381445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.\n\nLots of dimensions and facts needing to be processed differently from each sources to be correctly represented.\n\nIf I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).\n\nWhat would be the best way to create a documentation that can be shared with business? Is there \"one\" system or will there be a bunch of several artefacts, UMLs etc needed?", "author_fullname": "t2_e4028dc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you do documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fv52f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687413946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.&lt;/p&gt;\n\n&lt;p&gt;Lots of dimensions and facts needing to be processed differently from each sources to be correctly represented.&lt;/p&gt;\n\n&lt;p&gt;If I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to create a documentation that can be shared with business? Is there &amp;quot;one&amp;quot; system or will there be a bunch of several artefacts, UMLs etc needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fv52f", "is_robot_indexable": true, "report_reasons": null, "author": "tzt1324", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fv52f/how_do_you_do_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fv52f/how_do_you_do_documentation/", "subreddit_subscribers": 111707, "created_utc": 1687413946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  \n\n\nBut I particularly like the practice of **\"keeping code changes small and simple, and getting them into production fast by testing &amp; automating\"** (if you can call this a practice) - that, in my experience works pretty well in **data ingestion**.\n\nHere's how I'm going about it when working on data pipelines:\n\n1. Ingest all columns for a new table, but don\u2019t ingest additional tables unless necessary. Keep your ingestion code simple.\n2. Prefer log-based ingestion &gt; full table &gt; key based. Log-based is by far the simplest duplication method\u2014no need to hand code anything.\n3. Always assume duplicates in all data tables. If you don\u2019t want them, add a test to find problems before a dashboard user does!\n4. Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.\n5. Have a prod-like environment for every developer, e.g., a {user\\_prefix}\\_data snowflake schema, to work fast.\n6. Run everything through CI/CD. Why test by hand if a machine can do it for you?\n\n&amp;#x200B;\n\nDo you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  \n", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering practices that translate well to data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fxs4p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687422900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  &lt;/p&gt;\n\n&lt;p&gt;But I particularly like the practice of &lt;strong&gt;&amp;quot;keeping code changes small and simple, and getting them into production fast by testing &amp;amp; automating&amp;quot;&lt;/strong&gt; (if you can call this a practice) - that, in my experience works pretty well in &lt;strong&gt;data ingestion&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how I&amp;#39;m going about it when working on data pipelines:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ingest all columns for a new table, but don\u2019t ingest additional tables unless necessary. Keep your ingestion code simple.&lt;/li&gt;\n&lt;li&gt;Prefer log-based ingestion &amp;gt; full table &amp;gt; key based. Log-based is by far the simplest duplication method\u2014no need to hand code anything.&lt;/li&gt;\n&lt;li&gt;Always assume duplicates in all data tables. If you don\u2019t want them, add a test to find problems before a dashboard user does!&lt;/li&gt;\n&lt;li&gt;Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.&lt;/li&gt;\n&lt;li&gt;Have a prod-like environment for every developer, e.g., a {user_prefix}_data snowflake schema, to work fast.&lt;/li&gt;\n&lt;li&gt;Run everything through CI/CD. Why test by hand if a machine can do it for you?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fxs4p", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fxs4p/software_engineering_practices_that_translate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fxs4p/software_engineering_practices_that_translate/", "subreddit_subscribers": 111707, "created_utc": 1687422900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "No transformation is required, just replicating tables as in source. \n\nHere are options we have tried:\n\nDataflow/ DataStream: Too costly.\n\nFivetran: Too slow (and forces to create new columns in sink to be able to support the incremental changes)\n\nApache NiFi: too much Java code &amp; hard to code for inc sync.\n\nHevo data/ Integrateio: Eval in progress.\n\nAirbyte: seems not much useful for high volume\n\nSo far we found Fivetran to hold our use case better but the performance is so poor that we would like to explore options. Would really appreciate any idea to solve this on. ", "author_fullname": "t2_1l9tu7ql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools / Best way to ingest large 20TB plus data from Bigquery to Google CloudSQL Postgres with 2TB daily changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fkmbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687384326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No transformation is required, just replicating tables as in source. &lt;/p&gt;\n\n&lt;p&gt;Here are options we have tried:&lt;/p&gt;\n\n&lt;p&gt;Dataflow/ DataStream: Too costly.&lt;/p&gt;\n\n&lt;p&gt;Fivetran: Too slow (and forces to create new columns in sink to be able to support the incremental changes)&lt;/p&gt;\n\n&lt;p&gt;Apache NiFi: too much Java code &amp;amp; hard to code for inc sync.&lt;/p&gt;\n\n&lt;p&gt;Hevo data/ Integrateio: Eval in progress.&lt;/p&gt;\n\n&lt;p&gt;Airbyte: seems not much useful for high volume&lt;/p&gt;\n\n&lt;p&gt;So far we found Fivetran to hold our use case better but the performance is so poor that we would like to explore options. Would really appreciate any idea to solve this on. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fkmbu", "is_robot_indexable": true, "report_reasons": null, "author": "onksssss", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fkmbu/tools_best_way_to_ingest_large_20tb_plus_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fkmbu/tools_best_way_to_ingest_large_20tb_plus_data/", "subreddit_subscribers": 111707, "created_utc": 1687384326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering how long would it take in average for a person with healthcare background to enter into data engineer field? I have a bachelors in medical sciences and a masters in healthcare Informatics. I have a certification in SAS, have basic understanding and programming knowledge of SQL and R. I have zero knowledge of python java or anyother programming tool. Is it gonna be very challenging for me to work as data engineer?", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Break into Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ffv95", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687373172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering how long would it take in average for a person with healthcare background to enter into data engineer field? I have a bachelors in medical sciences and a masters in healthcare Informatics. I have a certification in SAS, have basic understanding and programming knowledge of SQL and R. I have zero knowledge of python java or anyother programming tool. Is it gonna be very challenging for me to work as data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14ffv95", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ffv95/break_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ffv95/break_into_data_engineering/", "subreddit_subscribers": 111707, "created_utc": 1687373172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am very new to the data governance field. What we are looking for is a tool which allows our teams to explore the data catalog and request to access to a dataset. \n\nOpenMetadata seems to be a good fit for the first requirement, but how do you provide access to the underlying data, not from the OpenMetadata UI, but via a connector. If all access rules are defined here but the access management is not covered within the same tool, how do you ensure the access is limited to data as defined by the policies.", "author_fullname": "t2_di0ejkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which open source or commercial tools are used for Data Governance and access management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fzpiz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687429373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very new to the data governance field. What we are looking for is a tool which allows our teams to explore the data catalog and request to access to a dataset. &lt;/p&gt;\n\n&lt;p&gt;OpenMetadata seems to be a good fit for the first requirement, but how do you provide access to the underlying data, not from the OpenMetadata UI, but via a connector. If all access rules are defined here but the access management is not covered within the same tool, how do you ensure the access is limited to data as defined by the policies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fzpiz", "is_robot_indexable": true, "report_reasons": null, "author": "smshuja", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fzpiz/which_open_source_or_commercial_tools_are_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fzpiz/which_open_source_or_commercial_tools_are_used/", "subreddit_subscribers": 111707, "created_utc": 1687429373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Or just your go-to tools?\n\nOr do you just not have any real preference, more like \"whatever tool fits the job\"?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your favorite Data Scraping tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fc8fj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687364624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or just your go-to tools?&lt;/p&gt;\n\n&lt;p&gt;Or do you just not have any real preference, more like &amp;quot;whatever tool fits the job&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fc8fj", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fc8fj/what_are_your_favorite_data_scraping_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fc8fj/what_are_your_favorite_data_scraping_tools/", "subreddit_subscribers": 111707, "created_utc": 1687364624.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DEs! Thanks for so much info over the last year, this is a great resource for those of us in the field. \n\nI have been working as a \"Data Engineer\" for the last year and a half. The reason I put this in quotes is because I don't feel like I live up to the title, as cool as it sounds. I read through these posts daily and have no idea what most of you are talking about, most of the time. A little background is that I came into this position by luck because I had a good resume within the company and the company doesn't have much internal competition for technical positions. I have no background in CS, DWH, DE, or really any type of coding. I've entirely learned on the job. \n\nThe problem with that is this is a very large company ( a large government organization ) and I work on one specific team that handles ETL exclusively. If I understand the term correctly our stack is: Ab Initio (ETL apps), oracle db for staging tables, and UNIX servers. My daily work consists of making updates to these ab initio applications. Within those applications are a bunch of unix shell scripts that I also have to work with. As far as the oracle db, I don't really do any SQL work. We have a modeling team that handles all of that. So the primary skills would be Ab Initio development &amp; shell scripting, and then navigating the metric ton of red tape for promotions to prod. \n\nMy question is , am I wasting my time? From everything I read on here, it seems like ab initio is a dying tool on its way out. It also seems like shell scripting isn't used much. Everyone seems to be using python, cloud, and a litany of other tools I've never heard of. This type of work does not come easy to me, as much as I wish it did. I work with a bunch of people who are either not great at the job, and if they are they aren't good communicators, nor do they want to mentor a new developer. \n\nTLDR: It feels like I am fighting an uphill battle to learn outdated technology that is only useful in my current role but doesn't actually build me into a true DE. I'm still relatively young and at a pivotal moment of my career. It feels like if I want a career in this field, this isn't the right place. What would you data Jedis do?", "author_fullname": "t2_73cw9sv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dinosaur tools &amp; a dead stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fajwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687360785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DEs! Thanks for so much info over the last year, this is a great resource for those of us in the field. &lt;/p&gt;\n\n&lt;p&gt;I have been working as a &amp;quot;Data Engineer&amp;quot; for the last year and a half. The reason I put this in quotes is because I don&amp;#39;t feel like I live up to the title, as cool as it sounds. I read through these posts daily and have no idea what most of you are talking about, most of the time. A little background is that I came into this position by luck because I had a good resume within the company and the company doesn&amp;#39;t have much internal competition for technical positions. I have no background in CS, DWH, DE, or really any type of coding. I&amp;#39;ve entirely learned on the job. &lt;/p&gt;\n\n&lt;p&gt;The problem with that is this is a very large company ( a large government organization ) and I work on one specific team that handles ETL exclusively. If I understand the term correctly our stack is: Ab Initio (ETL apps), oracle db for staging tables, and UNIX servers. My daily work consists of making updates to these ab initio applications. Within those applications are a bunch of unix shell scripts that I also have to work with. As far as the oracle db, I don&amp;#39;t really do any SQL work. We have a modeling team that handles all of that. So the primary skills would be Ab Initio development &amp;amp; shell scripting, and then navigating the metric ton of red tape for promotions to prod. &lt;/p&gt;\n\n&lt;p&gt;My question is , am I wasting my time? From everything I read on here, it seems like ab initio is a dying tool on its way out. It also seems like shell scripting isn&amp;#39;t used much. Everyone seems to be using python, cloud, and a litany of other tools I&amp;#39;ve never heard of. This type of work does not come easy to me, as much as I wish it did. I work with a bunch of people who are either not great at the job, and if they are they aren&amp;#39;t good communicators, nor do they want to mentor a new developer. &lt;/p&gt;\n\n&lt;p&gt;TLDR: It feels like I am fighting an uphill battle to learn outdated technology that is only useful in my current role but doesn&amp;#39;t actually build me into a true DE. I&amp;#39;m still relatively young and at a pivotal moment of my career. It feels like if I want a career in this field, this isn&amp;#39;t the right place. What would you data Jedis do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14fajwv", "is_robot_indexable": true, "report_reasons": null, "author": "ApatheticRart", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fajwv/dinosaur_tools_a_dead_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fajwv/dinosaur_tools_a_dead_stack/", "subreddit_subscribers": 111707, "created_utc": 1687360785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Backstory: I run the eCommerce for a company that also has a storefront &amp; warehouse on location where we ship out orders. For years we have been using RetailPro as our inventory system and 24seven as a middleware to send our product to Shopify.\n\nOur parent company is making us switch to DataWorks, with Boomi sending product data to Magento as our new eCommerce platform. We\u2019ve been going through this new process with an outside company doing the integration work for us. \n\nJust recently we were told that Boomi can only send product data from 1 location in DataWorks, such as the Warehouse. The way we\u2019ve always done it before with 24seven is ALL inventory data between the Store &amp; Warehouse would be sent over to Shopify. This has always been the preference since a lot of product doesn\u2019t have inventory in the Warehouse, as it\u2019s seasonal. If needed we could pull from the Store to fulfill orders.\n\nI\u2019m in no means a data engineer, nor is anybody on my team. But I just need to know if there is a solution for this issue.\n\nTLDR; we want Boomi to send over inventory data from multiple locations in DataWorks over to Magento.", "author_fullname": "t2_3ps9svrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boomi related issue for eCommerce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fjbr3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687381345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Backstory: I run the eCommerce for a company that also has a storefront &amp;amp; warehouse on location where we ship out orders. For years we have been using RetailPro as our inventory system and 24seven as a middleware to send our product to Shopify.&lt;/p&gt;\n\n&lt;p&gt;Our parent company is making us switch to DataWorks, with Boomi sending product data to Magento as our new eCommerce platform. We\u2019ve been going through this new process with an outside company doing the integration work for us. &lt;/p&gt;\n\n&lt;p&gt;Just recently we were told that Boomi can only send product data from 1 location in DataWorks, such as the Warehouse. The way we\u2019ve always done it before with 24seven is ALL inventory data between the Store &amp;amp; Warehouse would be sent over to Shopify. This has always been the preference since a lot of product doesn\u2019t have inventory in the Warehouse, as it\u2019s seasonal. If needed we could pull from the Store to fulfill orders.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m in no means a data engineer, nor is anybody on my team. But I just need to know if there is a solution for this issue.&lt;/p&gt;\n\n&lt;p&gt;TLDR; we want Boomi to send over inventory data from multiple locations in DataWorks over to Magento.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fjbr3", "is_robot_indexable": true, "report_reasons": null, "author": "i-race-goats", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fjbr3/boomi_related_issue_for_ecommerce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fjbr3/boomi_related_issue_for_ecommerce/", "subreddit_subscribers": 111707, "created_utc": 1687381345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to learn Apache Iceberg for building a data lake. We have late arriving data and the data is partitioned on date column. I will have a spark job that will transform the incoming data to iceberg format. Consider a scenario where the ingestion pipeline fails mid way, since dynamic partition overwrite is enabled, a rerun of the task will overwrite only the partitions that were created in previous run. This will work if there is no late arriving data. But consider a situation where I have data from the last day and the partition for that has already been created when I ingested the data in the yesterday\u2019s run. Now that the current day (which has late arriving data) run fails and since I have set dynamic partition overwrite, the partition that was created yesterday will also get rewritten. Is there a better way to handle dynamic partition overwrite for atomicity as well and late arriving data", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg Dynamic Partition Overwrite.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fc6i0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687364500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to learn Apache Iceberg for building a data lake. We have late arriving data and the data is partitioned on date column. I will have a spark job that will transform the incoming data to iceberg format. Consider a scenario where the ingestion pipeline fails mid way, since dynamic partition overwrite is enabled, a rerun of the task will overwrite only the partitions that were created in previous run. This will work if there is no late arriving data. But consider a situation where I have data from the last day and the partition for that has already been created when I ingested the data in the yesterday\u2019s run. Now that the current day (which has late arriving data) run fails and since I have set dynamic partition overwrite, the partition that was created yesterday will also get rewritten. Is there a better way to handle dynamic partition overwrite for atomicity as well and late arriving data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fc6i0", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fc6i0/apache_iceberg_dynamic_partition_overwrite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fc6i0/apache_iceberg_dynamic_partition_overwrite/", "subreddit_subscribers": 111707, "created_utc": 1687364500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an upcoming interview for a hybrid role that blends Data Engineering and DevOps. The role leans more towards DevOps but involves some crucial Data Engineering responsibilities as well. The company is embarking on an ambitious project to develop and deploy a new data streaming service integrating their clients' existing tech architecture using the Google Cloud Platform (GCP).  \n\n\nThe role includes:\n\n* Defining project workbooks, supporting the delivery of the new platform\n* Designing and implementing CI/CD pipelines\n* Developing and maintaining Infrastructure as Code (IaC) scripts\n* Monitoring and optimizing data streaming service performance\n* Ensuring security and compliance\n* Troubleshooting production and non-production environments\n* Some Data Engineering experience is preferred - Python/Pandas/SQL/PySpark\n*  Excellent understanding of managed GCP Services, Docker, Kubernetes\n* Experienced in using a modern CI SaaS platform such as Github Actions, CircleCI, GitLab, etc  \n\n\nGiven these factors, I'm seeking advice on a few specific areas:\n\n1. Which GCP-specific practices/concepts and Data Engineering tasks would be valuable to showcase in relation to a data streaming service?\n2. How can I best demonstrate my Python scripting and automation skills in a DevOps/Data Engineering context?\n3. How should I approach discussing the challenges and strategies of handling the hybrid nature of this role?\n4. What are some potential obstacles I might encounter in managing the performance, scalability, and reliability of a data streaming service?\n5. Are there additional tools, libraries, or frameworks that would be beneficial to learn for this role, especially in the context of GCP?\n\nI genuinely appreciate any insights, experiences, or resources you can share. I believe this is an incredible learning opportunity and I'm excited about the potential challenges that await me. Thanks in advance for your invaluable guidance.", "author_fullname": "t2_co7b8hy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preparing for a Hybrid Data Engineer/DevOps Interview Involving a GCP-Based Data Streaming Service - Seeking Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g076n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687430961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an upcoming interview for a hybrid role that blends Data Engineering and DevOps. The role leans more towards DevOps but involves some crucial Data Engineering responsibilities as well. The company is embarking on an ambitious project to develop and deploy a new data streaming service integrating their clients&amp;#39; existing tech architecture using the Google Cloud Platform (GCP).  &lt;/p&gt;\n\n&lt;p&gt;The role includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Defining project workbooks, supporting the delivery of the new platform&lt;/li&gt;\n&lt;li&gt;Designing and implementing CI/CD pipelines&lt;/li&gt;\n&lt;li&gt;Developing and maintaining Infrastructure as Code (IaC) scripts&lt;/li&gt;\n&lt;li&gt;Monitoring and optimizing data streaming service performance&lt;/li&gt;\n&lt;li&gt;Ensuring security and compliance&lt;/li&gt;\n&lt;li&gt;Troubleshooting production and non-production environments&lt;/li&gt;\n&lt;li&gt;Some Data Engineering experience is preferred - Python/Pandas/SQL/PySpark&lt;/li&gt;\n&lt;li&gt; Excellent understanding of managed GCP Services, Docker, Kubernetes&lt;/li&gt;\n&lt;li&gt;Experienced in using a modern CI SaaS platform such as Github Actions, CircleCI, GitLab, etc&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Given these factors, I&amp;#39;m seeking advice on a few specific areas:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which GCP-specific practices/concepts and Data Engineering tasks would be valuable to showcase in relation to a data streaming service?&lt;/li&gt;\n&lt;li&gt;How can I best demonstrate my Python scripting and automation skills in a DevOps/Data Engineering context?&lt;/li&gt;\n&lt;li&gt;How should I approach discussing the challenges and strategies of handling the hybrid nature of this role?&lt;/li&gt;\n&lt;li&gt;What are some potential obstacles I might encounter in managing the performance, scalability, and reliability of a data streaming service?&lt;/li&gt;\n&lt;li&gt;Are there additional tools, libraries, or frameworks that would be beneficial to learn for this role, especially in the context of GCP?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I genuinely appreciate any insights, experiences, or resources you can share. I believe this is an incredible learning opportunity and I&amp;#39;m excited about the potential challenges that await me. Thanks in advance for your invaluable guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14g076n", "is_robot_indexable": true, "report_reasons": null, "author": "DaCharisma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g076n/preparing_for_a_hybrid_data_engineerdevops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g076n/preparing_for_a_hybrid_data_engineerdevops/", "subreddit_subscribers": 111707, "created_utc": 1687430961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conceptual vs Logical vs Physical Data Models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14fzp4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Z_HueKSwldii1S8qUy67aCaXZjvKJkAg1z6modf8F0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687429336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thoughtspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thoughtspot.com/data-trends/data-modeling/conceptual-vs-logical-vs-physical-data-models", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?auto=webp&amp;v=enabled&amp;s=470b761fd754eaf537a0a155fd9d6e9b3d63c460", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e71ffe38058b5fd5c000f61d4032947638b40283", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0dcc8ceeb5c6643bad3dc788896755b15eb62d95", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adeceec78ad7614f06f0665f00f241c7c6d53192", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c70ac6892f5fd31df37e1cf74f3c27ed10a242b9", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9927f79503b60f9b53dcaa45f812edc9f26f0fa8", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a64cafc13f298b1724e6f7c2eccbc9436711f2b5", "width": 1080, "height": 565}], "variants": {}, "id": "RT3blwaFNDO7eUz6Pj4gd5FZXhod7maQjBHzvInU8JU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14fzp4i", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fzp4i/conceptual_vs_logical_vs_physical_data_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thoughtspot.com/data-trends/data-modeling/conceptual-vs-logical-vs-physical-data-models", "subreddit_subscribers": 111707, "created_utc": 1687429336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nCurrently working as a DE in a small company but I want to get into banking considering it\u2019s job stability and future career development. \n\nAny books or maybe tutorials to study banking knowledge? Looks like their tech stacks are normally old.", "author_fullname": "t2_oorup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Want to get banking/fintech DE job. Any books or tutorials to learn related knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fysdp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687426314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working as a DE in a small company but I want to get into banking considering it\u2019s job stability and future career development. &lt;/p&gt;\n\n&lt;p&gt;Any books or maybe tutorials to study banking knowledge? Looks like their tech stacks are normally old.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fysdp", "is_robot_indexable": true, "report_reasons": null, "author": "GeForceKawaiiyo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fysdp/want_to_get_bankingfintech_de_job_any_books_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fysdp/want_to_get_bankingfintech_de_job_any_books_or/", "subreddit_subscribers": 111707, "created_utc": 1687426314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a pipeline (Python) where I am pulling from an API and pushing a CSV file into S3  \nNext step is to COPY INTO a Snowflake table from that file I just dropped into S3 \n\nCurrently debating using one of these two approaches\n\nMake a call with all the information needed (secrets are stored separately and called at runtime).\n\n    COPY INTO mytable\n      FROM s3://mybucket/path/file.csv credentials=(AWS_KEY_ID='$AWS_ACCESS_KEY_ID' AWS_SECRET_KEY='$AWS_SECRET_ACCESS_KEY')\n      FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1);\n\nOr create a named stage with my credentials and an associated file format, then simply \"use\" it...\n\n     COPY INTO mytable FROM @my_ext_stage FILES='file.csv'; \n\nThe only pros I can see for using a named stage is that my credentials would not be needed in the query  \n\n\nOn the other hand, the first method is more flexible, as I can change the path in the bucket within python. Plus I don't have to bother creating a separate stage for each table I'll be landing into, as well as an associated file format (in each and every schema).\n\nHaving the named stage go to a specific URL ( 's3://mybucket/path/') seems incredibly limiting.  \n", "author_fullname": "t2_qhsi5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Stages Best Practices - Named stages seem limiting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fqpkh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687400422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a pipeline (Python) where I am pulling from an API and pushing a CSV file into S3&lt;br/&gt;\nNext step is to COPY INTO a Snowflake table from that file I just dropped into S3 &lt;/p&gt;\n\n&lt;p&gt;Currently debating using one of these two approaches&lt;/p&gt;\n\n&lt;p&gt;Make a call with all the information needed (secrets are stored separately and called at runtime).&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;COPY INTO mytable\n  FROM s3://mybucket/path/file.csv credentials=(AWS_KEY_ID=&amp;#39;$AWS_ACCESS_KEY_ID&amp;#39; AWS_SECRET_KEY=&amp;#39;$AWS_SECRET_ACCESS_KEY&amp;#39;)\n  FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = &amp;#39;,&amp;#39; SKIP_HEADER = 1);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or create a named stage with my credentials and an associated file format, then simply &amp;quot;use&amp;quot; it...&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; COPY INTO mytable FROM @my_ext_stage FILES=&amp;#39;file.csv&amp;#39;; \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The only pros I can see for using a named stage is that my credentials would not be needed in the query  &lt;/p&gt;\n\n&lt;p&gt;On the other hand, the first method is more flexible, as I can change the path in the bucket within python. Plus I don&amp;#39;t have to bother creating a separate stage for each table I&amp;#39;ll be landing into, as well as an associated file format (in each and every schema).&lt;/p&gt;\n\n&lt;p&gt;Having the named stage go to a specific URL ( &amp;#39;s3://mybucket/path/&amp;#39;) seems incredibly limiting.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fqpkh", "is_robot_indexable": true, "report_reasons": null, "author": "NoUsernames1eft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fqpkh/snowflake_stages_best_practices_named_stages_seem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fqpkh/snowflake_stages_best_practices_named_stages_seem/", "subreddit_subscribers": 111707, "created_utc": 1687400422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying Databricks for the first time. I created a 'standard' workspace using 'Quickstart on AWS' option.\n\nCloudformation runs into error during 'assign Metastore' step.\n\nFollowing is the error from Cloudwatch:\n\nHTTP content: b'\n\n{ \"error\\_code\": \"PERMISSION\\_DENIED\", \"message\": \"Cannot assign metastore to STANDARD tier workspace xyz\" }\n\nWhat is the solution for this?", "author_fullname": "t2_jl74ygnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Error while creating workspace with 'Quickstart' on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14faojz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687361087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying Databricks for the first time. I created a &amp;#39;standard&amp;#39; workspace using &amp;#39;Quickstart on AWS&amp;#39; option.&lt;/p&gt;\n\n&lt;p&gt;Cloudformation runs into error during &amp;#39;assign Metastore&amp;#39; step.&lt;/p&gt;\n\n&lt;p&gt;Following is the error from Cloudwatch:&lt;/p&gt;\n\n&lt;p&gt;HTTP content: b&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;error_code&amp;quot;: &amp;quot;PERMISSION_DENIED&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;Cannot assign metastore to STANDARD tier workspace xyz&amp;quot; }&lt;/p&gt;\n\n&lt;p&gt;What is the solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14faojz", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Device689", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14faojz/databricks_error_while_creating_workspace_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14faojz/databricks_error_while_creating_workspace_with/", "subreddit_subscribers": 111707, "created_utc": 1687361087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So far I've made one airflow project: Airflow running on an EC2 instance pulls data from an API, transforms it and loads it to AWS S3. \n\nI am a CSE student learning DE, how much Airflow should be sufficient to be able to impress a recruiter or maybe a potential mentor? \n\nI already have some base built on the basics of DE and want to spend the next 1-2 weeks learning airflow. What projects should I make?", "author_fullname": "t2_ens6kw85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much Airflow do I learn as a student?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14g58uf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687444604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So far I&amp;#39;ve made one airflow project: Airflow running on an EC2 instance pulls data from an API, transforms it and loads it to AWS S3. &lt;/p&gt;\n\n&lt;p&gt;I am a CSE student learning DE, how much Airflow should be sufficient to be able to impress a recruiter or maybe a potential mentor? &lt;/p&gt;\n\n&lt;p&gt;I already have some base built on the basics of DE and want to spend the next 1-2 weeks learning airflow. What projects should I make?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14g58uf", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent-Tadpole-564", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g58uf/how_much_airflow_do_i_learn_as_a_student/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g58uf/how_much_airflow_do_i_learn_as_a_student/", "subreddit_subscribers": 111707, "created_utc": 1687444604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to add a final validation step to a pipeline that outputs a report each time it\u2019s run for compliance reasons.\n\nPart of the validation is to ensure that all of the records at the end of the pipeline correspond to records at the start of the pipeline, and we haven\u2019t had stuff go missing, slip through business rules etc.\n\nIn SQL I\u2019d just do this as a join between two tables with some appropriate conditions and check the output.\n\nIs GX right for doing this, or is it more appropriate for a different sort of validation?", "author_fullname": "t2_xr8z9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this the correct use case for Great Expectations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14g53cn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687444238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to add a final validation step to a pipeline that outputs a report each time it\u2019s run for compliance reasons.&lt;/p&gt;\n\n&lt;p&gt;Part of the validation is to ensure that all of the records at the end of the pipeline correspond to records at the start of the pipeline, and we haven\u2019t had stuff go missing, slip through business rules etc.&lt;/p&gt;\n\n&lt;p&gt;In SQL I\u2019d just do this as a join between two tables with some appropriate conditions and check the output.&lt;/p&gt;\n\n&lt;p&gt;Is GX right for doing this, or is it more appropriate for a different sort of validation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14g53cn", "is_robot_indexable": true, "report_reasons": null, "author": "NeuralHijacker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g53cn/is_this_the_correct_use_case_for_great/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g53cn/is_this_the_correct_use_case_for_great/", "subreddit_subscribers": 111707, "created_utc": 1687444238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn Dbt, I have an incremental model called Test_a. I  have also a second model called Test_b that references model Test_a. I want the model Test_b to update only the rows that where updated in model Test_a. Can you help me?\n\nThanks", "author_fullname": "t2_5kcd2wet", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g0n1v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687432326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In Dbt, I have an incremental model called Test_a. I  have also a second model called Test_b that references model Test_a. I want the model Test_b to update only the rows that where updated in model Test_a. Can you help me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14g0n1v", "is_robot_indexable": true, "report_reasons": null, "author": "redtiger2019", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g0n1v/question_on_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g0n1v/question_on_dbt/", "subreddit_subscribers": 111707, "created_utc": 1687432326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Friends,   \nI am currently working as senior data engineer with 8yoe. I am looking forward to move into consulting role in the same organisation, But problem is i think I lacks soft skill, I feel like I don't have a flow in communication, I don't recall business terms, and don't have conviction in what I say.   \nSo, I am looking forward to know:  \n1. Best way to learn the business communication.  \n2. How to handle the when you don't know something, or say something incorrect.  \n3. How to be a pro consultant.  \nI know the most common answer for these is practise, but how do I prepare before I directly jump into that. \n\nPlease also recommend if you know any books, course, video etc.", "author_fullname": "t2_u1vbo568", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need guidance on consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fu5wp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687410769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Friends,&lt;br/&gt;\nI am currently working as senior data engineer with 8yoe. I am looking forward to move into consulting role in the same organisation, But problem is i think I lacks soft skill, I feel like I don&amp;#39;t have a flow in communication, I don&amp;#39;t recall business terms, and don&amp;#39;t have conviction in what I say.&lt;br/&gt;\nSo, I am looking forward to know:&lt;br/&gt;\n1. Best way to learn the business communication.&lt;br/&gt;\n2. How to handle the when you don&amp;#39;t know something, or say something incorrect.&lt;br/&gt;\n3. How to be a pro consultant.&lt;br/&gt;\nI know the most common answer for these is practise, but how do I prepare before I directly jump into that. &lt;/p&gt;\n\n&lt;p&gt;Please also recommend if you know any books, course, video etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fu5wp", "is_robot_indexable": true, "report_reasons": null, "author": "manu13891", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fu5wp/need_guidance_on_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fu5wp/need_guidance_on_consulting/", "subreddit_subscribers": 111707, "created_utc": 1687410769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a database, obviously it's common to enforce things like \"all rows in the \\`purchases\\` table must have a foreign key relationship to the \\`users\\` table (we have to know which user made each purchase!)\"\n\nWhen ELT'ing data to a data warehouse, I wrote a test in dbt to check the same thing.\n\nThis test periodically fails though, due to the fact that sometimes the ELT process transfers the \\`users\\` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the \\`purchases\\` table to the data warehouse. The result being that there's a purchase in the data warehouse which does not have a corresponding user in the user's table. Which means my dbt test fails.\n\nIs this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship \"for purchases made more than X hours ago\", where X is greater than the most recent load into the dw.", "author_fullname": "t2_116mtw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle table relationships in data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fnn1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687391877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a database, obviously it&amp;#39;s common to enforce things like &amp;quot;all rows in the `purchases` table must have a foreign key relationship to the `users` table (we have to know which user made each purchase!)&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;When ELT&amp;#39;ing data to a data warehouse, I wrote a test in dbt to check the same thing.&lt;/p&gt;\n\n&lt;p&gt;This test periodically fails though, due to the fact that sometimes the ELT process transfers the `users` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the `purchases` table to the data warehouse. The result being that there&amp;#39;s a purchase in the data warehouse which does not have a corresponding user in the user&amp;#39;s table. Which means my dbt test fails.&lt;/p&gt;\n\n&lt;p&gt;Is this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship &amp;quot;for purchases made more than X hours ago&amp;quot;, where X is greater than the most recent load into the dw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fnn1s", "is_robot_indexable": true, "report_reasons": null, "author": "PM_ME_SCIENCEY_STUFF", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "subreddit_subscribers": 111707, "created_utc": 1687391877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nI'm trying to implement  delta architecture to spark pipelines in my organization. I understand most of the idea but i still wonder what's the correct approach to cache last processed delta commit id from CDF in downstream pipelines. This will emable me to process only new data there. In most tutorials and articles CDC capabilities of delta are shown with those values hardcoded. I plan store it there as kv 'pipelinename':'last_read_version' in either separate bookmark table or some kv store solution. Is that the correct approach to solve it or am I missing something?", "author_fullname": "t2_bf5ei6em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC im delta lake - how to store commit id downstream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14feh1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687374132.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687369853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nI&amp;#39;m trying to implement  delta architecture to spark pipelines in my organization. I understand most of the idea but i still wonder what&amp;#39;s the correct approach to cache last processed delta commit id from CDF in downstream pipelines. This will emable me to process only new data there. In most tutorials and articles CDC capabilities of delta are shown with those values hardcoded. I plan store it there as kv &amp;#39;pipelinename&amp;#39;:&amp;#39;last_read_version&amp;#39; in either separate bookmark table or some kv store solution. Is that the correct approach to solve it or am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14feh1u", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning_Hurry2611", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14feh1u/cdc_im_delta_lake_how_to_store_commit_id/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14feh1u/cdc_im_delta_lake_how_to_store_commit_id/", "subreddit_subscribers": 111707, "created_utc": 1687369853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9msdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inside Starburst's hackathon: Into the great wide open(AI)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_14fbehd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/voccu5Jpp747AIKYmZfqwNf18sCRWpOlm7toVOuASio.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687362748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dev.to", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dev.to/starburstengineering/inside-starbursts-hackathon-into-the-great-wide-openai-3nl8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?auto=webp&amp;v=enabled&amp;s=aee7695eb34a88bed0b0b023121ef0e73fe7d268", "width": 1128, "height": 598}, "resolutions": [{"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b037eecc6379553b7a6d02fa3e40f8afc7c10d45", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92c758f2e18da31cd4db9c6acbf98f1911b54a10", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e40fdfe9460a0bb901a24bbb24284530349ff50f", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6dac3ddb39d735a2121b81201836dbf740d831e", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d489266392bfc1dacc29bac02181411a714a3175", "width": 960, "height": 508}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13aff827560a0748b74ae432b23b63849518e2fe", "width": 1080, "height": 572}], "variants": {}, "id": "k883IOz-8cn8hz390a3aFIjLzjAXzcFAs5flDn8iqwo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14fbehd", "is_robot_indexable": true, "report_reasons": null, "author": "randgalt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fbehd/inside_starbursts_hackathon_into_the_great_wide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dev.to/starburstengineering/inside-starbursts-hackathon-into-the-great-wide-openai-3nl8", "subreddit_subscribers": 111707, "created_utc": 1687362748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering that I am a Cse student who's learning data engineering while making medium blogs on my projects. \nI want to grow my network as it's the best way to find mentors and eventually jobs.", "author_fullname": "t2_ens6kw85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I network with data engineers considering...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14g3zmb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687441561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering that I am a Cse student who&amp;#39;s learning data engineering while making medium blogs on my projects. \nI want to grow my network as it&amp;#39;s the best way to find mentors and eventually jobs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14g3zmb", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent-Tadpole-564", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g3zmb/how_do_i_network_with_data_engineers_considering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g3zmb/how_do_i_network_with_data_engineers_considering/", "subreddit_subscribers": 111707, "created_utc": 1687441561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I'm aware of describe history and querying the delta table according to version. I was wondering if there is a way to find history of a record based on its I'd efficiently. Currently I have to sift through all versions to find out the history.", "author_fullname": "t2_rthmzez0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I get SCD2 like output by using Delta table time travel feature?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g0y4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687433283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m aware of describe history and querying the delta table according to version. I was wondering if there is a way to find history of a record based on its I&amp;#39;d efficiently. Currently I have to sift through all versions to find out the history.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14g0y4x", "is_robot_indexable": true, "report_reasons": null, "author": "rainybuzz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g0y4x/can_i_get_scd2_like_output_by_using_delta_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g0y4x/can_i_get_scd2_like_output_by_using_delta_table/", "subreddit_subscribers": 111707, "created_utc": 1687433283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning and doing certification in big query to get into data analytics /engineer field, i wanted to ask how in real time in companies , on job people to interact with big query framework ,  CLI or api ?", "author_fullname": "t2_t526hbv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interface with BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fp7f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687396115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning and doing certification in big query to get into data analytics /engineer field, i wanted to ask how in real time in companies , on job people to interact with big query framework ,  CLI or api ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fp7f7", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Ticket6016", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fp7f7/interface_with_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fp7f7/interface_with_bigquery/", "subreddit_subscribers": 111707, "created_utc": 1687396115.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}