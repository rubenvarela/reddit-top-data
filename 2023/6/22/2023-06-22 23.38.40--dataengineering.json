{"kind": "Listing", "data": {"after": "t3_14g0y4x", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.\n\nLots of dimensions and facts needing to be processed differently from each sources to be correctly represented.\n\nIf I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).\n\nWhat would be the best way to create a documentation that can be shared with business? Is there \"one\" system or will there be a bunch of several artefacts, UMLs etc needed?", "author_fullname": "t2_e4028dc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you do documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fv52f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687413946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.&lt;/p&gt;\n\n&lt;p&gt;Lots of dimensions and facts needing to be processed differently from each sources to be correctly represented.&lt;/p&gt;\n\n&lt;p&gt;If I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to create a documentation that can be shared with business? Is there &amp;quot;one&amp;quot; system or will there be a bunch of several artefacts, UMLs etc needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fv52f", "is_robot_indexable": true, "report_reasons": null, "author": "tzt1324", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fv52f/how_do_you_do_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fv52f/how_do_you_do_documentation/", "subreddit_subscribers": 111754, "created_utc": 1687413946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  \n\n\nBut I particularly like the practice of **\"keeping code changes small and simple, and getting them into production fast by testing &amp; automating\"** (if you can call this a practice) - that, in my experience works pretty well in **data ingestion**.\n\nHere's how I'm going about it when working on data pipelines:\n\n1. Ingest all columns for a new table, but don\u2019t ingest additional tables unless necessary. Keep your ingestion code simple.\n2. Prefer log-based ingestion &gt; full table &gt; key based. Log-based is by far the simplest duplication method\u2014no need to hand code anything.\n3. Always assume duplicates in all data tables. If you don\u2019t want them, add a test to find problems before a dashboard user does!\n4. Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.\n5. Have a prod-like environment for every developer, e.g., a {user\\_prefix}\\_data snowflake schema, to work fast.\n6. Run everything through CI/CD. Why test by hand if a machine can do it for you?\n\n&amp;#x200B;\n\nDo you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  \n", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering practices that translate well to data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fxs4p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687422900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  &lt;/p&gt;\n\n&lt;p&gt;But I particularly like the practice of &lt;strong&gt;&amp;quot;keeping code changes small and simple, and getting them into production fast by testing &amp;amp; automating&amp;quot;&lt;/strong&gt; (if you can call this a practice) - that, in my experience works pretty well in &lt;strong&gt;data ingestion&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how I&amp;#39;m going about it when working on data pipelines:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ingest all columns for a new table, but don\u2019t ingest additional tables unless necessary. Keep your ingestion code simple.&lt;/li&gt;\n&lt;li&gt;Prefer log-based ingestion &amp;gt; full table &amp;gt; key based. Log-based is by far the simplest duplication method\u2014no need to hand code anything.&lt;/li&gt;\n&lt;li&gt;Always assume duplicates in all data tables. If you don\u2019t want them, add a test to find problems before a dashboard user does!&lt;/li&gt;\n&lt;li&gt;Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.&lt;/li&gt;\n&lt;li&gt;Have a prod-like environment for every developer, e.g., a {user_prefix}_data snowflake schema, to work fast.&lt;/li&gt;\n&lt;li&gt;Run everything through CI/CD. Why test by hand if a machine can do it for you?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fxs4p", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fxs4p/software_engineering_practices_that_translate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fxs4p/software_engineering_practices_that_translate/", "subreddit_subscribers": 111754, "created_utc": 1687422900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uncovering Stack Overflow's Shocking Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_14g775a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fKc050dvNIE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Uncovering Stack Overflow&amp;#39;s Shocking Architecture\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Uncovering Stack Overflow's Shocking Architecture", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fKc050dvNIE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Uncovering Stack Overflow&amp;#39;s Shocking Architecture\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fKc050dvNIE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fKc050dvNIE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Uncovering Stack Overflow&amp;#39;s Shocking Architecture\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/14g775a", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/yoMm50fLX8cDIp6choKIGKe5W9RQKWeVPGMwIhrL-yE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687449255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=fKc050dvNIE", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/64fQ8qTHWxoEZ_Fwpo0wK9Qv0vOJh72hPR1YDg70QKs.jpg?auto=webp&amp;v=enabled&amp;s=65332583204e1b05d7033b57632bb1f2ec44f4d6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/64fQ8qTHWxoEZ_Fwpo0wK9Qv0vOJh72hPR1YDg70QKs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cd325b3407c83db4fe421e7792a4abf0c42f697", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/64fQ8qTHWxoEZ_Fwpo0wK9Qv0vOJh72hPR1YDg70QKs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4eace538ec055a445947ae965e267213ce6cf42e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/64fQ8qTHWxoEZ_Fwpo0wK9Qv0vOJh72hPR1YDg70QKs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=931339ffcc856bd9a84cfdd33e4d82a138313dac", "width": 320, "height": 240}], "variants": {}, "id": "yTsw0sByiMAAmMy9juwEG9jZEX2AIKsGB_uxPFXgpzU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14g775a", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g775a/uncovering_stack_overflows_shocking_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=fKc050dvNIE", "subreddit_subscribers": 111754, "created_utc": 1687449255.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Uncovering Stack Overflow's Shocking Architecture", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fKc050dvNIE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Uncovering Stack Overflow&amp;#39;s Shocking Architecture\"&gt;&lt;/iframe&gt;", "author_name": "ByteByteGo", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fKc050dvNIE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ByteByteGo"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So far I've made one airflow project: Airflow running on an EC2 instance pulls data from an API, transforms it and loads it to AWS S3. \n\nI am a CSE student learning DE, how much Airflow should be sufficient to be able to impress a recruiter or maybe a potential mentor? \n\nI already have some base built on the basics of DE and want to spend the next 1-2 weeks learning airflow. What projects should I make?", "author_fullname": "t2_ens6kw85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much Airflow do I learn as a student?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g58uf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687444604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So far I&amp;#39;ve made one airflow project: Airflow running on an EC2 instance pulls data from an API, transforms it and loads it to AWS S3. &lt;/p&gt;\n\n&lt;p&gt;I am a CSE student learning DE, how much Airflow should be sufficient to be able to impress a recruiter or maybe a potential mentor? &lt;/p&gt;\n\n&lt;p&gt;I already have some base built on the basics of DE and want to spend the next 1-2 weeks learning airflow. What projects should I make?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14g58uf", "is_robot_indexable": true, "report_reasons": null, "author": "Diligent-Tadpole-564", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g58uf/how_much_airflow_do_i_learn_as_a_student/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g58uf/how_much_airflow_do_i_learn_as_a_student/", "subreddit_subscribers": 111754, "created_utc": 1687444604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am very new to the data governance field. What we are looking for is a tool which allows our teams to explore the data catalog and request to access to a dataset. \n\nOpenMetadata seems to be a good fit for the first requirement, but how do you provide access to the underlying data, not from the OpenMetadata UI, but via a connector. If all access rules are defined here but the access management is not covered within the same tool, how do you ensure the access is limited to data as defined by the policies.", "author_fullname": "t2_di0ejkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which open source or commercial tools are used for Data Governance and access management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fzpiz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687429373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very new to the data governance field. What we are looking for is a tool which allows our teams to explore the data catalog and request to access to a dataset. &lt;/p&gt;\n\n&lt;p&gt;OpenMetadata seems to be a good fit for the first requirement, but how do you provide access to the underlying data, not from the OpenMetadata UI, but via a connector. If all access rules are defined here but the access management is not covered within the same tool, how do you ensure the access is limited to data as defined by the policies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fzpiz", "is_robot_indexable": true, "report_reasons": null, "author": "smshuja", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fzpiz/which_open_source_or_commercial_tools_are_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fzpiz/which_open_source_or_commercial_tools_are_used/", "subreddit_subscribers": 111754, "created_utc": 1687429373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI-powered Data Experiences with Semantic Layers, or How to Prevent LLMs from Hallucinating", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14g76w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/orA9oXzWYt67Ba9ulUAeNoMxkUuNTRjDy8FxIEdEdB8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687449235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/semantic-layer-the-backbone-of-ai-powered-data-experiences", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?auto=webp&amp;v=enabled&amp;s=67dcc1fe43e19d91b47e6ebb96e61e5f4ab5c982", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d20f7414a707c8bdd656c3770afbf436482613d9", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96b4433b055900089ad9145852557986e806afd7", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0001dbc7c1d25381890b7632e6dacb6b13172232", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d91ec8b048550e2bf24cef42613fe734084df5d7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fd042a6cd87dc387d301df48da68ad398fa40a4", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fPeIoEkrbBg5PTdABtnKe5_Kd30PCkIstXx6BBcg0EU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd265588f0abd8f6788380c109157e17a5ef10b9", "width": 1080, "height": 567}], "variants": {}, "id": "QfpFSHP8aypbw80FFqhIjy7PUgEl0k2jpslNwCYKaeU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14g76w3", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g76w3/aipowered_data_experiences_with_semantic_layers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/semantic-layer-the-backbone-of-ai-powered-data-experiences", "subreddit_subscribers": 111754, "created_utc": 1687449235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Conceptual vs Logical vs Physical Data Models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14fzp4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Z_HueKSwldii1S8qUy67aCaXZjvKJkAg1z6modf8F0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687429336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thoughtspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thoughtspot.com/data-trends/data-modeling/conceptual-vs-logical-vs-physical-data-models", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?auto=webp&amp;v=enabled&amp;s=470b761fd754eaf537a0a155fd9d6e9b3d63c460", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e71ffe38058b5fd5c000f61d4032947638b40283", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0dcc8ceeb5c6643bad3dc788896755b15eb62d95", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adeceec78ad7614f06f0665f00f241c7c6d53192", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c70ac6892f5fd31df37e1cf74f3c27ed10a242b9", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9927f79503b60f9b53dcaa45f812edc9f26f0fa8", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/wKvjshGdZTKMOM2IkTQ90bQpDIkvW5NHPBbU6bT9ArY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a64cafc13f298b1724e6f7c2eccbc9436711f2b5", "width": 1080, "height": 565}], "variants": {}, "id": "RT3blwaFNDO7eUz6Pj4gd5FZXhod7maQjBHzvInU8JU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14fzp4i", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fzp4i/conceptual_vs_logical_vs_physical_data_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thoughtspot.com/data-trends/data-modeling/conceptual-vs-logical-vs-physical-data-models", "subreddit_subscribers": 111754, "created_utc": 1687429336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn Dbt, I have an incremental model called Test_a. I  have also a second model called Test_b that references model Test_a. I want the model Test_b to update only the rows that where updated in model Test_a. Can you help me?\n\nThanks", "author_fullname": "t2_5kcd2wet", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g0n1v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687432326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In Dbt, I have an incremental model called Test_a. I  have also a second model called Test_b that references model Test_a. I want the model Test_b to update only the rows that where updated in model Test_a. Can you help me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14g0n1v", "is_robot_indexable": true, "report_reasons": null, "author": "redtiger2019", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g0n1v/question_on_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g0n1v/question_on_dbt/", "subreddit_subscribers": 111754, "created_utc": 1687432326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nCurrently working as a DE in a small company but I want to get into banking considering it\u2019s job stability and future career development. \n\nAny books or maybe tutorials to study banking knowledge? Looks like their tech stacks are normally old.", "author_fullname": "t2_oorup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Want to get banking/fintech DE job. Any books or tutorials to learn related knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fysdp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687426314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working as a DE in a small company but I want to get into banking considering it\u2019s job stability and future career development. &lt;/p&gt;\n\n&lt;p&gt;Any books or maybe tutorials to study banking knowledge? Looks like their tech stacks are normally old.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fysdp", "is_robot_indexable": true, "report_reasons": null, "author": "GeForceKawaiiyo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fysdp/want_to_get_bankingfintech_de_job_any_books_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fysdp/want_to_get_bankingfintech_de_job_any_books_or/", "subreddit_subscribers": 111754, "created_utc": 1687426314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone done this before? Did you use a connector or Did you write a custom connector?", "author_fullname": "t2_1h4yf7ed", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redis to Snowflake sync", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gev4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687467574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone done this before? Did you use a connector or Did you write a custom connector?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14gev4r", "is_robot_indexable": true, "report_reasons": null, "author": "botmentor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gev4r/redis_to_snowflake_sync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gev4r/redis_to_snowflake_sync/", "subreddit_subscribers": 111754, "created_utc": 1687467574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, hope I don\u2019t bother you with my stupid question. Currently I got multiple data engineering jobs open but I find it a hard time to find the right people. \n\nWhat are some key things that are important for a data engineer? What drives a data engineer to apply on a job add? What would attract you to a job add or message? \n\nOn a daily basis I send more then 40 inmails without any results \n\nHope you guys can help me out!\n\nThank you in advance", "author_fullname": "t2_a4oaxf06", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consultant asking for advise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14galtx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687458874.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687457705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, hope I don\u2019t bother you with my stupid question. Currently I got multiple data engineering jobs open but I find it a hard time to find the right people. &lt;/p&gt;\n\n&lt;p&gt;What are some key things that are important for a data engineer? What drives a data engineer to apply on a job add? What would attract you to a job add or message? &lt;/p&gt;\n\n&lt;p&gt;On a daily basis I send more then 40 inmails without any results &lt;/p&gt;\n\n&lt;p&gt;Hope you guys can help me out!&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14galtx", "is_robot_indexable": true, "report_reasons": null, "author": "King17zhc", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14galtx/consultant_asking_for_advise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14galtx/consultant_asking_for_advise/", "subreddit_subscribers": 111754, "created_utc": 1687457705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to add a final validation step to a pipeline that outputs a report each time it\u2019s run for compliance reasons.\n\nPart of the validation is to ensure that all of the records at the end of the pipeline correspond to records at the start of the pipeline, and we haven\u2019t had stuff go missing, slip through business rules etc.\n\nIn SQL I\u2019d just do this as a join between two tables with some appropriate conditions and check the output.\n\nIs GX right for doing this, or is it more appropriate for a different sort of validation?", "author_fullname": "t2_xr8z9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this the correct use case for Great Expectations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g53cn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687444238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to add a final validation step to a pipeline that outputs a report each time it\u2019s run for compliance reasons.&lt;/p&gt;\n\n&lt;p&gt;Part of the validation is to ensure that all of the records at the end of the pipeline correspond to records at the start of the pipeline, and we haven\u2019t had stuff go missing, slip through business rules etc.&lt;/p&gt;\n\n&lt;p&gt;In SQL I\u2019d just do this as a join between two tables with some appropriate conditions and check the output.&lt;/p&gt;\n\n&lt;p&gt;Is GX right for doing this, or is it more appropriate for a different sort of validation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14g53cn", "is_robot_indexable": true, "report_reasons": null, "author": "NeuralHijacker", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g53cn/is_this_the_correct_use_case_for_great/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g53cn/is_this_the_correct_use_case_for_great/", "subreddit_subscribers": 111754, "created_utc": 1687444238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an upcoming interview for a hybrid role that blends Data Engineering and DevOps. The role leans more towards DevOps but involves some crucial Data Engineering responsibilities as well. The company is embarking on an ambitious project to develop and deploy a new data streaming service integrating their clients' existing tech architecture using the Google Cloud Platform (GCP).  \n\n\nThe role includes:\n\n* Defining project workbooks, supporting the delivery of the new platform\n* Designing and implementing CI/CD pipelines\n* Developing and maintaining Infrastructure as Code (IaC) scripts\n* Monitoring and optimizing data streaming service performance\n* Ensuring security and compliance\n* Troubleshooting production and non-production environments\n* Some Data Engineering experience is preferred - Python/Pandas/SQL/PySpark\n*  Excellent understanding of managed GCP Services, Docker, Kubernetes\n* Experienced in using a modern CI SaaS platform such as Github Actions, CircleCI, GitLab, etc  \n\n\nGiven these factors, I'm seeking advice on a few specific areas:\n\n1. Which GCP-specific practices/concepts and Data Engineering tasks would be valuable to showcase in relation to a data streaming service?\n2. How can I best demonstrate my Python scripting and automation skills in a DevOps/Data Engineering context?\n3. How should I approach discussing the challenges and strategies of handling the hybrid nature of this role?\n4. What are some potential obstacles I might encounter in managing the performance, scalability, and reliability of a data streaming service?\n5. Are there additional tools, libraries, or frameworks that would be beneficial to learn for this role, especially in the context of GCP?\n\nI genuinely appreciate any insights, experiences, or resources you can share. I believe this is an incredible learning opportunity and I'm excited about the potential challenges that await me. Thanks in advance for your invaluable guidance.", "author_fullname": "t2_co7b8hy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preparing for a Hybrid Data Engineer/DevOps Interview Involving a GCP-Based Data Streaming Service - Seeking Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g076n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687430961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an upcoming interview for a hybrid role that blends Data Engineering and DevOps. The role leans more towards DevOps but involves some crucial Data Engineering responsibilities as well. The company is embarking on an ambitious project to develop and deploy a new data streaming service integrating their clients&amp;#39; existing tech architecture using the Google Cloud Platform (GCP).  &lt;/p&gt;\n\n&lt;p&gt;The role includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Defining project workbooks, supporting the delivery of the new platform&lt;/li&gt;\n&lt;li&gt;Designing and implementing CI/CD pipelines&lt;/li&gt;\n&lt;li&gt;Developing and maintaining Infrastructure as Code (IaC) scripts&lt;/li&gt;\n&lt;li&gt;Monitoring and optimizing data streaming service performance&lt;/li&gt;\n&lt;li&gt;Ensuring security and compliance&lt;/li&gt;\n&lt;li&gt;Troubleshooting production and non-production environments&lt;/li&gt;\n&lt;li&gt;Some Data Engineering experience is preferred - Python/Pandas/SQL/PySpark&lt;/li&gt;\n&lt;li&gt; Excellent understanding of managed GCP Services, Docker, Kubernetes&lt;/li&gt;\n&lt;li&gt;Experienced in using a modern CI SaaS platform such as Github Actions, CircleCI, GitLab, etc&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Given these factors, I&amp;#39;m seeking advice on a few specific areas:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which GCP-specific practices/concepts and Data Engineering tasks would be valuable to showcase in relation to a data streaming service?&lt;/li&gt;\n&lt;li&gt;How can I best demonstrate my Python scripting and automation skills in a DevOps/Data Engineering context?&lt;/li&gt;\n&lt;li&gt;How should I approach discussing the challenges and strategies of handling the hybrid nature of this role?&lt;/li&gt;\n&lt;li&gt;What are some potential obstacles I might encounter in managing the performance, scalability, and reliability of a data streaming service?&lt;/li&gt;\n&lt;li&gt;Are there additional tools, libraries, or frameworks that would be beneficial to learn for this role, especially in the context of GCP?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I genuinely appreciate any insights, experiences, or resources you can share. I believe this is an incredible learning opportunity and I&amp;#39;m excited about the potential challenges that await me. Thanks in advance for your invaluable guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14g076n", "is_robot_indexable": true, "report_reasons": null, "author": "DaCharisma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g076n/preparing_for_a_hybrid_data_engineerdevops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g076n/preparing_for_a_hybrid_data_engineerdevops/", "subreddit_subscribers": 111754, "created_utc": 1687430961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a pipeline (Python) where I am pulling from an API and pushing a CSV file into S3  \nNext step is to COPY INTO a Snowflake table from that file I just dropped into S3 \n\nCurrently debating using one of these two approaches\n\nMake a call with all the information needed (secrets are stored separately and called at runtime).\n\n    COPY INTO mytable\n      FROM s3://mybucket/path/file.csv credentials=(AWS_KEY_ID='$AWS_ACCESS_KEY_ID' AWS_SECRET_KEY='$AWS_SECRET_ACCESS_KEY')\n      FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1);\n\nOr create a named stage with my credentials and an associated file format, then simply \"use\" it...\n\n     COPY INTO mytable FROM @my_ext_stage FILES='file.csv'; \n\nThe only pros I can see for using a named stage is that my credentials would not be needed in the query  \n\n\nOn the other hand, the first method is more flexible, as I can change the path in the bucket within python. Plus I don't have to bother creating a separate stage for each table I'll be landing into, as well as an associated file format (in each and every schema).\n\nHaving the named stage go to a specific URL ( 's3://mybucket/path/') seems incredibly limiting.  \n", "author_fullname": "t2_qhsi5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Stages Best Practices - Named stages seem limiting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fqpkh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687400422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a pipeline (Python) where I am pulling from an API and pushing a CSV file into S3&lt;br/&gt;\nNext step is to COPY INTO a Snowflake table from that file I just dropped into S3 &lt;/p&gt;\n\n&lt;p&gt;Currently debating using one of these two approaches&lt;/p&gt;\n\n&lt;p&gt;Make a call with all the information needed (secrets are stored separately and called at runtime).&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;COPY INTO mytable\n  FROM s3://mybucket/path/file.csv credentials=(AWS_KEY_ID=&amp;#39;$AWS_ACCESS_KEY_ID&amp;#39; AWS_SECRET_KEY=&amp;#39;$AWS_SECRET_ACCESS_KEY&amp;#39;)\n  FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = &amp;#39;,&amp;#39; SKIP_HEADER = 1);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or create a named stage with my credentials and an associated file format, then simply &amp;quot;use&amp;quot; it...&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; COPY INTO mytable FROM @my_ext_stage FILES=&amp;#39;file.csv&amp;#39;; \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The only pros I can see for using a named stage is that my credentials would not be needed in the query  &lt;/p&gt;\n\n&lt;p&gt;On the other hand, the first method is more flexible, as I can change the path in the bucket within python. Plus I don&amp;#39;t have to bother creating a separate stage for each table I&amp;#39;ll be landing into, as well as an associated file format (in each and every schema).&lt;/p&gt;\n\n&lt;p&gt;Having the named stage go to a specific URL ( &amp;#39;s3://mybucket/path/&amp;#39;) seems incredibly limiting.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fqpkh", "is_robot_indexable": true, "report_reasons": null, "author": "NoUsernames1eft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fqpkh/snowflake_stages_best_practices_named_stages_seem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fqpkh/snowflake_stages_best_practices_named_stages_seem/", "subreddit_subscribers": 111754, "created_utc": 1687400422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,  \nData engineering is a pretty immature function at my current company with few best practices and tooling -- we have a lot of hand-rolled pipelines and monitoring alerts that might, for example, have a regularly scheduled spark instance grab newly ingested s3 files with a batch of records, perform transformations, and dump output to a redshift table. The few alerts we have are things like scheduled sql queries checking when the last record was added and alerting if that was more than X hours ago. I'm generally trying to implement best practices and get us to use industry-standard tooling that would help us do better, such as using an orchestration platform. \n\nMy focus in this post is on monitoring and observability. Our batch job pipelines operate on records that include an event timestamp. I'd like to get a slack alert if our output table ever has a gap in event time larger than, say, 15 minutes. Curious how you'd go about detecting and alerting such a gap. Right now I'm probably going to build a sql query triggered after each ingestion that uses a window function but maybe you have a different approach? Also, what are yall using to manage similar data quality, observability, and alerts?", "author_fullname": "t2_b4tn6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting data gaps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gbfl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687459630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nData engineering is a pretty immature function at my current company with few best practices and tooling -- we have a lot of hand-rolled pipelines and monitoring alerts that might, for example, have a regularly scheduled spark instance grab newly ingested s3 files with a batch of records, perform transformations, and dump output to a redshift table. The few alerts we have are things like scheduled sql queries checking when the last record was added and alerting if that was more than X hours ago. I&amp;#39;m generally trying to implement best practices and get us to use industry-standard tooling that would help us do better, such as using an orchestration platform. &lt;/p&gt;\n\n&lt;p&gt;My focus in this post is on monitoring and observability. Our batch job pipelines operate on records that include an event timestamp. I&amp;#39;d like to get a slack alert if our output table ever has a gap in event time larger than, say, 15 minutes. Curious how you&amp;#39;d go about detecting and alerting such a gap. Right now I&amp;#39;m probably going to build a sql query triggered after each ingestion that uses a window function but maybe you have a different approach? Also, what are yall using to manage similar data quality, observability, and alerts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14gbfl3", "is_robot_indexable": true, "report_reasons": null, "author": "acumenbeing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gbfl3/detecting_data_gaps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gbfl3/detecting_data_gaps/", "subreddit_subscribers": 111754, "created_utc": 1687459630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If I were to design a data warehouse table to capture Orders. Would the approach 1 or approach 2 be better? Please include your reasoning. I understand Kimball approach says go with the Fact table, but the Slowly changing approach can also solve the same problem. Or, am I missing anything here?\n\nhttps://preview.redd.it/yijknpy8gk7b1.png?width=1307&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9926e39ba023d4dce026832c987aaf28da62433d", "author_fullname": "t2_tov4xq2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ORDER_ACTIVITY_FACT or ORDER_SCD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 34, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yijknpy8gk7b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 26, "x": 108, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f05c83a591c5cabc51fe1cff189fd6ef31820ba"}, {"y": 53, "x": 216, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a41c4a75e9e85c621c65734b2feb4b904db1234"}, {"y": 78, "x": 320, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=422019322a4b3438aea8034fe6fe09f9a6ca6ba5"}, {"y": 157, "x": 640, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=712a41a90ab0bf081b4178a7c7fd72d71ef288d4"}, {"y": 235, "x": 960, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b81035381350bb9ceafab640c044063fdd127ad6"}, {"y": 265, "x": 1080, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4bac7648a121c95dab1976531c963f2a22130595"}], "s": {"y": 321, "x": 1307, "u": "https://preview.redd.it/yijknpy8gk7b1.png?width=1307&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9926e39ba023d4dce026832c987aaf28da62433d"}, "id": "yijknpy8gk7b1"}}, "name": "t3_14g2rqb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xWmwLo1kOv2NY10f4ppJc0gzWk-GJ3sAAHj88pgTTAk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687438395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I were to design a data warehouse table to capture Orders. Would the approach 1 or approach 2 be better? Please include your reasoning. I understand Kimball approach says go with the Fact table, but the Slowly changing approach can also solve the same problem. Or, am I missing anything here?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yijknpy8gk7b1.png?width=1307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9926e39ba023d4dce026832c987aaf28da62433d\"&gt;https://preview.redd.it/yijknpy8gk7b1.png?width=1307&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9926e39ba023d4dce026832c987aaf28da62433d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14g2rqb", "is_robot_indexable": true, "report_reasons": null, "author": "nanksk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g2rqb/order_activity_fact_or_order_scd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g2rqb/order_activity_fact_or_order_scd/", "subreddit_subscribers": 111754, "created_utc": 1687438395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Friends,   \nI am currently working as senior data engineer with 8yoe. I am looking forward to move into consulting role in the same organisation, But problem is i think I lacks soft skill, I feel like I don't have a flow in communication, I don't recall business terms, and don't have conviction in what I say.   \nSo, I am looking forward to know:  \n1. Best way to learn the business communication.  \n2. How to handle the when you don't know something, or say something incorrect.  \n3. How to be a pro consultant.  \nI know the most common answer for these is practise, but how do I prepare before I directly jump into that. \n\nPlease also recommend if you know any books, course, video etc.", "author_fullname": "t2_u1vbo568", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need guidance on consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fu5wp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687410769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Friends,&lt;br/&gt;\nI am currently working as senior data engineer with 8yoe. I am looking forward to move into consulting role in the same organisation, But problem is i think I lacks soft skill, I feel like I don&amp;#39;t have a flow in communication, I don&amp;#39;t recall business terms, and don&amp;#39;t have conviction in what I say.&lt;br/&gt;\nSo, I am looking forward to know:&lt;br/&gt;\n1. Best way to learn the business communication.&lt;br/&gt;\n2. How to handle the when you don&amp;#39;t know something, or say something incorrect.&lt;br/&gt;\n3. How to be a pro consultant.&lt;br/&gt;\nI know the most common answer for these is practise, but how do I prepare before I directly jump into that. &lt;/p&gt;\n\n&lt;p&gt;Please also recommend if you know any books, course, video etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fu5wp", "is_robot_indexable": true, "report_reasons": null, "author": "manu13891", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fu5wp/need_guidance_on_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fu5wp/need_guidance_on_consulting/", "subreddit_subscribers": 111754, "created_utc": 1687410769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning and doing certification in big query to get into data analytics /engineer field, i wanted to ask how in real time in companies , on job people to interact with big query framework ,  CLI or api ?", "author_fullname": "t2_t526hbv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interface with BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fp7f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687396115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning and doing certification in big query to get into data analytics /engineer field, i wanted to ask how in real time in companies , on job people to interact with big query framework ,  CLI or api ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fp7f7", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Ticket6016", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fp7f7/interface_with_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fp7f7/interface_with_bigquery/", "subreddit_subscribers": 111754, "created_utc": 1687396115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a database, obviously it's common to enforce things like \"all rows in the \\`purchases\\` table must have a foreign key relationship to the \\`users\\` table (we have to know which user made each purchase!)\"\n\nWhen ELT'ing data to a data warehouse, I wrote a test in dbt to check the same thing.\n\nThis test periodically fails though, due to the fact that sometimes the ELT process transfers the \\`users\\` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the \\`purchases\\` table to the data warehouse. The result being that there's a purchase in the data warehouse which does not have a corresponding user in the user's table. Which means my dbt test fails.\n\nIs this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship \"for purchases made more than X hours ago\", where X is greater than the most recent load into the dw.", "author_fullname": "t2_116mtw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle table relationships in data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fnn1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687391877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a database, obviously it&amp;#39;s common to enforce things like &amp;quot;all rows in the `purchases` table must have a foreign key relationship to the `users` table (we have to know which user made each purchase!)&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;When ELT&amp;#39;ing data to a data warehouse, I wrote a test in dbt to check the same thing.&lt;/p&gt;\n\n&lt;p&gt;This test periodically fails though, due to the fact that sometimes the ELT process transfers the `users` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the `purchases` table to the data warehouse. The result being that there&amp;#39;s a purchase in the data warehouse which does not have a corresponding user in the user&amp;#39;s table. Which means my dbt test fails.&lt;/p&gt;\n\n&lt;p&gt;Is this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship &amp;quot;for purchases made more than X hours ago&amp;quot;, where X is greater than the most recent load into the dw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fnn1s", "is_robot_indexable": true, "report_reasons": null, "author": "PM_ME_SCIENCEY_STUFF", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "subreddit_subscribers": 111754, "created_utc": 1687391877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Considering the importance od maintaining quality data in warehouse whats the best tool available in market", "author_fullname": "t2_s1yml2kq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which tool is the best to maintain data quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14gh2dx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687472768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Considering the importance od maintaining quality data in warehouse whats the best tool available in market&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14gh2dx", "is_robot_indexable": true, "report_reasons": null, "author": "uk-lad-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gh2dx/which_tool_is_the_best_to_maintain_data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gh2dx/which_tool_is_the_best_to_maintain_data_quality/", "subreddit_subscribers": 111754, "created_utc": 1687472768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey , iam electromechanical engineering student and iam asking if i can land a job as data engineer or i have to have experiences in data analyst first?\n\n\nAnd data engineering is senior role basically", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can i land data engineering job as junior?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gel0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687466936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey , iam electromechanical engineering student and iam asking if i can land a job as data engineer or i have to have experiences in data analyst first?&lt;/p&gt;\n\n&lt;p&gt;And data engineering is senior role basically&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14gel0t", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gel0t/can_i_land_data_engineering_job_as_junior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gel0t/can_i_land_data_engineering_job_as_junior/", "subreddit_subscribers": 111754, "created_utc": 1687466936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why? How? What did you do about egress costs and keeping lakes in sync if you had to serve the same data in two places? If you had to choose a vendor to build a core basic data lake from the major providers (AWS, Azure, GCP, Cloudflare (apparently no egress cost on R2 but low lake tooling)), which would you go with?", "author_fullname": "t2_uqrd0850", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone successfully implemented a multi cloud data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gcsly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687462819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why? How? What did you do about egress costs and keeping lakes in sync if you had to serve the same data in two places? If you had to choose a vendor to build a core basic data lake from the major providers (AWS, Azure, GCP, Cloudflare (apparently no egress cost on R2 but low lake tooling)), which would you go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14gcsly", "is_robot_indexable": true, "report_reasons": null, "author": "IncognitoEmployee", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gcsly/has_anyone_successfully_implemented_a_multi_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gcsly/has_anyone_successfully_implemented_a_multi_cloud/", "subreddit_subscribers": 111754, "created_utc": 1687462819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "what's the most challenging within retail data infrastructure as a DE?", "author_fullname": "t2_a4oaxf06", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenges in the retail data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gcqzp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687462708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what&amp;#39;s the most challenging within retail data infrastructure as a DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14gcqzp", "is_robot_indexable": true, "report_reasons": null, "author": "King17zhc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gcqzp/challenges_in_the_retail_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gcqzp/challenges_in_the_retail_data/", "subreddit_subscribers": 111754, "created_utc": 1687462708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I am a \"Data Scientist\" at a big trucking company. I have built several SQL programs and did some minor data engineering related work which is what I want to do. Stats is my graduate degree, and I want to be a ML Engineer in 3 more years. Thus, my current focus is to build my software engineering skillset in data.\n\nThis first year, I feel like most days I did little to nothing. I asked my manager (he is a very great guy) for more work (like model building or any projects I can take on), but there is nothing I can own after the first major project that I mainly drove when coming to the company.\n\nI currently feel very useless and depressed. I am studying two certs - Azure Data Fundamentals and then Azure Data Engineering Associate so I can job hope before next year and work somewhere I can develop more skills in data engineering. Our company uses Azure, and I like the platform. I also have a Data Engineering friend who is guiding me on skills I can improve on and a ML Engineering coach I am paying to help guide a path for my future. I just feel lost since I am not developing experience at the company now.\n\n**Would I be lucky to get another role with this 1 year experience** (which mainly consisted of building brute-force SQL programs that I need to optimize more on for big data)? I really like data and want to improve and learn more, but I am not lucky with actual work experience. :/\n\nAny advice?", "author_fullname": "t2_c790qclzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1 year into role and I am worried", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14gbseg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687460449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a &amp;quot;Data Scientist&amp;quot; at a big trucking company. I have built several SQL programs and did some minor data engineering related work which is what I want to do. Stats is my graduate degree, and I want to be a ML Engineer in 3 more years. Thus, my current focus is to build my software engineering skillset in data.&lt;/p&gt;\n\n&lt;p&gt;This first year, I feel like most days I did little to nothing. I asked my manager (he is a very great guy) for more work (like model building or any projects I can take on), but there is nothing I can own after the first major project that I mainly drove when coming to the company.&lt;/p&gt;\n\n&lt;p&gt;I currently feel very useless and depressed. I am studying two certs - Azure Data Fundamentals and then Azure Data Engineering Associate so I can job hope before next year and work somewhere I can develop more skills in data engineering. Our company uses Azure, and I like the platform. I also have a Data Engineering friend who is guiding me on skills I can improve on and a ML Engineering coach I am paying to help guide a path for my future. I just feel lost since I am not developing experience at the company now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Would I be lucky to get another role with this 1 year experience&lt;/strong&gt; (which mainly consisted of building brute-force SQL programs that I need to optimize more on for big data)? I really like data and want to improve and learn more, but I am not lucky with actual work experience. :/&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14gbseg", "is_robot_indexable": true, "report_reasons": null, "author": "Big_Pitch_9175", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14gbseg/1_year_into_role_and_i_am_worried/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14gbseg/1_year_into_role_and_i_am_worried/", "subreddit_subscribers": 111754, "created_utc": 1687460449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I'm aware of describe history and querying the delta table according to version. I was wondering if there is a way to find history of a record based on its I'd efficiently. Currently I have to sift through all versions to find out the history.", "author_fullname": "t2_rthmzez0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I get SCD2 like output by using Delta table time travel feature?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14g0y4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687433283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m aware of describe history and querying the delta table according to version. I was wondering if there is a way to find history of a record based on its I&amp;#39;d efficiently. Currently I have to sift through all versions to find out the history.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14g0y4x", "is_robot_indexable": true, "report_reasons": null, "author": "rainybuzz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14g0y4x/can_i_get_scd2_like_output_by_using_delta_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14g0y4x/can_i_get_scd2_like_output_by_using_delta_table/", "subreddit_subscribers": 111754, "created_utc": 1687433283.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}