{"kind": "Listing", "data": {"after": "t3_14fi6e6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It'll be so fun. Every week we'll invite a stakeholder to join us and make an absurd request for some view or something, and we'll role-play how we respond to keep the stakeholder happy to our own detriment.\n\nhttps://preview.redd.it/977fk80hkd7b1.png?width=910&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody want to join my book club?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"977fk80hkd7b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 143, "x": 108, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a236563704fb8a2a5295c901a49dedd9d42d0f6"}, {"y": 287, "x": 216, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb1a37c61e110db11ae11524371ffc4d785d039d"}, {"y": 425, "x": 320, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1107950234e6cefd1fffebc16a5103df0ff71ad"}, {"y": 851, "x": 640, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af838dc270b66d9070ce3060c486679e7167d9e9"}], "s": {"y": 1211, "x": 910, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=910&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3"}, "id": "977fk80hkd7b1"}}, "name": "t3_14f846e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jdrS8rYFm1Szp6wJdBpRHCXbPC0SNZXemW-X7IBQEPs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687354978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;ll be so fun. Every week we&amp;#39;ll invite a stakeholder to join us and make an absurd request for some view or something, and we&amp;#39;ll role-play how we respond to keep the stakeholder happy to our own detriment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/977fk80hkd7b1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3\"&gt;https://preview.redd.it/977fk80hkd7b1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14f846e", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f846e/anybody_want_to_join_my_book_club/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f846e/anybody_want_to_join_my_book_club/", "subreddit_subscribers": 111664, "created_utc": 1687354978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Playlist](https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;list=PLGVZCDnMOq0peDguAzds7kVmBr8avp46K)\n\nSome of the talks I found most intersting:\n\n* [ Use Spark from anywhere - A Spark client in Python powered by Spark Connect](https://www.youtube.com/watch?v=PzgPcvFDD4I&amp;t=1339s)\n* [Streamlit meets WebAssembly - stlite](https://www.youtube.com/watch?v=XivJYZUm1GY&amp;t=1368s)\n* [Pragmatic ways of using Rust in your data project](https://www.youtube.com/watch?v=Jk9NXfvgclU&amp;t=1374s)\n* [An Introduction to Apache Spark](https://www.youtube.com/watch?v=jOJceajwMGs&amp;t=749s)\n* [WebAssembly demystified](https://www.youtube.com/watch?v=VCkcv0ppYXs&amp;t=5s)\n* [Rusty Python - A Case Study](https://www.youtube.com/watch?v=Y5XQR0wUEyM&amp;t=52s)\n* [Towards Learned Database Systems](https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;t=13s)", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyCon DE &amp; PyData Berlin 2023 Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f73gz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687352384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;amp;list=PLGVZCDnMOq0peDguAzds7kVmBr8avp46K\"&gt;Playlist&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some of the talks I found most intersting:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=PzgPcvFDD4I&amp;amp;t=1339s\"&gt; Use Spark from anywhere - A Spark client in Python powered by Spark Connect&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=XivJYZUm1GY&amp;amp;t=1368s\"&gt;Streamlit meets WebAssembly - stlite&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=Jk9NXfvgclU&amp;amp;t=1374s\"&gt;Pragmatic ways of using Rust in your data project&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=jOJceajwMGs&amp;amp;t=749s\"&gt;An Introduction to Apache Spark&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=VCkcv0ppYXs&amp;amp;t=5s\"&gt;WebAssembly demystified&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=Y5XQR0wUEyM&amp;amp;t=52s\"&gt;Rusty Python - A Case Study&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;amp;t=13s\"&gt;Towards Learned Database Systems&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?auto=webp&amp;v=enabled&amp;s=a3492d7c1a5ad1a7128c98e245be41f067f20795", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c85fba682150a25e76d4b3f3d192b4e9d7729e59", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3d984e0b2729f9057c257397a25749f467a8337", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49ad477876c3a7b0ab159660bcff9ba2a6bdc865", "width": 320, "height": 240}], "variants": {}, "id": "qVYw_YvefBehG9llXEP7eXxaUqId0OOhkEobkd1GcbM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f73gz", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f73gz/pycon_de_pydata_berlin_2023_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f73gz/pycon_de_pydata_berlin_2023_playlist/", "subreddit_subscribers": 111664, "created_utc": 1687352384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you guys orchestrating your pipelines? \n\nCurrently our environment is : \nADF for orchestration\n,databricks for transformations\n,AZDL Gen 2 storage\n,synapse for reporting (which is slowly going away as we transition to Databricks). \n\nMy company is doing away with Azure (don\u2019t ask me why) and transitioning to GCP. We will keep databricks for transformations and most likely reporting, but now we lose our orchestration tool. \n\nCurrently we use ADF for connections to our source systems where we have 100s of sql ingestion queries (that are stored in a table and looped through) to copy data out of 40 different source system sql db\u2019s. Data is ingested into AZDL Gen2 storage as parquet files then picked up in databricks notebooks. \n\nWhat tools are you using that can replace ADF?", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fjdfm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687381445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you guys orchestrating your pipelines? &lt;/p&gt;\n\n&lt;p&gt;Currently our environment is : \nADF for orchestration\n,databricks for transformations\n,AZDL Gen 2 storage\n,synapse for reporting (which is slowly going away as we transition to Databricks). &lt;/p&gt;\n\n&lt;p&gt;My company is doing away with Azure (don\u2019t ask me why) and transitioning to GCP. We will keep databricks for transformations and most likely reporting, but now we lose our orchestration tool. &lt;/p&gt;\n\n&lt;p&gt;Currently we use ADF for connections to our source systems where we have 100s of sql ingestion queries (that are stored in a table and looped through) to copy data out of 40 different source system sql db\u2019s. Data is ingested into AZDL Gen2 storage as parquet files then picked up in databricks notebooks. &lt;/p&gt;\n\n&lt;p&gt;What tools are you using that can replace ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fjdfm", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fjdfm/orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fjdfm/orchestration/", "subreddit_subscribers": 111664, "created_utc": 1687381445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "No transformation is required, just replicating tables as in source. \n\nHere are options we have tried:\n\nDataflow/ DataStream: Too costly.\n\nFivetran: Too slow (and forces to create new columns in sink to be able to support the incremental changes)\n\nApache NiFi: too much Java code &amp; hard to code for inc sync.\n\nHevo data/ Integrateio: Eval in progress.\n\nAirbyte: seems not much useful for high volume\n\nSo far we found Fivetran to hold our use case better but the performance is so poor that we would like to explore options. Would really appreciate any idea to solve this on. ", "author_fullname": "t2_1l9tu7ql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools / Best way to ingest large 20TB plus data from Bigquery to Google CloudSQL Postgres with 2TB daily changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fkmbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687384326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No transformation is required, just replicating tables as in source. &lt;/p&gt;\n\n&lt;p&gt;Here are options we have tried:&lt;/p&gt;\n\n&lt;p&gt;Dataflow/ DataStream: Too costly.&lt;/p&gt;\n\n&lt;p&gt;Fivetran: Too slow (and forces to create new columns in sink to be able to support the incremental changes)&lt;/p&gt;\n\n&lt;p&gt;Apache NiFi: too much Java code &amp;amp; hard to code for inc sync.&lt;/p&gt;\n\n&lt;p&gt;Hevo data/ Integrateio: Eval in progress.&lt;/p&gt;\n\n&lt;p&gt;Airbyte: seems not much useful for high volume&lt;/p&gt;\n\n&lt;p&gt;So far we found Fivetran to hold our use case better but the performance is so poor that we would like to explore options. Would really appreciate any idea to solve this on. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fkmbu", "is_robot_indexable": true, "report_reasons": null, "author": "onksssss", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fkmbu/tools_best_way_to_ingest_large_20tb_plus_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fkmbu/tools_best_way_to_ingest_large_20tb_plus_data/", "subreddit_subscribers": 111664, "created_utc": 1687384326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering how long would it take in average for a person with healthcare background to enter into data engineer field? I have a bachelors in medical sciences and a masters in healthcare Informatics. I have a certification in SAS, have basic understanding and programming knowledge of SQL and R. I have zero knowledge of python java or anyother programming tool. Is it gonna be very challenging for me to work as data engineer?", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Break into Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ffv95", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687373172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering how long would it take in average for a person with healthcare background to enter into data engineer field? I have a bachelors in medical sciences and a masters in healthcare Informatics. I have a certification in SAS, have basic understanding and programming knowledge of SQL and R. I have zero knowledge of python java or anyother programming tool. Is it gonna be very challenging for me to work as data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14ffv95", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ffv95/break_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ffv95/break_into_data_engineering/", "subreddit_subscribers": 111664, "created_utc": 1687373172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, so to explain my problem to you, I need to ingest data from Google ADS DATA HUB (ADH) to Big Query and I have done the python code, the code needs to be run weekly (cron job), however we need a snapshot of the tables in the end of the month. The final code will be dockerized and run on Google Kubernetes Engine.\n\nWhat solutions do you suggest to me? I\u2019m open to any suggestion to further learn and improve my data engineering skills.\n\nTL;DR I need to schedule the python code to run weekly and last day of the month. Any suggestions?\n\nEdit: typo and added details", "author_fullname": "t2_5u3c1gj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Ingestion from google ads to big query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f60ba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687360316.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687349426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, so to explain my problem to you, I need to ingest data from Google ADS DATA HUB (ADH) to Big Query and I have done the python code, the code needs to be run weekly (cron job), however we need a snapshot of the tables in the end of the month. The final code will be dockerized and run on Google Kubernetes Engine.&lt;/p&gt;\n\n&lt;p&gt;What solutions do you suggest to me? I\u2019m open to any suggestion to further learn and improve my data engineering skills.&lt;/p&gt;\n\n&lt;p&gt;TL;DR I need to schedule the python code to run weekly and last day of the month. Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Edit: typo and added details&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14f60ba", "is_robot_indexable": true, "report_reasons": null, "author": "iGodFather302", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f60ba/data_ingestion_from_google_ads_to_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f60ba/data_ingestion_from_google_ads_to_big_query/", "subreddit_subscribers": 111664, "created_utc": 1687349426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Or just your go-to tools?\n\nOr do you just not have any real preference, more like \"whatever tool fits the job\"?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your favorite Data Scraping tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fc8fj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687364624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or just your go-to tools?&lt;/p&gt;\n\n&lt;p&gt;Or do you just not have any real preference, more like &amp;quot;whatever tool fits the job&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fc8fj", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fc8fj/what_are_your_favorite_data_scraping_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fc8fj/what_are_your_favorite_data_scraping_tools/", "subreddit_subscribers": 111664, "created_utc": 1687364624.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DEs! Thanks for so much info over the last year, this is a great resource for those of us in the field. \n\nI have been working as a \"Data Engineer\" for the last year and a half. The reason I put this in quotes is because I don't feel like I live up to the title, as cool as it sounds. I read through these posts daily and have no idea what most of you are talking about, most of the time. A little background is that I came into this position by luck because I had a good resume within the company and the company doesn't have much internal competition for technical positions. I have no background in CS, DWH, DE, or really any type of coding. I've entirely learned on the job. \n\nThe problem with that is this is a very large company ( a large government organization ) and I work on one specific team that handles ETL exclusively. If I understand the term correctly our stack is: Ab Initio (ETL apps), oracle db for staging tables, and UNIX servers. My daily work consists of making updates to these ab initio applications. Within those applications are a bunch of unix shell scripts that I also have to work with. As far as the oracle db, I don't really do any SQL work. We have a modeling team that handles all of that. So the primary skills would be Ab Initio development &amp; shell scripting, and then navigating the metric ton of red tape for promotions to prod. \n\nMy question is , am I wasting my time? From everything I read on here, it seems like ab initio is a dying tool on its way out. It also seems like shell scripting isn't used much. Everyone seems to be using python, cloud, and a litany of other tools I've never heard of. This type of work does not come easy to me, as much as I wish it did. I work with a bunch of people who are either not great at the job, and if they are they aren't good communicators, nor do they want to mentor a new developer. \n\nTLDR: It feels like I am fighting an uphill battle to learn outdated technology that is only useful in my current role but doesn't actually build me into a true DE. I'm still relatively young and at a pivotal moment of my career. It feels like if I want a career in this field, this isn't the right place. What would you data Jedis do?", "author_fullname": "t2_73cw9sv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dinosaur tools &amp; a dead stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fajwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687360785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DEs! Thanks for so much info over the last year, this is a great resource for those of us in the field. &lt;/p&gt;\n\n&lt;p&gt;I have been working as a &amp;quot;Data Engineer&amp;quot; for the last year and a half. The reason I put this in quotes is because I don&amp;#39;t feel like I live up to the title, as cool as it sounds. I read through these posts daily and have no idea what most of you are talking about, most of the time. A little background is that I came into this position by luck because I had a good resume within the company and the company doesn&amp;#39;t have much internal competition for technical positions. I have no background in CS, DWH, DE, or really any type of coding. I&amp;#39;ve entirely learned on the job. &lt;/p&gt;\n\n&lt;p&gt;The problem with that is this is a very large company ( a large government organization ) and I work on one specific team that handles ETL exclusively. If I understand the term correctly our stack is: Ab Initio (ETL apps), oracle db for staging tables, and UNIX servers. My daily work consists of making updates to these ab initio applications. Within those applications are a bunch of unix shell scripts that I also have to work with. As far as the oracle db, I don&amp;#39;t really do any SQL work. We have a modeling team that handles all of that. So the primary skills would be Ab Initio development &amp;amp; shell scripting, and then navigating the metric ton of red tape for promotions to prod. &lt;/p&gt;\n\n&lt;p&gt;My question is , am I wasting my time? From everything I read on here, it seems like ab initio is a dying tool on its way out. It also seems like shell scripting isn&amp;#39;t used much. Everyone seems to be using python, cloud, and a litany of other tools I&amp;#39;ve never heard of. This type of work does not come easy to me, as much as I wish it did. I work with a bunch of people who are either not great at the job, and if they are they aren&amp;#39;t good communicators, nor do they want to mentor a new developer. &lt;/p&gt;\n\n&lt;p&gt;TLDR: It feels like I am fighting an uphill battle to learn outdated technology that is only useful in my current role but doesn&amp;#39;t actually build me into a true DE. I&amp;#39;m still relatively young and at a pivotal moment of my career. It feels like if I want a career in this field, this isn&amp;#39;t the right place. What would you data Jedis do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14fajwv", "is_robot_indexable": true, "report_reasons": null, "author": "ApatheticRart", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fajwv/dinosaur_tools_a_dead_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fajwv/dinosaur_tools_a_dead_stack/", "subreddit_subscribers": 111664, "created_utc": 1687360785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.\n\nLots of dimensions and facts needing to be processed differently from each sources to be correctly represented.\n\nIf I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).\n\nWhat would be the best way to create a documentation that can be shared with business? Is there \"one\" system or will there be a bunch of several artefacts, UMLs etc needed?", "author_fullname": "t2_e4028dc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you do documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fv52f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687413946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.&lt;/p&gt;\n\n&lt;p&gt;Lots of dimensions and facts needing to be processed differently from each sources to be correctly represented.&lt;/p&gt;\n\n&lt;p&gt;If I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to create a documentation that can be shared with business? Is there &amp;quot;one&amp;quot; system or will there be a bunch of several artefacts, UMLs etc needed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fv52f", "is_robot_indexable": true, "report_reasons": null, "author": "tzt1324", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fv52f/how_do_you_do_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fv52f/how_do_you_do_documentation/", "subreddit_subscribers": 111664, "created_utc": 1687413946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Backstory: I run the eCommerce for a company that also has a storefront &amp; warehouse on location where we ship out orders. For years we have been using RetailPro as our inventory system and 24seven as a middleware to send our product to Shopify.\n\nOur parent company is making us switch to DataWorks, with Boomi sending product data to Magento as our new eCommerce platform. We\u2019ve been going through this new process with an outside company doing the integration work for us. \n\nJust recently we were told that Boomi can only send product data from 1 location in DataWorks, such as the Warehouse. The way we\u2019ve always done it before with 24seven is ALL inventory data between the Store &amp; Warehouse would be sent over to Shopify. This has always been the preference since a lot of product doesn\u2019t have inventory in the Warehouse, as it\u2019s seasonal. If needed we could pull from the Store to fulfill orders.\n\nI\u2019m in no means a data engineer, nor is anybody on my team. But I just need to know if there is a solution for this issue.\n\nTLDR; we want Boomi to send over inventory data from multiple locations in DataWorks over to Magento.", "author_fullname": "t2_3ps9svrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boomi related issue for eCommerce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fjbr3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687381345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Backstory: I run the eCommerce for a company that also has a storefront &amp;amp; warehouse on location where we ship out orders. For years we have been using RetailPro as our inventory system and 24seven as a middleware to send our product to Shopify.&lt;/p&gt;\n\n&lt;p&gt;Our parent company is making us switch to DataWorks, with Boomi sending product data to Magento as our new eCommerce platform. We\u2019ve been going through this new process with an outside company doing the integration work for us. &lt;/p&gt;\n\n&lt;p&gt;Just recently we were told that Boomi can only send product data from 1 location in DataWorks, such as the Warehouse. The way we\u2019ve always done it before with 24seven is ALL inventory data between the Store &amp;amp; Warehouse would be sent over to Shopify. This has always been the preference since a lot of product doesn\u2019t have inventory in the Warehouse, as it\u2019s seasonal. If needed we could pull from the Store to fulfill orders.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m in no means a data engineer, nor is anybody on my team. But I just need to know if there is a solution for this issue.&lt;/p&gt;\n\n&lt;p&gt;TLDR; we want Boomi to send over inventory data from multiple locations in DataWorks over to Magento.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fjbr3", "is_robot_indexable": true, "report_reasons": null, "author": "i-race-goats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fjbr3/boomi_related_issue_for_ecommerce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fjbr3/boomi_related_issue_for_ecommerce/", "subreddit_subscribers": 111664, "created_utc": 1687381345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to learn Apache Iceberg for building a data lake. We have late arriving data and the data is partitioned on date column. I will have a spark job that will transform the incoming data to iceberg format. Consider a scenario where the ingestion pipeline fails mid way, since dynamic partition overwrite is enabled, a rerun of the task will overwrite only the partitions that were created in previous run. This will work if there is no late arriving data. But consider a situation where I have data from the last day and the partition for that has already been created when I ingested the data in the yesterday\u2019s run. Now that the current day (which has late arriving data) run fails and since I have set dynamic partition overwrite, the partition that was created yesterday will also get rewritten. Is there a better way to handle dynamic partition overwrite for atomicity as well and late arriving data", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg Dynamic Partition Overwrite.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fc6i0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687364500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to learn Apache Iceberg for building a data lake. We have late arriving data and the data is partitioned on date column. I will have a spark job that will transform the incoming data to iceberg format. Consider a scenario where the ingestion pipeline fails mid way, since dynamic partition overwrite is enabled, a rerun of the task will overwrite only the partitions that were created in previous run. This will work if there is no late arriving data. But consider a situation where I have data from the last day and the partition for that has already been created when I ingested the data in the yesterday\u2019s run. Now that the current day (which has late arriving data) run fails and since I have set dynamic partition overwrite, the partition that was created yesterday will also get rewritten. Is there a better way to handle dynamic partition overwrite for atomicity as well and late arriving data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fc6i0", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fc6i0/apache_iceberg_dynamic_partition_overwrite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fc6i0/apache_iceberg_dynamic_partition_overwrite/", "subreddit_subscribers": 111664, "created_utc": 1687364500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common Anti-Scraping Measures on Websites and How to Bypass Them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14f9q8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gD9F-J0b8487yZfi24T6NRpDLgh22cSkGmohKTlzBgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687358880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "javascript.plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://javascript.plainenglish.io/common-anti-scraping-measures-on-websites-and-how-to-bypass-them-a32de1b066a2", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?auto=webp&amp;v=enabled&amp;s=42c59a73b3ad91dce8fcd997a95d024ff551cc8f", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04478cd9c6d2a35eb877e84520629c45354d80f3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a98eda0cbd3d0cfd4690879ac960bd381e9155c9", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bec1880dd7f6f7eefaa7bbf3bad9b37b13bba0c7", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8779f32e08b68a043c6684866de331b337660a3f", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15e3a4d1e59ac9aa7adc3772d0a940f425912392", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68007d0fa94969d328ff24d08a740023c51c6be1", "width": 1080, "height": 1080}], "variants": {}, "id": "2Qq3JmMuayyJv838L8VeYv47kDei9weoMUIDYVk3Q68"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14f9q8p", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f9q8p/common_antiscraping_measures_on_websites_and_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://javascript.plainenglish.io/common-anti-scraping-measures-on-websites-and-how-to-bypass-them-a32de1b066a2", "subreddit_subscribers": 111664, "created_utc": 1687358880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Offcourse I do understand that governance is a massive module and may not be linked to DE directly. \n\nI see lot of overlaps between the two modules: Lineage, basic catalog, and metadata mgmt \n\nI would like to learn from comments and views.", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MonteCarlo integrates with Atlan or vice-versa - I think Data observability should be unified with catalog / governance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f93du", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687357349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Offcourse I do understand that governance is a massive module and may not be linked to DE directly. &lt;/p&gt;\n\n&lt;p&gt;I see lot of overlaps between the two modules: Lineage, basic catalog, and metadata mgmt &lt;/p&gt;\n\n&lt;p&gt;I would like to learn from comments and views.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f93du", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f93du/montecarlo_integrates_with_atlan_or_viceversa_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f93du/montecarlo_integrates_with_atlan_or_viceversa_i/", "subreddit_subscribers": 111664, "created_utc": 1687357349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a pipeline (Python) where I am pulling from an API and pushing a CSV file into S3  \nNext step is to COPY INTO a Snowflake table from that file I just dropped into S3 \n\nCurrently debating using one of these two approaches\n\nMake a call with all the information needed (secrets are stored separately and called at runtime).\n\n    COPY INTO mytable\n      FROM s3://mybucket/path/file.csv credentials=(AWS_KEY_ID='$AWS_ACCESS_KEY_ID' AWS_SECRET_KEY='$AWS_SECRET_ACCESS_KEY')\n      FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1);\n\nOr create a named stage with my credentials and an associated file format, then simply \"use\" it...\n\n     COPY INTO mytable FROM @my_ext_stage FILES='file.csv'; \n\nThe only pros I can see for using a named stage is that my credentials would not be needed in the query  \n\n\nOn the other hand, the first method is more flexible, as I can change the path in the bucket within python. Plus I don't have to bother creating a separate stage for each table I'll be landing into, as well as an associated file format (in each and every schema).\n\nHaving the named stage go to a specific URL ( 's3://mybucket/path/') seems incredibly limiting.  \n", "author_fullname": "t2_qhsi5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Stages Best Practices - Named stages seem limiting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fqpkh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687400422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a pipeline (Python) where I am pulling from an API and pushing a CSV file into S3&lt;br/&gt;\nNext step is to COPY INTO a Snowflake table from that file I just dropped into S3 &lt;/p&gt;\n\n&lt;p&gt;Currently debating using one of these two approaches&lt;/p&gt;\n\n&lt;p&gt;Make a call with all the information needed (secrets are stored separately and called at runtime).&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;COPY INTO mytable\n  FROM s3://mybucket/path/file.csv credentials=(AWS_KEY_ID=&amp;#39;$AWS_ACCESS_KEY_ID&amp;#39; AWS_SECRET_KEY=&amp;#39;$AWS_SECRET_ACCESS_KEY&amp;#39;)\n  FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = &amp;#39;,&amp;#39; SKIP_HEADER = 1);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or create a named stage with my credentials and an associated file format, then simply &amp;quot;use&amp;quot; it...&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; COPY INTO mytable FROM @my_ext_stage FILES=&amp;#39;file.csv&amp;#39;; \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The only pros I can see for using a named stage is that my credentials would not be needed in the query  &lt;/p&gt;\n\n&lt;p&gt;On the other hand, the first method is more flexible, as I can change the path in the bucket within python. Plus I don&amp;#39;t have to bother creating a separate stage for each table I&amp;#39;ll be landing into, as well as an associated file format (in each and every schema).&lt;/p&gt;\n\n&lt;p&gt;Having the named stage go to a specific URL ( &amp;#39;s3://mybucket/path/&amp;#39;) seems incredibly limiting.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fqpkh", "is_robot_indexable": true, "report_reasons": null, "author": "NoUsernames1eft", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fqpkh/snowflake_stages_best_practices_named_stages_seem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fqpkh/snowflake_stages_best_practices_named_stages_seem/", "subreddit_subscribers": 111664, "created_utc": 1687400422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying Databricks for the first time. I created a 'standard' workspace using 'Quickstart on AWS' option.\n\nCloudformation runs into error during 'assign Metastore' step.\n\nFollowing is the error from Cloudwatch:\n\nHTTP content: b'\n\n{ \"error\\_code\": \"PERMISSION\\_DENIED\", \"message\": \"Cannot assign metastore to STANDARD tier workspace xyz\" }\n\nWhat is the solution for this?", "author_fullname": "t2_jl74ygnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Error while creating workspace with 'Quickstart' on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14faojz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687361087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying Databricks for the first time. I created a &amp;#39;standard&amp;#39; workspace using &amp;#39;Quickstart on AWS&amp;#39; option.&lt;/p&gt;\n\n&lt;p&gt;Cloudformation runs into error during &amp;#39;assign Metastore&amp;#39; step.&lt;/p&gt;\n\n&lt;p&gt;Following is the error from Cloudwatch:&lt;/p&gt;\n\n&lt;p&gt;HTTP content: b&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;error_code&amp;quot;: &amp;quot;PERMISSION_DENIED&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;Cannot assign metastore to STANDARD tier workspace xyz&amp;quot; }&lt;/p&gt;\n\n&lt;p&gt;What is the solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14faojz", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Device689", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14faojz/databricks_error_while_creating_workspace_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14faojz/databricks_error_while_creating_workspace_with/", "subreddit_subscribers": 111664, "created_utc": 1687361087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "a lot of talk around LLMs &amp; their potential impact on data teams workflows. \n\nMy guess is we'll see a lot of these pop-up in our favorite tools in the next few years (databricks, Snowflake, Tableau, Looker, dbt, etc).\n\ncan you add a quick explanation in comment to make it more interesting?\n\n[View Poll](https://www.reddit.com/poll/14f5mdr)", "author_fullname": "t2_bu8cw718", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think of the emerging concept of \"data copilots\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f5mdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687348355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;a lot of talk around LLMs &amp;amp; their potential impact on data teams workflows. &lt;/p&gt;\n\n&lt;p&gt;My guess is we&amp;#39;ll see a lot of these pop-up in our favorite tools in the next few years (databricks, Snowflake, Tableau, Looker, dbt, etc).&lt;/p&gt;\n\n&lt;p&gt;can you add a quick explanation in comment to make it more interesting?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/14f5mdr\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f5mdr", "is_robot_indexable": true, "report_reasons": null, "author": "castor-metadata", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1687607555683, "options": [{"text": "Over-hyped", "id": "23557093"}, {"text": "Game-changing", "id": "23557094"}, {"text": "No Opinion", "id": "23557095"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 97, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f5mdr/what_do_you_think_of_the_emerging_concept_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/14f5mdr/what_do_you_think_of_the_emerging_concept_of_data/", "subreddit_subscribers": 111664, "created_utc": 1687348355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Friends,   \nI am currently working as senior data engineer with 8yoe. I am looking forward to move into consulting role in the same organisation, But problem is i think I lacks soft skill, I feel like I don't have a flow in communication, I don't recall business terms, and don't have conviction in what I say.   \nSo, I am looking forward to know:  \n1. Best way to learn the business communication.  \n2. How to handle the when you don't know something, or say something incorrect.  \n3. How to be a pro consultant.  \nI know the most common answer for these is practise, but how do I prepare before I directly jump into that. \n\nPlease also recommend if you know any books, course, video etc.", "author_fullname": "t2_u1vbo568", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need guidance on consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fu5wp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687410769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Friends,&lt;br/&gt;\nI am currently working as senior data engineer with 8yoe. I am looking forward to move into consulting role in the same organisation, But problem is i think I lacks soft skill, I feel like I don&amp;#39;t have a flow in communication, I don&amp;#39;t recall business terms, and don&amp;#39;t have conviction in what I say.&lt;br/&gt;\nSo, I am looking forward to know:&lt;br/&gt;\n1. Best way to learn the business communication.&lt;br/&gt;\n2. How to handle the when you don&amp;#39;t know something, or say something incorrect.&lt;br/&gt;\n3. How to be a pro consultant.&lt;br/&gt;\nI know the most common answer for these is practise, but how do I prepare before I directly jump into that. &lt;/p&gt;\n\n&lt;p&gt;Please also recommend if you know any books, course, video etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fu5wp", "is_robot_indexable": true, "report_reasons": null, "author": "manu13891", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fu5wp/need_guidance_on_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fu5wp/need_guidance_on_consulting/", "subreddit_subscribers": 111664, "created_utc": 1687410769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a database, obviously it's common to enforce things like \"all rows in the \\`purchases\\` table must have a foreign key relationship to the \\`users\\` table (we have to know which user made each purchase!)\"\n\nWhen ELT'ing data to a data warehouse, I wrote a test in dbt to check the same thing.\n\nThis test periodically fails though, due to the fact that sometimes the ELT process transfers the \\`users\\` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the \\`purchases\\` table to the data warehouse. The result being that there's a purchase in the data warehouse which does not have a corresponding user in the user's table. Which means my dbt test fails.\n\nIs this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship \"for purchases made more than X hours ago\", where X is greater than the most recent load into the dw.", "author_fullname": "t2_116mtw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle table relationships in data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fnn1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687391877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a database, obviously it&amp;#39;s common to enforce things like &amp;quot;all rows in the `purchases` table must have a foreign key relationship to the `users` table (we have to know which user made each purchase!)&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;When ELT&amp;#39;ing data to a data warehouse, I wrote a test in dbt to check the same thing.&lt;/p&gt;\n\n&lt;p&gt;This test periodically fails though, due to the fact that sometimes the ELT process transfers the `users` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the `purchases` table to the data warehouse. The result being that there&amp;#39;s a purchase in the data warehouse which does not have a corresponding user in the user&amp;#39;s table. Which means my dbt test fails.&lt;/p&gt;\n\n&lt;p&gt;Is this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship &amp;quot;for purchases made more than X hours ago&amp;quot;, where X is greater than the most recent load into the dw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fnn1s", "is_robot_indexable": true, "report_reasons": null, "author": "PM_ME_SCIENCEY_STUFF", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "subreddit_subscribers": 111664, "created_utc": 1687391877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nI'm trying to implement  delta architecture to spark pipelines in my organization. I understand most of the idea but i still wonder what's the correct approach to cache last processed delta commit id from CDF in downstream pipelines. This will emable me to process only new data there. In most tutorials and articles CDC capabilities of delta are shown with those values hardcoded. I plan store it there as kv 'pipelinename':'last_read_version' in either separate bookmark table or some kv store solution. Is that the correct approach to solve it or am I missing something?", "author_fullname": "t2_bf5ei6em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC im delta lake - how to store commit id downstream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14feh1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687374132.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687369853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nI&amp;#39;m trying to implement  delta architecture to spark pipelines in my organization. I understand most of the idea but i still wonder what&amp;#39;s the correct approach to cache last processed delta commit id from CDF in downstream pipelines. This will emable me to process only new data there. In most tutorials and articles CDC capabilities of delta are shown with those values hardcoded. I plan store it there as kv &amp;#39;pipelinename&amp;#39;:&amp;#39;last_read_version&amp;#39; in either separate bookmark table or some kv store solution. Is that the correct approach to solve it or am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14feh1u", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning_Hurry2611", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14feh1u/cdc_im_delta_lake_how_to_store_commit_id/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14feh1u/cdc_im_delta_lake_how_to_store_commit_id/", "subreddit_subscribers": 111664, "created_utc": 1687369853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9msdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inside Starburst's hackathon: Into the great wide open(AI)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_14fbehd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/voccu5Jpp747AIKYmZfqwNf18sCRWpOlm7toVOuASio.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687362748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dev.to", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dev.to/starburstengineering/inside-starbursts-hackathon-into-the-great-wide-openai-3nl8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?auto=webp&amp;v=enabled&amp;s=aee7695eb34a88bed0b0b023121ef0e73fe7d268", "width": 1128, "height": 598}, "resolutions": [{"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b037eecc6379553b7a6d02fa3e40f8afc7c10d45", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92c758f2e18da31cd4db9c6acbf98f1911b54a10", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e40fdfe9460a0bb901a24bbb24284530349ff50f", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6dac3ddb39d735a2121b81201836dbf740d831e", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d489266392bfc1dacc29bac02181411a714a3175", "width": 960, "height": 508}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13aff827560a0748b74ae432b23b63849518e2fe", "width": 1080, "height": 572}], "variants": {}, "id": "k883IOz-8cn8hz390a3aFIjLzjAXzcFAs5flDn8iqwo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14fbehd", "is_robot_indexable": true, "report_reasons": null, "author": "randgalt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fbehd/inside_starbursts_hackathon_into_the_great_wide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dev.to/starburstengineering/inside-starbursts-hackathon-into-the-great-wide-openai-3nl8", "subreddit_subscribers": 111664, "created_utc": 1687362748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everyone,\n\nI have an issue relating to a local Oracle server (19c) creating a link to AWS via IAM user.\n\nI have an access key for the AWS IAM user, and AWS automatically generates a secret key (password) associated with the access key. Now the secret key is about 40chars long.  \nIn Oracle 19c the maximum length for an identifier is 30chars!?  \nMeaning when I try using JDBC to connect Oracle to AWS I get an error (ORA-00972: identifier is too long).  \n\n\nHas anyone experienced something like this? Any workarounds?  \nHelp would be much appreciated \ud83d\ude4f", "author_fullname": "t2_4wsp8k38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting to AWS From and Oracle Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f8dqg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687355639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt;\n\n&lt;p&gt;I have an issue relating to a local Oracle server (19c) creating a link to AWS via IAM user.&lt;/p&gt;\n\n&lt;p&gt;I have an access key for the AWS IAM user, and AWS automatically generates a secret key (password) associated with the access key. Now the secret key is about 40chars long.&lt;br/&gt;\nIn Oracle 19c the maximum length for an identifier is 30chars!?&lt;br/&gt;\nMeaning when I try using JDBC to connect Oracle to AWS I get an error (ORA-00972: identifier is too long).  &lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced something like this? Any workarounds?&lt;br/&gt;\nHelp would be much appreciated \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14f8dqg", "is_robot_indexable": true, "report_reasons": null, "author": "trendy_parker", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f8dqg/connecting_to_aws_from_and_oracle_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f8dqg/connecting_to_aws_from_and_oracle_server/", "subreddit_subscribers": 111664, "created_utc": 1687355639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So ill be doing a test this week for a job, before starting the test there are some instructions and its written \"You can write your solution(s) in SQL.\"\n\nDoes this means its a pure SQL test? Ive been studying python for this test in the last few days, now maybe i should stop and go full SQL.", "author_fullname": "t2_w5u5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick question about Codility test", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f7hg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687353344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So ill be doing a test this week for a job, before starting the test there are some instructions and its written &amp;quot;You can write your solution(s) in SQL.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Does this means its a pure SQL test? Ive been studying python for this test in the last few days, now maybe i should stop and go full SQL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14f7hg8", "is_robot_indexable": true, "report_reasons": null, "author": "Lenant", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f7hg8/quick_question_about_codility_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f7hg8/quick_question_about_codility_test/", "subreddit_subscribers": 111664, "created_utc": 1687353344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  \n\n\nBut I particularly like the practice of **\"keeping code changes small and simple, and getting them into production fast by testing &amp; automating\"** (if you can call this a practice) - that, in my experience works pretty well in **data ingestion**.\n\nHere's how I'm going about it when working on data pipelines:\n\n1. Ingest all columns for a new table, but don\u2019t ingest additional tables unless necessary. Keep your ingestion code simple.\n2. Prefer log-based ingestion &gt; full table &gt; key based. Log-based is by far the simplest duplication method\u2014no need to hand code anything.\n3. Always assume duplicates in all data tables. If you don\u2019t want them, add a test to find problems before a dashboard user does!\n4. Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.\n5. Have a prod-like environment for every developer, e.g., a {user\\_prefix}\\_data snowflake schema, to work fast.\n6. Run everything through CI/CD. Why test by hand if a machine can do it for you?\n\n&amp;#x200B;\n\nDo you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  \n", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering practices that translate well to data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14fxs4p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687422900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  &lt;/p&gt;\n\n&lt;p&gt;But I particularly like the practice of &lt;strong&gt;&amp;quot;keeping code changes small and simple, and getting them into production fast by testing &amp;amp; automating&amp;quot;&lt;/strong&gt; (if you can call this a practice) - that, in my experience works pretty well in &lt;strong&gt;data ingestion&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how I&amp;#39;m going about it when working on data pipelines:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ingest all columns for a new table, but don\u2019t ingest additional tables unless necessary. Keep your ingestion code simple.&lt;/li&gt;\n&lt;li&gt;Prefer log-based ingestion &amp;gt; full table &amp;gt; key based. Log-based is by far the simplest duplication method\u2014no need to hand code anything.&lt;/li&gt;\n&lt;li&gt;Always assume duplicates in all data tables. If you don\u2019t want them, add a test to find problems before a dashboard user does!&lt;/li&gt;\n&lt;li&gt;Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.&lt;/li&gt;\n&lt;li&gt;Have a prod-like environment for every developer, e.g., a {user_prefix}_data snowflake schema, to work fast.&lt;/li&gt;\n&lt;li&gt;Run everything through CI/CD. Why test by hand if a machine can do it for you?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fxs4p", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fxs4p/software_engineering_practices_that_translate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fxs4p/software_engineering_practices_that_translate/", "subreddit_subscribers": 111664, "created_utc": 1687422900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning and doing certification in big query to get into data analytics /engineer field, i wanted to ask how in real time in companies , on job people to interact with big query framework ,  CLI or api ?", "author_fullname": "t2_t526hbv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interface with BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fp7f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687396115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning and doing certification in big query to get into data analytics /engineer field, i wanted to ask how in real time in companies , on job people to interact with big query framework ,  CLI or api ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fp7f7", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Ticket6016", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fp7f7/interface_with_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fp7f7/interface_with_bigquery/", "subreddit_subscribers": 111664, "created_utc": 1687396115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team has created an integration for Datadog where users can connect their accounts and send us their error monitoring data directly from Datadog's WebHook API. We don't have our own DD account. Does anyone know of any way to test this webhook without our own account? Some tools i've seen offer a mock project or mock data in some sort of sandbox.", "author_fullname": "t2_g400fo1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for testing datadog webhook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fi6e6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687378670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has created an integration for Datadog where users can connect their accounts and send us their error monitoring data directly from Datadog&amp;#39;s WebHook API. We don&amp;#39;t have our own DD account. Does anyone know of any way to test this webhook without our own account? Some tools i&amp;#39;ve seen offer a mock project or mock data in some sort of sandbox.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fi6e6", "is_robot_indexable": true, "report_reasons": null, "author": "rmilaz18", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fi6e6/tips_for_testing_datadog_webhook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fi6e6/tips_for_testing_datadog_webhook/", "subreddit_subscribers": 111664, "created_utc": 1687378670.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}