{"kind": "Listing", "data": {"after": "t3_14fnn1s", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just received this requirement from our users.\n\nhttps://preview.redd.it/dkvbl0ytob7b1.png?width=207&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=284cf935784d2eb5c31122197f6b6f800a28f644", "author_fullname": "t2_wrxmx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best User Requirement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 43, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dkvbl0ytob7b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/dkvbl0ytob7b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=054c812515c43abbfdc945d62ce495737c34259a"}], "s": {"y": 64, "x": 207, "u": "https://preview.redd.it/dkvbl0ytob7b1.png?width=207&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=284cf935784d2eb5c31122197f6b6f800a28f644"}, "id": "dkvbl0ytob7b1"}}, "name": "t3_14f0msx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6tMBza5b6ErUTennUGN8xWA-okWWK-mxvvGe9Sjas2g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687332123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just received this requirement from our users.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dkvbl0ytob7b1.png?width=207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=284cf935784d2eb5c31122197f6b6f800a28f644\"&gt;https://preview.redd.it/dkvbl0ytob7b1.png?width=207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=284cf935784d2eb5c31122197f6b6f800a28f644&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14f0msx", "is_robot_indexable": true, "report_reasons": null, "author": "napstervab", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f0msx/best_user_requirement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f0msx/best_user_requirement/", "subreddit_subscribers": 111638, "created_utc": 1687332123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Playlist](https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;list=PLGVZCDnMOq0peDguAzds7kVmBr8avp46K)\n\nSome of the talks I found most intersting:\n\n* [ Use Spark from anywhere - A Spark client in Python powered by Spark Connect](https://www.youtube.com/watch?v=PzgPcvFDD4I&amp;t=1339s)\n* [Streamlit meets WebAssembly - stlite](https://www.youtube.com/watch?v=XivJYZUm1GY&amp;t=1368s)\n* [Pragmatic ways of using Rust in your data project](https://www.youtube.com/watch?v=Jk9NXfvgclU&amp;t=1374s)\n* [An Introduction to Apache Spark](https://www.youtube.com/watch?v=jOJceajwMGs&amp;t=749s)\n* [WebAssembly demystified](https://www.youtube.com/watch?v=VCkcv0ppYXs&amp;t=5s)\n* [Rusty Python - A Case Study](https://www.youtube.com/watch?v=Y5XQR0wUEyM&amp;t=52s)\n* [Towards Learned Database Systems](https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;t=13s)", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyCon DE &amp; PyData Berlin 2023 Playlist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f73gz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687352384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;amp;list=PLGVZCDnMOq0peDguAzds7kVmBr8avp46K\"&gt;Playlist&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some of the talks I found most intersting:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=PzgPcvFDD4I&amp;amp;t=1339s\"&gt; Use Spark from anywhere - A Spark client in Python powered by Spark Connect&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=XivJYZUm1GY&amp;amp;t=1368s\"&gt;Streamlit meets WebAssembly - stlite&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=Jk9NXfvgclU&amp;amp;t=1374s\"&gt;Pragmatic ways of using Rust in your data project&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=jOJceajwMGs&amp;amp;t=749s\"&gt;An Introduction to Apache Spark&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=VCkcv0ppYXs&amp;amp;t=5s\"&gt;WebAssembly demystified&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=Y5XQR0wUEyM&amp;amp;t=52s\"&gt;Rusty Python - A Case Study&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=VtL6Y4x10O0&amp;amp;t=13s\"&gt;Towards Learned Database Systems&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?auto=webp&amp;v=enabled&amp;s=a3492d7c1a5ad1a7128c98e245be41f067f20795", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c85fba682150a25e76d4b3f3d192b4e9d7729e59", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3d984e0b2729f9057c257397a25749f467a8337", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Z9edXAhh-aBR9kQgHf91R1BQAa8YHzOzrPzFgw-B22w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49ad477876c3a7b0ab159660bcff9ba2a6bdc865", "width": 320, "height": 240}], "variants": {}, "id": "qVYw_YvefBehG9llXEP7eXxaUqId0OOhkEobkd1GcbM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f73gz", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f73gz/pycon_de_pydata_berlin_2023_playlist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f73gz/pycon_de_pydata_berlin_2023_playlist/", "subreddit_subscribers": 111638, "created_utc": 1687352384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It'll be so fun. Every week we'll invite a stakeholder to join us and make an absurd request for some view or something, and we'll role-play how we respond to keep the stakeholder happy to our own detriment.\n\nhttps://preview.redd.it/977fk80hkd7b1.png?width=910&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody want to join my book club?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"977fk80hkd7b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 143, "x": 108, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a236563704fb8a2a5295c901a49dedd9d42d0f6"}, {"y": 287, "x": 216, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb1a37c61e110db11ae11524371ffc4d785d039d"}, {"y": 425, "x": 320, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1107950234e6cefd1fffebc16a5103df0ff71ad"}, {"y": 851, "x": 640, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af838dc270b66d9070ce3060c486679e7167d9e9"}], "s": {"y": 1211, "x": 910, "u": "https://preview.redd.it/977fk80hkd7b1.png?width=910&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3"}, "id": "977fk80hkd7b1"}}, "name": "t3_14f846e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jdrS8rYFm1Szp6wJdBpRHCXbPC0SNZXemW-X7IBQEPs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687354978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;ll be so fun. Every week we&amp;#39;ll invite a stakeholder to join us and make an absurd request for some view or something, and we&amp;#39;ll role-play how we respond to keep the stakeholder happy to our own detriment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/977fk80hkd7b1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3\"&gt;https://preview.redd.it/977fk80hkd7b1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6c89e2daf37736dd39d62a2783f354bb804b65f3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14f846e", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f846e/anybody_want_to_join_my_book_club/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f846e/anybody_want_to_join_my_book_club/", "subreddit_subscribers": 111638, "created_utc": 1687354978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had an interview the other day that was pretty standard technical and competency based questions. But the last one stumped me. \u201cHow would you support your new team and team manager\u201d . I fluffed some rubbish about looking for areas team members needed support in to take work off their plate and help me get up to speed. \n\nBut I\u2019m curious as to what everyone else would say and particularly anyone leading a data team would be thinking of?", "author_fullname": "t2_scnmi5ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you answer this interview question?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14eyqrg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687325998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had an interview the other day that was pretty standard technical and competency based questions. But the last one stumped me. \u201cHow would you support your new team and team manager\u201d . I fluffed some rubbish about looking for areas team members needed support in to take work off their plate and help me get up to speed. &lt;/p&gt;\n\n&lt;p&gt;But I\u2019m curious as to what everyone else would say and particularly anyone leading a data team would be thinking of?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14eyqrg", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Aardvark258", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14eyqrg/how_would_you_answer_this_interview_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14eyqrg/how_would_you_answer_this_interview_question/", "subreddit_subscribers": 111638, "created_utc": 1687325998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI am in the process of transitioning into Data Engineering. For context, I have a mechanical engineering degree and business masters degree from a reputable university and have worked as an engineer for a large supply chain/logistics/transportation company for a couple of years now. In my current role, I do a good bit of data analytics, although mainly using lower-level tools. In my free time, I have taken to learning Python (including pandas, NumPy, etc.) via online tutorials and text books and have implemented this in my current role by working on miscellaneous \"sandbox\" projects. I have also gained a fundamental understanding of database design and SQL and am taking some online trainings for Databricks.\n\nAm I crazy to think that I can pivot careers through my own dedication rather than seeking an additional degree? I am a technical learner and am dedicated to growing my knowledge gradually, but the terminology and vast number of tools can seem overwhelming at times. If it is possible to pivot on my own, does anyone have recommendations for materials/tutorials/online courses to grow my knowledge in ETL pipelines, distributed computing (Hadoop, Spark, etc.), and other cloud tools? I plan on working my way slowly through some of the recommendations from this subreddit's Learning Resources section. Apologies for the long post. Any advice is much appreciated in advance!\n\nThanks, all!", "author_fullname": "t2_5tzzasmn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Pivot Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14etouq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687311357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am in the process of transitioning into Data Engineering. For context, I have a mechanical engineering degree and business masters degree from a reputable university and have worked as an engineer for a large supply chain/logistics/transportation company for a couple of years now. In my current role, I do a good bit of data analytics, although mainly using lower-level tools. In my free time, I have taken to learning Python (including pandas, NumPy, etc.) via online tutorials and text books and have implemented this in my current role by working on miscellaneous &amp;quot;sandbox&amp;quot; projects. I have also gained a fundamental understanding of database design and SQL and am taking some online trainings for Databricks.&lt;/p&gt;\n\n&lt;p&gt;Am I crazy to think that I can pivot careers through my own dedication rather than seeking an additional degree? I am a technical learner and am dedicated to growing my knowledge gradually, but the terminology and vast number of tools can seem overwhelming at times. If it is possible to pivot on my own, does anyone have recommendations for materials/tutorials/online courses to grow my knowledge in ETL pipelines, distributed computing (Hadoop, Spark, etc.), and other cloud tools? I plan on working my way slowly through some of the recommendations from this subreddit&amp;#39;s Learning Resources section. Apologies for the long post. Any advice is much appreciated in advance!&lt;/p&gt;\n\n&lt;p&gt;Thanks, all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14etouq", "is_robot_indexable": true, "report_reasons": null, "author": "SmeagolsPrecious_", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14etouq/career_pivot_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14etouq/career_pivot_advice/", "subreddit_subscribers": 111638, "created_utc": 1687311357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I\u2019ve long worked with Databricks and want to step up my work on projects outside of my job. I really love developing in notebooks, but being able to version control with .py files and have all that git goodness. Also the flexibility of using that code in VS Code. \n\nMy question is, what open source alternatives are there such that I can develop in a \u201cnotebook\u201d, and version control it without a headache? Thanks!\n\nEDIT: I guess to clarify, I don\u2019t want to work out of Databricks for personal projects, so the aim is to retain the full functionality of working in notebooks that then have the benefits of easy version control, as eg Jupiter notebooks are horrible with being a bunch of json when directly vc\u2019d in a repo.", "author_fullname": "t2_7kdevi10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source Databricks notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f1fvs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687351642.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687334832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019ve long worked with Databricks and want to step up my work on projects outside of my job. I really love developing in notebooks, but being able to version control with .py files and have all that git goodness. Also the flexibility of using that code in VS Code. &lt;/p&gt;\n\n&lt;p&gt;My question is, what open source alternatives are there such that I can develop in a \u201cnotebook\u201d, and version control it without a headache? Thanks!&lt;/p&gt;\n\n&lt;p&gt;EDIT: I guess to clarify, I don\u2019t want to work out of Databricks for personal projects, so the aim is to retain the full functionality of working in notebooks that then have the benefits of easy version control, as eg Jupiter notebooks are horrible with being a bunch of json when directly vc\u2019d in a repo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f1fvs", "is_robot_indexable": true, "report_reasons": null, "author": "Impressive_Fact_6561", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f1fvs/open_source_databricks_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f1fvs/open_source_databricks_notebooks/", "subreddit_subscribers": 111638, "created_utc": 1687334832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering how long would it take in average for a person with healthcare background to enter into data engineer field? I have a bachelors in medical sciences and a masters in healthcare Informatics. I have a certification in SAS, have basic understanding and programming knowledge of SQL and R. I have zero knowledge of python java or anyother programming tool. Is it gonna be very challenging for me to work as data engineer?", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Break into Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ffv95", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687373172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering how long would it take in average for a person with healthcare background to enter into data engineer field? I have a bachelors in medical sciences and a masters in healthcare Informatics. I have a certification in SAS, have basic understanding and programming knowledge of SQL and R. I have zero knowledge of python java or anyother programming tool. Is it gonna be very challenging for me to work as data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14ffv95", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ffv95/break_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ffv95/break_into_data_engineering/", "subreddit_subscribers": 111638, "created_utc": 1687373172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "No transformation is required, just replicating tables as in source. \n\nHere are options we have tried:\n\nDataflow/ DataStream: Too costly.\n\nFivetran: Too slow (and forces to create new columns in sink to be able to support the incremental changes)\n\nApache NiFi: too much Java code &amp; hard to code for inc sync.\n\nHevo data/ Integrateio: Eval in progress.\n\nAirbyte: seems not much useful for high volume\n\nSo far we found Fivetran to hold our use case better but the performance is so poor that we would like to explore options. Would really appreciate any idea to solve this on. ", "author_fullname": "t2_1l9tu7ql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools / Best way to ingest large 20TB plus data from Bigquery to Google CloudSQL Postgres with 2TB daily changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fkmbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687384326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No transformation is required, just replicating tables as in source. &lt;/p&gt;\n\n&lt;p&gt;Here are options we have tried:&lt;/p&gt;\n\n&lt;p&gt;Dataflow/ DataStream: Too costly.&lt;/p&gt;\n\n&lt;p&gt;Fivetran: Too slow (and forces to create new columns in sink to be able to support the incremental changes)&lt;/p&gt;\n\n&lt;p&gt;Apache NiFi: too much Java code &amp;amp; hard to code for inc sync.&lt;/p&gt;\n\n&lt;p&gt;Hevo data/ Integrateio: Eval in progress.&lt;/p&gt;\n\n&lt;p&gt;Airbyte: seems not much useful for high volume&lt;/p&gt;\n\n&lt;p&gt;So far we found Fivetran to hold our use case better but the performance is so poor that we would like to explore options. Would really appreciate any idea to solve this on. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fkmbu", "is_robot_indexable": true, "report_reasons": null, "author": "onksssss", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fkmbu/tools_best_way_to_ingest_large_20tb_plus_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fkmbu/tools_best_way_to_ingest_large_20tb_plus_data/", "subreddit_subscribers": 111638, "created_utc": 1687384326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DEs! Thanks for so much info over the last year, this is a great resource for those of us in the field. \n\nI have been working as a \"Data Engineer\" for the last year and a half. The reason I put this in quotes is because I don't feel like I live up to the title, as cool as it sounds. I read through these posts daily and have no idea what most of you are talking about, most of the time. A little background is that I came into this position by luck because I had a good resume within the company and the company doesn't have much internal competition for technical positions. I have no background in CS, DWH, DE, or really any type of coding. I've entirely learned on the job. \n\nThe problem with that is this is a very large company ( a large government organization ) and I work on one specific team that handles ETL exclusively. If I understand the term correctly our stack is: Ab Initio (ETL apps), oracle db for staging tables, and UNIX servers. My daily work consists of making updates to these ab initio applications. Within those applications are a bunch of unix shell scripts that I also have to work with. As far as the oracle db, I don't really do any SQL work. We have a modeling team that handles all of that. So the primary skills would be Ab Initio development &amp; shell scripting, and then navigating the metric ton of red tape for promotions to prod. \n\nMy question is , am I wasting my time? From everything I read on here, it seems like ab initio is a dying tool on its way out. It also seems like shell scripting isn't used much. Everyone seems to be using python, cloud, and a litany of other tools I've never heard of. This type of work does not come easy to me, as much as I wish it did. I work with a bunch of people who are either not great at the job, and if they are they aren't good communicators, nor do they want to mentor a new developer. \n\nTLDR: It feels like I am fighting an uphill battle to learn outdated technology that is only useful in my current role but doesn't actually build me into a true DE. I'm still relatively young and at a pivotal moment of my career. It feels like if I want a career in this field, this isn't the right place. What would you data Jedis do?", "author_fullname": "t2_73cw9sv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dinosaur tools &amp; a dead stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fajwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687360785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DEs! Thanks for so much info over the last year, this is a great resource for those of us in the field. &lt;/p&gt;\n\n&lt;p&gt;I have been working as a &amp;quot;Data Engineer&amp;quot; for the last year and a half. The reason I put this in quotes is because I don&amp;#39;t feel like I live up to the title, as cool as it sounds. I read through these posts daily and have no idea what most of you are talking about, most of the time. A little background is that I came into this position by luck because I had a good resume within the company and the company doesn&amp;#39;t have much internal competition for technical positions. I have no background in CS, DWH, DE, or really any type of coding. I&amp;#39;ve entirely learned on the job. &lt;/p&gt;\n\n&lt;p&gt;The problem with that is this is a very large company ( a large government organization ) and I work on one specific team that handles ETL exclusively. If I understand the term correctly our stack is: Ab Initio (ETL apps), oracle db for staging tables, and UNIX servers. My daily work consists of making updates to these ab initio applications. Within those applications are a bunch of unix shell scripts that I also have to work with. As far as the oracle db, I don&amp;#39;t really do any SQL work. We have a modeling team that handles all of that. So the primary skills would be Ab Initio development &amp;amp; shell scripting, and then navigating the metric ton of red tape for promotions to prod. &lt;/p&gt;\n\n&lt;p&gt;My question is , am I wasting my time? From everything I read on here, it seems like ab initio is a dying tool on its way out. It also seems like shell scripting isn&amp;#39;t used much. Everyone seems to be using python, cloud, and a litany of other tools I&amp;#39;ve never heard of. This type of work does not come easy to me, as much as I wish it did. I work with a bunch of people who are either not great at the job, and if they are they aren&amp;#39;t good communicators, nor do they want to mentor a new developer. &lt;/p&gt;\n\n&lt;p&gt;TLDR: It feels like I am fighting an uphill battle to learn outdated technology that is only useful in my current role but doesn&amp;#39;t actually build me into a true DE. I&amp;#39;m still relatively young and at a pivotal moment of my career. It feels like if I want a career in this field, this isn&amp;#39;t the right place. What would you data Jedis do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14fajwv", "is_robot_indexable": true, "report_reasons": null, "author": "ApatheticRart", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fajwv/dinosaur_tools_a_dead_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fajwv/dinosaur_tools_a_dead_stack/", "subreddit_subscribers": 111638, "created_utc": 1687360785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, so to explain my problem to you, I need to ingest data from Google ADS DATA HUB (ADH) to Big Query and I have done the python code, the code needs to be run weekly (cron job), however we need a snapshot of the tables in the end of the month. The final code will be dockerized and run on Google Kubernetes Engine.\n\nWhat solutions do you suggest to me? I\u2019m open to any suggestion to further learn and improve my data engineering skills.\n\nTL;DR I need to schedule the python code to run weekly and last day of the month. Any suggestions?\n\nEdit: typo and added details", "author_fullname": "t2_5u3c1gj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Ingestion from google ads to big query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f60ba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687360316.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687349426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, so to explain my problem to you, I need to ingest data from Google ADS DATA HUB (ADH) to Big Query and I have done the python code, the code needs to be run weekly (cron job), however we need a snapshot of the tables in the end of the month. The final code will be dockerized and run on Google Kubernetes Engine.&lt;/p&gt;\n\n&lt;p&gt;What solutions do you suggest to me? I\u2019m open to any suggestion to further learn and improve my data engineering skills.&lt;/p&gt;\n\n&lt;p&gt;TL;DR I need to schedule the python code to run weekly and last day of the month. Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Edit: typo and added details&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14f60ba", "is_robot_indexable": true, "report_reasons": null, "author": "iGodFather302", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f60ba/data_ingestion_from_google_ads_to_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f60ba/data_ingestion_from_google_ads_to_big_query/", "subreddit_subscribers": 111638, "created_utc": 1687349426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you guys orchestrating your pipelines? \n\nCurrently our environment is : \nADF for orchestration\n,databricks for transformations\n,AZDL Gen 2 storage\n,synapse for reporting (which is slowly going away as we transition to Databricks). \n\nMy company is doing away with Azure (don\u2019t ask me why) and transitioning to GCP. We will keep databricks for transformations and most likely reporting, but now we lose our orchestration tool. \n\nCurrently we use ADF for connections to our source systems where we have 100s of sql ingestion queries (that are stored in a table and looped through) to copy data out of 40 different source system sql db\u2019s. Data is ingested into AZDL Gen2 storage as parquet files then picked up in databricks notebooks. \n\nWhat tools are you using that can replace ADF?", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fjdfm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687381445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you guys orchestrating your pipelines? &lt;/p&gt;\n\n&lt;p&gt;Currently our environment is : \nADF for orchestration\n,databricks for transformations\n,AZDL Gen 2 storage\n,synapse for reporting (which is slowly going away as we transition to Databricks). &lt;/p&gt;\n\n&lt;p&gt;My company is doing away with Azure (don\u2019t ask me why) and transitioning to GCP. We will keep databricks for transformations and most likely reporting, but now we lose our orchestration tool. &lt;/p&gt;\n\n&lt;p&gt;Currently we use ADF for connections to our source systems where we have 100s of sql ingestion queries (that are stored in a table and looped through) to copy data out of 40 different source system sql db\u2019s. Data is ingested into AZDL Gen2 storage as parquet files then picked up in databricks notebooks. &lt;/p&gt;\n\n&lt;p&gt;What tools are you using that can replace ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fjdfm", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fjdfm/orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fjdfm/orchestration/", "subreddit_subscribers": 111638, "created_utc": 1687381445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Or just your go-to tools?\n\nOr do you just not have any real preference, more like \"whatever tool fits the job\"?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your favorite Data Scraping tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fc8fj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687364624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or just your go-to tools?&lt;/p&gt;\n\n&lt;p&gt;Or do you just not have any real preference, more like &amp;quot;whatever tool fits the job&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fc8fj", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fc8fj/what_are_your_favorite_data_scraping_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fc8fj/what_are_your_favorite_data_scraping_tools/", "subreddit_subscribers": 111638, "created_utc": 1687364624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Offcourse I do understand that governance is a massive module and may not be linked to DE directly. \n\nI see lot of overlaps between the two modules: Lineage, basic catalog, and metadata mgmt \n\nI would like to learn from comments and views.", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MonteCarlo integrates with Atlan or vice-versa - I think Data observability should be unified with catalog / governance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f93du", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687357349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Offcourse I do understand that governance is a massive module and may not be linked to DE directly. &lt;/p&gt;\n\n&lt;p&gt;I see lot of overlaps between the two modules: Lineage, basic catalog, and metadata mgmt &lt;/p&gt;\n\n&lt;p&gt;I would like to learn from comments and views.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f93du", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f93du/montecarlo_integrates_with_atlan_or_viceversa_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f93du/montecarlo_integrates_with_atlan_or_viceversa_i/", "subreddit_subscribers": 111638, "created_utc": 1687357349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Backstory: I run the eCommerce for a company that also has a storefront &amp; warehouse on location where we ship out orders. For years we have been using RetailPro as our inventory system and 24seven as a middleware to send our product to Shopify.\n\nOur parent company is making us switch to DataWorks, with Boomi sending product data to Magento as our new eCommerce platform. We\u2019ve been going through this new process with an outside company doing the integration work for us. \n\nJust recently we were told that Boomi can only send product data from 1 location in DataWorks, such as the Warehouse. The way we\u2019ve always done it before with 24seven is ALL inventory data between the Store &amp; Warehouse would be sent over to Shopify. This has always been the preference since a lot of product doesn\u2019t have inventory in the Warehouse, as it\u2019s seasonal. If needed we could pull from the Store to fulfill orders.\n\nI\u2019m in no means a data engineer, nor is anybody on my team. But I just need to know if there is a solution for this issue.\n\nTLDR; we want Boomi to send over inventory data from multiple locations in DataWorks over to Magento.", "author_fullname": "t2_3ps9svrr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boomi related issue for eCommerce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fjbr3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687381345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Backstory: I run the eCommerce for a company that also has a storefront &amp;amp; warehouse on location where we ship out orders. For years we have been using RetailPro as our inventory system and 24seven as a middleware to send our product to Shopify.&lt;/p&gt;\n\n&lt;p&gt;Our parent company is making us switch to DataWorks, with Boomi sending product data to Magento as our new eCommerce platform. We\u2019ve been going through this new process with an outside company doing the integration work for us. &lt;/p&gt;\n\n&lt;p&gt;Just recently we were told that Boomi can only send product data from 1 location in DataWorks, such as the Warehouse. The way we\u2019ve always done it before with 24seven is ALL inventory data between the Store &amp;amp; Warehouse would be sent over to Shopify. This has always been the preference since a lot of product doesn\u2019t have inventory in the Warehouse, as it\u2019s seasonal. If needed we could pull from the Store to fulfill orders.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m in no means a data engineer, nor is anybody on my team. But I just need to know if there is a solution for this issue.&lt;/p&gt;\n\n&lt;p&gt;TLDR; we want Boomi to send over inventory data from multiple locations in DataWorks over to Magento.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fjbr3", "is_robot_indexable": true, "report_reasons": null, "author": "i-race-goats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fjbr3/boomi_related_issue_for_ecommerce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fjbr3/boomi_related_issue_for_ecommerce/", "subreddit_subscribers": 111638, "created_utc": 1687381345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to learn Apache Iceberg for building a data lake. We have late arriving data and the data is partitioned on date column. I will have a spark job that will transform the incoming data to iceberg format. Consider a scenario where the ingestion pipeline fails mid way, since dynamic partition overwrite is enabled, a rerun of the task will overwrite only the partitions that were created in previous run. This will work if there is no late arriving data. But consider a situation where I have data from the last day and the partition for that has already been created when I ingested the data in the yesterday\u2019s run. Now that the current day (which has late arriving data) run fails and since I have set dynamic partition overwrite, the partition that was created yesterday will also get rewritten. Is there a better way to handle dynamic partition overwrite for atomicity as well and late arriving data", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Iceberg Dynamic Partition Overwrite.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fc6i0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687364500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to learn Apache Iceberg for building a data lake. We have late arriving data and the data is partitioned on date column. I will have a spark job that will transform the incoming data to iceberg format. Consider a scenario where the ingestion pipeline fails mid way, since dynamic partition overwrite is enabled, a rerun of the task will overwrite only the partitions that were created in previous run. This will work if there is no late arriving data. But consider a situation where I have data from the last day and the partition for that has already been created when I ingested the data in the yesterday\u2019s run. Now that the current day (which has late arriving data) run fails and since I have set dynamic partition overwrite, the partition that was created yesterday will also get rewritten. Is there a better way to handle dynamic partition overwrite for atomicity as well and late arriving data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14fc6i0", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fc6i0/apache_iceberg_dynamic_partition_overwrite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fc6i0/apache_iceberg_dynamic_partition_overwrite/", "subreddit_subscribers": 111638, "created_utc": 1687364500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying Databricks for the first time. I created a 'standard' workspace using 'Quickstart on AWS' option.\n\nCloudformation runs into error during 'assign Metastore' step.\n\nFollowing is the error from Cloudwatch:\n\nHTTP content: b'\n\n{ \"error\\_code\": \"PERMISSION\\_DENIED\", \"message\": \"Cannot assign metastore to STANDARD tier workspace xyz\" }\n\nWhat is the solution for this?", "author_fullname": "t2_jl74ygnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Error while creating workspace with 'Quickstart' on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14faojz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687361087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying Databricks for the first time. I created a &amp;#39;standard&amp;#39; workspace using &amp;#39;Quickstart on AWS&amp;#39; option.&lt;/p&gt;\n\n&lt;p&gt;Cloudformation runs into error during &amp;#39;assign Metastore&amp;#39; step.&lt;/p&gt;\n\n&lt;p&gt;Following is the error from Cloudwatch:&lt;/p&gt;\n\n&lt;p&gt;HTTP content: b&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;{ &amp;quot;error_code&amp;quot;: &amp;quot;PERMISSION_DENIED&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;Cannot assign metastore to STANDARD tier workspace xyz&amp;quot; }&lt;/p&gt;\n\n&lt;p&gt;What is the solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14faojz", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Device689", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14faojz/databricks_error_while_creating_workspace_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14faojz/databricks_error_while_creating_workspace_with/", "subreddit_subscribers": 111638, "created_utc": 1687361087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common Anti-Scraping Measures on Websites and How to Bypass Them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14f9q8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gD9F-J0b8487yZfi24T6NRpDLgh22cSkGmohKTlzBgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687358880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "javascript.plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://javascript.plainenglish.io/common-anti-scraping-measures-on-websites-and-how-to-bypass-them-a32de1b066a2", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?auto=webp&amp;v=enabled&amp;s=42c59a73b3ad91dce8fcd997a95d024ff551cc8f", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04478cd9c6d2a35eb877e84520629c45354d80f3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a98eda0cbd3d0cfd4690879ac960bd381e9155c9", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bec1880dd7f6f7eefaa7bbf3bad9b37b13bba0c7", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8779f32e08b68a043c6684866de331b337660a3f", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15e3a4d1e59ac9aa7adc3772d0a940f425912392", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/7yKKS1kBAIsgjZdaj0xcTI76r8ow3utJYiE22CzcTRs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68007d0fa94969d328ff24d08a740023c51c6be1", "width": 1080, "height": 1080}], "variants": {}, "id": "2Qq3JmMuayyJv838L8VeYv47kDei9weoMUIDYVk3Q68"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14f9q8p", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f9q8p/common_antiscraping_measures_on_websites_and_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://javascript.plainenglish.io/common-anti-scraping-measures-on-websites-and-how-to-bypass-them-a32de1b066a2", "subreddit_subscribers": 111638, "created_utc": 1687358880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "a lot of talk around LLMs &amp; their potential impact on data teams workflows. \n\nMy guess is we'll see a lot of these pop-up in our favorite tools in the next few years (databricks, Snowflake, Tableau, Looker, dbt, etc).\n\ncan you add a quick explanation in comment to make it more interesting?\n\n[View Poll](https://www.reddit.com/poll/14f5mdr)", "author_fullname": "t2_bu8cw718", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think of the emerging concept of \"data copilots\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f5mdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687348355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;a lot of talk around LLMs &amp;amp; their potential impact on data teams workflows. &lt;/p&gt;\n\n&lt;p&gt;My guess is we&amp;#39;ll see a lot of these pop-up in our favorite tools in the next few years (databricks, Snowflake, Tableau, Looker, dbt, etc).&lt;/p&gt;\n\n&lt;p&gt;can you add a quick explanation in comment to make it more interesting?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/14f5mdr\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14f5mdr", "is_robot_indexable": true, "report_reasons": null, "author": "castor-metadata", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1687607555683, "options": [{"text": "Over-hyped", "id": "23557093"}, {"text": "Game-changing", "id": "23557094"}, {"text": "No Opinion", "id": "23557095"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 79, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f5mdr/what_do_you_think_of_the_emerging_concept_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/14f5mdr/what_do_you_think_of_the_emerging_concept_of_data/", "subreddit_subscribers": 111638, "created_utc": 1687348355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My first post here. I hope this is the right place to ask this question and have a meaningful discussion.\n\nWhat started as a pet project quickly became can internal tool for our company. When ever I am dealing with REST APIs, I have found building lots of bespoke data pipelines per API to manage the data ingestion. Especially to deal with authentication and pagination. I've come across some unconventional authentication mechanism as well as standard once like basic auth, Bearer token, api key etc. \n\nThen the problem of looping. For example load date related data using dynamic date parameters or load a list from one API endpoint and then loop through the list to load data from second endpoint etc.\n\nI wanted to simplify these implementation and haven't really found any tools for that task. So started working on one. Idea is to provide a single tool that can take instructions from data engineers, deal with authentications, manage the pagination, deal with loop and dynamic parameters and save the outcome to a storage (azure datalake, snowflake etc) or just return a JSON response at the end with all the data. Imagine postman with some added options that can be executed remotely to fetch data.\n\nI am interested to know if people face similar challenges, what use cases you deal/have dealt with and what's your approach been?\n\nHere is a small video of the tool in action. Please let me know if anyone is interested testing this. It's a fully hosted solution and I can set you up with an UI access. I found this works well with data pipelines in azure, remove the headaches, also tried it with other data ingestion tools such as Hevo and fivetran. ", "author_fullname": "t2_i9luvah0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "REST API connector tester and use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_14ey7cr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 1200, "fallback_url": "https://v.redd.it/muy5o5yf1b7b1/DASH_480.mp4?source=fallback", "has_audio": false, "height": 428, "width": 854, "scrubber_media_url": "https://v.redd.it/muy5o5yf1b7b1/DASH_96.mp4", "dash_url": "https://v.redd.it/muy5o5yf1b7b1/DASHPlaylist.mpd?a=1689991388%2CMWViOTViZTVjMDA0ZmZjMjM1OGFlMTc2NmVjNGNkOGViNzJmY2Y0NmUxNzE3MmMyNjg1NWJiMTUyNzUzMmE1OA%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/muy5o5yf1b7b1/HLSPlaylist.m3u8?a=1689991388%2CYzA2OTI5M2FhMmU1OTU2M2Y2ZWI5MDgwNmI5OGZjYjA1NzVmN2I1YmM3NWRjYzIzNjRhZjkwZTNhNWFkMDY5Nw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=d2b2a2623a936e5fd4baac4afe2d88cb23d51687", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687324299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My first post here. I hope this is the right place to ask this question and have a meaningful discussion.&lt;/p&gt;\n\n&lt;p&gt;What started as a pet project quickly became can internal tool for our company. When ever I am dealing with REST APIs, I have found building lots of bespoke data pipelines per API to manage the data ingestion. Especially to deal with authentication and pagination. I&amp;#39;ve come across some unconventional authentication mechanism as well as standard once like basic auth, Bearer token, api key etc. &lt;/p&gt;\n\n&lt;p&gt;Then the problem of looping. For example load date related data using dynamic date parameters or load a list from one API endpoint and then loop through the list to load data from second endpoint etc.&lt;/p&gt;\n\n&lt;p&gt;I wanted to simplify these implementation and haven&amp;#39;t really found any tools for that task. So started working on one. Idea is to provide a single tool that can take instructions from data engineers, deal with authentications, manage the pagination, deal with loop and dynamic parameters and save the outcome to a storage (azure datalake, snowflake etc) or just return a JSON response at the end with all the data. Imagine postman with some added options that can be executed remotely to fetch data.&lt;/p&gt;\n\n&lt;p&gt;I am interested to know if people face similar challenges, what use cases you deal/have dealt with and what&amp;#39;s your approach been?&lt;/p&gt;\n\n&lt;p&gt;Here is a small video of the tool in action. Please let me know if anyone is interested testing this. It&amp;#39;s a fully hosted solution and I can set you up with an UI access. I found this works well with data pipelines in azure, remove the headaches, also tried it with other data ingestion tools such as Hevo and fivetran. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/muy5o5yf1b7b1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3409cda296cf88e427a5f46726a9e4280ff62505", "width": 1080, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1f8fcbc2f749b468b64c8f2ece163cf06fcb2ddf", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=fac6181b17769b8d28a5d0f6c791ffd817e04aa3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7a94fbd36e068a47d82f11366a5e9e743aa00077", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=94a96526cd4f3a8f781a113522114da70dfbae58", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c6970740f45dd4e8146ef323cf94166ecda88b46", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=adde90c08d02cebaeb9a77006825f5085a8ac6c9", "width": 1080, "height": 540}], "variants": {}, "id": "NmhlcHEydmYxYjdiMevGc2-hs7PLVFc3ePDJV3vg671EdiMKQDcAFBOSYIKw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14ey7cr", "is_robot_indexable": true, "report_reasons": null, "author": "Kabir-rab", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ey7cr/rest_api_connector_tester_and_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/muy5o5yf1b7b1", "subreddit_subscribers": 111638, "created_utc": 1687324299.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 1200, "fallback_url": "https://v.redd.it/muy5o5yf1b7b1/DASH_480.mp4?source=fallback", "has_audio": false, "height": 428, "width": 854, "scrubber_media_url": "https://v.redd.it/muy5o5yf1b7b1/DASH_96.mp4", "dash_url": "https://v.redd.it/muy5o5yf1b7b1/DASHPlaylist.mpd?a=1689991388%2CMWViOTViZTVjMDA0ZmZjMjM1OGFlMTc2NmVjNGNkOGViNzJmY2Y0NmUxNzE3MmMyNjg1NWJiMTUyNzUzMmE1OA%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/muy5o5yf1b7b1/HLSPlaylist.m3u8?a=1689991388%2CYzA2OTI5M2FhMmU1OTU2M2Y2ZWI5MDgwNmI5OGZjYjA1NzVmN2I1YmM3NWRjYzIzNjRhZjkwZTNhNWFkMDY5Nw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nI'm trying to implement  delta architecture to spark pipelines in my organization. I understand most of the idea but i still wonder what's the correct approach to cache last processed delta commit id from CDF in downstream pipelines. This will emable me to process only new data there. In most tutorials and articles CDC capabilities of delta are shown with those values hardcoded. I plan store it there as kv 'pipelinename':'last_read_version' in either separate bookmark table or some kv store solution. Is that the correct approach to solve it or am I missing something?", "author_fullname": "t2_bf5ei6em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC im delta lake - how to store commit id downstream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14feh1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687374132.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687369853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nI&amp;#39;m trying to implement  delta architecture to spark pipelines in my organization. I understand most of the idea but i still wonder what&amp;#39;s the correct approach to cache last processed delta commit id from CDF in downstream pipelines. This will emable me to process only new data there. In most tutorials and articles CDC capabilities of delta are shown with those values hardcoded. I plan store it there as kv &amp;#39;pipelinename&amp;#39;:&amp;#39;last_read_version&amp;#39; in either separate bookmark table or some kv store solution. Is that the correct approach to solve it or am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14feh1u", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning_Hurry2611", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14feh1u/cdc_im_delta_lake_how_to_store_commit_id/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14feh1u/cdc_im_delta_lake_how_to_store_commit_id/", "subreddit_subscribers": 111638, "created_utc": 1687369853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI\u2019m conducting a survey for my research on Work-Life Balance in the life of Data Science Professionals. If you are a Data Scientist, Data Analyst, Data Architect, Report Developer, ML Engineer, SQL Developer, or ETL Developer and live in the United States. Please fill out the survey.\n\n[https://ncu.co1.qualtrics.com/jfe/form/SV\\_8vrPq0QOrNdXvG6](https://ncu.co1.qualtrics.com/jfe/form/SV_8vrPq0QOrNdXvG6)\n\nIf you have any questions or concerns, feel free to reach out to me: [m.sardar1513@o365.ncu.edu](mailto:m.sardar1513@o365.ncu.edu)", "author_fullname": "t2_a1n8j857", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SURVEY: Work-Life Balance in the Life of Data Science Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fe5qy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687369125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m conducting a survey for my research on Work-Life Balance in the life of Data Science Professionals. If you are a Data Scientist, Data Analyst, Data Architect, Report Developer, ML Engineer, SQL Developer, or ETL Developer and live in the United States. Please fill out the survey.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ncu.co1.qualtrics.com/jfe/form/SV_8vrPq0QOrNdXvG6\"&gt;https://ncu.co1.qualtrics.com/jfe/form/SV_8vrPq0QOrNdXvG6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions or concerns, feel free to reach out to me: [&lt;a href=\"mailto:m.sardar1513@o365.ncu.edu\"&gt;m.sardar1513@o365.ncu.edu&lt;/a&gt;](mailto:&lt;a href=\"mailto:m.sardar1513@o365.ncu.edu\"&gt;m.sardar1513@o365.ncu.edu&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fe5qy", "is_robot_indexable": true, "report_reasons": null, "author": "Money-News9333", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fe5qy/survey_worklife_balance_in_the_life_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fe5qy/survey_worklife_balance_in_the_life_of_data/", "subreddit_subscribers": 111638, "created_utc": 1687369125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9msdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inside Starburst's hackathon: Into the great wide open(AI)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_14fbehd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/voccu5Jpp747AIKYmZfqwNf18sCRWpOlm7toVOuASio.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687362748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dev.to", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dev.to/starburstengineering/inside-starbursts-hackathon-into-the-great-wide-openai-3nl8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?auto=webp&amp;v=enabled&amp;s=aee7695eb34a88bed0b0b023121ef0e73fe7d268", "width": 1128, "height": 598}, "resolutions": [{"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b037eecc6379553b7a6d02fa3e40f8afc7c10d45", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92c758f2e18da31cd4db9c6acbf98f1911b54a10", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e40fdfe9460a0bb901a24bbb24284530349ff50f", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6dac3ddb39d735a2121b81201836dbf740d831e", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d489266392bfc1dacc29bac02181411a714a3175", "width": 960, "height": 508}, {"url": "https://external-preview.redd.it/MJBCaxBeJbbUW2LTRJL2eYHmkb3y-gNB9zCg8xPNbT0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13aff827560a0748b74ae432b23b63849518e2fe", "width": 1080, "height": 572}], "variants": {}, "id": "k883IOz-8cn8hz390a3aFIjLzjAXzcFAs5flDn8iqwo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14fbehd", "is_robot_indexable": true, "report_reasons": null, "author": "randgalt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fbehd/inside_starbursts_hackathon_into_the_great_wide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dev.to/starburstengineering/inside-starbursts-hackathon-into-the-great-wide-openai-3nl8", "subreddit_subscribers": 111638, "created_utc": 1687362748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everyone,\n\nI have an issue relating to a local Oracle server (19c) creating a link to AWS via IAM user.\n\nI have an access key for the AWS IAM user, and AWS automatically generates a secret key (password) associated with the access key. Now the secret key is about 40chars long.  \nIn Oracle 19c the maximum length for an identifier is 30chars!?  \nMeaning when I try using JDBC to connect Oracle to AWS I get an error (ORA-00972: identifier is too long).  \n\n\nHas anyone experienced something like this? Any workarounds?  \nHelp would be much appreciated \ud83d\ude4f", "author_fullname": "t2_4wsp8k38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting to AWS From and Oracle Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f8dqg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687355639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt;\n\n&lt;p&gt;I have an issue relating to a local Oracle server (19c) creating a link to AWS via IAM user.&lt;/p&gt;\n\n&lt;p&gt;I have an access key for the AWS IAM user, and AWS automatically generates a secret key (password) associated with the access key. Now the secret key is about 40chars long.&lt;br/&gt;\nIn Oracle 19c the maximum length for an identifier is 30chars!?&lt;br/&gt;\nMeaning when I try using JDBC to connect Oracle to AWS I get an error (ORA-00972: identifier is too long).  &lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced something like this? Any workarounds?&lt;br/&gt;\nHelp would be much appreciated \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14f8dqg", "is_robot_indexable": true, "report_reasons": null, "author": "trendy_parker", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f8dqg/connecting_to_aws_from_and_oracle_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f8dqg/connecting_to_aws_from_and_oracle_server/", "subreddit_subscribers": 111638, "created_utc": 1687355639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So ill be doing a test this week for a job, before starting the test there are some instructions and its written \"You can write your solution(s) in SQL.\"\n\nDoes this means its a pure SQL test? Ive been studying python for this test in the last few days, now maybe i should stop and go full SQL.", "author_fullname": "t2_w5u5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick question about Codility test", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14f7hg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687353344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So ill be doing a test this week for a job, before starting the test there are some instructions and its written &amp;quot;You can write your solution(s) in SQL.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Does this means its a pure SQL test? Ive been studying python for this test in the last few days, now maybe i should stop and go full SQL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14f7hg8", "is_robot_indexable": true, "report_reasons": null, "author": "Lenant", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14f7hg8/quick_question_about_codility_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14f7hg8/quick_question_about_codility_test/", "subreddit_subscribers": 111638, "created_utc": 1687353344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a database, obviously it's common to enforce things like \"all rows in the \\`purchases\\` table must have a foreign key relationship to the \\`users\\` table (we have to know which user made each purchase!)\"\n\nWhen ELT'ing data to a data warehouse, I wrote a test in dbt to check the same thing.\n\nThis test periodically fails though, due to the fact that sometimes the ELT process transfers the \\`users\\` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the \\`purchases\\` table to the data warehouse. The result being that there's a purchase in the data warehouse which does not have a corresponding user in the user's table. Which means my dbt test fails.\n\nIs this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship \"for purchases made more than X hours ago\", where X is greater than the most recent load into the dw.", "author_fullname": "t2_116mtw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle table relationships in data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14fnn1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687391877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a database, obviously it&amp;#39;s common to enforce things like &amp;quot;all rows in the `purchases` table must have a foreign key relationship to the `users` table (we have to know which user made each purchase!)&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;When ELT&amp;#39;ing data to a data warehouse, I wrote a test in dbt to check the same thing.&lt;/p&gt;\n\n&lt;p&gt;This test periodically fails though, due to the fact that sometimes the ELT process transfers the `users` table to the data warehouse, then a new user creates an account + makes a purchase, then the ELT process transfers the `purchases` table to the data warehouse. The result being that there&amp;#39;s a purchase in the data warehouse which does not have a corresponding user in the user&amp;#39;s table. Which means my dbt test fails.&lt;/p&gt;\n\n&lt;p&gt;Is this avoidable? The only thing I can think of is putting a condition on the test so that it only checks this relationship &amp;quot;for purchases made more than X hours ago&amp;quot;, where X is greater than the most recent load into the dw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14fnn1s", "is_robot_indexable": true, "report_reasons": null, "author": "PM_ME_SCIENCEY_STUFF", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14fnn1s/how_do_you_handle_table_relationships_in_data/", "subreddit_subscribers": 111638, "created_utc": 1687391877.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}