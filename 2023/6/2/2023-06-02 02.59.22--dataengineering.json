{"kind": "Listing", "data": {"after": "t3_13xw7re", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve read the articles, looked at the websites, but want to hear from people who\u2019ve actually done it. How do the three compare? What are the downsides of each? What\u2019s your thought process in choosing an orchestrator anyway?", "author_fullname": "t2_vxxrqrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration: Thoughts on Dagster, Airflow and Prefect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xkeov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 68, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685633011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve read the articles, looked at the websites, but want to hear from people who\u2019ve actually done it. How do the three compare? What are the downsides of each? What\u2019s your thought process in choosing an orchestrator anyway?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13xkeov", "is_robot_indexable": true, "report_reasons": null, "author": "MrMosBiggestFan", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xkeov/orchestration_thoughts_on_dagster_airflow_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xkeov/orchestration_thoughts_on_dagster_airflow_and/", "subreddit_subscribers": 108464, "created_utc": 1685633011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had an interview with a mid-sized company, where the interviewer asked me, 'What is your data engineering philosophy?'. I was caught off guard by the question and just responded, 'The simpler, the better'. \n\nWhat would you say if an interviewer asked you this question?", "author_fullname": "t2_l4f3vumi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your data engineering philosophy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xb1m3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685604454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an interview with a mid-sized company, where the interviewer asked me, &amp;#39;What is your data engineering philosophy?&amp;#39;. I was caught off guard by the question and just responded, &amp;#39;The simpler, the better&amp;#39;. &lt;/p&gt;\n\n&lt;p&gt;What would you say if an interviewer asked you this question?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13xb1m3", "is_robot_indexable": true, "report_reasons": null, "author": "Which_Rutabaga2774", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xb1m3/what_is_your_data_engineering_philosophy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xb1m3/what_is_your_data_engineering_philosophy/", "subreddit_subscribers": 108464, "created_utc": 1685604454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_v4ncd2zz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UI for Apache Kafka - An open-source tool for monitoring and managing Apache Kafka Clusters - v0.17 release", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13xlb49", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/n-KEX0zzlef5Degyp6_V7pd1XoMk1cvBbdjsKHUr1U0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685635066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/provectus/kafka-ui", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?auto=webp&amp;v=enabled&amp;s=82e8e2662d0b2b64a28612fec2688992aa206933", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f309e6e194c0ee7e57d1b68dd60275d03d5afd83", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00a65096549a88adacf23e006865f0c7eb09f563", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fad1e1b71c14859a4465832ba58f7e61fb6af1e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7883691410932b7c90f7c3ed74b01c2cbe8be29b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=966d4dd55c580c52a3584da7cb05f24c60af319b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QMdLmsBVJs1xBmJA74gf0kOvG7tOqzdrcWJDEJs8fnI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a901a1923ce1acc9f3fa5447c56a4ab4b7bc8db4", "width": 1080, "height": 540}], "variants": {}, "id": "RcuEPg0I8fPEiPOV6vLeiVtieDOLcxOX0OWhC4xG2Ro"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13xlb49", "is_robot_indexable": true, "report_reasons": null, "author": "dahmedahe", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xlb49/ui_for_apache_kafka_an_opensource_tool_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/provectus/kafka-ui", "subreddit_subscribers": 108464, "created_utc": 1685635066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title es self-explanatory. How do you explain your job to people that don\u2019t know anything about data engineering?", "author_fullname": "t2_lpdlqzf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you explain your job to laymen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x79yg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685591077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title es self-explanatory. How do you explain your job to people that don\u2019t know anything about data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13x79yg", "is_robot_indexable": true, "report_reasons": null, "author": "aflyingtaco06", "discussion_type": null, "num_comments": 92, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13x79yg/how_do_you_explain_your_job_to_laymen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13x79yg/how_do_you_explain_your_job_to_laymen/", "subreddit_subscribers": 108464, "created_utc": 1685591077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Feels like DEs can get stuck in this box of having to use Snowflake, Databricks, or whatever your cloud offers. I know it's a big boost to have one of the \"Big 5\" as a skill on your resume, but that seems like a classic vendor lock-in strategy. Is anybody trying to break out of the box and do something unique/different with Data Warehousing? Just wanna hear your stories...", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody avoiding data warehouse vendor lock-in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xl68u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685634755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Feels like DEs can get stuck in this box of having to use Snowflake, Databricks, or whatever your cloud offers. I know it&amp;#39;s a big boost to have one of the &amp;quot;Big 5&amp;quot; as a skill on your resume, but that seems like a classic vendor lock-in strategy. Is anybody trying to break out of the box and do something unique/different with Data Warehousing? Just wanna hear your stories...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13xl68u", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xl68u/anybody_avoiding_data_warehouse_vendor_lockin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xl68u/anybody_avoiding_data_warehouse_vendor_lockin/", "subreddit_subscribers": 108464, "created_utc": 1685634755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:\n\n1. Current title\n\n2. Years of experience (YOE)\n\n3. Location\n\n4. Base salary &amp; currency (dollars, euro, pesos, etc.)\n\n5. Bonuses/Equity (optional)\n\n6. Industry (optional)\n\n7. Tech stack (optional)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quarterly Salary Discussion - Jun 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/ef3eb514-328d-4549-a705-94c26963d79b", "link_ids": ["t3_npxcqc", "t3_pfwuyg", "t3_r6jfnm", "t3_t4clep", "t3_v2ka3w", "t3_x3bb11", "t3_z9szj1", "t3_11f8yxo", "t3_13xldpd"], "description": "", "title": "Data Engineering Salaries", "created_at_utc": 1621559056.076, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "ef3eb514-328d-4549-a705-94c26963d79b", "author_id": "t2_2tv9i42n", "last_update_utc": 1685635229.941, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xldpd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685635229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Current title&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Years of experience (YOE)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Location&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Base salary &amp;amp; currency (dollars, euro, pesos, etc.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Bonuses/Equity (optional)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Industry (optional)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tech stack (optional)&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13xldpd", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 22, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xldpd/quarterly_salary_discussion_jun_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/13xldpd/quarterly_salary_discussion_jun_2023/", "subreddit_subscribers": 108464, "created_utc": 1685635229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am Krishna Thota. I am building an open-source integration and data platform([https://cptn.io](https://cptn.io/)). The product is MIT licensed and the repo is at [https://github.com/cptn-io/el-cptn](https://github.com/cptn-io/el-cptn).\n\nI have started on my startup journey an year ago and launched a monitoring platform called DevRaven. Unfortunately the product did not takeoff as expected. But during the course of building the product, I have built several integrations leveraging MQs and Cloud Functions. While building and deploying Cloud Functions for happy paths is easy, I had to monitor logs for failures, build retry mechanisms or manually process failed events, keep instances running to prevent cold start timeouts. It can also get expensive with charges for MQs, server time for running cloud functions etc and costs can be unpredictable.\n\nI thought of building a platform where I can build integrations and data pipelines quickly, have the ability to look at incoming/outgoing events, look at logs, retry any failed events etc. And finally, predictable costs for running the infrastructure. cptn.io provides all these capabilities and more. You can build pipelines to integrate with any cloud services, send data from your backend to data warehouses, listen to web hook events etc. The platform can be integrated into any stack by sending events to HTTP end points.\n\nThe platform w/ SSO is available under MIT license so any user or customer can use it. There is no ee folder or complex dual licensing. The plan is to offer a managed service in the cloud at a later time, accept sponsors for prioritizing features for enterprise customers and charge for enterprise support. The platform has ready to use apps for integrating or sending data to popular services like Snowflake, AWS S3, GCP buckets, Postgres, MySQL, ServiceNow, MongoDB etc.\n\nIt should take less than 5 minutes to get the platform running on your machine. Welcome any feedback, feature requests, PRs and bug reports.", "author_fullname": "t2_2o3oyfm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "cptn.io - open-source integration and data pipeline platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x8njo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685595621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am Krishna Thota. I am building an open-source integration and data platform(&lt;a href=\"https://cptn.io/\"&gt;https://cptn.io&lt;/a&gt;). The product is MIT licensed and the repo is at &lt;a href=\"https://github.com/cptn-io/el-cptn\"&gt;https://github.com/cptn-io/el-cptn&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I have started on my startup journey an year ago and launched a monitoring platform called DevRaven. Unfortunately the product did not takeoff as expected. But during the course of building the product, I have built several integrations leveraging MQs and Cloud Functions. While building and deploying Cloud Functions for happy paths is easy, I had to monitor logs for failures, build retry mechanisms or manually process failed events, keep instances running to prevent cold start timeouts. It can also get expensive with charges for MQs, server time for running cloud functions etc and costs can be unpredictable.&lt;/p&gt;\n\n&lt;p&gt;I thought of building a platform where I can build integrations and data pipelines quickly, have the ability to look at incoming/outgoing events, look at logs, retry any failed events etc. And finally, predictable costs for running the infrastructure. cptn.io provides all these capabilities and more. You can build pipelines to integrate with any cloud services, send data from your backend to data warehouses, listen to web hook events etc. The platform can be integrated into any stack by sending events to HTTP end points.&lt;/p&gt;\n\n&lt;p&gt;The platform w/ SSO is available under MIT license so any user or customer can use it. There is no ee folder or complex dual licensing. The plan is to offer a managed service in the cloud at a later time, accept sponsors for prioritizing features for enterprise customers and charge for enterprise support. The platform has ready to use apps for integrating or sending data to popular services like Snowflake, AWS S3, GCP buckets, Postgres, MySQL, ServiceNow, MongoDB etc.&lt;/p&gt;\n\n&lt;p&gt;It should take less than 5 minutes to get the platform running on your machine. Welcome any feedback, feature requests, PRs and bug reports.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?auto=webp&amp;v=enabled&amp;s=b328483e7c9d7536eded667be78eb8b42e18b17c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bec602c584bf8c2ef006f7de1cff54324b992a76", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=faa1a6cb39160180b51e581510048793c591aa34", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cd88fca2841244e68a1e12532e65ea7c04dcba8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81f436f600892e34a6de24eb0d7203a19e4d04a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2e87e31db476a80e5cc85781092dfa55dd9828f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-QjeOtRnwgyuJ0OV4k1mKQ3QF9dqP1vCM6JEt4-lM8A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9997f58d4493b238693c42be1fb8f25b5e7c9d12", "width": 1080, "height": 567}], "variants": {}, "id": "r45VCvwfxV3IY9dXLtkBr67O-3J8-P7jxU2-U1EB4bY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13x8njo", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-No", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13x8njo/cptnio_opensource_integration_and_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13x8njo/cptnio_opensource_integration_and_data_pipeline/", "subreddit_subscribers": 108464, "created_utc": 1685595621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title says it.", "author_fullname": "t2_fcn3yu96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What were the projects that made your resume/cv stand out from others while applying for internship or Job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xiv7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685629276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13xiv7h", "is_robot_indexable": true, "report_reasons": null, "author": "work_up", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xiv7h/what_were_the_projects_that_made_your_resumecv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xiv7h/what_were_the_projects_that_made_your_resumecv/", "subreddit_subscribers": 108464, "created_utc": 1685629276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have always been intrigued and slowly been falling inlove with the extensive capabilities of what Data Engineers can do in the industry. \n\nMy portfolio, I have been working as DMA in the Data Governance for 9 years. My job mainly consist of implementing data cleaning cycle to my company,  Generating metrics on my team\u2019s output monthly to check work efficiency. Not even sure if considered a technical skill but I\u2019m using powershell and automating tools to do the dirty work in migrating bulk data (terabytes). Not so fun at all.\n\nI have 0 interest in coding but since I\u2019ve been really into Data Engineering. Recently I\u2019ve been self studying, buying udemy bootcamps courses for Python and SQL. Also I\u2019ve been using our friendly neighborhood Youtube and Google as additional resources.\n\nKnowing how crucial and important data is in the industry. I\u2019m really considering in transitioning to this role due to its high demand,  high value and the skillset to acquire in this role is no joke. I can see that it takes continous dedication to upskill and pure love for data to be considered a good DE.\nOf course would also have to consider the bigger pay for this role as well.\n\nWill this be worth the jump?\n\nAny tips or advise would be highly appreciated!\n\n\n\nTLDR: I\u2019m emotionally fatigued with my current role, how can I find hope in transitioning to Data Engineer?", "author_fullname": "t2_p2kttfgn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Shift from Data Management Analyst to Data Engineer a good idea?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xqwrx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685647837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always been intrigued and slowly been falling inlove with the extensive capabilities of what Data Engineers can do in the industry. &lt;/p&gt;\n\n&lt;p&gt;My portfolio, I have been working as DMA in the Data Governance for 9 years. My job mainly consist of implementing data cleaning cycle to my company,  Generating metrics on my team\u2019s output monthly to check work efficiency. Not even sure if considered a technical skill but I\u2019m using powershell and automating tools to do the dirty work in migrating bulk data (terabytes). Not so fun at all.&lt;/p&gt;\n\n&lt;p&gt;I have 0 interest in coding but since I\u2019ve been really into Data Engineering. Recently I\u2019ve been self studying, buying udemy bootcamps courses for Python and SQL. Also I\u2019ve been using our friendly neighborhood Youtube and Google as additional resources.&lt;/p&gt;\n\n&lt;p&gt;Knowing how crucial and important data is in the industry. I\u2019m really considering in transitioning to this role due to its high demand,  high value and the skillset to acquire in this role is no joke. I can see that it takes continous dedication to upskill and pure love for data to be considered a good DE.\nOf course would also have to consider the bigger pay for this role as well.&lt;/p&gt;\n\n&lt;p&gt;Will this be worth the jump?&lt;/p&gt;\n\n&lt;p&gt;Any tips or advise would be highly appreciated!&lt;/p&gt;\n\n&lt;p&gt;TLDR: I\u2019m emotionally fatigued with my current role, how can I find hope in transitioning to Data Engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13xqwrx", "is_robot_indexable": true, "report_reasons": null, "author": "SheeshKebabPls", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xqwrx/career_shift_from_data_management_analyst_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xqwrx/career_shift_from_data_management_analyst_to_data/", "subreddit_subscribers": 108464, "created_utc": 1685647837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have recently created [a template DBT project](https://github.com/PaddyAlton/DBT-GA4) that can be used to transform data exported directly from Google Analytics to BigQuery.\n\nIt's open source, so I hope it helps anyone wrestling with the same problem to get off the ground a bit faster.\n\nI've written an article (linked) to explain more about it. Feedback welcome.", "author_fullname": "t2_7920orfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wrangling Google Analytics data with DBT: a guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_13xlika", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8lq1xomm0byeQo9USYejnnzm1794mQQxsjPR_EjwjMI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685635509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have recently created &lt;a href=\"https://github.com/PaddyAlton/DBT-GA4\"&gt;a template DBT project&lt;/a&gt; that can be used to transform data exported directly from Google Analytics to BigQuery.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s open source, so I hope it helps anyone wrestling with the same problem to get off the ground a bit faster.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve written an article (linked) to explain more about it. Feedback welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/OTr2KLRphAb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?auto=webp&amp;v=enabled&amp;s=2cf6aba44a6dc4bd5a0e24f4f4b580731ea3267a", "width": 1200, "height": 705}, "resolutions": [{"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=469c64f5b91b61f35a0b9081da96011d966c5ef8", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=479d429e9bca569de737f0d595d23878b94f2620", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d50f06e9e54a70ba93cd7426b9d69147698e491", "width": 320, "height": 188}, {"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40ac752dd1a94f9e7b831293d6a6732f498fb6b1", "width": 640, "height": 376}, {"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35052a8759cf9365a98cc155aed25c7b589d4ee7", "width": 960, "height": 564}, {"url": "https://external-preview.redd.it/aRVRURSmnrO_vgy5Uv79JgYGiNKOGQSvNErj7N6KHak.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caab8a8337d763eaa1c1351d64c5d8b4580b312e", "width": 1080, "height": 634}], "variants": {}, "id": "O2tDTvcCts4gpy9FjJGB8yuocGEoKNW-gKrvu9u-oIQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13xlika", "is_robot_indexable": true, "report_reasons": null, "author": "PaddyAlton", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xlika/wrangling_google_analytics_data_with_dbt_a_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/OTr2KLRphAb", "subreddit_subscribers": 108464, "created_utc": 1685635509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All,\n\nIm early on my journey into data science (got put on a project adjacent to the space and am trying to learn all that I can).\n\n&amp;#x200B;\n\nAnyway I recently read this blog post out of Airbyte and found it a good overview on data modeling from the 10,000 foot view.\n\n&amp;#x200B;\n\nA lot of it still required some googling, however I found the pattern section on ADAPT and Beam super interesting and helpful.  \n\n\n[https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-architecture-pattern-tools](https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-architecture-pattern-tools)", "author_fullname": "t2_11uwgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Primer on Modern Data Modeling: Taxonomy, Adapt vs Beam, Declaritive vs Imperitive, Tools, Patterns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xjc6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685630467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;Im early on my journey into data science (got put on a project adjacent to the space and am trying to learn all that I can).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyway I recently read this blog post out of Airbyte and found it a good overview on data modeling from the 10,000 foot view.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A lot of it still required some googling, however I found the pattern section on ADAPT and Beam super interesting and helpful.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-architecture-pattern-tools\"&gt;https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-architecture-pattern-tools&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?auto=webp&amp;v=enabled&amp;s=599c82e767a3201a3c6302ef161e4f110098d8c1", "width": 1398, "height": 759}, "resolutions": [{"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7680cedf77711c90e0d700c01da53e0f78d7e406", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f7f78c758d4078ad9fb3b427e0e15f377738c71", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fe30d4c96e6242e76fb36425e95d2e52e595e84", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=223dbeabd36a7250183e060ce2d6d05ca46209e9", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf170c8e4ea9b2d6511842b5dcf9b8a514741633", "width": 960, "height": 521}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3792a5f6a75292c0274147d75a5668eaf117713", "width": 1080, "height": 586}], "variants": {}, "id": "2QFm0XH2goPLQbXBxCMz_DUtGQBe_lv10iSMNlX0-3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13xjc6w", "is_robot_indexable": true, "report_reasons": null, "author": "bnchrch", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xjc6w/primer_on_modern_data_modeling_taxonomy_adapt_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xjc6w/primer_on_modern_data_modeling_taxonomy_adapt_vs/", "subreddit_subscribers": 108464, "created_utc": 1685630467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I am hoping to get your perspective on Microsoft Purview in case you are currently using it or have evaluated it in the past. \nWhat was/is your opinion in terms of setup (connecting to both on-Prem and cloud sources), data catalog and lineage functionality? \n\nI am currently doing a pilot using Purview and start to question the usability of it given the level effort that goes in connecting with non-Microsoft or on-Prem sources such as oracle dbs or SAP. \n\nAppreciate your thoughts and advice.", "author_fullname": "t2_dswp11x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Microsoft Purview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xmqym", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685638255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I am hoping to get your perspective on Microsoft Purview in case you are currently using it or have evaluated it in the past. \nWhat was/is your opinion in terms of setup (connecting to both on-Prem and cloud sources), data catalog and lineage functionality? &lt;/p&gt;\n\n&lt;p&gt;I am currently doing a pilot using Purview and start to question the usability of it given the level effort that goes in connecting with non-Microsoft or on-Prem sources such as oracle dbs or SAP. &lt;/p&gt;\n\n&lt;p&gt;Appreciate your thoughts and advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13xmqym", "is_robot_indexable": true, "report_reasons": null, "author": "abskiing403", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xmqym/thoughts_on_microsoft_purview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xmqym/thoughts_on_microsoft_purview/", "subreddit_subscribers": 108464, "created_utc": 1685638255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a project that requires filtering of huge volumes of data with extremely low latency. \n\nSome general context: My organisation's product is a web app. On this webapp, we have to build a feature where the frontend UI will have multiple filters that the user can select, these selected filters should be applied on a huge amount of data and after the filters are applied, some very basic summary metrics should be calculated and shown to the user in frontend.\n\nA bit more specific context(I am assuming the absolute worst case numbers here):\n\n1. The webapp is assumed to have a total of 100 concurrent users\n2. Huge amount of data  = we're planning to store 1 month of data, and average is 1 million records per day with 50 columns. So lets say 30 million records **per user**. At the moment, nobody is using any of this data, and its just sitting in log files in S3.\n3. Basic summary metrics = count of email IDs, count of IP addresses, count of mobile numbers etc\n4. It's ok if the data is not absolute real-time. What I mean to say here is that if the max(date) in the dashboard is yesterday's, its fine.\n\nI am a rookie DE working in a team of SWEs, and we can build something here from scratch(means that the data which is currently stored in S3, we can process that and store in a DW if required) \n\nCan you help me with a solution as to what should be my best data storage + data processing solution for the above use case, considering the below concerns:\n\n1. data volume\n2. latency(after all the filters are applied, a latency of max 10 seconds is affordable)\n3. cost\n\n&amp;#x200B;\n\nThanks in advance!!", "author_fullname": "t2_yl8ul14", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low latency filtering and basic operations on huge volume of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xg3d9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685621962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project that requires filtering of huge volumes of data with extremely low latency. &lt;/p&gt;\n\n&lt;p&gt;Some general context: My organisation&amp;#39;s product is a web app. On this webapp, we have to build a feature where the frontend UI will have multiple filters that the user can select, these selected filters should be applied on a huge amount of data and after the filters are applied, some very basic summary metrics should be calculated and shown to the user in frontend.&lt;/p&gt;\n\n&lt;p&gt;A bit more specific context(I am assuming the absolute worst case numbers here):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The webapp is assumed to have a total of 100 concurrent users&lt;/li&gt;\n&lt;li&gt;Huge amount of data  = we&amp;#39;re planning to store 1 month of data, and average is 1 million records per day with 50 columns. So lets say 30 million records &lt;strong&gt;per user&lt;/strong&gt;. At the moment, nobody is using any of this data, and its just sitting in log files in S3.&lt;/li&gt;\n&lt;li&gt;Basic summary metrics = count of email IDs, count of IP addresses, count of mobile numbers etc&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s ok if the data is not absolute real-time. What I mean to say here is that if the max(date) in the dashboard is yesterday&amp;#39;s, its fine.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am a rookie DE working in a team of SWEs, and we can build something here from scratch(means that the data which is currently stored in S3, we can process that and store in a DW if required) &lt;/p&gt;\n\n&lt;p&gt;Can you help me with a solution as to what should be my best data storage + data processing solution for the above use case, considering the below concerns:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;data volume&lt;/li&gt;\n&lt;li&gt;latency(after all the filters are applied, a latency of max 10 seconds is affordable)&lt;/li&gt;\n&lt;li&gt;cost&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13xg3d9", "is_robot_indexable": true, "report_reasons": null, "author": "MavSidharth", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xg3d9/low_latency_filtering_and_basic_operations_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xg3d9/low_latency_filtering_and_basic_operations_on/", "subreddit_subscribers": 108464, "created_utc": 1685621962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://open.substack.com/pub/hubertdulay/p/real-time-streaming-ecosystem-part-2ff?utm_source=direct&amp;r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real time streaming ecosystem-stream processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xfmw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685620661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/hubertdulay/p/real-time-streaming-ecosystem-part-2ff?utm_source=direct&amp;amp;r=46sqk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web\"&gt;https://open.substack.com/pub/hubertdulay/p/real-time-streaming-ecosystem-part-2ff?utm_source=direct&amp;amp;r=46sqk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?auto=webp&amp;v=enabled&amp;s=9039439d79ae61917cf8885f532b7b7490a066d2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90eaa90b1c2d1495437c8dac4acac015be061160", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e8d55472311d3d2ac5cfced51131a56aeb1d61f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c010e862d23a5b8afa0f32bf0c70ebbfba3dc5b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8abb149dc0f6bb9bce51055cbe3160febd08d841", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45cdffb7a1a3cdda36c737eb4ce0f1bb43c693c7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/2pykRI79o054eCTsDmowfNh87bi1WZwbWdh6LIJBGHo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8046d9fd46e4cace60dc7b6aeef2cf8a6c7dd86e", "width": 1080, "height": 540}], "variants": {}, "id": "uu0FWsUL8b93Bzt8U6jRxYZoPIZic3G2dzxU7AiAt18"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13xfmw0", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xfmw0/real_time_streaming_ecosystemstream_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xfmw0/real_time_streaming_ecosystemstream_processing/", "subreddit_subscribers": 108464, "created_utc": 1685620661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to build out a data model for a pet project that I want to be able to run on my laptop (8GB RAM). The data is shaped in such a way that I can have one 4-byte INT key link just about everything together that I need to. It's not necessarily a star schema, but for the purposes of the question you can think of it that way without much loss of generality. The central table is around \\~15M rows and the other tables are in that order of magnitude as well.\n\nI really like DuckDB and it was a logical choice for the project, but at the moment it doesn't have a good way of leveraging predictability when performing full-table joins (it also uses a lot of memory on joins when you're grabbing a wide set of columns from the tables, which is a problem for my case). Ideally I'd have something like Redshift's distkeys and sortkeys, where the data from the different tables would already be collocated by the join key. A standard PK/FK constraint would also work if declaring them actually sped up full-table joins.\n\nAny suggestions? Clickhouse seems like it might be a good fit, but I'm not sure how well it would handle the low-memory part specifically.", "author_fullname": "t2_3rnvqshi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good open-source analytics DB for fast, low-memory joins on predictable keys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xxcmq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685663922.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685663531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to build out a data model for a pet project that I want to be able to run on my laptop (8GB RAM). The data is shaped in such a way that I can have one 4-byte INT key link just about everything together that I need to. It&amp;#39;s not necessarily a star schema, but for the purposes of the question you can think of it that way without much loss of generality. The central table is around ~15M rows and the other tables are in that order of magnitude as well.&lt;/p&gt;\n\n&lt;p&gt;I really like DuckDB and it was a logical choice for the project, but at the moment it doesn&amp;#39;t have a good way of leveraging predictability when performing full-table joins (it also uses a lot of memory on joins when you&amp;#39;re grabbing a wide set of columns from the tables, which is a problem for my case). Ideally I&amp;#39;d have something like Redshift&amp;#39;s distkeys and sortkeys, where the data from the different tables would already be collocated by the join key. A standard PK/FK constraint would also work if declaring them actually sped up full-table joins.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions? Clickhouse seems like it might be a good fit, but I&amp;#39;m not sure how well it would handle the low-memory part specifically.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13xxcmq", "is_robot_indexable": true, "report_reasons": null, "author": "PaginatedSalmon", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xxcmq/good_opensource_analytics_db_for_fast_lowmemory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xxcmq/good_opensource_analytics_db_for_fast_lowmemory/", "subreddit_subscribers": 108464, "created_utc": 1685663531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alright so let me give a bit of a background:\n\nI\u2019ve been working as a software consultant for 1.5yr which means 50% of my job is working directly with our clients, understanding their business needs and figuring out the technical requirements. \n\nThe other 50% goes like this:\n\n-\tWriting large SQL queries for ad hoc reports used in business analytics/intelligence\n-\tWriting backend code in C# and VB.Net (legacy codebase) for ETL operations (typically used for populating our data warehouse), for batch updates of databases (we receive bulk data from various government agencies), creating streams to automate various file jobs, as well as application side data\n-\tOptimizing SQL queries (some queries involve joining several tables with hundreds of millions of rows each), indexing tables, creating new tables, etc\n\nTypical tools I use are SSMS and Visual Studio. Nothing fancy. \n\nI\u2019m also doing a masters in computer and information technology at University of Pennsylvania. I\u2019ve taken data structures, algorithms, machine learning courses and currently studying big data analytics. So I\u2019m proficient with Java and Python (as well as data science tools like pandas, scikitlearn) and currently learning Spark and Hadoop in the big data course. \n\nUPenn also partnered with AWS and offered us a project opportunity to set up EC2, S3, RDS. I just received my cloud practitioner certificate. \n\nWhat are my chances that I break in to the data engineering field given I don\u2019t use any popular cloud or recent data engineering technology? All job listings I\u2019ve looked at requires 3+ years of Azure, GCP, AWS, Databricks, Snowflake etc. \n\nI\u2019m open to learning these technologies on my own but due to my companies old tech stack I wouldn\u2019t be professionally using them. At best at a personal portfolio/project level.", "author_fullname": "t2_a0n6r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How likely that I can transition from software consultancy to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xoqv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685644629.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685642773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright so let me give a bit of a background:&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working as a software consultant for 1.5yr which means 50% of my job is working directly with our clients, understanding their business needs and figuring out the technical requirements. &lt;/p&gt;\n\n&lt;p&gt;The other 50% goes like this:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  Writing large SQL queries for ad hoc reports used in business analytics/intelligence&lt;/li&gt;\n&lt;li&gt;  Writing backend code in C# and VB.Net (legacy codebase) for ETL operations (typically used for populating our data warehouse), for batch updates of databases (we receive bulk data from various government agencies), creating streams to automate various file jobs, as well as application side data&lt;/li&gt;\n&lt;li&gt;  Optimizing SQL queries (some queries involve joining several tables with hundreds of millions of rows each), indexing tables, creating new tables, etc&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Typical tools I use are SSMS and Visual Studio. Nothing fancy. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m also doing a masters in computer and information technology at University of Pennsylvania. I\u2019ve taken data structures, algorithms, machine learning courses and currently studying big data analytics. So I\u2019m proficient with Java and Python (as well as data science tools like pandas, scikitlearn) and currently learning Spark and Hadoop in the big data course. &lt;/p&gt;\n\n&lt;p&gt;UPenn also partnered with AWS and offered us a project opportunity to set up EC2, S3, RDS. I just received my cloud practitioner certificate. &lt;/p&gt;\n\n&lt;p&gt;What are my chances that I break in to the data engineering field given I don\u2019t use any popular cloud or recent data engineering technology? All job listings I\u2019ve looked at requires 3+ years of Azure, GCP, AWS, Databricks, Snowflake etc. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m open to learning these technologies on my own but due to my companies old tech stack I wouldn\u2019t be professionally using them. At best at a personal portfolio/project level.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13xoqv2", "is_robot_indexable": true, "report_reasons": null, "author": "Soknardalr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xoqv2/how_likely_that_i_can_transition_from_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xoqv2/how_likely_that_i_can_transition_from_software/", "subreddit_subscribers": 108464, "created_utc": 1685642773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Jun 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1685635253.015, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xle38", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685635252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13xle38", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xle38/monthly_general_discussion_jun_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/13xle38/monthly_general_discussion_jun_2023/", "subreddit_subscribers": 108464, "created_utc": 1685635252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, I am seeking some advice on how I should go about allocating resources to Spark applications. Here is more context on this:\n\n* Spark is running within a Kubernetes pod. All of the Spark applications that will be scheduled are batch jobs. In the future I may incorporate streaming.\n* These Spark applications are written and submitted by users. Once submitted they can be automatically scheduled to run daily, weekly, etc.. There is some quality control to ensure that they are not submitting inefficient code.\n* I am using spark-submit to launch each application on the cluster. For now I am manually specifying to Kubernetes the CPU, Memory, and number of executors to use. **This is where I would want to dynamically allocate resources**.\n* My idea is that I could fetch metrics from both the Spark History Server and Kubernetes and save them into some SQL table. The next time that this Spark application runs, the scheduler will have a better idea on how much resources should be allocated. I don't know if this is a reasonable approach and what metrics would best indicate things like lack of CPU resources.\n\nAny advice is greatly appreciated.", "author_fullname": "t2_e7pkp6q3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Scheduler Resource Allocation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xgdxl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685622779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, I am seeking some advice on how I should go about allocating resources to Spark applications. Here is more context on this:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark is running within a Kubernetes pod. All of the Spark applications that will be scheduled are batch jobs. In the future I may incorporate streaming.&lt;/li&gt;\n&lt;li&gt;These Spark applications are written and submitted by users. Once submitted they can be automatically scheduled to run daily, weekly, etc.. There is some quality control to ensure that they are not submitting inefficient code.&lt;/li&gt;\n&lt;li&gt;I am using spark-submit to launch each application on the cluster. For now I am manually specifying to Kubernetes the CPU, Memory, and number of executors to use. &lt;strong&gt;This is where I would want to dynamically allocate resources&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;My idea is that I could fetch metrics from both the Spark History Server and Kubernetes and save them into some SQL table. The next time that this Spark application runs, the scheduler will have a better idea on how much resources should be allocated. I don&amp;#39;t know if this is a reasonable approach and what metrics would best indicate things like lack of CPU resources.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13xgdxl", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Dish-1171", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xgdxl/spark_scheduler_resource_allocation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xgdxl/spark_scheduler_resource_allocation/", "subreddit_subscribers": 108464, "created_utc": 1685622779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, thanks for passing by.  \nI'm working on a project where we need (among other things) to geographically enrich ingested data in NRT.  \nThe currently implemented architecture is simply as follows:  \nEvents -&gt; kafka -&gt; spark streaming -&gt; kafka  \n\nThe spark job is is concerned with:  \n1. Reading decoding avro data packets from a kafka topic\n2. joining packets with customer data read from Hbase  \n3. calling a reverse geocoding service (written by some folks here) for geographical enrichment for each packet. \n4. Writing data back to another kafka topic  \n\nAs of now, the reverse geocoder is a single instance REST service which loads the whole 80+ gb OSM map in memory, and i really have the feeling this is by no means the correct way to implement it (although i do not have experience with geographical data) and I'm therefore looking to find a better, more robust way to accomplish this requirement.  \n\nIs there a commonly agreed upon way to do this? The map data is obviously static, so the solution needs to be optimized for fast data retrieval (key-based). I'd like to get rid of the rest service altogether and to store this map (which will soon include other countries hence will be bigger than 80gb) in some data store.  \n\nThe first solution that i came up with was to store the map in Hbase, since it's what we already use for customer data and also because it would unify our data enrichment process, but somehow CA colleagues say that \"it would overload Hbase and negatively impact performance\". Are they just being lazy or mine is actually a bad idea? (I feel like enhancement proposals are not very welcome here, but yet i try). \nIs there any other technology that would be fit for the task? Postgres, Elasticsearch? Alternatively, is keeping these big ass maps in memory a good idea? \n\nJust for info, we are talking about 1000 to 2000 msg/s and no, we cannot use external geocoding services. Also manually implementing sharding, load balancing, replication and automatic fallbacks is a lot of unnecessary work to me.\n\nThanks a lot for taking the time to read this.  \n\nEdit: i'd add that while the msg rate is around 1000-2000 /s we process them in micro-batches every 30 seconds.", "author_fullname": "t2_3xvidrz9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Implementing a Reverse Geocoding service for data enrichment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xsrft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685652572.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685652159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, thanks for passing by.&lt;br/&gt;\nI&amp;#39;m working on a project where we need (among other things) to geographically enrich ingested data in NRT.&lt;br/&gt;\nThe currently implemented architecture is simply as follows:&lt;br/&gt;\nEvents -&amp;gt; kafka -&amp;gt; spark streaming -&amp;gt; kafka  &lt;/p&gt;\n\n&lt;p&gt;The spark job is is concerned with:&lt;br/&gt;\n1. Reading decoding avro data packets from a kafka topic\n2. joining packets with customer data read from Hbase&lt;br/&gt;\n3. calling a reverse geocoding service (written by some folks here) for geographical enrichment for each packet. \n4. Writing data back to another kafka topic  &lt;/p&gt;\n\n&lt;p&gt;As of now, the reverse geocoder is a single instance REST service which loads the whole 80+ gb OSM map in memory, and i really have the feeling this is by no means the correct way to implement it (although i do not have experience with geographical data) and I&amp;#39;m therefore looking to find a better, more robust way to accomplish this requirement.  &lt;/p&gt;\n\n&lt;p&gt;Is there a commonly agreed upon way to do this? The map data is obviously static, so the solution needs to be optimized for fast data retrieval (key-based). I&amp;#39;d like to get rid of the rest service altogether and to store this map (which will soon include other countries hence will be bigger than 80gb) in some data store.  &lt;/p&gt;\n\n&lt;p&gt;The first solution that i came up with was to store the map in Hbase, since it&amp;#39;s what we already use for customer data and also because it would unify our data enrichment process, but somehow CA colleagues say that &amp;quot;it would overload Hbase and negatively impact performance&amp;quot;. Are they just being lazy or mine is actually a bad idea? (I feel like enhancement proposals are not very welcome here, but yet i try). \nIs there any other technology that would be fit for the task? Postgres, Elasticsearch? Alternatively, is keeping these big ass maps in memory a good idea? &lt;/p&gt;\n\n&lt;p&gt;Just for info, we are talking about 1000 to 2000 msg/s and no, we cannot use external geocoding services. Also manually implementing sharding, load balancing, replication and automatic fallbacks is a lot of unnecessary work to me.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for taking the time to read this.  &lt;/p&gt;\n\n&lt;p&gt;Edit: i&amp;#39;d add that while the msg rate is around 1000-2000 /s we process them in micro-batches every 30 seconds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13xsrft", "is_robot_indexable": true, "report_reasons": null, "author": "RAT-000", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xsrft/implementing_a_reverse_geocoding_service_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xsrft/implementing_a_reverse_geocoding_service_for_data/", "subreddit_subscribers": 108464, "created_utc": 1685652159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have in place a process that archives Snowflake query history for the longer default, into a table that we created. \n\nDoes anybody know of a SQL parser or analysis tool that would be easy to set up and run that would scrape the QUERY\\_TEXT values from this table to build a table, dataframe, or csv file showing how frequently each table/view/field is accessed? I'm trying to do this on a per-team basis so that we can find assets and work being duplicated across teams/schemas as a way to reduce duplication of effort (and expense). \n\nThis is a one-off piece of a larger strategic process, so I don't need a robust tool that will be in place for a long time, like a data catalog. I don't mind installing and using one if it's the easiest way to get this data. Given the ephemeral nature of the work, I'd prefer to use a free or open source tool.\n\nThanks!", "author_fullname": "t2_ysn7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick way to extract table/column usage from Snowflake query history?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xhuum", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685626717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have in place a process that archives Snowflake query history for the longer default, into a table that we created. &lt;/p&gt;\n\n&lt;p&gt;Does anybody know of a SQL parser or analysis tool that would be easy to set up and run that would scrape the QUERY_TEXT values from this table to build a table, dataframe, or csv file showing how frequently each table/view/field is accessed? I&amp;#39;m trying to do this on a per-team basis so that we can find assets and work being duplicated across teams/schemas as a way to reduce duplication of effort (and expense). &lt;/p&gt;\n\n&lt;p&gt;This is a one-off piece of a larger strategic process, so I don&amp;#39;t need a robust tool that will be in place for a long time, like a data catalog. I don&amp;#39;t mind installing and using one if it&amp;#39;s the easiest way to get this data. Given the ephemeral nature of the work, I&amp;#39;d prefer to use a free or open source tool.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13xhuum", "is_robot_indexable": true, "report_reasons": null, "author": "i_hmm_some", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xhuum/quick_way_to_extract_tablecolumn_usage_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xhuum/quick_way_to_extract_tablecolumn_usage_from/", "subreddit_subscribers": 108464, "created_utc": 1685626717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi ,\n\nCan you please let me know what services in Azure are required for an Azure Data Engineer.\n\nProfessional BackGround\n\nAround 10 Years of experience in DE roles using Informatica ,SQL Server and Netezza.\n\nCurrently working on Azure but limited to Python and DataBricks with target DB Snowflake.\n\nThis is my role since last 6 Months , So just wanted to utilize this opportunity and learn Other Tools related to DE in Azure\n\nI know ADF ,COSMOS DB and PowerBI comes in this DE stack but what else is mandatory for an Azure DataEngineer.\n\nThankyou!", "author_fullname": "t2_c1ly5ik3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Services to Focus for Azure Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xe17a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685615577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi ,&lt;/p&gt;\n\n&lt;p&gt;Can you please let me know what services in Azure are required for an Azure Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;Professional BackGround&lt;/p&gt;\n\n&lt;p&gt;Around 10 Years of experience in DE roles using Informatica ,SQL Server and Netezza.&lt;/p&gt;\n\n&lt;p&gt;Currently working on Azure but limited to Python and DataBricks with target DB Snowflake.&lt;/p&gt;\n\n&lt;p&gt;This is my role since last 6 Months , So just wanted to utilize this opportunity and learn Other Tools related to DE in Azure&lt;/p&gt;\n\n&lt;p&gt;I know ADF ,COSMOS DB and PowerBI comes in this DE stack but what else is mandatory for an Azure DataEngineer.&lt;/p&gt;\n\n&lt;p&gt;Thankyou!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13xe17a", "is_robot_indexable": true, "report_reasons": null, "author": "nifty60", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xe17a/services_to_focus_for_azure_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xe17a/services_to_focus_for_azure_data_engineer/", "subreddit_subscribers": 108464, "created_utc": 1685615577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know there are plenty of materials for this question. But I just can't help but stuck in data retrieval question. If I know the hash key of hashed data , do I have access to it ?\n\nE.g\n\n`DECLARE @HashThis NVARCHAR(32)`\n\n`SET @HashThis = 'data'`\n\n`SELECT HASHBYTES('SHA2_256', @HashThis);`\n\nLet's imagine hash key is `0x741238C01D9DB821CF171BF61D72260B998F7C7881D90091099945E0B9E0C2E3`\n\nNow, if I know this hash key, do I have access to data ?\n\nBasically, all attacker needs to know is this hash key ?", "author_fullname": "t2_gy4d61n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hashing works ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13x7037", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685590226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there are plenty of materials for this question. But I just can&amp;#39;t help but stuck in data retrieval question. If I know the hash key of hashed data , do I have access to it ?&lt;/p&gt;\n\n&lt;p&gt;E.g&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;DECLARE @HashThis NVARCHAR(32)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SET @HashThis = &amp;#39;data&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT HASHBYTES(&amp;#39;SHA2_256&amp;#39;, @HashThis);&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s imagine hash key is &lt;code&gt;0x741238C01D9DB821CF171BF61D72260B998F7C7881D90091099945E0B9E0C2E3&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, if I know this hash key, do I have access to data ?&lt;/p&gt;\n\n&lt;p&gt;Basically, all attacker needs to know is this hash key ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13x7037", "is_robot_indexable": true, "report_reasons": null, "author": "SolariDoma", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13x7037/how_hashing_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13x7037/how_hashing_works/", "subreddit_subscribers": 108464, "created_utc": 1685590226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Embedding Pipelines For Generative AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_13xzpt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/H7VGUc9R5gFecU5Ez20Voch7M-JUlDQ0FW_iAoTLGzI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685670118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bytewax.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bytewax.io/blog/embedding-pipelines-for-generative-ai", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?auto=webp&amp;v=enabled&amp;s=508e36832dcdeb2bc9f42531a3224e7c5f9e6cfc", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2221d29d33827fb9c45a1bd9a811e44475b47a7", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=442f4ea278032bdda0e13616933e73c99662a6fd", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa994bb2165b79bc75a9a548cd1f0d1425ee6e87", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c21bb53529abd65816f0b2935254605f4fa330e2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d47c4f7c1122199e5f3b9214633846d5d289578b", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/L5YMBKz240JSZtVKLof0hBtrdtcN9SZdn6JrDNu6LdE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2ed7d5bf7eba7af3c8eb8ac8fa0c65c8d7be082", "width": 1080, "height": 607}], "variants": {}, "id": "aZU66TyrS3pTtx2nstLcKFQ6BhqRUJp3xVigMlCxRJg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13xzpt2", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xzpt2/embedding_pipelines_for_generative_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bytewax.io/blog/embedding-pipelines-for-generative-ai", "subreddit_subscribers": 108464, "created_utc": 1685670118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have a system where a lot of data arrives in a pleasant, standard format (let's say there are \\~100 standard forms) but a lot of data arrives in Excel or text files with some descriptive header, many rows of CSV content, some more descriptive cruft, another set of CSV content, etc.\n\n\"Get the users to fix the data\" isn't a viable response given our pricing model.\n\nI'm starting to write some tools to allow users to provide processing instructions, such as \n\n* split an Excel doc into multiple sheets\n* split the file at some user provided content (e.g. \"Report #2 xyz\")\n* skip n header rows (easy) and n footer rows (less easy)\n* date format\n* the usual delimiter, character quoting stuff\n\nAll of this is achievable with some code, but this isn't a new or unique problem so there must be some options already available out there. Right?", "author_fullname": "t2_rmmatfaw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tooling for messy ingestions (e.g. excel, non-tabular text files, etc)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13xzejs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685669224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a system where a lot of data arrives in a pleasant, standard format (let&amp;#39;s say there are ~100 standard forms) but a lot of data arrives in Excel or text files with some descriptive header, many rows of CSV content, some more descriptive cruft, another set of CSV content, etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Get the users to fix the data&amp;quot; isn&amp;#39;t a viable response given our pricing model.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m starting to write some tools to allow users to provide processing instructions, such as &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;split an Excel doc into multiple sheets&lt;/li&gt;\n&lt;li&gt;split the file at some user provided content (e.g. &amp;quot;Report #2 xyz&amp;quot;)&lt;/li&gt;\n&lt;li&gt;skip n header rows (easy) and n footer rows (less easy)&lt;/li&gt;\n&lt;li&gt;date format&lt;/li&gt;\n&lt;li&gt;the usual delimiter, character quoting stuff&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All of this is achievable with some code, but this isn&amp;#39;t a new or unique problem so there must be some options already available out there. Right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13xzejs", "is_robot_indexable": true, "report_reasons": null, "author": "realitydevice", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xzejs/tooling_for_messy_ingestions_eg_excel_nontabular/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xzejs/tooling_for_messy_ingestions_eg_excel_nontabular/", "subreddit_subscribers": 108464, "created_utc": 1685669224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,   \n\n\nI know that Fivetran has functionality that enables the creation of Custom Connectors e.g. instructions here: [https://www.biztory.com/blog/building-your-own-custom-connector-for-fivetran-part-ii](https://www.biztory.com/blog/building-your-own-custom-connector-for-fivetran-part-ii)  \n\n\nBut does anybody know what their protocol is for the creation of Native connectors? For example if a native connector doesn't exist for your data source - can you request for one to be built and what is the likelihood of that happening?   \n\n\nThanks", "author_fullname": "t2_gmutm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creation of native connectors with FiveTran", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13xw7re", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685660529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,   &lt;/p&gt;\n\n&lt;p&gt;I know that Fivetran has functionality that enables the creation of Custom Connectors e.g. instructions here: &lt;a href=\"https://www.biztory.com/blog/building-your-own-custom-connector-for-fivetran-part-ii\"&gt;https://www.biztory.com/blog/building-your-own-custom-connector-for-fivetran-part-ii&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;But does anybody know what their protocol is for the creation of Native connectors? For example if a native connector doesn&amp;#39;t exist for your data source - can you request for one to be built and what is the likelihood of that happening?   &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13xw7re", "is_robot_indexable": true, "report_reasons": null, "author": "CyberVoyeur", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13xw7re/creation_of_native_connectors_with_fivetran/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13xw7re/creation_of_native_connectors_with_fivetran/", "subreddit_subscribers": 108464, "created_utc": 1685660529.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}