{"kind": "Listing", "data": {"after": "t3_145iahj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Are there specific skills? Or is it just the same as everywhere else?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Scientist who are good at office politics, how did you increase your political skill?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kh8b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 172, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 172, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686353823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there specific skills? Or is it just the same as everywhere else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145kh8b", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 112, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145kh8b/data_scientist_who_are_good_at_office_politics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145kh8b/data_scientist_who_are_good_at_office_politics/", "subreddit_subscribers": 922013, "created_utc": 1686353823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry if this is odd to post here - I completed 11 out of 12 courses for my MSDS at Northwestern and just felt like the program was extremely lackluster. The required courses in management rather than the technicals. It was 55k and it would have been obvious to transfer quickly to OMSA or the newer programs.The landscape looked different when I was applying for programs, and I was initially skeptical of entirely MOOC based programs. However, I ended up just watching YT videos as my lectures anyway.\n\nI would just like to warn others when deciding whether or not to go back to school. I\u2019m still taking time to patch up on knowledge that I felt like I did not gain via the program. Although with that being said, that would be the case with any masters program. I am almost considering not even doing the last capstone just because I know that there are other things that I would rather learn.\n\nI literally just have the capstone left but I am almost considering just letting it go", "author_fullname": "t2_35qsk3el", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Off my chest: No need for costly masters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145fo6w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686356525.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686342059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is odd to post here - I completed 11 out of 12 courses for my MSDS at Northwestern and just felt like the program was extremely lackluster. The required courses in management rather than the technicals. It was 55k and it would have been obvious to transfer quickly to OMSA or the newer programs.The landscape looked different when I was applying for programs, and I was initially skeptical of entirely MOOC based programs. However, I ended up just watching YT videos as my lectures anyway.&lt;/p&gt;\n\n&lt;p&gt;I would just like to warn others when deciding whether or not to go back to school. I\u2019m still taking time to patch up on knowledge that I felt like I did not gain via the program. Although with that being said, that would be the case with any masters program. I am almost considering not even doing the last capstone just because I know that there are other things that I would rather learn.&lt;/p&gt;\n\n&lt;p&gt;I literally just have the capstone left but I am almost considering just letting it go&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145fo6w", "is_robot_indexable": true, "report_reasons": null, "author": "peachyjiang", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145fo6w/off_my_chest_no_need_for_costly_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145fo6w/off_my_chest_no_need_for_costly_masters/", "subreddit_subscribers": 922013, "created_utc": 1686342059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have come to problems that I have solved in SQL (postgress) but doing the analysis in R or even Python would be easier.\n\nI needed from a zip code the geometry column twice:1 for the orgin source and 1 for the destination.  There would be no easy solution in SQL other use a nested query or use left join twice. Where as in R mapvalues(collum\\_to\\_transforn, zipcode, geometry) is sufficient. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nOr you want to calculate the avg use per zipcode per day but\n\nusing the global maximum and minimum date to calculate the number of  days and not the days between per zipcode, as the first time using it was e.g. in February but available since January.\n\nThis can be done with SQL but  R makes lives easier again.\n\n&amp;#x200B;\n\nSo when do you use SQL for data manipulation and when another language. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_67cz0s3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL or programming language. When to use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145syvc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686379865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have come to problems that I have solved in SQL (postgress) but doing the analysis in R or even Python would be easier.&lt;/p&gt;\n\n&lt;p&gt;I needed from a zip code the geometry column twice:1 for the orgin source and 1 for the destination.  There would be no easy solution in SQL other use a nested query or use left join twice. Where as in R mapvalues(collum_to_transforn, zipcode, geometry) is sufficient. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or you want to calculate the avg use per zipcode per day but&lt;/p&gt;\n\n&lt;p&gt;using the global maximum and minimum date to calculate the number of  days and not the days between per zipcode, as the first time using it was e.g. in February but available since January.&lt;/p&gt;\n\n&lt;p&gt;This can be done with SQL but  R makes lives easier again.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So when do you use SQL for data manipulation and when another language. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145syvc", "is_robot_indexable": true, "report_reasons": null, "author": "TywinASOIAF", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145syvc/sql_or_programming_language_when_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145syvc/sql_or_programming_language_when_to_use/", "subreddit_subscribers": 922013, "created_utc": 1686379865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lrllrd1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any one working on smart manufacturing? What are your use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145q3z8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686370304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145q3z8", "is_robot_indexable": true, "report_reasons": null, "author": "vishal-vora", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145q3z8/any_one_working_on_smart_manufacturing_what_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145q3z8/any_one_working_on_smart_manufacturing_what_are/", "subreddit_subscribers": 922013, "created_utc": 1686370304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing SlimPajama-627B: the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145gqcx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_voqxwypq", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "LanguageTechnology", "selftext": "SlimPajama cleans and deduplicates RedPajama-1T, reducing the total token count and file size by 50%. It's half the size and trains twice as fast!\n\nIt\u2019s the highest quality dataset when training to 600B tokens and, when upsampled, performs equal or better than RedPajama.  It was no mean feat to deduplicate data on this scale \u2013 existing tools do not scale to a trillion tokens. We built a custom parallel data pre-processing pipeline and are sharing the code open source with the community.\n\nWe\u2019d like to thank our partner Opentensor for supporting this project. And credit goes to Together Compute and the entire team that created the RedPajama dataset!\n\n&amp;#x200B;\n\n* SlimPajama dataset - [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)\n* Libraries for data pre-processing - [https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data\\_processing/slimpajama](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama)\n* Read our blog - [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)", "author_fullname": "t2_voqxwypq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing SlimPajama-627B: the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/LanguageTechnology", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145gowe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686356247.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686344467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LanguageTechnology", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SlimPajama cleans and deduplicates RedPajama-1T, reducing the total token count and file size by 50%. It&amp;#39;s half the size and trains twice as fast!&lt;/p&gt;\n\n&lt;p&gt;It\u2019s the highest quality dataset when training to 600B tokens and, when upsampled, performs equal or better than RedPajama.  It was no mean feat to deduplicate data on this scale \u2013 existing tools do not scale to a trillion tokens. We built a custom parallel data pre-processing pipeline and are sharing the code open source with the community.&lt;/p&gt;\n\n&lt;p&gt;We\u2019d like to thank our partner Opentensor for supporting this project. And credit goes to Together Compute and the entire team that created the RedPajama dataset!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SlimPajama dataset - &lt;a href=\"https://huggingface.co/datasets/cerebras/SlimPajama-627B\"&gt;https://huggingface.co/datasets/cerebras/SlimPajama-627B&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Libraries for data pre-processing - &lt;a href=\"https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama\"&gt;https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Read our blog - &lt;a href=\"https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama\"&gt;https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?auto=webp&amp;v=enabled&amp;s=a583c96a3aca547884f7609054ef900ea6e1b937", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffa89dc772d5b0c5c209fe190156897d3bf3ab93", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e4f4987b044c263bd6abf9f14b3897f11eceedb", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=292e53558b985987a9004fa3c28fa47da1c1f2b9", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ccb4c70274741ca419e6fa85f5a4ed3c30f3cf2", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8328eb49c040bcae227faa60bf28f84f705cc4a0", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aae668a15dd053f27c3c00579ccf95137d96a1c", "width": 1080, "height": 583}], "variants": {}, "id": "qZu7xwSlcLKCNhCBJSU5hdBnG2HpXv6XxLw_0LnqVnA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rkr2", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145gowe", "is_robot_indexable": true, "report_reasons": null, "author": "CS-fan-101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "subreddit_subscribers": 42650, "created_utc": 1686344467.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1686344549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LanguageTechnology", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?auto=webp&amp;v=enabled&amp;s=a583c96a3aca547884f7609054ef900ea6e1b937", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffa89dc772d5b0c5c209fe190156897d3bf3ab93", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e4f4987b044c263bd6abf9f14b3897f11eceedb", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=292e53558b985987a9004fa3c28fa47da1c1f2b9", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ccb4c70274741ca419e6fa85f5a4ed3c30f3cf2", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8328eb49c040bcae227faa60bf28f84f705cc4a0", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aae668a15dd053f27c3c00579ccf95137d96a1c", "width": 1080, "height": 583}], "variants": {}, "id": "qZu7xwSlcLKCNhCBJSU5hdBnG2HpXv6XxLw_0LnqVnA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145gqcx", "is_robot_indexable": true, "report_reasons": null, "author": "CS-fan-101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_145gowe", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145gqcx/introducing_slimpajama627b_the_largest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "subreddit_subscribers": 922013, "created_utc": 1686344549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[OC] Teaching DataViz to kids via Board Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_145d7r5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_51o3wf0o", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/85BRd3d4PsXPnzvB0yCnfyt467HKEfL53NuS5MbknEU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "dataisbeautiful", "selftext": "", "author_fullname": "t2_51o3wf0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[OC] Teaching DataViz to kids via Board Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataisbeautiful", "hidden": false, "pwls": 6, "link_flair_css_class": "oc", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_144gxzk", "quarantine": false, "link_flair_text_color": null, "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "1c7d62a6-099d-11e7-9b3c-0ee50bfd7a4c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OC", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/85BRd3d4PsXPnzvB0yCnfyt467HKEfL53NuS5MbknEU.jpg", "edited": false, "author_flair_css_class": "ocmaker", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1686248807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bxaaztw87u4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?auto=webp&amp;v=enabled&amp;s=496ac0bb9575ca95229257a777fd1ce94900f593", "width": 720, "height": 540}, "resolutions": [{"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d643590fc8f6d02c76e0865c2f60a1b6d596e3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=615653e24d163ceb024b6456c7a7f67b1ec01130", "width": 216, "height": 162}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4e4a4953c341c8ba247a233877864ee4c3aa356", "width": 320, "height": 240}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b9c83630fdc1ec7e0ea1a5febd8ba3b81987a9", "width": 640, "height": 480}], "variants": {}, "id": "nPqUNFFdipGnezYZE44prAzG_4rtsYukP-D_eWwmj08"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OC: 11", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2tk95", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "144gxzk", "is_robot_indexable": true, "report_reasons": null, "author": "DataVizzdom", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataisbeautiful/comments/144gxzk/oc_teaching_dataviz_to_kids_via_board_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bxaaztw87u4b1.jpg", "subreddit_subscribers": 19643679, "created_utc": 1686248807.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1686336122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bxaaztw87u4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?auto=webp&amp;v=enabled&amp;s=496ac0bb9575ca95229257a777fd1ce94900f593", "width": 720, "height": 540}, "resolutions": [{"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d643590fc8f6d02c76e0865c2f60a1b6d596e3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=615653e24d163ceb024b6456c7a7f67b1ec01130", "width": 216, "height": 162}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4e4a4953c341c8ba247a233877864ee4c3aa356", "width": 320, "height": 240}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b9c83630fdc1ec7e0ea1a5febd8ba3b81987a9", "width": 640, "height": 480}], "variants": {}, "id": "nPqUNFFdipGnezYZE44prAzG_4rtsYukP-D_eWwmj08"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145d7r5", "is_robot_indexable": true, "report_reasons": null, "author": "DataVizzdom", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_144gxzk", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145d7r5/oc_teaching_dataviz_to_kids_via_board_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bxaaztw87u4b1.jpg", "subreddit_subscribers": 922013, "created_utc": 1686336122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a beginner and I am stuck in this program. \n\nWhen I use model.fit() the program says \u2018InvalidArgumentError\u2019\n\nI tried many ways to solve the problem.\n\n[https://www.kaggle.com/code/raghavdecoded/infosys-agropy](https://www.kaggle.com/code/raghavdecoded/infosys-agropy)", "author_fullname": "t2_rlkg9ceg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plant Disease Identification using DenseNet201", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1461l5m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686410693.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686407016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a beginner and I am stuck in this program. &lt;/p&gt;\n\n&lt;p&gt;When I use model.fit() the program says \u2018InvalidArgumentError\u2019&lt;/p&gt;\n\n&lt;p&gt;I tried many ways to solve the problem.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kaggle.com/code/raghavdecoded/infosys-agropy\"&gt;https://www.kaggle.com/code/raghavdecoded/infosys-agropy&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uOl0zx6CKFscwZpm-VzUbN7G5fPVilXvzxsHD2hXZIg.jpg?auto=webp&amp;v=enabled&amp;s=cf506407538bb3c2c6ab5eabbb232e5ab21766cd", "width": 100, "height": 100}, "resolutions": [], "variants": {}, "id": "XdcBgGBtA9rRL9U_i8ErY-agZXiiCqClRnMacYcZRZY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1461l5m", "is_robot_indexable": true, "report_reasons": null, "author": "supreme-raghav", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1461l5m/plant_disease_identification_using_densenet201/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1461l5m/plant_disease_identification_using_densenet201/", "subreddit_subscribers": 922013, "created_utc": 1686407016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'll use a API service to provide me with a bunch of website urls then use python beautiful soup to extract the actual text content the pile up all the content from about 30 web pages and send it to a python script that extracts keywords. \n\nBut it's slow. How can I find the bottle necks? Or is there a way to speed this up using another language like R or Java? \n\nQuite new to this so I do not know how to run tests etc.\n\nI just want to mention the the API is really fast and I'm sure is not causing any latency issue.", "author_fullname": "t2_r3147swk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm using python to scrape web page content and extract keywords, how can I make it faster to process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145vvsx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686389983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll use a API service to provide me with a bunch of website urls then use python beautiful soup to extract the actual text content the pile up all the content from about 30 web pages and send it to a python script that extracts keywords. &lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s slow. How can I find the bottle necks? Or is there a way to speed this up using another language like R or Java? &lt;/p&gt;\n\n&lt;p&gt;Quite new to this so I do not know how to run tests etc.&lt;/p&gt;\n\n&lt;p&gt;I just want to mention the the API is really fast and I&amp;#39;m sure is not causing any latency issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145vvsx", "is_robot_indexable": true, "report_reasons": null, "author": "flipsnapnet", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145vvsx/im_using_python_to_scrape_web_page_content_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145vvsx/im_using_python_to_scrape_web_page_content_and/", "subreddit_subscribers": 922013, "created_utc": 1686389983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For each feature, how do go about finding the best transformations?  \n\n\nCurrently what I do is I look at a scatter graph of my feature vs the target and trial various different functions as lines of best fit to see which has the best R-Squared and go with that. I know there are various weakness to this approach however, not to mention it's time consuming, at least how I do it.  \n\n\nWhat methods do you have find better transformation or to find them faster/more effeciently?", "author_fullname": "t2_1055bs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you find the best Transformations? (Linear Regression)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1460ek4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686403978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For each feature, how do go about finding the best transformations?  &lt;/p&gt;\n\n&lt;p&gt;Currently what I do is I look at a scatter graph of my feature vs the target and trial various different functions as lines of best fit to see which has the best R-Squared and go with that. I know there are various weakness to this approach however, not to mention it&amp;#39;s time consuming, at least how I do it.  &lt;/p&gt;\n\n&lt;p&gt;What methods do you have find better transformation or to find them faster/more effeciently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1460ek4", "is_robot_indexable": true, "report_reasons": null, "author": "Gef_1_Man_Army", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1460ek4/how_do_you_find_the_best_transformations_linear/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1460ek4/how_do_you_find_the_best_transformations_linear/", "subreddit_subscribers": 922013, "created_utc": 1686403978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys!\n\nWe are hosting another Live event for our ML community.. you're all welcome to join =)\n\nTime: This Sunday at 1PM UTC+0 / 9AM EST / 7AM CST\n\nThe topic is: \"Machine Learning Implementation on the Insurance/Finance Industry\"\n\nThe Speaker: Cornellius Yudha Wijaya ([https://www.linkedin.com/in/cornellius-yudha-wijaya/](https://www.linkedin.com/in/cornellius-yudha-wijaya/))\n\n&amp;#x200B;\n\nIf you want the event on your Calendar and also to hear about future events you can signup here:\n\n[https://forms.gle/zTiezLukGRhiLQ4DA](https://forms.gle/zTiezLukGRhiLQ4DA)\n\n&amp;#x200B;\n\nIf you prefer not to share any info - I get it... you can simply join here when it starts:\n\n[https://meet.google.com/mcm-gxrd-prb](https://meet.google.com/mcm-gxrd-prb)", "author_fullname": "t2_8lm3cr0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Live Event - \"Machine Learning Implementation on the Insurance/Finance Industry\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1464x32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686415476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;We are hosting another Live event for our ML community.. you&amp;#39;re all welcome to join =)&lt;/p&gt;\n\n&lt;p&gt;Time: This Sunday at 1PM UTC+0 / 9AM EST / 7AM CST&lt;/p&gt;\n\n&lt;p&gt;The topic is: &amp;quot;Machine Learning Implementation on the Insurance/Finance Industry&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The Speaker: Cornellius Yudha Wijaya (&lt;a href=\"https://www.linkedin.com/in/cornellius-yudha-wijaya/\"&gt;https://www.linkedin.com/in/cornellius-yudha-wijaya/&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you want the event on your Calendar and also to hear about future events you can signup here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.gle/zTiezLukGRhiLQ4DA\"&gt;https://forms.gle/zTiezLukGRhiLQ4DA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you prefer not to share any info - I get it... you can simply join here when it starts:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://meet.google.com/mcm-gxrd-prb\"&gt;https://meet.google.com/mcm-gxrd-prb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1464x32", "is_robot_indexable": true, "report_reasons": null, "author": "__god_bless_you_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1464x32/live_event_machine_learning_implementation_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1464x32/live_event_machine_learning_implementation_on_the/", "subreddit_subscribers": 922013, "created_utc": 1686415476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a23h93a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the 2023 raises and bonuses thread up yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14627hr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686408566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14627hr", "is_robot_indexable": true, "report_reasons": null, "author": "BlackPlasmaX", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/14627hr/is_the_2023_raises_and_bonuses_thread_up_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/14627hr/is_the_2023_raises_and_bonuses_thread_up_yet/", "subreddit_subscribers": 922013, "created_utc": 1686408566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This encoding system claim that it can state all IPA symbols and its pronounciation in a Binary format \n\nPhonBinary Encoding System V6\n\n- Analyzes each sound into its featural components to translate into binary. \n- Features are encoded using a variable number of bits based on the number of possibilities for each feature.\n- Enables representation of the full IPA  to use in various NLP applications. \n- Includes all manners of articulation, secondary articulations, prosodic features, airstream mechanisms and vowel qualities.\n\nSound Analysis:  \n\n- The first step involves analyzing each sound into its featural components. \n- Full understanding of the IPA and phonetic terminology required.\n\nConsonants:\n- Produced by modifying airstream with articulators. \n- Factors: airstream mechanism (3 bits), manner of articulation (5 bits), place of articulation (6 bits), phonation (4 bits), coarticulation (5 bits).\n\nAirstream mechanism (3 bits):\n- Pulmonic egressive: 000 \n- Pulmonic ingressive: 001\n- Glottalic egressive: 010\n- Glottalic ingressive: 011\n- Lingual-glottalic: 100\n- Velaric: 101 \n\nManner of articulation (5 bits):\n- Plosive: 00000   \n- Nasal: 00001\n- Trill: 00010\n- Tap: 00011\n- Fricative: 00100\n- Approximant: 00101\n- Lateral approximant: 00110\n- Flap: 00111\n- Affricate: 01000 \n- Clitic: 01001  \n- Implosive  01010\n- Ejective: 01011\n- Click: 01100\n\nPlace of articulation (6 bits): \n- Bilabial: 000000 \n- Labiodental: 000001\n- Interdental: 000010\n- Alveolar: 000011 \n- Postalveolar: 000100\n- Retroflex: 000101\n- Palatal: 000110\n- Velar: 000111 \n- Uvular: 001000\n- Pharyngeal: 001001  \n- Glottal: 001010 \n- Epiglottal: 001011\n- Labial-palatal: 001100\n- Labial-velar: 001101 \n- Labial-uvular: 001110\n- Velar-pharyngeal: 001111\n- Velar-uvular: 010000  \n- Velar-epilaryngeal: 010001\n- Uvular-pharyngeal : 010010\n\nPhonation (4 bits):\n- Voiceless: 0000\n- Voiced: 0001   \n- Creaky: 0010 \n- Breathy: 0011\n- Slack: 0100\n- Stiff: 0101\n- Creaky voiced: 0110\n- Whispered: 0111   \n- Harsh: 1000 \n- Laryngealized : 1001\n- Closed glottis: 1010\n- Velaric aspiration: 1011\n\nVowels:\n- Produced by shaping airstream resonated through oral and nasal cavities. \n- Factors: height (4 bits), backness (4 bits), roundness (4 bits), nasalization (2 bits), length (3 bits). \n\nHeight (4 bits):\n- Close: 0000\n- Near-close: 0001  \n- Close-mid: 0010  \n- Mid: 0011\n- Open-mid: 0100 \n- Near-open: 0101   \n- Open: 0110\n\nBackness(4 bits):\n- Front: 0000\n- Near-front: 0001\n- Central: 0010  \n- Near-back: 0011\n- Back: 0100 \n- Near-central: 0101\n\nRoundness (4 bits):\n- Unrounded: 0000  \n- Rounded: 0001\n- Compressed: 0010\n- Protruded: 0011\n- More rounded: 0100\n- Less rounded: 0101\n\nTenseness  (3 bits):\n- Extra-short: 000 \n- Short: 001\n- Long: 010  \n- Overlong: 011\n\nNasalization (2 bits):\n- Oral: 00\n- Nasal: 01\n- Nasalized: 10\n- Denasalized: 11\n\nSecondary articulations, prosodic features,  diacritics and delimiters will also be included with variable bit encoding. Morphological information  can be  added as an additional layer of information.\n\nDelimiters:\nMorpheme boundary (+)\nWord boundary (#)\nSyllable boundary (-)\nStress boundary (*)\nIntonation boundary ( ^ )\nProcess boundary (%)\n\nEncoding:\n\n- Analyze each sound into its featural components using the IPA chart and the feature values given in the question.\n- Arrange the features in a consistent order for consonants (airstream mechanism \u2192 manner \u2192 place \u2192 phonation \u2192 coarticulation)  and vowels (height \u2192 backness \u2192 roundness  \u2192 nasalization \u2192 length).\n- Concatenate the binary codes for each feature to form the binary code for each sound.\n- Concatenate the binary codes for each sound to form the binary code for each word.\n- Use delimiters to mark boundaries between words, syllables and morphemes.\n\nDecoding:\n\n- Use delimiters to identify boundaries between words, syllables and morphemes.\n- Split the binary code for each word into segments of variable length based on the number of bits for each feature.\n- Identify the feature values for each segment using the feature values given in the question.\n- Identify the sound corresponding to each segment using the IPA chart and the feature values.\n- Write the sounds using IPA symbols or orthographic symbols.\n\nExample:\n\n\"Hello World\"\n\nEncoding:\n\n- Analyze each sound into its featural components:\n\n  - /h/: pulmonic egressive, fricative, glottal, voiceless, no coarticulation\n  - /\u025b/: open-mid, front, unrounded, oral, short\n  - /l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation\n  - /o\u028a/: close-mid, back, rounded, oral, long\n  - /w/: pulmonic egressive, approximant, labial-velar, voiced, no coarticulation\n  - /\u025c\u02d0/: mid, central, unrounded, oral, overlong\n  - /l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation\n  - /d/: pulmonic egressive, plosive, alveolar, voiced, no coarticulation\n\n- Arrange the features in a consistent order and concatenate the binary codes for each feature:\n\n  - /h/: 000001000010100000000000\n  - /\u025b/: 010000000000000001\n  - /l/: 0000010110000110000100000\n  - /o\u028a/: 001001010001000010\n  - /w/: 0000010100111101000100000\n  - /\u025c\u02d0/: 001100010000000011\n  - /l/: 0000010110000110000100000\n  - /d/: 0000000000011001000100000\n\n- Concatenate the binary codes for each sound and use delimiters to mark boundaries:\n\n  - Hello: 000001000010100000000000#010000000000000001#-#0000010110000110000100000#001001010001000010#\n  - World: 0000010100111101000100000#-#001100010000000011#-#0000010110000110000100000#-#0000000000011001000100000#\n\nDecoding:\n\n- Identify the boundaries between words, syllables and morphemes using the delimiters:\n\n  - Hello: [h][\u025b]-[l][o\u028a]\n  - World: [w]-[\u025c\u02d0]-[l]-[d]\n\n- Divide the binary code for each word into chunks of bits corresponding to each feature:\n\n  - Hello: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[4][4][4][2][3]\n    - [h]: [000] [00] [010] [1000] [00000]\n    - [\u025b]: [0100] [0000] [0000] [00] [001]\n    - [l]: [000] [01] [1000] [0110] [00010]\n    - [o\u028a]:[0010] [0100] [0100] [00] [010]\n  - World: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[3][5][6][4][5]\n    - [w]: [000] [01] [0011] [1101] [00010]\n    - [\u025c\u02d0]:[0011] [0001] [0000] [00] [011]\n    - [l]: [000] [01] [1000] [0110] [00010]\n    - [d]: [000] [00] [0001] [1001] [00010]\n\n- Match each chunk of bits with the feature value given in the question:\n\n  - Hello:\n    - /h/: pulmonic egressive (000), fricative (00), glottal (010), voiceless (1000), no coarticulation (00000)\n    - /\u025b/: open-mid (0100), front (0000), unrounded (0000), oral (00), short (001)\n    - /l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)\n    - /o\u028a/: close-mid (0010), back (0100), rounded (0100), oral (00), long (010)\n  - World:\n    - /w/: pulmonic egressive (000), approximant (01), labial-velar (0011), voiced (1101), no coarticulation (00010)\n    - /\u025c\u02d0/: mid (0011), central (0001), unrounded (0000), oral (00), overlong (011)\n    - /l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)\n    - /d/: pulmonic egressive (000), plosive (00), alveolar (0001), voiced (1001), no coarticulation (00010)\n\n- Find the sound that corresponds to each combination of feature values using the IPA chart and the feature values:\n\n  - Hello: /h/ /\u025b/ /l/ /o\u028a/\n  - World: /w/ /\u025c\u02d0/ /l/ /d/\n\n- Write the sounds using IPA symbols or orthographic symbols:\n\n  - Hello: /h\u025blo\u028a/ or hello\n  - World: /w\u025c\u02d0ld/ or world", "author_fullname": "t2_d1l6mocox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Experimental Phonetics to Binary Encoding System", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145y56v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686397622.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686397432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This encoding system claim that it can state all IPA symbols and its pronounciation in a Binary format &lt;/p&gt;\n\n&lt;p&gt;PhonBinary Encoding System V6&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Analyzes each sound into its featural components to translate into binary. &lt;/li&gt;\n&lt;li&gt;Features are encoded using a variable number of bits based on the number of possibilities for each feature.&lt;/li&gt;\n&lt;li&gt;Enables representation of the full IPA  to use in various NLP applications. &lt;/li&gt;\n&lt;li&gt;Includes all manners of articulation, secondary articulations, prosodic features, airstream mechanisms and vowel qualities.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sound Analysis:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The first step involves analyzing each sound into its featural components. &lt;/li&gt;\n&lt;li&gt;Full understanding of the IPA and phonetic terminology required.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Consonants:\n- Produced by modifying airstream with articulators. \n- Factors: airstream mechanism (3 bits), manner of articulation (5 bits), place of articulation (6 bits), phonation (4 bits), coarticulation (5 bits).&lt;/p&gt;\n\n&lt;p&gt;Airstream mechanism (3 bits):\n- Pulmonic egressive: 000 \n- Pulmonic ingressive: 001\n- Glottalic egressive: 010\n- Glottalic ingressive: 011\n- Lingual-glottalic: 100\n- Velaric: 101 &lt;/p&gt;\n\n&lt;p&gt;Manner of articulation (5 bits):\n- Plosive: 00000&lt;br/&gt;\n- Nasal: 00001\n- Trill: 00010\n- Tap: 00011\n- Fricative: 00100\n- Approximant: 00101\n- Lateral approximant: 00110\n- Flap: 00111\n- Affricate: 01000 \n- Clitic: 01001&lt;br/&gt;\n- Implosive  01010\n- Ejective: 01011\n- Click: 01100&lt;/p&gt;\n\n&lt;p&gt;Place of articulation (6 bits): \n- Bilabial: 000000 \n- Labiodental: 000001\n- Interdental: 000010\n- Alveolar: 000011 \n- Postalveolar: 000100\n- Retroflex: 000101\n- Palatal: 000110\n- Velar: 000111 \n- Uvular: 001000\n- Pharyngeal: 001001&lt;br/&gt;\n- Glottal: 001010 \n- Epiglottal: 001011\n- Labial-palatal: 001100\n- Labial-velar: 001101 \n- Labial-uvular: 001110\n- Velar-pharyngeal: 001111\n- Velar-uvular: 010000&lt;br/&gt;\n- Velar-epilaryngeal: 010001\n- Uvular-pharyngeal : 010010&lt;/p&gt;\n\n&lt;p&gt;Phonation (4 bits):\n- Voiceless: 0000\n- Voiced: 0001&lt;br/&gt;\n- Creaky: 0010 \n- Breathy: 0011\n- Slack: 0100\n- Stiff: 0101\n- Creaky voiced: 0110\n- Whispered: 0111&lt;br/&gt;\n- Harsh: 1000 \n- Laryngealized : 1001\n- Closed glottis: 1010\n- Velaric aspiration: 1011&lt;/p&gt;\n\n&lt;p&gt;Vowels:\n- Produced by shaping airstream resonated through oral and nasal cavities. \n- Factors: height (4 bits), backness (4 bits), roundness (4 bits), nasalization (2 bits), length (3 bits). &lt;/p&gt;\n\n&lt;p&gt;Height (4 bits):\n- Close: 0000\n- Near-close: 0001&lt;br/&gt;\n- Close-mid: 0010&lt;br/&gt;\n- Mid: 0011\n- Open-mid: 0100 \n- Near-open: 0101&lt;br/&gt;\n- Open: 0110&lt;/p&gt;\n\n&lt;p&gt;Backness(4 bits):\n- Front: 0000\n- Near-front: 0001\n- Central: 0010&lt;br/&gt;\n- Near-back: 0011\n- Back: 0100 \n- Near-central: 0101&lt;/p&gt;\n\n&lt;p&gt;Roundness (4 bits):\n- Unrounded: 0000&lt;br/&gt;\n- Rounded: 0001\n- Compressed: 0010\n- Protruded: 0011\n- More rounded: 0100\n- Less rounded: 0101&lt;/p&gt;\n\n&lt;p&gt;Tenseness  (3 bits):\n- Extra-short: 000 \n- Short: 001\n- Long: 010&lt;br/&gt;\n- Overlong: 011&lt;/p&gt;\n\n&lt;p&gt;Nasalization (2 bits):\n- Oral: 00\n- Nasal: 01\n- Nasalized: 10\n- Denasalized: 11&lt;/p&gt;\n\n&lt;p&gt;Secondary articulations, prosodic features,  diacritics and delimiters will also be included with variable bit encoding. Morphological information  can be  added as an additional layer of information.&lt;/p&gt;\n\n&lt;p&gt;Delimiters:\nMorpheme boundary (+)\nWord boundary (#)\nSyllable boundary (-)\nStress boundary (*)\nIntonation boundary ( ^ )\nProcess boundary (%)&lt;/p&gt;\n\n&lt;p&gt;Encoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Analyze each sound into its featural components using the IPA chart and the feature values given in the question.&lt;/li&gt;\n&lt;li&gt;Arrange the features in a consistent order for consonants (airstream mechanism \u2192 manner \u2192 place \u2192 phonation \u2192 coarticulation)  and vowels (height \u2192 backness \u2192 roundness  \u2192 nasalization \u2192 length).&lt;/li&gt;\n&lt;li&gt;Concatenate the binary codes for each feature to form the binary code for each sound.&lt;/li&gt;\n&lt;li&gt;Concatenate the binary codes for each sound to form the binary code for each word.&lt;/li&gt;\n&lt;li&gt;Use delimiters to mark boundaries between words, syllables and morphemes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Decoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use delimiters to identify boundaries between words, syllables and morphemes.&lt;/li&gt;\n&lt;li&gt;Split the binary code for each word into segments of variable length based on the number of bits for each feature.&lt;/li&gt;\n&lt;li&gt;Identify the feature values for each segment using the feature values given in the question.&lt;/li&gt;\n&lt;li&gt;Identify the sound corresponding to each segment using the IPA chart and the feature values.&lt;/li&gt;\n&lt;li&gt;Write the sounds using IPA symbols or orthographic symbols.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Hello World&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Encoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Analyze each sound into its featural components:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/h/: pulmonic egressive, fricative, glottal, voiceless, no coarticulation&lt;/li&gt;\n&lt;li&gt;/\u025b/: open-mid, front, unrounded, oral, short&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation&lt;/li&gt;\n&lt;li&gt;/o\u028a/: close-mid, back, rounded, oral, long&lt;/li&gt;\n&lt;li&gt;/w/: pulmonic egressive, approximant, labial-velar, voiced, no coarticulation&lt;/li&gt;\n&lt;li&gt;/\u025c\u02d0/: mid, central, unrounded, oral, overlong&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation&lt;/li&gt;\n&lt;li&gt;/d/: pulmonic egressive, plosive, alveolar, voiced, no coarticulation&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Arrange the features in a consistent order and concatenate the binary codes for each feature:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/h/: 000001000010100000000000&lt;/li&gt;\n&lt;li&gt;/\u025b/: 010000000000000001&lt;/li&gt;\n&lt;li&gt;/l/: 0000010110000110000100000&lt;/li&gt;\n&lt;li&gt;/o\u028a/: 001001010001000010&lt;/li&gt;\n&lt;li&gt;/w/: 0000010100111101000100000&lt;/li&gt;\n&lt;li&gt;/\u025c\u02d0/: 001100010000000011&lt;/li&gt;\n&lt;li&gt;/l/: 0000010110000110000100000&lt;/li&gt;\n&lt;li&gt;/d/: 0000000000011001000100000&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Concatenate the binary codes for each sound and use delimiters to mark boundaries:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: 000001000010100000000000#010000000000000001#-#0000010110000110000100000#001001010001000010#&lt;/li&gt;\n&lt;li&gt;World: 0000010100111101000100000#-#001100010000000011#-#0000010110000110000100000#-#0000000000011001000100000#&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Decoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Identify the boundaries between words, syllables and morphemes using the delimiters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: [h][\u025b]-[l][o\u028a]&lt;/li&gt;\n&lt;li&gt;World: [w]-[\u025c\u02d0]-[l]-[d]&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Divide the binary code for each word into chunks of bits corresponding to each feature:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[4][4][4][2][3]&lt;/li&gt;\n&lt;li&gt;[h]: [000] [00] [010] [1000] [00000]&lt;/li&gt;\n&lt;li&gt;[\u025b]: [0100] [0000] [0000] [00] [001]&lt;/li&gt;\n&lt;li&gt;[l]: [000] [01] [1000] [0110] [00010]&lt;/li&gt;\n&lt;li&gt;[o\u028a]:[0010] [0100] [0100] [00] [010]&lt;/li&gt;\n&lt;li&gt;World: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[3][5][6][4][5]&lt;/li&gt;\n&lt;li&gt;[w]: [000] [01] [0011] [1101] [00010]&lt;/li&gt;\n&lt;li&gt;[\u025c\u02d0]:[0011] [0001] [0000] [00] [011]&lt;/li&gt;\n&lt;li&gt;[l]: [000] [01] [1000] [0110] [00010]&lt;/li&gt;\n&lt;li&gt;[d]: [000] [00] [0001] [1001] [00010]&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Match each chunk of bits with the feature value given in the question:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello:&lt;/li&gt;\n&lt;li&gt;/h/: pulmonic egressive (000), fricative (00), glottal (010), voiceless (1000), no coarticulation (00000)&lt;/li&gt;\n&lt;li&gt;/\u025b/: open-mid (0100), front (0000), unrounded (0000), oral (00), short (001)&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)&lt;/li&gt;\n&lt;li&gt;/o\u028a/: close-mid (0010), back (0100), rounded (0100), oral (00), long (010)&lt;/li&gt;\n&lt;li&gt;World:&lt;/li&gt;\n&lt;li&gt;/w/: pulmonic egressive (000), approximant (01), labial-velar (0011), voiced (1101), no coarticulation (00010)&lt;/li&gt;\n&lt;li&gt;/\u025c\u02d0/: mid (0011), central (0001), unrounded (0000), oral (00), overlong (011)&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)&lt;/li&gt;\n&lt;li&gt;/d/: pulmonic egressive (000), plosive (00), alveolar (0001), voiced (1001), no coarticulation (00010)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Find the sound that corresponds to each combination of feature values using the IPA chart and the feature values:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: /h/ /\u025b/ /l/ /o\u028a/&lt;/li&gt;\n&lt;li&gt;World: /w/ /\u025c\u02d0/ /l/ /d/&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Write the sounds using IPA symbols or orthographic symbols:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: /h\u025blo\u028a/ or hello&lt;/li&gt;\n&lt;li&gt;World: /w\u025c\u02d0ld/ or world&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145y56v", "is_robot_indexable": true, "report_reasons": null, "author": "B-acccount", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145y56v/my_experimental_phonetics_to_binary_encoding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145y56v/my_experimental_phonetics_to_binary_encoding/", "subreddit_subscribers": 922013, "created_utc": 1686397432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing FeatureEng: A Community for Feature Engineering Enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145oag6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_cvej3qatd", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "FeatureEng", "selftext": "Hey everyone, I am Gxav. I used to be very active on Kaggle [https://www.kaggle.com/xavierconort](https://www.kaggle.com/xavierconort) and I owe my Kaggle GrandMaster title to feature engineering. Building skills in feature engineering is an ongoing journey and I am really missing my Kaggle days where I could learn new tricks from my fellow Kagglers.\n\nI started the FeatureEng community because I couldn't find any communities specifically focused on feature engineering, which I believe deserves its own dedicated space. I hope this community will be a place where you and I will find our new sources of inspiration!\n\nCheers,\n\nGxav", "author_fullname": "t2_cvej3qatd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing FeatureEng: A Community for Feature Engineering Enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/FeatureEng", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145nb4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686361712.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.FeatureEng", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am Gxav. I used to be very active on Kaggle &lt;a href=\"https://www.kaggle.com/xavierconort\"&gt;https://www.kaggle.com/xavierconort&lt;/a&gt; and I owe my Kaggle GrandMaster title to feature engineering. Building skills in feature engineering is an ongoing journey and I am really missing my Kaggle days where I could learn new tricks from my fellow Kagglers.&lt;/p&gt;\n\n&lt;p&gt;I started the FeatureEng community because I couldn&amp;#39;t find any communities specifically focused on feature engineering, which I believe deserves its own dedicated space. I hope this community will be a place where you and I will find our new sources of inspiration!&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Gxav&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_8kc7h0", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145nb4g", "is_robot_indexable": true, "report_reasons": null, "author": "Gxav73", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "subreddit_subscribers": 15, "created_utc": 1686361712.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686364639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.FeatureEng", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145oag6", "is_robot_indexable": true, "report_reasons": null, "author": "Gxav73", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_145nb4g", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145oag6/introducing_featureeng_a_community_for_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "subreddit_subscribers": 922013, "created_utc": 1686364639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Anyone have experience interviewing at cruise? Specifically looking for experience with the live python coding piece. I have plenty of experience, just looking for resources or level of difficulty so I can study properly. Gracias fam!", "author_fullname": "t2_9d3tunn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cruise python technical interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145goxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686344469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience interviewing at cruise? Specifically looking for experience with the live python coding piece. I have plenty of experience, just looking for resources or level of difficulty so I can study properly. Gracias fam!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145goxl", "is_robot_indexable": true, "report_reasons": null, "author": "IllPoem4426", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145goxl/cruise_python_technical_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145goxl/cruise_python_technical_interview/", "subreddit_subscribers": 922013, "created_utc": 1686344469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ajskcqg7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - H4NM/Groppy: Facilitating regex creation and deploying custom grok patterns in an ELK environment \ud83e\udd8c\ud83d\udcdc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_145gd6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ye34yg6kbY7LbF7SF6nWRaiqYevZUoJaQ-qiRvUiqQQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686343690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/H4NM/Groppy", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?auto=webp&amp;v=enabled&amp;s=47a883acbbc6cfcb1ad03b617ff5d6148b9c03b4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=137f09c0b56404e7391a3e00960672e44546f560", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2192a1761052879ed3d684097620f0c931bfa269", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0284861e9739b5324448a15e56c9d35e9cb8a51", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2b106400924e31522dc736e5ac00dc2df536aca", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343d458647e02212080ae14aaca929dcf05e0cd8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57cf8c00930eaa4468530c17c02e73206f7da4fa", "width": 1080, "height": 540}], "variants": {}, "id": "sXOGV_7LAGwkwaJUdEH5X0q6us7QJfYTKogbfMSh8EM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "145gd6z", "is_robot_indexable": true, "report_reasons": null, "author": "73637269707420", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145gd6z/github_h4nmgroppy_facilitating_regex_creation_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/H4NM/Groppy", "subreddit_subscribers": 922013, "created_utc": 1686343690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was wondering if there was a good way to simulate sales data (like for a retail store or a restaurant) that could be analyzed for how time of day/season/etc affects what is sold\n\n(Alternatively, are there any companies that happen to share this info online? I assume not but worth a shot haha)", "author_fullname": "t2_bxa84d34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simulating sales data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145fptb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686342168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if there was a good way to simulate sales data (like for a retail store or a restaurant) that could be analyzed for how time of day/season/etc affects what is sold&lt;/p&gt;\n\n&lt;p&gt;(Alternatively, are there any companies that happen to share this info online? I assume not but worth a shot haha)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145fptb", "is_robot_indexable": true, "report_reasons": null, "author": "_new_name_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145fptb/simulating_sales_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145fptb/simulating_sales_data/", "subreddit_subscribers": 922013, "created_utc": 1686342168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\nI'm a 3rd year undergrad student. I want to have a career in data science. I've done some machine learning projects in Python, and plan to learn SQL and AWS in summer while also doing more projects. \n\nI have a very limited budget that I want to invest in myself for my education: literally 35 dollars LOL. It's not a lot, but I want to buy/subscribe to something with it if possible. What kind of online education websites can I subscribe to except Udemy courses? I've thought about getting a premium membership for Leetcode, but is it really beneficial? I don't really feel the need to watch videos to understand code solutions and I don't know if it offers anything more useful. My budget is also not enough for the crash course it offers.\n\nAny help/insight is appreciated!", "author_fullname": "t2_vqbr3noo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to invest in myself as a math undergrad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1466nvf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686419851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nI&amp;#39;m a 3rd year undergrad student. I want to have a career in data science. I&amp;#39;ve done some machine learning projects in Python, and plan to learn SQL and AWS in summer while also doing more projects. &lt;/p&gt;\n\n&lt;p&gt;I have a very limited budget that I want to invest in myself for my education: literally 35 dollars LOL. It&amp;#39;s not a lot, but I want to buy/subscribe to something with it if possible. What kind of online education websites can I subscribe to except Udemy courses? I&amp;#39;ve thought about getting a premium membership for Leetcode, but is it really beneficial? I don&amp;#39;t really feel the need to watch videos to understand code solutions and I don&amp;#39;t know if it offers anything more useful. My budget is also not enough for the crash course it offers.&lt;/p&gt;\n\n&lt;p&gt;Any help/insight is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1466nvf", "is_robot_indexable": true, "report_reasons": null, "author": "kirmi_zek", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1466nvf/how_to_invest_in_myself_as_a_math_undergrad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1466nvf/how_to_invest_in_myself_as_a_math_undergrad/", "subreddit_subscribers": 922013, "created_utc": 1686419851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone, I'm looking how could I encrypt and decrypt SHA-512 password in R  \nDo you have any documentation about it ?", "author_fullname": "t2_gnhzyiiu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encrypt and Decrypt in R", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1465k3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686417062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m looking how could I encrypt and decrypt SHA-512 password in R&lt;br/&gt;\nDo you have any documentation about it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1465k3q", "is_robot_indexable": true, "report_reasons": null, "author": "hlama26", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1465k3q/encrypt_and_decrypt_in_r/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1465k3q/encrypt_and_decrypt_in_r/", "subreddit_subscribers": 922013, "created_utc": 1686417062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI tools for Excel/Google Sheets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1465gt8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2u5c1o7i", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "ChatGPT", "selftext": "While vanilla ChatGPT is powerful, it's just not quite there as a spreadsheet tool. However, I've noticed quite a few AI tools powered by GPT recently that integrate with Excel or Google Sheets so I've spent the better half of this week trying them out. Here's my rundown on several of what I thought were the most useful tools:\n\n# Coefficient - Google Sheets\n\nThis one is a free extension and it works as a Google Sheets Add-on. Essentially there's several prompts you can use to use ChatGPT within your spreadsheet itself- I've listed a few examples below:\n\n    // Generates text using GPT\n    =GPTX(prompt) \n    \n    // Generates table of values based on prompt and headers\n    =GPTX_TABLE(prompt, [header_row])\n    \n    // Classifies text according to given labels\n    =GPTX_CLASSIFY(text, labels)\n    \n    // Converts text into a specific text format\n    =GPTX_FORMAT(text, language)\n    \n    // Extracts info from text\n    =GPTX_EXTRACT(text, info_to_extract) \n\nAlong with the prompts/formulas, Coefficient can also generate formulas, pivot tables, and charts for you from natural language. For example I can ask it to create a pie chart that shows the distribution of my expenses, or pviot tables based on what information I want to organize/summarize.\n\n# Flowshot.ai - Google Sheets\n\nLike Coefficient, you can use GPT inside your spreadsheet however it also allows you to autocomplete/fill cells using AI, without needing to use formulas. It also allows you to train custom AI models using your own spreadsheet data and use it for the autocomplete feature or deploy it with Zapier or API. This one is also an add-on for Google Sheets and you get 100k characters for free.\n\n# SheetAI - Google Sheets\n\nAlternative to Coefficient for using GPT inside your spreadsheet. Available as Google Sheets add-on and you get 50 free generations a month, however you have to use your own OpenAI API key.\n\n# GPT for Sheets and Docs\n\nAnother great alternative to Coefficient. Note this one works with Google Docs as well as a writing co-pilot. It's a free add-on however you do have to use your own API key like SheetAI.\n\n# AskCSV - CSV and TSV files\n\nA free web app you can use to chat with your dataset and gain insights/analysis. As a very simple example, if I had a dataset of survey results with educational attainment and income, I could ask the chatbot to plot something where it compares the Income by Education and tell me insights about this. It will also give you questions and ideas to explore your data further. It's available as a web app and you get 5 generations a day for free. ***According to their website*** the data storage and analysis is only done on your browser and not sent to a server.\n\n# Zia - Zoho Sheet\n\nZia comes with Zoho Sheet and is basically an AI assistant for your data. You can use it to generate charts, pivot tables, and use it for data analysis questions. Works as a web and mobile app however you or your organization needs to be on a WorkDrive plan to use it.\n\n# Excelformulabot - Excel &amp; Google Sheets\n\nExcellent formula generator that does VBA, Regex, and Google Apps scripts as well. You could use ChatGPT to do this however it makes generating formulas a lot faster since they have preset prompts and you don't have to switch tabs. It's available as a web app as well as Excel &amp; Google Sheets add-on. It's a paid service but you get a limited number of generations for free each month.\n\n# Ajelix - Excel &amp; Google Sheets\n\nThis one is like Excelformulabot on steroids- except it can also generate entire Excel spreadsheets based on your description. Works both as a web app and plugin for Sheets &amp; Excel and has a free plan with limited generations.\n\n# Arcwise - Google Sheets\n\nGreat overall AI assistant for Google sheets. Like Excelformulabot and Ajelix, it functions as a formula copilot however it has a simple built in function to clean/extract data. However what really set it apart for me is its ability to answer questions regarding a spreadsheet. For example, you can ask it to explain outputs and calculation interdependencies. This one is a free Chrome extension.\n\nDon't want to sound like a broken record- but use these tools with sensitive data at your own risk. If your using this for work obviously be smart about it.\n\n\\---\n\n**P.S. If you liked this,** I've created a [free directory](https://aiscout.net/) with over 1000 AI tools listed for almost any use case. It's updated daily and there's also a GPT-powered chatbot to help you find AI tools for your needs. Feel free to check it out if there's something specific you are looking for.", "author_fullname": "t2_a12an6q01", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Tools to use GPT with Excel/Google Sheets", "link_flair_richtext": [{"e": "text", "t": "Resources "}], "subreddit_name_prefixed": "r/ChatGPT", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1459tt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1117, "total_awards_received": 4, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources ", "can_mod_post": false, "score": 1117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686328111.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ChatGPT", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While vanilla ChatGPT is powerful, it&amp;#39;s just not quite there as a spreadsheet tool. However, I&amp;#39;ve noticed quite a few AI tools powered by GPT recently that integrate with Excel or Google Sheets so I&amp;#39;ve spent the better half of this week trying them out. Here&amp;#39;s my rundown on several of what I thought were the most useful tools:&lt;/p&gt;\n\n&lt;h1&gt;Coefficient - Google Sheets&lt;/h1&gt;\n\n&lt;p&gt;This one is a free extension and it works as a Google Sheets Add-on. Essentially there&amp;#39;s several prompts you can use to use ChatGPT within your spreadsheet itself- I&amp;#39;ve listed a few examples below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;// Generates text using GPT\n=GPTX(prompt) \n\n// Generates table of values based on prompt and headers\n=GPTX_TABLE(prompt, [header_row])\n\n// Classifies text according to given labels\n=GPTX_CLASSIFY(text, labels)\n\n// Converts text into a specific text format\n=GPTX_FORMAT(text, language)\n\n// Extracts info from text\n=GPTX_EXTRACT(text, info_to_extract) \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Along with the prompts/formulas, Coefficient can also generate formulas, pivot tables, and charts for you from natural language. For example I can ask it to create a pie chart that shows the distribution of my expenses, or pviot tables based on what information I want to organize/summarize.&lt;/p&gt;\n\n&lt;h1&gt;Flowshot.ai - Google Sheets&lt;/h1&gt;\n\n&lt;p&gt;Like Coefficient, you can use GPT inside your spreadsheet however it also allows you to autocomplete/fill cells using AI, without needing to use formulas. It also allows you to train custom AI models using your own spreadsheet data and use it for the autocomplete feature or deploy it with Zapier or API. This one is also an add-on for Google Sheets and you get 100k characters for free.&lt;/p&gt;\n\n&lt;h1&gt;SheetAI - Google Sheets&lt;/h1&gt;\n\n&lt;p&gt;Alternative to Coefficient for using GPT inside your spreadsheet. Available as Google Sheets add-on and you get 50 free generations a month, however you have to use your own OpenAI API key.&lt;/p&gt;\n\n&lt;h1&gt;GPT for Sheets and Docs&lt;/h1&gt;\n\n&lt;p&gt;Another great alternative to Coefficient. Note this one works with Google Docs as well as a writing co-pilot. It&amp;#39;s a free add-on however you do have to use your own API key like SheetAI.&lt;/p&gt;\n\n&lt;h1&gt;AskCSV - CSV and TSV files&lt;/h1&gt;\n\n&lt;p&gt;A free web app you can use to chat with your dataset and gain insights/analysis. As a very simple example, if I had a dataset of survey results with educational attainment and income, I could ask the chatbot to plot something where it compares the Income by Education and tell me insights about this. It will also give you questions and ideas to explore your data further. It&amp;#39;s available as a web app and you get 5 generations a day for free. &lt;strong&gt;&lt;em&gt;According to their website&lt;/em&gt;&lt;/strong&gt; the data storage and analysis is only done on your browser and not sent to a server.&lt;/p&gt;\n\n&lt;h1&gt;Zia - Zoho Sheet&lt;/h1&gt;\n\n&lt;p&gt;Zia comes with Zoho Sheet and is basically an AI assistant for your data. You can use it to generate charts, pivot tables, and use it for data analysis questions. Works as a web and mobile app however you or your organization needs to be on a WorkDrive plan to use it.&lt;/p&gt;\n\n&lt;h1&gt;Excelformulabot - Excel &amp;amp; Google Sheets&lt;/h1&gt;\n\n&lt;p&gt;Excellent formula generator that does VBA, Regex, and Google Apps scripts as well. You could use ChatGPT to do this however it makes generating formulas a lot faster since they have preset prompts and you don&amp;#39;t have to switch tabs. It&amp;#39;s available as a web app as well as Excel &amp;amp; Google Sheets add-on. It&amp;#39;s a paid service but you get a limited number of generations for free each month.&lt;/p&gt;\n\n&lt;h1&gt;Ajelix - Excel &amp;amp; Google Sheets&lt;/h1&gt;\n\n&lt;p&gt;This one is like Excelformulabot on steroids- except it can also generate entire Excel spreadsheets based on your description. Works both as a web app and plugin for Sheets &amp;amp; Excel and has a free plan with limited generations.&lt;/p&gt;\n\n&lt;h1&gt;Arcwise - Google Sheets&lt;/h1&gt;\n\n&lt;p&gt;Great overall AI assistant for Google sheets. Like Excelformulabot and Ajelix, it functions as a formula copilot however it has a simple built in function to clean/extract data. However what really set it apart for me is its ability to answer questions regarding a spreadsheet. For example, you can ask it to explain outputs and calculation interdependencies. This one is a free Chrome extension.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t want to sound like a broken record- but use these tools with sensitive data at your own risk. If your using this for work obviously be smart about it.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S. If you liked this,&lt;/strong&gt; I&amp;#39;ve created a &lt;a href=\"https://aiscout.net/\"&gt;free directory&lt;/a&gt; with over 1000 AI tools listed for almost any use case. It&amp;#39;s updated daily and there&amp;#39;s also a GPT-powered chatbot to help you find AI tools for your needs. Feel free to check it out if there&amp;#39;s something specific you are looking for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?auto=webp&amp;v=enabled&amp;s=04cdf591f6815ce8deb90a3e4ffae7ba454d249e", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20000c576ef7c106e8e5cc3b02b566ec45759e35", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=868b10488dddd77106d6ee0bbc4bb6499de1a64f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=270984fa58d1a57a90df9ff788cf753e41cc8a1a", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0baa55f84564a184a3acdd93672bf7aff92a3b60", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efaeb9e2f744d1f2c4359b05590e6ca40b001291", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cec84d601c33a898248e8ca14254c973370514b1", "width": 1080, "height": 607}], "variants": {}, "id": "A-eQ3J32nUFfJaKEDoNNYiT2VDRppiycWWRtZ-chcq0"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_b4ff447e-05a5-42dc-9002-63568807cfe6", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A glowing commendation for all to see", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "All-Seeing Upvote", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=25880d00e4283ff6a3851b64c83fea465c3fac48", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=67fae0c2af26488b9d70cb7afe877086707cf1d1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=3033c6583809a27b998883b7c56c158102cb0420", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=cccb48d7518eaba72894a7ac4214bb70c7120793", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=4101e0eff424bbd818195853387db358bec74ed0", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_a67d649d-5aa5-407e-a98b-32fd9e3a9696", "penny_donate": null, "award_sub_type": "APPRECIATION", "coin_reward": 100, "icon_url": "https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=b0fc6c7d2285e538ecae47fdbbe9772fdbc3a282", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=dbffe82e3fce6160d908e52dcd74bfa77e9fcd63", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=5d33f2655adcc8b1b31e18fed59d6ac9394baf2c", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=f58ebd214a9668ad9c2d680ce396c2f209bbb37a", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=549c7e4359a14fe2877e91162c3f1941fc8c6711", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "The more you know... Gives %{coin_symbol}100 Coins to both the author and the community.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 100, "count": 1, "static_icon_height": 2048, "name": "Today I Learned", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=b0fc6c7d2285e538ecae47fdbbe9772fdbc3a282", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=dbffe82e3fce6160d908e52dcd74bfa77e9fcd63", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=5d33f2655adcc8b1b31e18fed59d6ac9394baf2c", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=f58ebd214a9668ad9c2d680ce396c2f209bbb37a", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=549c7e4359a14fe2877e91162c3f1941fc8c6711", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "c8a7075c-9962-11ed-86d0-b2c48b46bf22", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_7hqomg", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#373c3f", "id": "1459tt1", "is_robot_indexable": true, "report_reasons": null, "author": "AI_Scout_Official", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ChatGPT/comments/1459tt1/tools_to_use_gpt_with_excelgoogle_sheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/ChatGPT/comments/1459tt1/tools_to_use_gpt_with_excelgoogle_sheets/", "subreddit_subscribers": 2008011, "created_utc": 1686328111.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1686416830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ChatGPT", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/ChatGPT/comments/1459tt1/tools_to_use_gpt_with_excelgoogle_sheets/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?auto=webp&amp;v=enabled&amp;s=04cdf591f6815ce8deb90a3e4ffae7ba454d249e", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20000c576ef7c106e8e5cc3b02b566ec45759e35", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=868b10488dddd77106d6ee0bbc4bb6499de1a64f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=270984fa58d1a57a90df9ff788cf753e41cc8a1a", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0baa55f84564a184a3acdd93672bf7aff92a3b60", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efaeb9e2f744d1f2c4359b05590e6ca40b001291", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/blYDgdvS6ArVk1sdgBtW4GHJScIzVRYSiWku_qVF7Gw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cec84d601c33a898248e8ca14254c973370514b1", "width": 1080, "height": 607}], "variants": {}, "id": "A-eQ3J32nUFfJaKEDoNNYiT2VDRppiycWWRtZ-chcq0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1465gt8", "is_robot_indexable": true, "report_reasons": null, "author": "safwanadnan19", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1459tt1", "author_flair_text_color": null, "permalink": "/r/datascience/comments/1465gt8/ai_tools_for_excelgoogle_sheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/ChatGPT/comments/1459tt1/tools_to_use_gpt_with_excelgoogle_sheets/", "subreddit_subscribers": 922013, "created_utc": 1686416830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Check it out.   \n[The Data Trend](https://twitter.com/The_Data_Trend)", "author_fullname": "t2_b0yxowhs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Found this Twitter page for Data Science and Machine Learning news, memes and updates.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1464ed0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686414183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check it out.&lt;br/&gt;\n&lt;a href=\"https://twitter.com/The_Data_Trend\"&gt;The Data Trend&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mweMiAud3dRpl_1hGIw-ngmTx1YtB_XjmdNPdcUdy_U.jpg?auto=webp&amp;v=enabled&amp;s=b317b757dd20c1540a217fbc014b11be74d9ce21", "width": 48, "height": 48}, "resolutions": [], "variants": {}, "id": "Ppn8lQdRHSLk2n7yFevfF4XxKFit9zpleSrj5_PJxTo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1464ed0", "is_robot_indexable": true, "report_reasons": null, "author": "Roomour5", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1464ed0/found_this_twitter_page_for_data_science_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1464ed0/found_this_twitter_page_for_data_science_and/", "subreddit_subscribers": 922013, "created_utc": 1686414183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Recently, I conducted research and developed an exploratory data and sentiment analysis from Reddit in the r/movies subreddit. It took me several hours, but I believe the results are good!  \n\nPlease check out my notebook on Kaggle (link: [https://www.kaggle.com/code/curiel/r-movies-eda-sa](https://www.kaggle.com/code/curiel/r-movies-eda-sa)) and consider upvoting, analyzing, and providing feedback. All comments are welcome \ud83d\udc9c.  \n\nI compared how the posts and comments are organized based on tags, hour, and sentiment. It's beneficial because we can observe the sentiment of the post authors through tags and their corresponding comments, and analyze potential correlations.  \n\nIf you find this interesting, please consider sharing or linking it! Your support would be greatly appreciated and will help make this notebook accessible to a wider audience \ud83d\udc9c.  \n\n&amp;#x200B;\n\nDataset used: [https://www.kaggle.com/datasets/curiel/rmovies-posts-and-comments](https://www.kaggle.com/datasets/curiel/rmovies-posts-and-comments) \n\nScript that feeds and updates the dataset: [https://www.kaggle.com/code/curiel/update-r-movies-dataset](https://www.kaggle.com/code/curiel/update-r-movies-dataset)", "author_fullname": "t2_86qi4nrq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploratory Data and Sentiment Analysis of r/movies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1462i6f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686409387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, I conducted research and developed an exploratory data and sentiment analysis from Reddit in the &lt;a href=\"/r/movies\"&gt;r/movies&lt;/a&gt; subreddit. It took me several hours, but I believe the results are good!  &lt;/p&gt;\n\n&lt;p&gt;Please check out my notebook on Kaggle (link: &lt;a href=\"https://www.kaggle.com/code/curiel/r-movies-eda-sa\"&gt;https://www.kaggle.com/code/curiel/r-movies-eda-sa&lt;/a&gt;) and consider upvoting, analyzing, and providing feedback. All comments are welcome \ud83d\udc9c.  &lt;/p&gt;\n\n&lt;p&gt;I compared how the posts and comments are organized based on tags, hour, and sentiment. It&amp;#39;s beneficial because we can observe the sentiment of the post authors through tags and their corresponding comments, and analyze potential correlations.  &lt;/p&gt;\n\n&lt;p&gt;If you find this interesting, please consider sharing or linking it! Your support would be greatly appreciated and will help make this notebook accessible to a wider audience \ud83d\udc9c.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Dataset used: &lt;a href=\"https://www.kaggle.com/datasets/curiel/rmovies-posts-and-comments\"&gt;https://www.kaggle.com/datasets/curiel/rmovies-posts-and-comments&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Script that feeds and updates the dataset: &lt;a href=\"https://www.kaggle.com/code/curiel/update-r-movies-dataset\"&gt;https://www.kaggle.com/code/curiel/update-r-movies-dataset&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bAF3nu34sqx9JiQ8p2IC1qpunRvsj1lY6gdZCTglVR4.jpg?auto=webp&amp;v=enabled&amp;s=f9a4ebbdf25c141e393ac0f5de16dfb5de3c35e8", "width": 100, "height": 100}, "resolutions": [], "variants": {}, "id": "nf-qTd7Cm4Nu8vE8A8JoHkZUwetTZo9OzjS48fKRCdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1462i6f", "is_robot_indexable": true, "report_reasons": null, "author": "data-dreamr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1462i6f/exploratory_data_and_sentiment_analysis_of_rmovies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1462i6f/exploratory_data_and_sentiment_analysis_of_rmovies/", "subreddit_subscribers": 922013, "created_utc": 1686409387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I wanted to find out the top 50 or 100 college for masters in Data science. But no matter where I looked i can't find a trustworthy list. \nUnlike the traditional subjects there is no full list with courses listed as such. Can anyone help me?", "author_fullname": "t2_6f4kci1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I can't find a good list for MS in DS colleges. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145so54", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686378866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to find out the top 50 or 100 college for masters in Data science. But no matter where I looked i can&amp;#39;t find a trustworthy list. \nUnlike the traditional subjects there is no full list with courses listed as such. Can anyone help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145so54", "is_robot_indexable": true, "report_reasons": null, "author": "Famous-Gas798", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145so54/i_cant_find_a_good_list_for_ms_in_ds_colleges_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145so54/i_cant_find_a_good_list_for_ms_in_ds_colleges_why/", "subreddit_subscribers": 922013, "created_utc": 1686378866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I've been working on a business analytics degree with the hopes of getting an MBA afterwards. But now as I get closer to finishing my MSBA, I realized I really like the data science part better. So I'm think of going after an MS in Data Science, specifically for the machine learning and artificial intelligence aspect of it. Any tips for it? Money is not a factor.", "author_fullname": "t2_903k40ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business analytics to data science.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145nmjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686362656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been working on a business analytics degree with the hopes of getting an MBA afterwards. But now as I get closer to finishing my MSBA, I realized I really like the data science part better. So I&amp;#39;m think of going after an MS in Data Science, specifically for the machine learning and artificial intelligence aspect of it. Any tips for it? Money is not a factor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145nmjv", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Cry-495", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145nmjv/business_analytics_to_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145nmjv/business_analytics_to_data_science/", "subreddit_subscribers": 922013, "created_utc": 1686362656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I found the model weight to a machine learning problem in the tar file but I need it in an h5 file format. What do I do? I only use TensorFlow. I'm unfamiliar with Pytorch, but the person wrote their model in Pytorch. Thanks for any help in advance.", "author_fullname": "t2_n76zm04c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tar file to h5 file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145lz0q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686357883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found the model weight to a machine learning problem in the tar file but I need it in an h5 file format. What do I do? I only use TensorFlow. I&amp;#39;m unfamiliar with Pytorch, but the person wrote their model in Pytorch. Thanks for any help in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145lz0q", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy-Currency-8127", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145lz0q/tar_file_to_h5_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145lz0q/tar_file_to_h5_file/", "subreddit_subscribers": 922013, "created_utc": 1686357883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Have an interview coming up for a jr level position, can anyone recommend some resources to freshen up on basic probability theory / statistical methods?", "author_fullname": "t2_6htadx20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic probability theory resource", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145iahj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686348266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have an interview coming up for a jr level position, can anyone recommend some resources to freshen up on basic probability theory / statistical methods?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145iahj", "is_robot_indexable": true, "report_reasons": null, "author": "jannsbababa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145iahj/basic_probability_theory_resource/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145iahj/basic_probability_theory_resource/", "subreddit_subscribers": 922013, "created_utc": 1686348266.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}