{"kind": "Listing", "data": {"after": "t3_145ghzi", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I'm applying for some hiring processes for a machine learning engineer role.\n\nWhen I have the interview, I always try to ask:\n\n\\- How many senior MLE/DS do you have?\n\n\\- Which business problems do you want to solve?\n\n\\- How many models do you currently have in production?\n\n\\- What's the level of MLOps your company is at today?", "author_fullname": "t2_w7ap5hsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to find red flags in the interview for machine learning engineer (or data science) role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1455mm2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 152, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 152, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686318157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m applying for some hiring processes for a machine learning engineer role.&lt;/p&gt;\n\n&lt;p&gt;When I have the interview, I always try to ask:&lt;/p&gt;\n\n&lt;p&gt;- How many senior MLE/DS do you have?&lt;/p&gt;\n\n&lt;p&gt;- Which business problems do you want to solve?&lt;/p&gt;\n\n&lt;p&gt;- How many models do you currently have in production?&lt;/p&gt;\n\n&lt;p&gt;- What&amp;#39;s the level of MLOps your company is at today?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1455mm2", "is_robot_indexable": true, "report_reasons": null, "author": "Muted_Standard175", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1455mm2/how_to_find_red_flags_in_the_interview_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1455mm2/how_to_find_red_flags_in_the_interview_for/", "subreddit_subscribers": 921898, "created_utc": 1686318157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Are there specific skills? Or is it just the same as everywhere else?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Scientist who are good at office politics, how did you increase your political skill?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kh8b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 105, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 105, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686353823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there specific skills? Or is it just the same as everywhere else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145kh8b", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145kh8b/data_scientist_who_are_good_at_office_politics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145kh8b/data_scientist_who_are_good_at_office_politics/", "subreddit_subscribers": 921898, "created_utc": 1686353823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry if this is odd to post here - I completed 11 out of 12 courses for my MSDS at Northwestern and just felt like the program was extremely lackluster. The required courses in management rather than the technicals. It was 55k and it would have been obvious to transfer quickly to OMSA or the newer programs.The landscape looked different when I was applying for programs, and I was initially skeptical of entirely MOOC based programs. However, I ended up just watching YT videos as my lectures anyway.\n\nI would just like to warn others when deciding whether or not to go back to school. I\u2019m still taking time to patch up on knowledge that I felt like I did not gain via the program. Although with that being said, that would be the case with any masters program. I am almost considering not even doing the last capstone just because I know that there are other things that I would rather learn.\n\nI literally just have the capstone left but I am almost considering just letting it go", "author_fullname": "t2_35qsk3el", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Off my chest: No need for costly masters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145fo6w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686356525.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686342059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is odd to post here - I completed 11 out of 12 courses for my MSDS at Northwestern and just felt like the program was extremely lackluster. The required courses in management rather than the technicals. It was 55k and it would have been obvious to transfer quickly to OMSA or the newer programs.The landscape looked different when I was applying for programs, and I was initially skeptical of entirely MOOC based programs. However, I ended up just watching YT videos as my lectures anyway.&lt;/p&gt;\n\n&lt;p&gt;I would just like to warn others when deciding whether or not to go back to school. I\u2019m still taking time to patch up on knowledge that I felt like I did not gain via the program. Although with that being said, that would be the case with any masters program. I am almost considering not even doing the last capstone just because I know that there are other things that I would rather learn.&lt;/p&gt;\n\n&lt;p&gt;I literally just have the capstone left but I am almost considering just letting it go&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145fo6w", "is_robot_indexable": true, "report_reasons": null, "author": "peachyjiang", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145fo6w/off_my_chest_no_need_for_costly_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145fo6w/off_my_chest_no_need_for_costly_masters/", "subreddit_subscribers": 921898, "created_utc": 1686342059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[OC] Teaching DataViz to kids via Board Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_145d7r5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_51o3wf0o", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/85BRd3d4PsXPnzvB0yCnfyt467HKEfL53NuS5MbknEU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "dataisbeautiful", "selftext": "", "author_fullname": "t2_51o3wf0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[OC] Teaching DataViz to kids via Board Games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataisbeautiful", "hidden": false, "pwls": 6, "link_flair_css_class": "oc", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_144gxzk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "1c7d62a6-099d-11e7-9b3c-0ee50bfd7a4c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OC", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/85BRd3d4PsXPnzvB0yCnfyt467HKEfL53NuS5MbknEU.jpg", "edited": false, "author_flair_css_class": "ocmaker", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1686248807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bxaaztw87u4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?auto=webp&amp;v=enabled&amp;s=496ac0bb9575ca95229257a777fd1ce94900f593", "width": 720, "height": 540}, "resolutions": [{"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d643590fc8f6d02c76e0865c2f60a1b6d596e3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=615653e24d163ceb024b6456c7a7f67b1ec01130", "width": 216, "height": 162}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4e4a4953c341c8ba247a233877864ee4c3aa356", "width": 320, "height": 240}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b9c83630fdc1ec7e0ea1a5febd8ba3b81987a9", "width": 640, "height": 480}], "variants": {}, "id": "nPqUNFFdipGnezYZE44prAzG_4rtsYukP-D_eWwmj08"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OC: 11", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2tk95", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "144gxzk", "is_robot_indexable": true, "report_reasons": null, "author": "DataVizzdom", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataisbeautiful/comments/144gxzk/oc_teaching_dataviz_to_kids_via_board_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bxaaztw87u4b1.jpg", "subreddit_subscribers": 19642924, "created_utc": 1686248807.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1686336122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bxaaztw87u4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?auto=webp&amp;v=enabled&amp;s=496ac0bb9575ca95229257a777fd1ce94900f593", "width": 720, "height": 540}, "resolutions": [{"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d643590fc8f6d02c76e0865c2f60a1b6d596e3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=615653e24d163ceb024b6456c7a7f67b1ec01130", "width": 216, "height": 162}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4e4a4953c341c8ba247a233877864ee4c3aa356", "width": 320, "height": 240}, {"url": "https://preview.redd.it/bxaaztw87u4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b9c83630fdc1ec7e0ea1a5febd8ba3b81987a9", "width": 640, "height": 480}], "variants": {}, "id": "nPqUNFFdipGnezYZE44prAzG_4rtsYukP-D_eWwmj08"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145d7r5", "is_robot_indexable": true, "report_reasons": null, "author": "DataVizzdom", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_144gxzk", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145d7r5/oc_teaching_dataviz_to_kids_via_board_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bxaaztw87u4b1.jpg", "subreddit_subscribers": 921898, "created_utc": 1686336122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I used chatGPT to create a comprehensive list of 99 projects taking one from a beginner to expert level in Data Science.\n\nThese projects cover both key ML techniques and underappreciated technical skills, from basic code structuring to advanced deep learning.\n\nAs I progress, I'll be sharing my work here for us to discuss, learn, and grow together.\n\nCurious? Want to start your own journey? Here's the prompt I used:\n\n\"I am interested in {insert\\_topic\\_here}. List data science projects from beginner to expert that include AI or generative AI. Use a table format with columns - Name, Topic, Difficulty, Data Science/ML Technique, and a Description.\"\n\nMy first project is a recipe recommendation model. I'm focusing on Github repository setup, version control best practices, and project planning.\n\nStay tuned for more updates and happy learning!", "author_fullname": "t2_qyfyulzl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Embarking on a Journey of 99 Data Science Projects - From Beginner to Expert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145c4ap", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686333520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used chatGPT to create a comprehensive list of 99 projects taking one from a beginner to expert level in Data Science.&lt;/p&gt;\n\n&lt;p&gt;These projects cover both key ML techniques and underappreciated technical skills, from basic code structuring to advanced deep learning.&lt;/p&gt;\n\n&lt;p&gt;As I progress, I&amp;#39;ll be sharing my work here for us to discuss, learn, and grow together.&lt;/p&gt;\n\n&lt;p&gt;Curious? Want to start your own journey? Here&amp;#39;s the prompt I used:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I am interested in {insert_topic_here}. List data science projects from beginner to expert that include AI or generative AI. Use a table format with columns - Name, Topic, Difficulty, Data Science/ML Technique, and a Description.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;My first project is a recipe recommendation model. I&amp;#39;m focusing on Github repository setup, version control best practices, and project planning.&lt;/p&gt;\n\n&lt;p&gt;Stay tuned for more updates and happy learning!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145c4ap", "is_robot_indexable": true, "report_reasons": null, "author": "Front_Newspaper3600", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145c4ap/embarking_on_a_journey_of_99_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145c4ap/embarking_on_a_journey_of_99_data_science/", "subreddit_subscribers": 921898, "created_utc": 1686333520.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have come to problems that I have solved in SQL (postgress) but doing the analysis in R or even Python would be easier.\n\nI needed from a zip code the geometry column twice:1 for the orgin source and 1 for the destination.  There would be no easy solution in SQL other use a nested query or use left join twice. Where as in R mapvalues(collum\\_to\\_transforn, zipcode, geometry) is sufficient. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nOr you want to calculate the avg use per zipcode per day but\n\nusing the global maximum and minimum date to calculate the number of  days and not the days between per zipcode, as the first time using it was e.g. in February but available since January.\n\nThis can be done with SQL but  R makes lives easier again.\n\n&amp;#x200B;\n\nSo when do you use SQL for data manipulation and when another language. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_67cz0s3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL or programming language. When to use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145syvc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686379865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have come to problems that I have solved in SQL (postgress) but doing the analysis in R or even Python would be easier.&lt;/p&gt;\n\n&lt;p&gt;I needed from a zip code the geometry column twice:1 for the orgin source and 1 for the destination.  There would be no easy solution in SQL other use a nested query or use left join twice. Where as in R mapvalues(collum_to_transforn, zipcode, geometry) is sufficient. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or you want to calculate the avg use per zipcode per day but&lt;/p&gt;\n\n&lt;p&gt;using the global maximum and minimum date to calculate the number of  days and not the days between per zipcode, as the first time using it was e.g. in February but available since January.&lt;/p&gt;\n\n&lt;p&gt;This can be done with SQL but  R makes lives easier again.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So when do you use SQL for data manipulation and when another language. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145syvc", "is_robot_indexable": true, "report_reasons": null, "author": "TywinASOIAF", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145syvc/sql_or_programming_language_when_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145syvc/sql_or_programming_language_when_to_use/", "subreddit_subscribers": 921898, "created_utc": 1686379865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing SlimPajama-627B: the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145gqcx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_voqxwypq", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "LanguageTechnology", "selftext": "SlimPajama cleans and deduplicates RedPajama-1T, reducing the total token count and file size by 50%. It's half the size and trains twice as fast!\n\nIt\u2019s the highest quality dataset when training to 600B tokens and, when upsampled, performs equal or better than RedPajama.  It was no mean feat to deduplicate data on this scale \u2013 existing tools do not scale to a trillion tokens. We built a custom parallel data pre-processing pipeline and are sharing the code open source with the community.\n\nWe\u2019d like to thank our partner Opentensor for supporting this project. And credit goes to Together Compute and the entire team that created the RedPajama dataset!\n\n&amp;#x200B;\n\n* SlimPajama dataset - [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)\n* Libraries for data pre-processing - [https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data\\_processing/slimpajama](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama)\n* Read our blog - [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)", "author_fullname": "t2_voqxwypq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing SlimPajama-627B: the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/LanguageTechnology", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145gowe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686356247.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686344467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LanguageTechnology", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SlimPajama cleans and deduplicates RedPajama-1T, reducing the total token count and file size by 50%. It&amp;#39;s half the size and trains twice as fast!&lt;/p&gt;\n\n&lt;p&gt;It\u2019s the highest quality dataset when training to 600B tokens and, when upsampled, performs equal or better than RedPajama.  It was no mean feat to deduplicate data on this scale \u2013 existing tools do not scale to a trillion tokens. We built a custom parallel data pre-processing pipeline and are sharing the code open source with the community.&lt;/p&gt;\n\n&lt;p&gt;We\u2019d like to thank our partner Opentensor for supporting this project. And credit goes to Together Compute and the entire team that created the RedPajama dataset!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SlimPajama dataset - &lt;a href=\"https://huggingface.co/datasets/cerebras/SlimPajama-627B\"&gt;https://huggingface.co/datasets/cerebras/SlimPajama-627B&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Libraries for data pre-processing - &lt;a href=\"https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama\"&gt;https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Read our blog - &lt;a href=\"https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama\"&gt;https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?auto=webp&amp;v=enabled&amp;s=a583c96a3aca547884f7609054ef900ea6e1b937", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffa89dc772d5b0c5c209fe190156897d3bf3ab93", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e4f4987b044c263bd6abf9f14b3897f11eceedb", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=292e53558b985987a9004fa3c28fa47da1c1f2b9", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ccb4c70274741ca419e6fa85f5a4ed3c30f3cf2", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8328eb49c040bcae227faa60bf28f84f705cc4a0", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aae668a15dd053f27c3c00579ccf95137d96a1c", "width": 1080, "height": 583}], "variants": {}, "id": "qZu7xwSlcLKCNhCBJSU5hdBnG2HpXv6XxLw_0LnqVnA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rkr2", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145gowe", "is_robot_indexable": true, "report_reasons": null, "author": "CS-fan-101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "subreddit_subscribers": 42643, "created_utc": 1686344467.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686344549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LanguageTechnology", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?auto=webp&amp;v=enabled&amp;s=a583c96a3aca547884f7609054ef900ea6e1b937", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffa89dc772d5b0c5c209fe190156897d3bf3ab93", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e4f4987b044c263bd6abf9f14b3897f11eceedb", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=292e53558b985987a9004fa3c28fa47da1c1f2b9", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ccb4c70274741ca419e6fa85f5a4ed3c30f3cf2", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8328eb49c040bcae227faa60bf28f84f705cc4a0", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/e1NAAte95XVh6EVzu0k_0u2uONi5Ztr5_Gn6Gwul7Lk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aae668a15dd053f27c3c00579ccf95137d96a1c", "width": 1080, "height": 583}], "variants": {}, "id": "qZu7xwSlcLKCNhCBJSU5hdBnG2HpXv6XxLw_0LnqVnA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145gqcx", "is_robot_indexable": true, "report_reasons": null, "author": "CS-fan-101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_145gowe", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145gqcx/introducing_slimpajama627b_the_largest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/LanguageTechnology/comments/145gowe/introducing_slimpajama627b_the_largest/", "subreddit_subscribers": 921898, "created_utc": 1686344549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lrllrd1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any one working on smart manufacturing? What are your use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145q3z8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686370304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145q3z8", "is_robot_indexable": true, "report_reasons": null, "author": "vishal-vora", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145q3z8/any_one_working_on_smart_manufacturing_what_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145q3z8/any_one_working_on_smart_manufacturing_what_are/", "subreddit_subscribers": 921898, "created_utc": 1686370304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I wanted to find out the top 50 or 100 college for masters in Data science. But no matter where I looked i can't find a trustworthy list. \nUnlike the traditional subjects there is no full list with courses listed as such. Can anyone help me?", "author_fullname": "t2_6f4kci1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I can't find a good list for MS in DS colleges. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145so54", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686378866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to find out the top 50 or 100 college for masters in Data science. But no matter where I looked i can&amp;#39;t find a trustworthy list. \nUnlike the traditional subjects there is no full list with courses listed as such. Can anyone help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145so54", "is_robot_indexable": true, "report_reasons": null, "author": "Famous-Gas798", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145so54/i_cant_find_a_good_list_for_ms_in_ds_colleges_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145so54/i_cant_find_a_good_list_for_ms_in_ds_colleges_why/", "subreddit_subscribers": 921898, "created_utc": 1686378866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This encoding system claim that it can state all IPA symbols and its pronounciation in a Binary format \n\nPhonBinary Encoding System V6\n\n- Analyzes each sound into its featural components to translate into binary. \n- Features are encoded using a variable number of bits based on the number of possibilities for each feature.\n- Enables representation of the full IPA  to use in various NLP applications. \n- Includes all manners of articulation, secondary articulations, prosodic features, airstream mechanisms and vowel qualities.\n\nSound Analysis:  \n\n- The first step involves analyzing each sound into its featural components. \n- Full understanding of the IPA and phonetic terminology required.\n\nConsonants:\n- Produced by modifying airstream with articulators. \n- Factors: airstream mechanism (3 bits), manner of articulation (5 bits), place of articulation (6 bits), phonation (4 bits), coarticulation (5 bits).\n\nAirstream mechanism (3 bits):\n- Pulmonic egressive: 000 \n- Pulmonic ingressive: 001\n- Glottalic egressive: 010\n- Glottalic ingressive: 011\n- Lingual-glottalic: 100\n- Velaric: 101 \n\nManner of articulation (5 bits):\n- Plosive: 00000   \n- Nasal: 00001\n- Trill: 00010\n- Tap: 00011\n- Fricative: 00100\n- Approximant: 00101\n- Lateral approximant: 00110\n- Flap: 00111\n- Affricate: 01000 \n- Clitic: 01001  \n- Implosive  01010\n- Ejective: 01011\n- Click: 01100\n\nPlace of articulation (6 bits): \n- Bilabial: 000000 \n- Labiodental: 000001\n- Interdental: 000010\n- Alveolar: 000011 \n- Postalveolar: 000100\n- Retroflex: 000101\n- Palatal: 000110\n- Velar: 000111 \n- Uvular: 001000\n- Pharyngeal: 001001  \n- Glottal: 001010 \n- Epiglottal: 001011\n- Labial-palatal: 001100\n- Labial-velar: 001101 \n- Labial-uvular: 001110\n- Velar-pharyngeal: 001111\n- Velar-uvular: 010000  \n- Velar-epilaryngeal: 010001\n- Uvular-pharyngeal : 010010\n\nPhonation (4 bits):\n- Voiceless: 0000\n- Voiced: 0001   \n- Creaky: 0010 \n- Breathy: 0011\n- Slack: 0100\n- Stiff: 0101\n- Creaky voiced: 0110\n- Whispered: 0111   \n- Harsh: 1000 \n- Laryngealized : 1001\n- Closed glottis: 1010\n- Velaric aspiration: 1011\n\nVowels:\n- Produced by shaping airstream resonated through oral and nasal cavities. \n- Factors: height (4 bits), backness (4 bits), roundness (4 bits), nasalization (2 bits), length (3 bits). \n\nHeight (4 bits):\n- Close: 0000\n- Near-close: 0001  \n- Close-mid: 0010  \n- Mid: 0011\n- Open-mid: 0100 \n- Near-open: 0101   \n- Open: 0110\n\nBackness(4 bits):\n- Front: 0000\n- Near-front: 0001\n- Central: 0010  \n- Near-back: 0011\n- Back: 0100 \n- Near-central: 0101\n\nRoundness (4 bits):\n- Unrounded: 0000  \n- Rounded: 0001\n- Compressed: 0010\n- Protruded: 0011\n- More rounded: 0100\n- Less rounded: 0101\n\nTenseness  (3 bits):\n- Extra-short: 000 \n- Short: 001\n- Long: 010  \n- Overlong: 011\n\nNasalization (2 bits):\n- Oral: 00\n- Nasal: 01\n- Nasalized: 10\n- Denasalized: 11\n\nSecondary articulations, prosodic features,  diacritics and delimiters will also be included with variable bit encoding. Morphological information  can be  added as an additional layer of information.\n\nDelimiters:\nMorpheme boundary (+)\nWord boundary (#)\nSyllable boundary (-)\nStress boundary (*)\nIntonation boundary ( ^ )\nProcess boundary (%)\n\nEncoding:\n\n- Analyze each sound into its featural components using the IPA chart and the feature values given in the question.\n- Arrange the features in a consistent order for consonants (airstream mechanism \u2192 manner \u2192 place \u2192 phonation \u2192 coarticulation)  and vowels (height \u2192 backness \u2192 roundness  \u2192 nasalization \u2192 length).\n- Concatenate the binary codes for each feature to form the binary code for each sound.\n- Concatenate the binary codes for each sound to form the binary code for each word.\n- Use delimiters to mark boundaries between words, syllables and morphemes.\n\nDecoding:\n\n- Use delimiters to identify boundaries between words, syllables and morphemes.\n- Split the binary code for each word into segments of variable length based on the number of bits for each feature.\n- Identify the feature values for each segment using the feature values given in the question.\n- Identify the sound corresponding to each segment using the IPA chart and the feature values.\n- Write the sounds using IPA symbols or orthographic symbols.\n\nExample:\n\n\"Hello World\"\n\nEncoding:\n\n- Analyze each sound into its featural components:\n\n  - /h/: pulmonic egressive, fricative, glottal, voiceless, no coarticulation\n  - /\u025b/: open-mid, front, unrounded, oral, short\n  - /l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation\n  - /o\u028a/: close-mid, back, rounded, oral, long\n  - /w/: pulmonic egressive, approximant, labial-velar, voiced, no coarticulation\n  - /\u025c\u02d0/: mid, central, unrounded, oral, overlong\n  - /l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation\n  - /d/: pulmonic egressive, plosive, alveolar, voiced, no coarticulation\n\n- Arrange the features in a consistent order and concatenate the binary codes for each feature:\n\n  - /h/: 000001000010100000000000\n  - /\u025b/: 010000000000000001\n  - /l/: 0000010110000110000100000\n  - /o\u028a/: 001001010001000010\n  - /w/: 0000010100111101000100000\n  - /\u025c\u02d0/: 001100010000000011\n  - /l/: 0000010110000110000100000\n  - /d/: 0000000000011001000100000\n\n- Concatenate the binary codes for each sound and use delimiters to mark boundaries:\n\n  - Hello: 000001000010100000000000#010000000000000001#-#0000010110000110000100000#001001010001000010#\n  - World: 0000010100111101000100000#-#001100010000000011#-#0000010110000110000100000#-#0000000000011001000100000#\n\nDecoding:\n\n- Identify the boundaries between words, syllables and morphemes using the delimiters:\n\n  - Hello: [h][\u025b]-[l][o\u028a]\n  - World: [w]-[\u025c\u02d0]-[l]-[d]\n\n- Divide the binary code for each word into chunks of bits corresponding to each feature:\n\n  - Hello: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[4][4][4][2][3]\n    - [h]: [000] [00] [010] [1000] [00000]\n    - [\u025b]: [0100] [0000] [0000] [00] [001]\n    - [l]: [000] [01] [1000] [0110] [00010]\n    - [o\u028a]:[0010] [0100] [0100] [00] [010]\n  - World: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[3][5][6][4][5]\n    - [w]: [000] [01] [0011] [1101] [00010]\n    - [\u025c\u02d0]:[0011] [0001] [0000] [00] [011]\n    - [l]: [000] [01] [1000] [0110] [00010]\n    - [d]: [000] [00] [0001] [1001] [00010]\n\n- Match each chunk of bits with the feature value given in the question:\n\n  - Hello:\n    - /h/: pulmonic egressive (000), fricative (00), glottal (010), voiceless (1000), no coarticulation (00000)\n    - /\u025b/: open-mid (0100), front (0000), unrounded (0000), oral (00), short (001)\n    - /l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)\n    - /o\u028a/: close-mid (0010), back (0100), rounded (0100), oral (00), long (010)\n  - World:\n    - /w/: pulmonic egressive (000), approximant (01), labial-velar (0011), voiced (1101), no coarticulation (00010)\n    - /\u025c\u02d0/: mid (0011), central (0001), unrounded (0000), oral (00), overlong (011)\n    - /l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)\n    - /d/: pulmonic egressive (000), plosive (00), alveolar (0001), voiced (1001), no coarticulation (00010)\n\n- Find the sound that corresponds to each combination of feature values using the IPA chart and the feature values:\n\n  - Hello: /h/ /\u025b/ /l/ /o\u028a/\n  - World: /w/ /\u025c\u02d0/ /l/ /d/\n\n- Write the sounds using IPA symbols or orthographic symbols:\n\n  - Hello: /h\u025blo\u028a/ or hello\n  - World: /w\u025c\u02d0ld/ or world", "author_fullname": "t2_d1l6mocox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Experimental Phonetics to Binary Encoding System", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_145y56v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686397622.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686397432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This encoding system claim that it can state all IPA symbols and its pronounciation in a Binary format &lt;/p&gt;\n\n&lt;p&gt;PhonBinary Encoding System V6&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Analyzes each sound into its featural components to translate into binary. &lt;/li&gt;\n&lt;li&gt;Features are encoded using a variable number of bits based on the number of possibilities for each feature.&lt;/li&gt;\n&lt;li&gt;Enables representation of the full IPA  to use in various NLP applications. &lt;/li&gt;\n&lt;li&gt;Includes all manners of articulation, secondary articulations, prosodic features, airstream mechanisms and vowel qualities.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sound Analysis:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The first step involves analyzing each sound into its featural components. &lt;/li&gt;\n&lt;li&gt;Full understanding of the IPA and phonetic terminology required.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Consonants:\n- Produced by modifying airstream with articulators. \n- Factors: airstream mechanism (3 bits), manner of articulation (5 bits), place of articulation (6 bits), phonation (4 bits), coarticulation (5 bits).&lt;/p&gt;\n\n&lt;p&gt;Airstream mechanism (3 bits):\n- Pulmonic egressive: 000 \n- Pulmonic ingressive: 001\n- Glottalic egressive: 010\n- Glottalic ingressive: 011\n- Lingual-glottalic: 100\n- Velaric: 101 &lt;/p&gt;\n\n&lt;p&gt;Manner of articulation (5 bits):\n- Plosive: 00000&lt;br/&gt;\n- Nasal: 00001\n- Trill: 00010\n- Tap: 00011\n- Fricative: 00100\n- Approximant: 00101\n- Lateral approximant: 00110\n- Flap: 00111\n- Affricate: 01000 \n- Clitic: 01001&lt;br/&gt;\n- Implosive  01010\n- Ejective: 01011\n- Click: 01100&lt;/p&gt;\n\n&lt;p&gt;Place of articulation (6 bits): \n- Bilabial: 000000 \n- Labiodental: 000001\n- Interdental: 000010\n- Alveolar: 000011 \n- Postalveolar: 000100\n- Retroflex: 000101\n- Palatal: 000110\n- Velar: 000111 \n- Uvular: 001000\n- Pharyngeal: 001001&lt;br/&gt;\n- Glottal: 001010 \n- Epiglottal: 001011\n- Labial-palatal: 001100\n- Labial-velar: 001101 \n- Labial-uvular: 001110\n- Velar-pharyngeal: 001111\n- Velar-uvular: 010000&lt;br/&gt;\n- Velar-epilaryngeal: 010001\n- Uvular-pharyngeal : 010010&lt;/p&gt;\n\n&lt;p&gt;Phonation (4 bits):\n- Voiceless: 0000\n- Voiced: 0001&lt;br/&gt;\n- Creaky: 0010 \n- Breathy: 0011\n- Slack: 0100\n- Stiff: 0101\n- Creaky voiced: 0110\n- Whispered: 0111&lt;br/&gt;\n- Harsh: 1000 \n- Laryngealized : 1001\n- Closed glottis: 1010\n- Velaric aspiration: 1011&lt;/p&gt;\n\n&lt;p&gt;Vowels:\n- Produced by shaping airstream resonated through oral and nasal cavities. \n- Factors: height (4 bits), backness (4 bits), roundness (4 bits), nasalization (2 bits), length (3 bits). &lt;/p&gt;\n\n&lt;p&gt;Height (4 bits):\n- Close: 0000\n- Near-close: 0001&lt;br/&gt;\n- Close-mid: 0010&lt;br/&gt;\n- Mid: 0011\n- Open-mid: 0100 \n- Near-open: 0101&lt;br/&gt;\n- Open: 0110&lt;/p&gt;\n\n&lt;p&gt;Backness(4 bits):\n- Front: 0000\n- Near-front: 0001\n- Central: 0010&lt;br/&gt;\n- Near-back: 0011\n- Back: 0100 \n- Near-central: 0101&lt;/p&gt;\n\n&lt;p&gt;Roundness (4 bits):\n- Unrounded: 0000&lt;br/&gt;\n- Rounded: 0001\n- Compressed: 0010\n- Protruded: 0011\n- More rounded: 0100\n- Less rounded: 0101&lt;/p&gt;\n\n&lt;p&gt;Tenseness  (3 bits):\n- Extra-short: 000 \n- Short: 001\n- Long: 010&lt;br/&gt;\n- Overlong: 011&lt;/p&gt;\n\n&lt;p&gt;Nasalization (2 bits):\n- Oral: 00\n- Nasal: 01\n- Nasalized: 10\n- Denasalized: 11&lt;/p&gt;\n\n&lt;p&gt;Secondary articulations, prosodic features,  diacritics and delimiters will also be included with variable bit encoding. Morphological information  can be  added as an additional layer of information.&lt;/p&gt;\n\n&lt;p&gt;Delimiters:\nMorpheme boundary (+)\nWord boundary (#)\nSyllable boundary (-)\nStress boundary (*)\nIntonation boundary ( ^ )\nProcess boundary (%)&lt;/p&gt;\n\n&lt;p&gt;Encoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Analyze each sound into its featural components using the IPA chart and the feature values given in the question.&lt;/li&gt;\n&lt;li&gt;Arrange the features in a consistent order for consonants (airstream mechanism \u2192 manner \u2192 place \u2192 phonation \u2192 coarticulation)  and vowels (height \u2192 backness \u2192 roundness  \u2192 nasalization \u2192 length).&lt;/li&gt;\n&lt;li&gt;Concatenate the binary codes for each feature to form the binary code for each sound.&lt;/li&gt;\n&lt;li&gt;Concatenate the binary codes for each sound to form the binary code for each word.&lt;/li&gt;\n&lt;li&gt;Use delimiters to mark boundaries between words, syllables and morphemes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Decoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use delimiters to identify boundaries between words, syllables and morphemes.&lt;/li&gt;\n&lt;li&gt;Split the binary code for each word into segments of variable length based on the number of bits for each feature.&lt;/li&gt;\n&lt;li&gt;Identify the feature values for each segment using the feature values given in the question.&lt;/li&gt;\n&lt;li&gt;Identify the sound corresponding to each segment using the IPA chart and the feature values.&lt;/li&gt;\n&lt;li&gt;Write the sounds using IPA symbols or orthographic symbols.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Hello World&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Encoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Analyze each sound into its featural components:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/h/: pulmonic egressive, fricative, glottal, voiceless, no coarticulation&lt;/li&gt;\n&lt;li&gt;/\u025b/: open-mid, front, unrounded, oral, short&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation&lt;/li&gt;\n&lt;li&gt;/o\u028a/: close-mid, back, rounded, oral, long&lt;/li&gt;\n&lt;li&gt;/w/: pulmonic egressive, approximant, labial-velar, voiced, no coarticulation&lt;/li&gt;\n&lt;li&gt;/\u025c\u02d0/: mid, central, unrounded, oral, overlong&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive, lateral approximant, alveolar, voiced, no coarticulation&lt;/li&gt;\n&lt;li&gt;/d/: pulmonic egressive, plosive, alveolar, voiced, no coarticulation&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Arrange the features in a consistent order and concatenate the binary codes for each feature:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/h/: 000001000010100000000000&lt;/li&gt;\n&lt;li&gt;/\u025b/: 010000000000000001&lt;/li&gt;\n&lt;li&gt;/l/: 0000010110000110000100000&lt;/li&gt;\n&lt;li&gt;/o\u028a/: 001001010001000010&lt;/li&gt;\n&lt;li&gt;/w/: 0000010100111101000100000&lt;/li&gt;\n&lt;li&gt;/\u025c\u02d0/: 001100010000000011&lt;/li&gt;\n&lt;li&gt;/l/: 0000010110000110000100000&lt;/li&gt;\n&lt;li&gt;/d/: 0000000000011001000100000&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Concatenate the binary codes for each sound and use delimiters to mark boundaries:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: 000001000010100000000000#010000000000000001#-#0000010110000110000100000#001001010001000010#&lt;/li&gt;\n&lt;li&gt;World: 0000010100111101000100000#-#001100010000000011#-#0000010110000110000100000#-#0000000000011001000100000#&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Decoding:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Identify the boundaries between words, syllables and morphemes using the delimiters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: [h][\u025b]-[l][o\u028a]&lt;/li&gt;\n&lt;li&gt;World: [w]-[\u025c\u02d0]-[l]-[d]&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Divide the binary code for each word into chunks of bits corresponding to each feature:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[4][4][4][2][3]&lt;/li&gt;\n&lt;li&gt;[h]: [000] [00] [010] [1000] [00000]&lt;/li&gt;\n&lt;li&gt;[\u025b]: [0100] [0000] [0000] [00] [001]&lt;/li&gt;\n&lt;li&gt;[l]: [000] [01] [1000] [0110] [00010]&lt;/li&gt;\n&lt;li&gt;[o\u028a]:[0010] [0100] [0100] [00] [010]&lt;/li&gt;\n&lt;li&gt;World: [3][5][6][4][5]-[4][4][4][2][3]-[3][5][6][4][5]-[3][5][6][4][5]&lt;/li&gt;\n&lt;li&gt;[w]: [000] [01] [0011] [1101] [00010]&lt;/li&gt;\n&lt;li&gt;[\u025c\u02d0]:[0011] [0001] [0000] [00] [011]&lt;/li&gt;\n&lt;li&gt;[l]: [000] [01] [1000] [0110] [00010]&lt;/li&gt;\n&lt;li&gt;[d]: [000] [00] [0001] [1001] [00010]&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Match each chunk of bits with the feature value given in the question:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello:&lt;/li&gt;\n&lt;li&gt;/h/: pulmonic egressive (000), fricative (00), glottal (010), voiceless (1000), no coarticulation (00000)&lt;/li&gt;\n&lt;li&gt;/\u025b/: open-mid (0100), front (0000), unrounded (0000), oral (00), short (001)&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)&lt;/li&gt;\n&lt;li&gt;/o\u028a/: close-mid (0010), back (0100), rounded (0100), oral (00), long (010)&lt;/li&gt;\n&lt;li&gt;World:&lt;/li&gt;\n&lt;li&gt;/w/: pulmonic egressive (000), approximant (01), labial-velar (0011), voiced (1101), no coarticulation (00010)&lt;/li&gt;\n&lt;li&gt;/\u025c\u02d0/: mid (0011), central (0001), unrounded (0000), oral (00), overlong (011)&lt;/li&gt;\n&lt;li&gt;/l/: pulmonic egressive (000), lateral approximant (01), alveolar (1000), voiced (0110), no coarticulation (00010)&lt;/li&gt;\n&lt;li&gt;/d/: pulmonic egressive (000), plosive (00), alveolar (0001), voiced (1001), no coarticulation (00010)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Find the sound that corresponds to each combination of feature values using the IPA chart and the feature values:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: /h/ /\u025b/ /l/ /o\u028a/&lt;/li&gt;\n&lt;li&gt;World: /w/ /\u025c\u02d0/ /l/ /d/&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Write the sounds using IPA symbols or orthographic symbols:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hello: /h\u025blo\u028a/ or hello&lt;/li&gt;\n&lt;li&gt;World: /w\u025c\u02d0ld/ or world&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145y56v", "is_robot_indexable": true, "report_reasons": null, "author": "B-acccount", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145y56v/my_experimental_phonetics_to_binary_encoding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145y56v/my_experimental_phonetics_to_binary_encoding/", "subreddit_subscribers": 921898, "created_utc": 1686397432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'll use a API service to provide me with a bunch of website urls then use python beautiful soup to extract the actual text content the pile up all the content from about 30 web pages and send it to a python script that extracts keywords. \n\nBut it's slow. How can I find the bottle necks? Or is there a way to speed this up using another language like R or Java? \n\nQuite new to this so I do not know how to run tests etc.\n\nI just want to mention the the API is really fast and I'm sure is not causing any latency issue.", "author_fullname": "t2_r3147swk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm using python to scrape web page content and extract keywords, how can I make it faster to process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145vvsx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686389983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll use a API service to provide me with a bunch of website urls then use python beautiful soup to extract the actual text content the pile up all the content from about 30 web pages and send it to a python script that extracts keywords. &lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s slow. How can I find the bottle necks? Or is there a way to speed this up using another language like R or Java? &lt;/p&gt;\n\n&lt;p&gt;Quite new to this so I do not know how to run tests etc.&lt;/p&gt;\n\n&lt;p&gt;I just want to mention the the API is really fast and I&amp;#39;m sure is not causing any latency issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145vvsx", "is_robot_indexable": true, "report_reasons": null, "author": "flipsnapnet", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145vvsx/im_using_python_to_scrape_web_page_content_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145vvsx/im_using_python_to_scrape_web_page_content_and/", "subreddit_subscribers": 921898, "created_utc": 1686389983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing FeatureEng: A Community for Feature Engineering Enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145oag6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_cvej3qatd", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "FeatureEng", "selftext": "Hey everyone, I am Gxav. I used to be very active on Kaggle [https://www.kaggle.com/xavierconort](https://www.kaggle.com/xavierconort) and I owe my Kaggle GrandMaster title to feature engineering. Building skills in feature engineering is an ongoing journey and I am really missing my Kaggle days where I could learn new tricks from my fellow Kagglers.\n\nI started the FeatureEng community because I couldn't find any communities specifically focused on feature engineering, which I believe deserves its own dedicated space. I hope this community will be a place where you and I will find our new sources of inspiration!\n\nCheers,\n\nGxav", "author_fullname": "t2_cvej3qatd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing FeatureEng: A Community for Feature Engineering Enthusiasts!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/FeatureEng", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145nb4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686361712.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.FeatureEng", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am Gxav. I used to be very active on Kaggle &lt;a href=\"https://www.kaggle.com/xavierconort\"&gt;https://www.kaggle.com/xavierconort&lt;/a&gt; and I owe my Kaggle GrandMaster title to feature engineering. Building skills in feature engineering is an ongoing journey and I am really missing my Kaggle days where I could learn new tricks from my fellow Kagglers.&lt;/p&gt;\n\n&lt;p&gt;I started the FeatureEng community because I couldn&amp;#39;t find any communities specifically focused on feature engineering, which I believe deserves its own dedicated space. I hope this community will be a place where you and I will find our new sources of inspiration!&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Gxav&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_8kc7h0", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145nb4g", "is_robot_indexable": true, "report_reasons": null, "author": "Gxav73", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "subreddit_subscribers": 9, "created_utc": 1686361712.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1686364639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.FeatureEng", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145oag6", "is_robot_indexable": true, "report_reasons": null, "author": "Gxav73", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_145nb4g", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145oag6/introducing_featureeng_a_community_for_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/FeatureEng/comments/145nb4g/introducing_featureeng_a_community_for_feature/", "subreddit_subscribers": 921898, "created_utc": 1686364639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Anyone have experience interviewing at cruise? Specifically looking for experience with the live python coding piece. I have plenty of experience, just looking for resources or level of difficulty so I can study properly. Gracias fam!", "author_fullname": "t2_9d3tunn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cruise python technical interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145goxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686344469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience interviewing at cruise? Specifically looking for experience with the live python coding piece. I have plenty of experience, just looking for resources or level of difficulty so I can study properly. Gracias fam!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145goxl", "is_robot_indexable": true, "report_reasons": null, "author": "IllPoem4426", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145goxl/cruise_python_technical_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145goxl/cruise_python_technical_interview/", "subreddit_subscribers": 921898, "created_utc": 1686344469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ajskcqg7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - H4NM/Groppy: Facilitating regex creation and deploying custom grok patterns in an ELK environment \ud83e\udd8c\ud83d\udcdc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_145gd6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ye34yg6kbY7LbF7SF6nWRaiqYevZUoJaQ-qiRvUiqQQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686343690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/H4NM/Groppy", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?auto=webp&amp;v=enabled&amp;s=47a883acbbc6cfcb1ad03b617ff5d6148b9c03b4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=137f09c0b56404e7391a3e00960672e44546f560", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2192a1761052879ed3d684097620f0c931bfa269", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0284861e9739b5324448a15e56c9d35e9cb8a51", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2b106400924e31522dc736e5ac00dc2df536aca", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343d458647e02212080ae14aaca929dcf05e0cd8", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/H8Z38UsjEOniTS-CeIOYGPT41km18oxXPhS3_RoKovA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57cf8c00930eaa4468530c17c02e73206f7da4fa", "width": 1080, "height": 540}], "variants": {}, "id": "sXOGV_7LAGwkwaJUdEH5X0q6us7QJfYTKogbfMSh8EM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "145gd6z", "is_robot_indexable": true, "report_reasons": null, "author": "73637269707420", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145gd6z/github_h4nmgroppy_facilitating_regex_creation_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/H4NM/Groppy", "subreddit_subscribers": 921898, "created_utc": 1686343690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was wondering if there was a good way to simulate sales data (like for a retail store or a restaurant) that could be analyzed for how time of day/season/etc affects what is sold\n\n(Alternatively, are there any companies that happen to share this info online? I assume not but worth a shot haha)", "author_fullname": "t2_bxa84d34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simulating sales data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145fptb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686342168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if there was a good way to simulate sales data (like for a retail store or a restaurant) that could be analyzed for how time of day/season/etc affects what is sold&lt;/p&gt;\n\n&lt;p&gt;(Alternatively, are there any companies that happen to share this info online? I assume not but worth a shot haha)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145fptb", "is_robot_indexable": true, "report_reasons": null, "author": "_new_name_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145fptb/simulating_sales_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145fptb/simulating_sales_data/", "subreddit_subscribers": 921898, "created_utc": 1686342168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Xgboost gurus,\n\nDoes label encoding categorical features affect xgboost in anyway? My fear is that it would introduce some ordinality in the data and affect predictions. \n\nThe documentation suggests this and for categories with very high cardinality they suggest using techniques like Hash encoding.", "author_fullname": "t2_9atv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Label Encoding for categorical columns - xgboost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145d04o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686335618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Xgboost gurus,&lt;/p&gt;\n\n&lt;p&gt;Does label encoding categorical features affect xgboost in anyway? My fear is that it would introduce some ordinality in the data and affect predictions. &lt;/p&gt;\n\n&lt;p&gt;The documentation suggests this and for categories with very high cardinality they suggest using techniques like Hash encoding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145d04o", "is_robot_indexable": true, "report_reasons": null, "author": "longgamma", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145d04o/label_encoding_for_categorical_columns_xgboost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145d04o/label_encoding_for_categorical_columns_xgboost/", "subreddit_subscribers": 921898, "created_utc": 1686335618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How can I start this new path if I need to become a data analyst in less than a year?", "author_fullname": "t2_d4ehdg1f1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analyst", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_145zr9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686402178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How can I start this new path if I need to become a data analyst in less than a year?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145zr9o", "is_robot_indexable": true, "report_reasons": null, "author": "Fide0Udon", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145zr9o/data_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145zr9o/data_analyst/", "subreddit_subscribers": 921898, "created_utc": 1686402178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Many businesses have started using Generative AI as a powerful tool for their business.  It can help them create more accurate predictions, identify new opportunities, and optimize processes. However, every business owners and decision makers should not overlook the possible and obvious disadvantages in using Generative AI. What do you think are the disadvantages of using generative AI in the business aside from being expensive to implement, operate and maintain?", "author_fullname": "t2_8glql2df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the disadvantages in using Generative AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_145znyj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686401939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many businesses have started using Generative AI as a powerful tool for their business.  It can help them create more accurate predictions, identify new opportunities, and optimize processes. However, every business owners and decision makers should not overlook the possible and obvious disadvantages in using Generative AI. What do you think are the disadvantages of using generative AI in the business aside from being expensive to implement, operate and maintain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145znyj", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Nerd1979", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145znyj/what_are_the_disadvantages_in_using_generative_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145znyj/what_are_the_disadvantages_in_using_generative_ai/", "subreddit_subscribers": 921898, "created_utc": 1686401939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I've been working on a business analytics degree with the hopes of getting an MBA afterwards. But now as I get closer to finishing my MSBA, I realized I really like the data science part better. So I'm think of going after an MS in Data Science, specifically for the machine learning and artificial intelligence aspect of it. Any tips for it? Money is not a factor.", "author_fullname": "t2_903k40ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business analytics to data science.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145nmjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686362656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been working on a business analytics degree with the hopes of getting an MBA afterwards. But now as I get closer to finishing my MSBA, I realized I really like the data science part better. So I&amp;#39;m think of going after an MS in Data Science, specifically for the machine learning and artificial intelligence aspect of it. Any tips for it? Money is not a factor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145nmjv", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Cry-495", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145nmjv/business_analytics_to_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145nmjv/business_analytics_to_data_science/", "subreddit_subscribers": 921898, "created_utc": 1686362656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I found the model weight to a machine learning problem in the tar file but I need it in an h5 file format. What do I do? I only use TensorFlow. I'm unfamiliar with Pytorch, but the person wrote their model in Pytorch. Thanks for any help in advance.", "author_fullname": "t2_n76zm04c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tar file to h5 file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145lz0q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686357883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found the model weight to a machine learning problem in the tar file but I need it in an h5 file format. What do I do? I only use TensorFlow. I&amp;#39;m unfamiliar with Pytorch, but the person wrote their model in Pytorch. Thanks for any help in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145lz0q", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy-Currency-8127", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145lz0q/tar_file_to_h5_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145lz0q/tar_file_to_h5_file/", "subreddit_subscribers": 921898, "created_utc": 1686357883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Have an interview coming up for a jr level position, can anyone recommend some resources to freshen up on basic probability theory / statistical methods?", "author_fullname": "t2_6htadx20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic probability theory resource", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145iahj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686348266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have an interview coming up for a jr level position, can anyone recommend some resources to freshen up on basic probability theory / statistical methods?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145iahj", "is_robot_indexable": true, "report_reasons": null, "author": "jannsbababa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145iahj/basic_probability_theory_resource/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145iahj/basic_probability_theory_resource/", "subreddit_subscribers": 921898, "created_utc": 1686348266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to automate data extraction using custom trained AI models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145b16s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_32tnavmg", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "u_UBIAI", "selftext": " \n\nIf you're a financial professional and you're tired of manual data entry from bank statements, we highly recommend reading this article on how to automate data extraction using custom trained AI models. \ud83d\udc47\n\nData is growing at an unprecedented rate, and automating data extraction is crucial for maintaining efficiency and accuracy in financial transactions. With manual data entry becoming increasingly inefficient, the use of AI technology has become a game-changer. \ud83d\udcaa\ud83d\udcbb\n\nIn this article, you'll learn how to automate data extraction from bank statements using custom trained AI models and automated table extraction. It also covers the process of table extraction, training AI models for information extraction, and creating custom workflows with the new AI Builder. \u2699\ufe0f\ud83d\udca1\n\nSo if you want to increase efficiency in handling bank statements, don't miss out on the transformative potential of AI technology in the financial industry. Read the full article here: \\[[https://walidamamou.medium.com/how-to-automate-data-extraction-from-bank-statements-572c0147654](https://walidamamou.medium.com/how-to-automate-data-extraction-from-bank-statements-572c0147654) \\] \ud83d\udcda\ud83d\udd17\n\n\\#FinancialAutomation #DataExtractionSolutions #AIinFinance #StreamlineProcesses #EfficientBanking #AutomatedWorkflows #TechInFinance #DigitalTransformation #FinancialProductivity #AIAdvancements", "author_fullname": "t2_32tnavmg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to automate data extraction using custom trained AI models", "link_flair_richtext": [], "subreddit_name_prefixed": "u/UBIAI", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ww4zy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "user", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685562740.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.UBIAI", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re a financial professional and you&amp;#39;re tired of manual data entry from bank statements, we highly recommend reading this article on how to automate data extraction using custom trained AI models. \ud83d\udc47&lt;/p&gt;\n\n&lt;p&gt;Data is growing at an unprecedented rate, and automating data extraction is crucial for maintaining efficiency and accuracy in financial transactions. With manual data entry becoming increasingly inefficient, the use of AI technology has become a game-changer. \ud83d\udcaa\ud83d\udcbb&lt;/p&gt;\n\n&lt;p&gt;In this article, you&amp;#39;ll learn how to automate data extraction from bank statements using custom trained AI models and automated table extraction. It also covers the process of table extraction, training AI models for information extraction, and creating custom workflows with the new AI Builder. \u2699\ufe0f\ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;So if you want to increase efficiency in handling bank statements, don&amp;#39;t miss out on the transformative potential of AI technology in the financial industry. Read the full article here: [&lt;a href=\"https://walidamamou.medium.com/how-to-automate-data-extraction-from-bank-statements-572c0147654\"&gt;https://walidamamou.medium.com/how-to-automate-data-extraction-from-bank-statements-572c0147654&lt;/a&gt; ] \ud83d\udcda\ud83d\udd17&lt;/p&gt;\n\n&lt;p&gt;#FinancialAutomation #DataExtractionSolutions #AIinFinance #StreamlineProcesses #EfficientBanking #AutomatedWorkflows #TechInFinance #DigitalTransformation #FinancialProductivity #AIAdvancements&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "qa", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?auto=webp&amp;v=enabled&amp;s=9f0d6ddd7478695375bd3e670b58144bfd56e75a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b81999850d1b44619eaedec121806f9f984356b", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b79cee30d0c2e1b0fdfeef6e84be55399188215e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6de3eb168e3bfa0ebea944d7c6afda05bf8e0494", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a422eaaa6dc866c8dc0244218508d07d080e1cb9", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72893e6a4300e60cca9c9bca7319be28290cea22", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab102a533234a4e1fe8f1a05986296ad9c037cfb", "width": 1080, "height": 720}], "variants": {}, "id": "7P_tY0x1uaRvOwBIKSy9n_H5V6Onw1hTKj1Xb5EaQnc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2lnnxo", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ww4zy", "is_robot_indexable": true, "report_reasons": null, "author": "UBIAI", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/u_UBIAI/comments/13ww4zy/how_to_automate_data_extraction_using_custom/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/u_UBIAI/comments/13ww4zy/how_to_automate_data_extraction_using_custom/", "subreddit_subscribers": 0, "created_utc": 1685562740.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1686330909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.UBIAI", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/user/UBIAI/comments/13ww4zy/how_to_automate_data_extraction_using_custom/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?auto=webp&amp;v=enabled&amp;s=9f0d6ddd7478695375bd3e670b58144bfd56e75a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b81999850d1b44619eaedec121806f9f984356b", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b79cee30d0c2e1b0fdfeef6e84be55399188215e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6de3eb168e3bfa0ebea944d7c6afda05bf8e0494", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a422eaaa6dc866c8dc0244218508d07d080e1cb9", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72893e6a4300e60cca9c9bca7319be28290cea22", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/kJkEQfF7QKjFU8jyLMMNK0klvtl-h7ATXPmHGXDHW4k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab102a533234a4e1fe8f1a05986296ad9c037cfb", "width": 1080, "height": 720}], "variants": {}, "id": "7P_tY0x1uaRvOwBIKSy9n_H5V6Onw1hTKj1Xb5EaQnc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145b16s", "is_robot_indexable": true, "report_reasons": null, "author": "UBIAI", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13ww4zy", "author_flair_text_color": null, "permalink": "/r/datascience/comments/145b16s/how_to_automate_data_extraction_using_custom/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/user/UBIAI/comments/13ww4zy/how_to_automate_data_extraction_using_custom/", "subreddit_subscribers": 921898, "created_utc": 1686330909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've just completed my first year in bachelor's data science, but I haven't worked on a real world data, so I haven't seen how I do practically without guidance can you guys telle where I can get data for practice and also what to do with it.", "author_fullname": "t2_flfng4ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real world data practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145qzgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686373210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just completed my first year in bachelor&amp;#39;s data science, but I haven&amp;#39;t worked on a real world data, so I haven&amp;#39;t seen how I do practically without guidance can you guys telle where I can get data for practice and also what to do with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145qzgm", "is_robot_indexable": true, "report_reasons": null, "author": "Bulky-Top3782", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145qzgm/real_world_data_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145qzgm/real_world_data_practice/", "subreddit_subscribers": 921898, "created_utc": 1686373210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If you were asked to pick how you wanted to be interviewed for a DS/ML role knowing that other candidates were given the same option what steps would you pick?", "author_fullname": "t2_5bfewy53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview - Pick Your Own Adventure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145odga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686364892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you were asked to pick how you wanted to be interviewed for a DS/ML role knowing that other candidates were given the same option what steps would you pick?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145odga", "is_robot_indexable": true, "report_reasons": null, "author": "hownottopetacat", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145odga/interview_pick_your_own_adventure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145odga/interview_pick_your_own_adventure/", "subreddit_subscribers": 921898, "created_utc": 1686364892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is anyone able to comment if there are any red flags for [AIcore](https://www.theaicore.com/course)?", "author_fullname": "t2_7bl8nnsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AICore course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145ghzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686344003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone able to comment if there are any red flags for &lt;a href=\"https://www.theaicore.com/course\"&gt;AIcore&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?auto=webp&amp;v=enabled&amp;s=3ab8f6c55f8169f099c9cc9308a087aa25858e89", "width": 1240, "height": 610}, "resolutions": [{"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1095eb71deeaa13cf7603a78c86e3034295dfb61", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d17b8525e3bba5f66e42686a9abecf3dfb7bc824", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd7ae13ce5fa5bb9a1a6f83a91bc0531078aaa0d", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=910665178816e0b0b4d7ea9b5835b312c727f276", "width": 640, "height": 314}, {"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=24cede66d8db04c693ebaa4ba77319ff422a363b", "width": 960, "height": 472}, {"url": "https://external-preview.redd.it/T5PjxVtJ5fLnVD-lien7olOz1fn0VjRvL2C1E3GWp24.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1aa45b409bf9b4efc388c93ba78e100520da238b", "width": 1080, "height": 531}], "variants": {}, "id": "qBElg6l4Bmzpo_AONeuGwPp4lJLqCIF5bvY1xq4O4h8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "145ghzi", "is_robot_indexable": true, "report_reasons": null, "author": "Q7893", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/145ghzi/aicore_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/145ghzi/aicore_course/", "subreddit_subscribers": 921898, "created_utc": 1686344003.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}