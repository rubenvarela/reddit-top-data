{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My employer has spent sickening amounts of money on various \u201cself serve\u201d analytics/bi/de tools. I have been successful on some small projects like setting up tools to generate commonly requested excel files and dashboards to let people filter and play around with stuff that been largely defined I.e return on sales\u2026. But that\u2019s akin to letting somebody drive the car. You can drive it however you want, but we built the roads so you can only go where we say you can go and if you want to drive somewhere new, the engineering team is going to have to build you a new road.  \n\nHave you ever witnessed the ever elusive \u201cself serve\u201d where entry level hourly workers are solving complex problems and generating value for companies? Or is somebody doing AVG/SUM in some carefully curated DB view in power bi and calling it ML the absolute ceiling?", "author_fullname": "t2_4nbvm2s0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you ever seen a successful \u201cself-serve\u201d implementation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1454ghq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 87, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 87, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686315161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My employer has spent sickening amounts of money on various \u201cself serve\u201d analytics/bi/de tools. I have been successful on some small projects like setting up tools to generate commonly requested excel files and dashboards to let people filter and play around with stuff that been largely defined I.e return on sales\u2026. But that\u2019s akin to letting somebody drive the car. You can drive it however you want, but we built the roads so you can only go where we say you can go and if you want to drive somewhere new, the engineering team is going to have to build you a new road.  &lt;/p&gt;\n\n&lt;p&gt;Have you ever witnessed the ever elusive \u201cself serve\u201d where entry level hourly workers are solving complex problems and generating value for companies? Or is somebody doing AVG/SUM in some carefully curated DB view in power bi and calling it ML the absolute ceiling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1454ghq", "is_robot_indexable": true, "report_reasons": null, "author": "Visible-Tennis4144", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1454ghq/have_you_ever_seen_a_successful_selfserve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1454ghq/have_you_ever_seen_a_successful_selfserve/", "subreddit_subscribers": 109904, "created_utc": 1686315161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just use DBT and snowflake to transform data for a BI layer at my job.  No python, aws, or ETL.\n\n&amp;#x200B;\n\nHow common is this, and does my role fit into data engineering?  I'm trying to figure out how my current knowledge, Snowflake and DBT, fits into the wider world of data engineering and where I can branch out next to learn more.  I'm currently snowflake core certified and was considering getting AWS certified, but it all feels like rote memorization, and I want to figure out my connection to this field first.  \n\n\nI've been reading articles, medium blogs and am hoping to read  \n\n# The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling\n\n \n\n# Fundamentals of Data Engineering: Plan and Build Robust Data Systems \n\n Designing Data-Intensive Applications: The Big Ideas", "author_fullname": "t2_c6pz2borl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just DBT and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145f8f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686341002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just use DBT and snowflake to transform data for a BI layer at my job.  No python, aws, or ETL.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How common is this, and does my role fit into data engineering?  I&amp;#39;m trying to figure out how my current knowledge, Snowflake and DBT, fits into the wider world of data engineering and where I can branch out next to learn more.  I&amp;#39;m currently snowflake core certified and was considering getting AWS certified, but it all feels like rote memorization, and I want to figure out my connection to this field first.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reading articles, medium blogs and am hoping to read  &lt;/p&gt;\n\n&lt;h1&gt;The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling&lt;/h1&gt;\n\n&lt;h1&gt;Fundamentals of Data Engineering: Plan and Build Robust Data Systems&lt;/h1&gt;\n\n&lt;p&gt;Designing Data-Intensive Applications: The Big Ideas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "145f8f7", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious-Fox-9625", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145f8f7/just_dbt_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145f8f7/just_dbt_and_snowflake/", "subreddit_subscribers": 109904, "created_utc": 1686341002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a bit of an analytical SQL a-ha moment today while working out a BI calculation, something I would previously have leaned on looping in Python for. It felt good and someone might get something out of it so I thought I'd share.\n\nThe puzzle: Calculate how much time a service desk ticket has been worked during business hours (09:00 - 17:00 Mon-Fri, no holidays). Note: Ticket duration can be multiple days.\n\nfact_service_desk_tickets\n--------------------------\nticket_id | start_timestamp | end_timestamp | date_key | time_key\n.\n\ndimension_date\n---------------\ndate_key | full_date | year | month | calendar_week | day_of_week | weekend_indicator | holiday_indicator\n.\n\nMy initial thought was that I need to generate a 1-minute time series for each ticket, which would have taken forever to run. The a-ha moment was realizing I can use least(), greatest(), and a where clause on a table with 1 day granularity.\n\nSQL (Postgres)\n-----\n-- Break the tickets into 1-day rows and add the _date column to represent the day.\n\nwith\n\n  day_series as (\n\n    select \n\n      ticket_id\n\n      ,start_timestamp\n\n      ,end_timestamp\n\n      ,generate_series(date_trunc('day', start_timestamp), date_trunc('day', end_timestamp), '1 day'::interval)::date as _date\n\n    from fact_service_desk_tickets\n\n),\n\n-- Add start and end times for each day the ticket was worked on (total, not business hours), as well as columns with business hours start and end time.\n\n  main as (\n\n    select\n      ticket_id\n\n      ,start_timestamp\n\n      ,end_timestamp\n\n      ,_date\n\n      ,case \n\n        when start_timestamp::date = _date then start_timestamp::date\n\n        else '00:00:00'::time\n\n      end as start_time\n\n      ,case \n\n        when end_timestamp::date = _date then end_timestamp::date\n\n        else '23:59:59'::time\n\n      end as end_time\n\n      ,'09:30'::time as start_business_time\n\n      ,'15:30'::time as end_business_time\n\n    from day_series ds\n\n    join dimension_date dd on ds._date = dd.full_date\n\n    where\n\n      dd.weekday_indicator = 'Weekday'\n\n      and dd.holiday_indicator = 'Non-holiday'\n\nselect\n\n    ticket_id\n\n    ,sum(extract(epoch from \n\n      least(end_time, end_business_time) - greatest(start_time, start_business_time) as business_hours_duration_seconds\n\nfrom main\n\nwhere\n\n  least(end_time, end_business_time) &gt; greatest(start_time, start_business_time)\n\ngroup by ticket_id\n\n\nUsing least() and greatest() in the final select clause lets us pick the correct start and end times for that day (for start time: either start of business hours or when the ticket was submitted, whichever is later. vice versa for end time).\n\nAnd then in the where clause, we filter out days where the ticket was not worked during business hours (resulting in a negative number).\n\n\nCongrats if you made it here and thanks for reading. Feel free to share some of your data a-ha moments in comments!\n\n\np.s. I have never done leetcode or interview prep, are the problems ever this tough?\n      \n p.p.s. Sorry about this cursed formatting", "author_fullname": "t2_cqvp4nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A-ha moments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145ig4o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686348660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a bit of an analytical SQL a-ha moment today while working out a BI calculation, something I would previously have leaned on looping in Python for. It felt good and someone might get something out of it so I thought I&amp;#39;d share.&lt;/p&gt;\n\n&lt;p&gt;The puzzle: Calculate how much time a service desk ticket has been worked during business hours (09:00 - 17:00 Mon-Fri, no holidays). Note: Ticket duration can be multiple days.&lt;/p&gt;\n\n&lt;h2&gt;fact_service_desk_tickets&lt;/h2&gt;\n\n&lt;p&gt;ticket_id | start_timestamp | end_timestamp | date_key | time_key\n.&lt;/p&gt;\n\n&lt;h2&gt;dimension_date&lt;/h2&gt;\n\n&lt;p&gt;date_key | full_date | year | month | calendar_week | day_of_week | weekend_indicator | holiday_indicator\n.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was that I need to generate a 1-minute time series for each ticket, which would have taken forever to run. The a-ha moment was realizing I can use least(), greatest(), and a where clause on a table with 1 day granularity.&lt;/p&gt;\n\n&lt;h2&gt;SQL (Postgres)&lt;/h2&gt;\n\n&lt;p&gt;-- Break the tickets into 1-day rows and add the _date column to represent the day.&lt;/p&gt;\n\n&lt;p&gt;with&lt;/p&gt;\n\n&lt;p&gt;day_series as (&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select \n\n  ticket_id\n\n  ,start_timestamp\n\n  ,end_timestamp\n\n  ,generate_series(date_trunc(&amp;#39;day&amp;#39;, start_timestamp), date_trunc(&amp;#39;day&amp;#39;, end_timestamp), &amp;#39;1 day&amp;#39;::interval)::date as _date\n\nfrom fact_service_desk_tickets\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;),&lt;/p&gt;\n\n&lt;p&gt;-- Add start and end times for each day the ticket was worked on (total, not business hours), as well as columns with business hours start and end time.&lt;/p&gt;\n\n&lt;p&gt;main as (&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select\n  ticket_id\n\n  ,start_timestamp\n\n  ,end_timestamp\n\n  ,_date\n\n  ,case \n\n    when start_timestamp::date = _date then start_timestamp::date\n\n    else &amp;#39;00:00:00&amp;#39;::time\n\n  end as start_time\n\n  ,case \n\n    when end_timestamp::date = _date then end_timestamp::date\n\n    else &amp;#39;23:59:59&amp;#39;::time\n\n  end as end_time\n\n  ,&amp;#39;09:30&amp;#39;::time as start_business_time\n\n  ,&amp;#39;15:30&amp;#39;::time as end_business_time\n\nfrom day_series ds\n\njoin dimension_date dd on ds._date = dd.full_date\n\nwhere\n\n  dd.weekday_indicator = &amp;#39;Weekday&amp;#39;\n\n  and dd.holiday_indicator = &amp;#39;Non-holiday&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;select&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ticket_id\n\n,sum(extract(epoch from \n\n  least(end_time, end_business_time) - greatest(start_time, start_business_time) as business_hours_duration_seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;from main&lt;/p&gt;\n\n&lt;p&gt;where&lt;/p&gt;\n\n&lt;p&gt;least(end_time, end_business_time) &amp;gt; greatest(start_time, start_business_time)&lt;/p&gt;\n\n&lt;p&gt;group by ticket_id&lt;/p&gt;\n\n&lt;p&gt;Using least() and greatest() in the final select clause lets us pick the correct start and end times for that day (for start time: either start of business hours or when the ticket was submitted, whichever is later. vice versa for end time).&lt;/p&gt;\n\n&lt;p&gt;And then in the where clause, we filter out days where the ticket was not worked during business hours (resulting in a negative number).&lt;/p&gt;\n\n&lt;p&gt;Congrats if you made it here and thanks for reading. Feel free to share some of your data a-ha moments in comments!&lt;/p&gt;\n\n&lt;p&gt;p.s. I have never done leetcode or interview prep, are the problems ever this tough?&lt;/p&gt;\n\n&lt;p&gt;p.p.s. Sorry about this cursed formatting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "145ig4o", "is_robot_indexable": true, "report_reasons": null, "author": "udonthave2call", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145ig4o/aha_moments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145ig4o/aha_moments/", "subreddit_subscribers": 109904, "created_utc": 1686348660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am a student working in the area of data lineage and data provenance. I have created this Python library called LineageX, which it aims to generate the column-level lineage information for the inputted SQLs. This tool can create an interactive graph on a webpage to explore the column level lineage, it works with or without a database connection(Currently only supports Postgres for connection, other connection types or dialects are under development). It is also implemented as a dbt package using the same core (also only Postgres connection, and an active connection is a must).\n\nIf you are interested, you are welcome to try it out and any feedback is much appreciated!\n\nGithub:[https://github.com/sfu-db/lineagex](https://github.com/sfu-db/lineagex), dbt package: [https://github.com/sfu-db/dbt-lineagex](https://github.com/sfu-db/dbt-lineagex)\n\nPypi: [https://pypi.org/project/lineagex/](https://pypi.org/project/lineagex/)\n\nBlog: [https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3](https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3)\n\nThank you very much in advance!", "author_fullname": "t2_14ztj9sa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing LineageX - The Python library for your lineage needs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kaqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686353343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am a student working in the area of data lineage and data provenance. I have created this Python library called LineageX, which it aims to generate the column-level lineage information for the inputted SQLs. This tool can create an interactive graph on a webpage to explore the column level lineage, it works with or without a database connection(Currently only supports Postgres for connection, other connection types or dialects are under development). It is also implemented as a dbt package using the same core (also only Postgres connection, and an active connection is a must).&lt;/p&gt;\n\n&lt;p&gt;If you are interested, you are welcome to try it out and any feedback is much appreciated!&lt;/p&gt;\n\n&lt;p&gt;Github:&lt;a href=\"https://github.com/sfu-db/lineagex\"&gt;https://github.com/sfu-db/lineagex&lt;/a&gt;, dbt package: &lt;a href=\"https://github.com/sfu-db/dbt-lineagex\"&gt;https://github.com/sfu-db/dbt-lineagex&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Pypi: &lt;a href=\"https://pypi.org/project/lineagex/\"&gt;https://pypi.org/project/lineagex/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3\"&gt;https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?auto=webp&amp;v=enabled&amp;s=b5b3976ea79f0ec74994df21318c60a665d68b27", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20a3eff69e35037a875e682de9359a42d017f885", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b04c7c7fd21657219c4ad3a1b739e5a62753119a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d9d015cdb5eb4b081c1cda343162d54d34806a2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f50446b6e9e85e28d8d4675ab447f29e5975424c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecbf9d8396494782f1414d86dc0397ca4b125d53", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ca672e3800f980ee24dbf59e1f459f8dc13b66f", "width": 1080, "height": 540}], "variants": {}, "id": "c8YbTeknXpj4L5WxZHfgC4nLTCIowkaTIaAAvDkTpwo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "145kaqy", "is_robot_indexable": true, "report_reasons": null, "author": "zshandy1994", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145kaqy/introducing_lineagex_the_python_library_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145kaqy/introducing_lineagex_the_python_library_for_your/", "subreddit_subscribers": 109904, "created_utc": 1686353343.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A lot of code we have in our pipelines is written in R, and I am hoping to swap over to python so that more folks on the team can read/write code.\n\nAny tips for transitioning someone from R to pandas?  I am not an expert in R so trying to find good resources.\n\nAlso what would be the equivalent of shiny in python?  \n\nWe have two apps we would like to port over - has a dropdown that controls data in a timeline.  \n\nThanks!", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from R to Python - any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1450x5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686304362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of code we have in our pipelines is written in R, and I am hoping to swap over to python so that more folks on the team can read/write code.&lt;/p&gt;\n\n&lt;p&gt;Any tips for transitioning someone from R to pandas?  I am not an expert in R so trying to find good resources.&lt;/p&gt;\n\n&lt;p&gt;Also what would be the equivalent of shiny in python?  &lt;/p&gt;\n\n&lt;p&gt;We have two apps we would like to port over - has a dropdown that controls data in a timeline.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1450x5k", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1450x5k/transitioning_from_r_to_python_any_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1450x5k/transitioning_from_r_to_python_any_tips/", "subreddit_subscribers": 109904, "created_utc": 1686304362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nI'm currently managing a data lake architecture on AWS. Our data pipelines consist of workflows in Step Functions and some Lambda functions that do the ETL and move the data through all the different zones. We handle relatively low volume data.\n\nThe lambda functions are python scripts that use the awswrangler library for the whole ETL.  \nI'm running a query against different databases, [creating the glue table](https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.catalog.create_parquet_table.html#awswrangler.catalog.create_parquet_table) in case it doesn't exist, generating the corresponding path  (eg: domain/subdomain/year/month/day/&lt;process\\_date&gt;.parquet), [writing the parquet](https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html#awswrangler.s3.to_parquet) (with dataset=False), add a **process\\_date** column and then run an **ALTER TABLE ADD PARTITION** statement against Athena to add the recently file as a partition for the column **process\\_date.**   \nI'm doing all of this because i need to store the parquet file with a partition path as I mentioned above, yet I need the dataset to be partitioned by process\\_date (with a format YYYYMMDD), i don't want process\\_date to be part of the path like a hive partition.  \n\n\nIs this approach correct? I still don't totally get if I 100% need to use a partition\\_by() method or partition\\_cols parameter (in this case) and if that produces a change in the physical output file. Will I make use of partition pruning this way? The partitions are correctly recognized on Athena\n\nIf someone could provide some resources/book to read it'll be great, from what i searched there is no real difference between using a partition method and generating the folder structure myself. Adding partitions manually was the only alternative I found on this [Athena doc page](https://docs.aws.amazon.com/athena/latest/ug/partitions.html) (Scenario 2).\n\nI also read about bucketing, i think it doesn't suits me for this use case but it will in the future maybe if I need to improve performance for a high cardinality field. Does bucketing affect the final .parquet file?", "author_fullname": "t2_dpj60sgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correct way of partitioning parquet files on AWS data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kz4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686355155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently managing a data lake architecture on AWS. Our data pipelines consist of workflows in Step Functions and some Lambda functions that do the ETL and move the data through all the different zones. We handle relatively low volume data.&lt;/p&gt;\n\n&lt;p&gt;The lambda functions are python scripts that use the awswrangler library for the whole ETL.&lt;br/&gt;\nI&amp;#39;m running a query against different databases, &lt;a href=\"https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.catalog.create_parquet_table.html#awswrangler.catalog.create_parquet_table\"&gt;creating the glue table&lt;/a&gt; in case it doesn&amp;#39;t exist, generating the corresponding path  (eg: domain/subdomain/year/month/day/&amp;lt;process\\_date&amp;gt;.parquet), &lt;a href=\"https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html#awswrangler.s3.to_parquet\"&gt;writing the parquet&lt;/a&gt; (with dataset=False), add a &lt;strong&gt;process_date&lt;/strong&gt; column and then run an &lt;strong&gt;ALTER TABLE ADD PARTITION&lt;/strong&gt; statement against Athena to add the recently file as a partition for the column &lt;strong&gt;process_date.&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m doing all of this because i need to store the parquet file with a partition path as I mentioned above, yet I need the dataset to be partitioned by process_date (with a format YYYYMMDD), i don&amp;#39;t want process_date to be part of the path like a hive partition.  &lt;/p&gt;\n\n&lt;p&gt;Is this approach correct? I still don&amp;#39;t totally get if I 100% need to use a partition_by() method or partition_cols parameter (in this case) and if that produces a change in the physical output file. Will I make use of partition pruning this way? The partitions are correctly recognized on Athena&lt;/p&gt;\n\n&lt;p&gt;If someone could provide some resources/book to read it&amp;#39;ll be great, from what i searched there is no real difference between using a partition method and generating the folder structure myself. Adding partitions manually was the only alternative I found on this &lt;a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\"&gt;Athena doc page&lt;/a&gt; (Scenario 2).&lt;/p&gt;\n\n&lt;p&gt;I also read about bucketing, i think it doesn&amp;#39;t suits me for this use case but it will in the future maybe if I need to improve performance for a high cardinality field. Does bucketing affect the final .parquet file?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145kz4n", "is_robot_indexable": true, "report_reasons": null, "author": "_unwin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145kz4n/correct_way_of_partitioning_parquet_files_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145kz4n/correct_way_of_partitioning_parquet_files_on_aws/", "subreddit_subscribers": 109904, "created_utc": 1686355155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On the interwebs, I feel like I am constantly reading different ways to structure the gold layer for serving data and analytics in the Lakehouse and want to clarify how you all approach it.\n\n**TLDR:** I see two approaches for the Gold Layer. What is preferred, and how do you approach the gold layer?\n\n* **(1)** Data in the Gold Layer is used to build data products or \n* **(2)** Data in the Gold Layer is the data product. \n\nI commonly see that the gold layer is your \"Kimball-style Warehouse\" layer. To me, that means facts and conformed dimensions, a bit more generic. I can easily create specific views and OBT's for reporting from the facts and dimensions as needed. This seems like a more traditional warehousing approach. *I guess is more of a Bronze (raw) -&gt; Silver (3nf) -&gt; Gold (Kimball) -&gt; Presentation*\n\nOn the other hand, I also see that the gold layer consists of \"Project-Specific\" Databases. When I think of something being \"project-specific\" I don't think of a Kimball warehouse with conformed dimensions and so on. I think of a process where I create these facts and dims for project 1, and then I create new facts and dims for project 2. To me, this seems like I would create multiple independent dimensional models for each project, where dimensions are not shared across models. This would be more of a *Bronze (raw) -&gt; Silver (3nf) -&gt; Gold (independent stars)*\n\nMaybe like most things, both a valid and it just depends on the org. Happy to expand and discuss with you all!\n\n&amp;#x200B;\n\nEDIT: Added TLDR", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring the Gold Layer in the Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1458fne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686325276.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686324882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the interwebs, I feel like I am constantly reading different ways to structure the gold layer for serving data and analytics in the Lakehouse and want to clarify how you all approach it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I see two approaches for the Gold Layer. What is preferred, and how do you approach the gold layer?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;(1)&lt;/strong&gt; Data in the Gold Layer is used to build data products or &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;(2)&lt;/strong&gt; Data in the Gold Layer is the data product. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I commonly see that the gold layer is your &amp;quot;Kimball-style Warehouse&amp;quot; layer. To me, that means facts and conformed dimensions, a bit more generic. I can easily create specific views and OBT&amp;#39;s for reporting from the facts and dimensions as needed. This seems like a more traditional warehousing approach. &lt;em&gt;I guess is more of a Bronze (raw) -&amp;gt; Silver (3nf) -&amp;gt; Gold (Kimball) -&amp;gt; Presentation&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I also see that the gold layer consists of &amp;quot;Project-Specific&amp;quot; Databases. When I think of something being &amp;quot;project-specific&amp;quot; I don&amp;#39;t think of a Kimball warehouse with conformed dimensions and so on. I think of a process where I create these facts and dims for project 1, and then I create new facts and dims for project 2. To me, this seems like I would create multiple independent dimensional models for each project, where dimensions are not shared across models. This would be more of a &lt;em&gt;Bronze (raw) -&amp;gt; Silver (3nf) -&amp;gt; Gold (independent stars)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Maybe like most things, both a valid and it just depends on the org. Happy to expand and discuss with you all!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT: Added TLDR&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1458fne", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1458fne/structuring_the_gold_layer_in_the_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1458fne/structuring_the_gold_layer_in_the_lakehouse/", "subreddit_subscribers": 109904, "created_utc": 1686324882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone! I hope you are well. I have a problem with one scraper in my job, is too slow. It was made with selenium, and the scraper download files too. I was searching for a technology that helps me to run multiples instances of the same scraper (for example 8). I have access to GCP and I was thinking to use cloud jobs, but I'm not sure if is the best option, because this scraper run all the time, I mean 24/7, I can edit them so they can stop every 5 minutes and then continue. I'm not sure which is the best solution or technology. Sorry if my English is bad!\nHave a nice day:D\nThanks.", "author_fullname": "t2_cwa2139f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run multiples instances of a scraper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145eln2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686339430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I hope you are well. I have a problem with one scraper in my job, is too slow. It was made with selenium, and the scraper download files too. I was searching for a technology that helps me to run multiples instances of the same scraper (for example 8). I have access to GCP and I was thinking to use cloud jobs, but I&amp;#39;m not sure if is the best option, because this scraper run all the time, I mean 24/7, I can edit them so they can stop every 5 minutes and then continue. I&amp;#39;m not sure which is the best solution or technology. Sorry if my English is bad!\nHave a nice day:D\nThanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145eln2", "is_robot_indexable": true, "report_reasons": null, "author": "Ramcer2", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145eln2/how_to_run_multiples_instances_of_a_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145eln2/how_to_run_multiples_instances_of_a_scraper/", "subreddit_subscribers": 109904, "created_utc": 1686339430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I haven't taken the course yet in school, I'm an upcoming senior undergrad\n\nStarted learning dsa slowly so I can start leetcoding python easy's for a technical interview. I have a score of 60 but it is all SQL, can do mediums and easys.\n\nI'm just wondering if I'll be able to pass an interview for winter internships -\n\nand if I should use this month to cover as much as I can with DSA. I realize that's not the major focus of a DE internship, but I'm afraid it will weed me out of nice prospects if I can't pass any technical python questions.\n\n\\*would delay me organizing my github\n\n\\*would rather work on third project\n\n&amp;#x200B;\n\n* 2 pipeline projects\n* Databricks and Azure Data Factory project\n* Dimensional Modeling knowledge\n\nPySpark, Scala, SQL\n\n&amp;#x200B;\n\n(0 experience, absolutely no experience with internships)", "author_fullname": "t2_pwk2f3iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I continue DSA before applying to internships?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145lxvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686357787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t taken the course yet in school, I&amp;#39;m an upcoming senior undergrad&lt;/p&gt;\n\n&lt;p&gt;Started learning dsa slowly so I can start leetcoding python easy&amp;#39;s for a technical interview. I have a score of 60 but it is all SQL, can do mediums and easys.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just wondering if I&amp;#39;ll be able to pass an interview for winter internships -&lt;/p&gt;\n\n&lt;p&gt;and if I should use this month to cover as much as I can with DSA. I realize that&amp;#39;s not the major focus of a DE internship, but I&amp;#39;m afraid it will weed me out of nice prospects if I can&amp;#39;t pass any technical python questions.&lt;/p&gt;\n\n&lt;p&gt;*would delay me organizing my github&lt;/p&gt;\n\n&lt;p&gt;*would rather work on third project&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2 pipeline projects&lt;/li&gt;\n&lt;li&gt;Databricks and Azure Data Factory project&lt;/li&gt;\n&lt;li&gt;Dimensional Modeling knowledge&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PySpark, Scala, SQL&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(0 experience, absolutely no experience with internships)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145lxvb", "is_robot_indexable": true, "report_reasons": null, "author": "CowUnfair4318", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145lxvb/should_i_continue_dsa_before_applying_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145lxvb/should_i_continue_dsa_before_applying_to/", "subreddit_subscribers": 109904, "created_utc": 1686357787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the challenges u faced on azure adf and databricks from your experience", "author_fullname": "t2_5nx7csx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mention azure adf and azure darabricks challenges u faced from your experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kwll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686354961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the challenges u faced on azure adf and databricks from your experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "145kwll", "is_robot_indexable": true, "report_reasons": null, "author": "pavan449", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145kwll/mention_azure_adf_and_azure_darabricks_challenges/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145kwll/mention_azure_adf_and_azure_darabricks_challenges/", "subreddit_subscribers": 109904, "created_utc": 1686354961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to set up a pipeline to load data from Postgres to OpenSearch. The OpenSearch model includes some fields that are lists of IDs from many to many mappings and some other more complex transformations. I am looking for non-expensive solutions that are not super hard to implement and manage. I looked into AWS DMS but the transformations seem super limited? If anyone has any suggestions I'm happy to listen!", "author_fullname": "t2_w1tdhbrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage loading data from RDS to OpenSearch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145098s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686302123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to set up a pipeline to load data from Postgres to OpenSearch. The OpenSearch model includes some fields that are lists of IDs from many to many mappings and some other more complex transformations. I am looking for non-expensive solutions that are not super hard to implement and manage. I looked into AWS DMS but the transformations seem super limited? If anyone has any suggestions I&amp;#39;m happy to listen!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145098s", "is_robot_indexable": true, "report_reasons": null, "author": "data_pie3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145098s/how_do_you_manage_loading_data_from_rds_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145098s/how_do_you_manage_loading_data_from_rds_to/", "subreddit_subscribers": 109904, "created_utc": 1686302123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I\u2019ve been tasked with recommending improvements to the current stack that\u2019s struggling with performance but I need some help. \n\nThe setup is RDS Postgres as a data warehouse and glue for transformations. \n\nWe have a team of only two mid level data engineering who are familiar with PySpark but not familiar with optimising spark for performance. \n\nI\u2019m considering recommending using DBT (which I\u2019m familiar with) and Athena or Snowflake (the company is considering redshift but I\u2019m not seeing good things about it). \n\nWe ingest about 2 - 3 million records per day and it\u2019s currently not in a compressed format.  I can\u2019t quite remember the size but it\u2019s in gigabytes, and will compress to megabytes in parquet. That being said, this is an MVP and is expected to grow to terabytes within a year. \n\nWe\u2019re likely to ingest more data types from more sources in the near future. \n\nMy current thinking is that we could stick with glue and S3 and build a pure data lake, or introduce DBT and Snowflake which I think can handle small-ish file sizes and will scale nicely. \n\nI don\u2019t have enough experience to know what considerations I might be missing so any advice would be amazing!", "author_fullname": "t2_brgs3nc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Selecting tools for a small team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145hpo2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686346893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I\u2019ve been tasked with recommending improvements to the current stack that\u2019s struggling with performance but I need some help. &lt;/p&gt;\n\n&lt;p&gt;The setup is RDS Postgres as a data warehouse and glue for transformations. &lt;/p&gt;\n\n&lt;p&gt;We have a team of only two mid level data engineering who are familiar with PySpark but not familiar with optimising spark for performance. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m considering recommending using DBT (which I\u2019m familiar with) and Athena or Snowflake (the company is considering redshift but I\u2019m not seeing good things about it). &lt;/p&gt;\n\n&lt;p&gt;We ingest about 2 - 3 million records per day and it\u2019s currently not in a compressed format.  I can\u2019t quite remember the size but it\u2019s in gigabytes, and will compress to megabytes in parquet. That being said, this is an MVP and is expected to grow to terabytes within a year. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re likely to ingest more data types from more sources in the near future. &lt;/p&gt;\n\n&lt;p&gt;My current thinking is that we could stick with glue and S3 and build a pure data lake, or introduce DBT and Snowflake which I think can handle small-ish file sizes and will scale nicely. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have enough experience to know what considerations I might be missing so any advice would be amazing!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145hpo2", "is_robot_indexable": true, "report_reasons": null, "author": "hippiecampus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145hpo2/selecting_tools_for_a_small_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145hpo2/selecting_tools_for_a_small_team/", "subreddit_subscribers": 109904, "created_utc": 1686346893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently entered the field of data and have been working as a DA intern at a small startup for a year. The company utilizes GCP services, and my usual responsibilities include scheduling with Airflow, data modeling with dbt, and providing reports to the business team. I have noticed that I am more interested in the governance and operational aspects of data pipelines compared to being a DA.\n\nIs it necessary to learn Hadoop and Spark for cloud-native trends if I want to be a DE in the future ?", "author_fullname": "t2_846xgjv4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it necessary to learn Hadoop and Spark for cloud-native trends ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145b31e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686333302.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686331029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently entered the field of data and have been working as a DA intern at a small startup for a year. The company utilizes GCP services, and my usual responsibilities include scheduling with Airflow, data modeling with dbt, and providing reports to the business team. I have noticed that I am more interested in the governance and operational aspects of data pipelines compared to being a DA.&lt;/p&gt;\n\n&lt;p&gt;Is it necessary to learn Hadoop and Spark for cloud-native trends if I want to be a DE in the future ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "145b31e", "is_robot_indexable": true, "report_reasons": null, "author": "Ssnakei", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145b31e/is_it_necessary_to_learn_hadoop_and_spark_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145b31e/is_it_necessary_to_learn_hadoop_and_spark_for/", "subreddit_subscribers": 109904, "created_utc": 1686331029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently implementing a data mesh on an Azure eco system and a major part of our work is building data products for reusability between multiple use cases. My manager asked me to explore the opportunities of synthetic data generation to help with some of the data product implementation. Only a very high level direction is given. As per my understanding (and whatever further research I did ) , this is mostly used for Data Science space for Model training and similar stuff.\n\nAnyone here had a need to use synthetic data generation mechanism for their data platform? And how did it help you . Please suggest.", "author_fullname": "t2_qshu8mn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone worked on synthetic data generation for Data Product Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14558lq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686317169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently implementing a data mesh on an Azure eco system and a major part of our work is building data products for reusability between multiple use cases. My manager asked me to explore the opportunities of synthetic data generation to help with some of the data product implementation. Only a very high level direction is given. As per my understanding (and whatever further research I did ) , this is mostly used for Data Science space for Model training and similar stuff.&lt;/p&gt;\n\n&lt;p&gt;Anyone here had a need to use synthetic data generation mechanism for their data platform? And how did it help you . Please suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14558lq", "is_robot_indexable": true, "report_reasons": null, "author": "Global_Industry_6801", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14558lq/anyone_worked_on_synthetic_data_generation_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14558lq/anyone_worked_on_synthetic_data_generation_for/", "subreddit_subscribers": 109904, "created_utc": 1686317169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm trying to set up an ETL data pipeline from scratch, and need ideas/suggestions around the **E** &amp; **T** part of **ETL**. Let me explain it right from the data source to the final sink.\n\n# E\n\nAs the data source, what I have is a gRPC stream. I get data in protobuf encoded format from it. This is a fixed part in the overall system, there is no other way to extract the data. We plan to ingest this data in [delta lake](https://github.com/delta-io/delta), but before we do that there are a few problems.\n\nEach protobuf that I get from the stream, contains 4-5 different kinds of entities. Depending on the kind of entity, it should be inserted in the correct table in the delta lake. So, I need to first decode the protobuf and then figure out those entities and then accordingly write to delta. One particular constraint I have here is that a specific kind of entity comes encoded in such a format for which decoding is only possible in Golang, that encoding library doesn't exist in any other languages, and neither do we want to implement it for other langs. Also, to date, there is no good delta writer for Go. So, the way we were thinking to go about this problem was through two passthrough services.\n\n* gRPC Stream ---(protobuf)---&gt; Go decoder service ---(json)---&gt; Java/Scala Delta writer service ---(add entries to 4-5 different tables)---&gt; Delta lake\n* The Go decoder service would decode all the entities from within the protobuf, and put them all in a JSON object.\n* The Java/Scala delta writer service would use the JSON objects from the Go service, and write those entities to their respective tables in delta. If this service can write the entities in the order of the stream, that would be best, as then we won't have to run Z-ordering optimizations on all the different delta lake tables.\n* The problems we see with this are:\n   * The overhead of state management around consuming the gRPC Stream so that we can correctly resume the stream from the right place, if it or any of the downstream services broke in between. The gRPC stream does allow us to resume given we provide it a cursor, but we will have to manage that cursor on our end.\n   * Service management for the decoder and delta writer services. Ensuring that they never go down.\n   * There might be more decoder services in different languages in the future (depending on the libs that exist only in those languages), which will introduce more headache around service management.\n\n***Q1***: Does anyone have ideas or know an existing solution/workflow/framework that can help easily manage writing gRPC streams to delta with some (language agnostic) transformation involved in between? We just started looking into Apache Flink, but we don't know about its full capabilities yet!\n\n# T\n\nOnce we have the data in delta lake, there are two kinds of transformations that we foresee:\n\n1. Complex ones involving joins across tables. Expressing these transforms is feasible in SQL. We plan to use Trino for carrying out these transforms. Also, such transforms would be known in advance, so we plan to use Dagster+DBT for the orchestration part.\n2. Simple ones that map each row in one table to a new row in another table. Expressing these transforms is not feasible in SQL. There are some computations that we need to do on the row content that is easily expressible only in a programming language like Go/Python etc. This is sort of comparable to the decoder service we were previously talking about in the **E** (Extract) part. The only thing is that there would be a lot of such transforms, and we want to spend as less time on a single transform as possible. Any such transform can divide its work into 10 different chunks all of which can run in parallel as the transform of one row doesn't affect the transform of another row. Trino seems to support custom functions in Java, so even expressing these transforms via SQL to trino might be feasible. But, we aren't sure around the orchestration part. Such transforms won't be known upfront. Anyone can request for such transforms anytime. We know Databricks DLT with Python support can be helpful here, but we don't want to stick to any vendor-specific solutions. We want to build it out using open-source tools so that we should be able to manage it ourselves.\n\n**Q2**: Any advice/ideas around the transformation part? Specifically, the 2nd point. Would it be better to merge it within the decoder service in the Extraction phase? If so, any way to parallelize the compute involved in such a transform in an easy way?\n\n# L\n\nThis again is a fixed part of the system. Trino on top of Delta Lake is something that we are finalized with to serve the user queries.", "author_fullname": "t2_5wv31mqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas/Suggestions around setting up a data pipeline from scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14547rb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686314526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to set up an ETL data pipeline from scratch, and need ideas/suggestions around the &lt;strong&gt;E&lt;/strong&gt; &amp;amp; &lt;strong&gt;T&lt;/strong&gt; part of &lt;strong&gt;ETL&lt;/strong&gt;. Let me explain it right from the data source to the final sink.&lt;/p&gt;\n\n&lt;h1&gt;E&lt;/h1&gt;\n\n&lt;p&gt;As the data source, what I have is a gRPC stream. I get data in protobuf encoded format from it. This is a fixed part in the overall system, there is no other way to extract the data. We plan to ingest this data in &lt;a href=\"https://github.com/delta-io/delta\"&gt;delta lake&lt;/a&gt;, but before we do that there are a few problems.&lt;/p&gt;\n\n&lt;p&gt;Each protobuf that I get from the stream, contains 4-5 different kinds of entities. Depending on the kind of entity, it should be inserted in the correct table in the delta lake. So, I need to first decode the protobuf and then figure out those entities and then accordingly write to delta. One particular constraint I have here is that a specific kind of entity comes encoded in such a format for which decoding is only possible in Golang, that encoding library doesn&amp;#39;t exist in any other languages, and neither do we want to implement it for other langs. Also, to date, there is no good delta writer for Go. So, the way we were thinking to go about this problem was through two passthrough services.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;gRPC Stream ---(protobuf)---&amp;gt; Go decoder service ---(json)---&amp;gt; Java/Scala Delta writer service ---(add entries to 4-5 different tables)---&amp;gt; Delta lake&lt;/li&gt;\n&lt;li&gt;The Go decoder service would decode all the entities from within the protobuf, and put them all in a JSON object.&lt;/li&gt;\n&lt;li&gt;The Java/Scala delta writer service would use the JSON objects from the Go service, and write those entities to their respective tables in delta. If this service can write the entities in the order of the stream, that would be best, as then we won&amp;#39;t have to run Z-ordering optimizations on all the different delta lake tables.&lt;/li&gt;\n&lt;li&gt;The problems we see with this are:\n\n&lt;ul&gt;\n&lt;li&gt;The overhead of state management around consuming the gRPC Stream so that we can correctly resume the stream from the right place, if it or any of the downstream services broke in between. The gRPC stream does allow us to resume given we provide it a cursor, but we will have to manage that cursor on our end.&lt;/li&gt;\n&lt;li&gt;Service management for the decoder and delta writer services. Ensuring that they never go down.&lt;/li&gt;\n&lt;li&gt;There might be more decoder services in different languages in the future (depending on the libs that exist only in those languages), which will introduce more headache around service management.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Q1&lt;/em&gt;&lt;/strong&gt;: Does anyone have ideas or know an existing solution/workflow/framework that can help easily manage writing gRPC streams to delta with some (language agnostic) transformation involved in between? We just started looking into Apache Flink, but we don&amp;#39;t know about its full capabilities yet!&lt;/p&gt;\n\n&lt;h1&gt;T&lt;/h1&gt;\n\n&lt;p&gt;Once we have the data in delta lake, there are two kinds of transformations that we foresee:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Complex ones involving joins across tables. Expressing these transforms is feasible in SQL. We plan to use Trino for carrying out these transforms. Also, such transforms would be known in advance, so we plan to use Dagster+DBT for the orchestration part.&lt;/li&gt;\n&lt;li&gt;Simple ones that map each row in one table to a new row in another table. Expressing these transforms is not feasible in SQL. There are some computations that we need to do on the row content that is easily expressible only in a programming language like Go/Python etc. This is sort of comparable to the decoder service we were previously talking about in the &lt;strong&gt;E&lt;/strong&gt; (Extract) part. The only thing is that there would be a lot of such transforms, and we want to spend as less time on a single transform as possible. Any such transform can divide its work into 10 different chunks all of which can run in parallel as the transform of one row doesn&amp;#39;t affect the transform of another row. Trino seems to support custom functions in Java, so even expressing these transforms via SQL to trino might be feasible. But, we aren&amp;#39;t sure around the orchestration part. Such transforms won&amp;#39;t be known upfront. Anyone can request for such transforms anytime. We know Databricks DLT with Python support can be helpful here, but we don&amp;#39;t want to stick to any vendor-specific solutions. We want to build it out using open-source tools so that we should be able to manage it ourselves.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Q2&lt;/strong&gt;: Any advice/ideas around the transformation part? Specifically, the 2nd point. Would it be better to merge it within the decoder service in the Extraction phase? If so, any way to parallelize the compute involved in such a transform in an easy way?&lt;/p&gt;\n\n&lt;h1&gt;L&lt;/h1&gt;\n\n&lt;p&gt;This again is a fixed part of the system. Trino on top of Delta Lake is something that we are finalized with to serve the user queries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?auto=webp&amp;v=enabled&amp;s=fa58310d4ff1d938c13a4bd89e90c9bba275089e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee96d192170a46719444a777d2130ddd3490de45", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=426fe3a6df115d3aa40ffd5b9ecd43206b9840e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d76b33a58cdd34ff3992647ad7e46af5724915ea", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=832d3f585ad218ec3ea821c5bcbe9c79adb9a434", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a6f8c57c9ac45b52413b6a9cf36a40e200822e9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Yx7MixRJduy8TvN4F-qXPzGTgO7iQ9UzyiFq7LHOBiw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39244e3eb2a9d424a843d53b0e3eb52338b79581", "width": 1080, "height": 540}], "variants": {}, "id": "DVCDhaaj9zVfsdKJ_bz5ixrpUB7CFHkJma1GKr7iYdQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14547rb", "is_robot_indexable": true, "report_reasons": null, "author": "abhimanyusinghgaur", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14547rb/ideassuggestions_around_setting_up_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14547rb/ideassuggestions_around_setting_up_a_data/", "subreddit_subscribers": 109904, "created_utc": 1686314526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Usually in our projects requirements change continuously, even for the same feature. Do you prefer a particular tool for keeping track of this? Can you give me an example of how you would use it?", "author_fullname": "t2_e4qv08k8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which tool do you use to keep track of requirements?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_144yp78", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686296633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Usually in our projects requirements change continuously, even for the same feature. Do you prefer a particular tool for keeping track of this? Can you give me an example of how you would use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "144yp78", "is_robot_indexable": true, "report_reasons": null, "author": "Busy_Elderberry8650", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/144yp78/which_tool_do_you_use_to_keep_track_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/144yp78/which_tool_do_you_use_to_keep_track_of/", "subreddit_subscribers": 109904, "created_utc": 1686296633.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}