{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am a student working in the area of data lineage and data provenance. I have created this Python library called LineageX, which it aims to generate the column-level lineage information for the inputted SQLs. This tool can create an interactive graph on a webpage to explore the column level lineage, it works with or without a database connection(Currently only supports Postgres for connection, other connection types or dialects are under development). It is also implemented as a dbt package using the same core (also only Postgres connection, and an active connection is a must).\n\nIf you are interested, you are welcome to try it out and any feedback is much appreciated!\n\nGithub:[https://github.com/sfu-db/lineagex](https://github.com/sfu-db/lineagex), dbt package: [https://github.com/sfu-db/dbt-lineagex](https://github.com/sfu-db/dbt-lineagex)\n\nPypi: [https://pypi.org/project/lineagex/](https://pypi.org/project/lineagex/)\n\nBlog: [https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3](https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3)\n\nThank you very much in advance!", "author_fullname": "t2_14ztj9sa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing LineageX - The Python library for your lineage needs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kaqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686353343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am a student working in the area of data lineage and data provenance. I have created this Python library called LineageX, which it aims to generate the column-level lineage information for the inputted SQLs. This tool can create an interactive graph on a webpage to explore the column level lineage, it works with or without a database connection(Currently only supports Postgres for connection, other connection types or dialects are under development). It is also implemented as a dbt package using the same core (also only Postgres connection, and an active connection is a must).&lt;/p&gt;\n\n&lt;p&gt;If you are interested, you are welcome to try it out and any feedback is much appreciated!&lt;/p&gt;\n\n&lt;p&gt;Github:&lt;a href=\"https://github.com/sfu-db/lineagex\"&gt;https://github.com/sfu-db/lineagex&lt;/a&gt;, dbt package: &lt;a href=\"https://github.com/sfu-db/dbt-lineagex\"&gt;https://github.com/sfu-db/dbt-lineagex&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Pypi: &lt;a href=\"https://pypi.org/project/lineagex/\"&gt;https://pypi.org/project/lineagex/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3\"&gt;https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?auto=webp&amp;v=enabled&amp;s=b5b3976ea79f0ec74994df21318c60a665d68b27", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20a3eff69e35037a875e682de9359a42d017f885", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b04c7c7fd21657219c4ad3a1b739e5a62753119a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d9d015cdb5eb4b081c1cda343162d54d34806a2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f50446b6e9e85e28d8d4675ab447f29e5975424c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecbf9d8396494782f1414d86dc0397ca4b125d53", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/g-cK0c4B6WCWdNAXdqZzDDuYqvucFJvDptM9BJU7ktM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ca672e3800f980ee24dbf59e1f459f8dc13b66f", "width": 1080, "height": 540}], "variants": {}, "id": "c8YbTeknXpj4L5WxZHfgC4nLTCIowkaTIaAAvDkTpwo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "145kaqy", "is_robot_indexable": true, "report_reasons": null, "author": "zshandy1994", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145kaqy/introducing_lineagex_the_python_library_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145kaqy/introducing_lineagex_the_python_library_for_your/", "subreddit_subscribers": 109982, "created_utc": 1686353343.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHello everyone,\n\n  I've seen (&amp; written) a lot of articles explaining batch data processing. However, not much content explains the concepts to be aware of when designing streaming data pipelines. \n\n  With that in mind, I wrote an article that covers the fundamental concepts to know when building a streaming data pipeline. It covers concepts such as \n\n  1. State\n  2. Watermarking\n  3. Backpressure\n  4. Joins in streaming systems and their caveats\n  5. Monitoring\n\nThe concepts are explained as you build a streaming data pipeline that does [clickstream attribution](https://www.shopify.com/blog/marketing-attribution#3) (which is very common in marketing).\n\n  Post: https://www.startdataengineering.com/post/data-engineering-project-for-beginners-stream-edition/\n\n  Code: https://github.com/josephmachado/beginner_de_project_stream\n\n  Appreciate any questions, feedback, or comments. I hope this helps someone.", "author_fullname": "t2_5srxspj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming data engineering project: Apache Flink, Apache Kafka, Prometheus, &amp; Graphana running on Docker.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1460qft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686405169.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686404870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen (&amp;amp; written) a lot of articles explaining batch data processing. However, not much content explains the concepts to be aware of when designing streaming data pipelines. &lt;/p&gt;\n\n&lt;p&gt;With that in mind, I wrote an article that covers the fundamental concepts to know when building a streaming data pipeline. It covers concepts such as &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;State&lt;/li&gt;\n&lt;li&gt;Watermarking&lt;/li&gt;\n&lt;li&gt;Backpressure&lt;/li&gt;\n&lt;li&gt;Joins in streaming systems and their caveats&lt;/li&gt;\n&lt;li&gt;Monitoring&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The concepts are explained as you build a streaming data pipeline that does &lt;a href=\"https://www.shopify.com/blog/marketing-attribution#3\"&gt;clickstream attribution&lt;/a&gt; (which is very common in marketing).&lt;/p&gt;\n\n&lt;p&gt;Post: &lt;a href=\"https://www.startdataengineering.com/post/data-engineering-project-for-beginners-stream-edition/\"&gt;https://www.startdataengineering.com/post/data-engineering-project-for-beginners-stream-edition/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/josephmachado/beginner_de_project_stream\"&gt;https://github.com/josephmachado/beginner_de_project_stream&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Appreciate any questions, feedback, or comments. I hope this helps someone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?auto=webp&amp;v=enabled&amp;s=70778482c60cd6bfed94df6ed567a8412a9da42f", "width": 1848, "height": 782}, "resolutions": [{"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d1f56556df76ebaf79be2e8a94f3288ccd9e9fa", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fc360f1c2f21294836f4743ef83eefcb9f9e72d", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e01063c7516663ce6170e5436bab1764cc8675bb", "width": 320, "height": 135}, {"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a38e0e3ca74a15e3933ac104f4ecda746786961c", "width": 640, "height": 270}, {"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9762565ec246613239a034f8aa168b2254f2ee0d", "width": 960, "height": 406}, {"url": "https://external-preview.redd.it/gKvUcQq6qaP2cyBDewNrVmxD0SunPbzzavemOk5U7n4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75fde1098b132f548cc67eb765c998e7dce779e1", "width": 1080, "height": 457}], "variants": {}, "id": "R19HPidXvKpirhIMGCelqrKGxVNn_syvLjPfAjnRteE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1460qft", "is_robot_indexable": true, "report_reasons": null, "author": "joseph_machado", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1460qft/streaming_data_engineering_project_apache_flink/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1460qft/streaming_data_engineering_project_apache_flink/", "subreddit_subscribers": 109982, "created_utc": 1686404870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a bit of an analytical SQL a-ha moment today while working out a BI calculation, something I would previously have leaned on looping in Python for. It felt good and someone might get something out of it so I thought I'd share.\n\nThe puzzle: Calculate how much time a service desk ticket has been worked during business hours (09:00 - 17:00 Mon-Fri, no holidays). Note: Ticket duration can be multiple days.\n\nfact_service_desk_tickets\n--------------------------\nticket_id | start_timestamp | end_timestamp | date_key | time_key\n.\n\ndimension_date\n---------------\ndate_key | full_date | year | month | calendar_week | day_of_week | weekend_indicator | holiday_indicator\n.\n\nMy initial thought was that I need to generate a 1-minute time series for each ticket, which would have taken forever to run. The a-ha moment was realizing I can use least(), greatest(), and a where clause on a table with 1 day granularity.\n\nSQL (Postgres)\n-----\n-- Break the tickets into 1-day rows and add the _date column to represent the day.\n\nwith\n\n  day_series as (\n\n    select \n\n      ticket_id\n\n      ,start_timestamp\n\n      ,end_timestamp\n\n      ,generate_series(date_trunc('day', start_timestamp), date_trunc('day', end_timestamp), '1 day'::interval)::date as _date\n\n    from fact_service_desk_tickets\n\n),\n\n-- Add start and end times for each day the ticket was worked on (total, not business hours), as well as columns with business hours start and end time.\n\n  main as (\n\n    select\n      ticket_id\n\n      ,start_timestamp\n\n      ,end_timestamp\n\n      ,_date\n\n      ,case \n\n        when start_timestamp::date = _date then start_timestamp::date\n\n        else '00:00:00'::time\n\n      end as start_time\n\n      ,case \n\n        when end_timestamp::date = _date then end_timestamp::date\n\n        else '23:59:59'::time\n\n      end as end_time\n\n      ,'09:30'::time as start_business_time\n\n      ,'15:30'::time as end_business_time\n\n    from day_series ds\n\n    join dimension_date dd on ds._date = dd.full_date\n\n    where\n\n      dd.weekday_indicator = 'Weekday'\n\n      and dd.holiday_indicator = 'Non-holiday'\n\nselect\n\n    ticket_id\n\n    ,sum(extract(epoch from \n\n      least(end_time, end_business_time) - greatest(start_time, start_business_time) as business_hours_duration_seconds\n\nfrom main\n\nwhere\n\n  least(end_time, end_business_time) &gt; greatest(start_time, start_business_time)\n\ngroup by ticket_id\n\n\nUsing least() and greatest() in the final select clause lets us pick the correct start and end times for that day (for start time: either start of business hours or when the ticket was submitted, whichever is later. vice versa for end time).\n\nAnd then in the where clause, we filter out days where the ticket was not worked during business hours (resulting in a negative number).\n\n\nCongrats if you made it here and thanks for reading. Feel free to share some of your data a-ha moments in comments!\n\n\np.s. I have never done leetcode or interview prep, are the problems ever this tough?\n      \n p.p.s. Sorry about this cursed formatting", "author_fullname": "t2_cqvp4nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A-ha moments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145ig4o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686348660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a bit of an analytical SQL a-ha moment today while working out a BI calculation, something I would previously have leaned on looping in Python for. It felt good and someone might get something out of it so I thought I&amp;#39;d share.&lt;/p&gt;\n\n&lt;p&gt;The puzzle: Calculate how much time a service desk ticket has been worked during business hours (09:00 - 17:00 Mon-Fri, no holidays). Note: Ticket duration can be multiple days.&lt;/p&gt;\n\n&lt;h2&gt;fact_service_desk_tickets&lt;/h2&gt;\n\n&lt;p&gt;ticket_id | start_timestamp | end_timestamp | date_key | time_key\n.&lt;/p&gt;\n\n&lt;h2&gt;dimension_date&lt;/h2&gt;\n\n&lt;p&gt;date_key | full_date | year | month | calendar_week | day_of_week | weekend_indicator | holiday_indicator\n.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was that I need to generate a 1-minute time series for each ticket, which would have taken forever to run. The a-ha moment was realizing I can use least(), greatest(), and a where clause on a table with 1 day granularity.&lt;/p&gt;\n\n&lt;h2&gt;SQL (Postgres)&lt;/h2&gt;\n\n&lt;p&gt;-- Break the tickets into 1-day rows and add the _date column to represent the day.&lt;/p&gt;\n\n&lt;p&gt;with&lt;/p&gt;\n\n&lt;p&gt;day_series as (&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select \n\n  ticket_id\n\n  ,start_timestamp\n\n  ,end_timestamp\n\n  ,generate_series(date_trunc(&amp;#39;day&amp;#39;, start_timestamp), date_trunc(&amp;#39;day&amp;#39;, end_timestamp), &amp;#39;1 day&amp;#39;::interval)::date as _date\n\nfrom fact_service_desk_tickets\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;),&lt;/p&gt;\n\n&lt;p&gt;-- Add start and end times for each day the ticket was worked on (total, not business hours), as well as columns with business hours start and end time.&lt;/p&gt;\n\n&lt;p&gt;main as (&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select\n  ticket_id\n\n  ,start_timestamp\n\n  ,end_timestamp\n\n  ,_date\n\n  ,case \n\n    when start_timestamp::date = _date then start_timestamp::date\n\n    else &amp;#39;00:00:00&amp;#39;::time\n\n  end as start_time\n\n  ,case \n\n    when end_timestamp::date = _date then end_timestamp::date\n\n    else &amp;#39;23:59:59&amp;#39;::time\n\n  end as end_time\n\n  ,&amp;#39;09:30&amp;#39;::time as start_business_time\n\n  ,&amp;#39;15:30&amp;#39;::time as end_business_time\n\nfrom day_series ds\n\njoin dimension_date dd on ds._date = dd.full_date\n\nwhere\n\n  dd.weekday_indicator = &amp;#39;Weekday&amp;#39;\n\n  and dd.holiday_indicator = &amp;#39;Non-holiday&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;select&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ticket_id\n\n,sum(extract(epoch from \n\n  least(end_time, end_business_time) - greatest(start_time, start_business_time) as business_hours_duration_seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;from main&lt;/p&gt;\n\n&lt;p&gt;where&lt;/p&gt;\n\n&lt;p&gt;least(end_time, end_business_time) &amp;gt; greatest(start_time, start_business_time)&lt;/p&gt;\n\n&lt;p&gt;group by ticket_id&lt;/p&gt;\n\n&lt;p&gt;Using least() and greatest() in the final select clause lets us pick the correct start and end times for that day (for start time: either start of business hours or when the ticket was submitted, whichever is later. vice versa for end time).&lt;/p&gt;\n\n&lt;p&gt;And then in the where clause, we filter out days where the ticket was not worked during business hours (resulting in a negative number).&lt;/p&gt;\n\n&lt;p&gt;Congrats if you made it here and thanks for reading. Feel free to share some of your data a-ha moments in comments!&lt;/p&gt;\n\n&lt;p&gt;p.s. I have never done leetcode or interview prep, are the problems ever this tough?&lt;/p&gt;\n\n&lt;p&gt;p.p.s. Sorry about this cursed formatting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "145ig4o", "is_robot_indexable": true, "report_reasons": null, "author": "udonthave2call", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145ig4o/aha_moments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145ig4o/aha_moments/", "subreddit_subscribers": 109982, "created_utc": 1686348660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just use DBT and snowflake to transform data for a BI layer at my job.  No python, aws, or ETL.\n\n&amp;#x200B;\n\nHow common is this, and does my role fit into data engineering?  I'm trying to figure out how my current knowledge, Snowflake and DBT, fits into the wider world of data engineering and where I can branch out next to learn more.  I'm currently snowflake core certified and was considering getting AWS certified, but it all feels like rote memorization, and I want to figure out my connection to this field first.  \n\n\nI've been reading articles, medium blogs and am hoping to read  \n\n# The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling\n\n \n\n# Fundamentals of Data Engineering: Plan and Build Robust Data Systems \n\n Designing Data-Intensive Applications: The Big Ideas", "author_fullname": "t2_c6pz2borl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just DBT and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145f8f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686341002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just use DBT and snowflake to transform data for a BI layer at my job.  No python, aws, or ETL.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How common is this, and does my role fit into data engineering?  I&amp;#39;m trying to figure out how my current knowledge, Snowflake and DBT, fits into the wider world of data engineering and where I can branch out next to learn more.  I&amp;#39;m currently snowflake core certified and was considering getting AWS certified, but it all feels like rote memorization, and I want to figure out my connection to this field first.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reading articles, medium blogs and am hoping to read  &lt;/p&gt;\n\n&lt;h1&gt;The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling&lt;/h1&gt;\n\n&lt;h1&gt;Fundamentals of Data Engineering: Plan and Build Robust Data Systems&lt;/h1&gt;\n\n&lt;p&gt;Designing Data-Intensive Applications: The Big Ideas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "145f8f7", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious-Fox-9625", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145f8f7/just_dbt_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145f8f7/just_dbt_and_snowflake/", "subreddit_subscribers": 109982, "created_utc": 1686341002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nI'm currently managing a data lake architecture on AWS. Our data pipelines consist of workflows in Step Functions and some Lambda functions that do the ETL and move the data through all the different zones. We handle relatively low volume data.\n\nThe lambda functions are python scripts that use the awswrangler library for the whole ETL.  \nI'm running a query against different databases, [creating the glue table](https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.catalog.create_parquet_table.html#awswrangler.catalog.create_parquet_table) in case it doesn't exist, generating the corresponding path  (eg: domain/subdomain/year/month/day/&lt;process\\_date&gt;.parquet), [writing the parquet](https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html#awswrangler.s3.to_parquet) (with dataset=False), add a **process\\_date** column and then run an **ALTER TABLE ADD PARTITION** statement against Athena to add the recently file as a partition for the column **process\\_date.**   \nI'm doing all of this because i need to store the parquet file with a partition path as I mentioned above, yet I need the dataset to be partitioned by process\\_date (with a format YYYYMMDD), i don't want process\\_date to be part of the path like a hive partition.  \n\n\nIs this approach correct? I still don't totally get if I 100% need to use a partition\\_by() method or partition\\_cols parameter (in this case) and if that produces a change in the physical output file. Will I make use of partition pruning this way? The partitions are correctly recognized on Athena\n\nIf someone could provide some resources/book to read it'll be great, from what i searched there is no real difference between using a partition method and generating the folder structure myself. Adding partitions manually was the only alternative I found on this [Athena doc page](https://docs.aws.amazon.com/athena/latest/ug/partitions.html) (Scenario 2).\n\nI also read about bucketing, i think it doesn't suits me for this use case but it will in the future maybe if I need to improve performance for a high cardinality field. Does bucketing affect the final .parquet file?", "author_fullname": "t2_dpj60sgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correct way of partitioning parquet files on AWS data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kz4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686355155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently managing a data lake architecture on AWS. Our data pipelines consist of workflows in Step Functions and some Lambda functions that do the ETL and move the data through all the different zones. We handle relatively low volume data.&lt;/p&gt;\n\n&lt;p&gt;The lambda functions are python scripts that use the awswrangler library for the whole ETL.&lt;br/&gt;\nI&amp;#39;m running a query against different databases, &lt;a href=\"https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.catalog.create_parquet_table.html#awswrangler.catalog.create_parquet_table\"&gt;creating the glue table&lt;/a&gt; in case it doesn&amp;#39;t exist, generating the corresponding path  (eg: domain/subdomain/year/month/day/&amp;lt;process\\_date&amp;gt;.parquet), &lt;a href=\"https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.to_parquet.html#awswrangler.s3.to_parquet\"&gt;writing the parquet&lt;/a&gt; (with dataset=False), add a &lt;strong&gt;process_date&lt;/strong&gt; column and then run an &lt;strong&gt;ALTER TABLE ADD PARTITION&lt;/strong&gt; statement against Athena to add the recently file as a partition for the column &lt;strong&gt;process_date.&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m doing all of this because i need to store the parquet file with a partition path as I mentioned above, yet I need the dataset to be partitioned by process_date (with a format YYYYMMDD), i don&amp;#39;t want process_date to be part of the path like a hive partition.  &lt;/p&gt;\n\n&lt;p&gt;Is this approach correct? I still don&amp;#39;t totally get if I 100% need to use a partition_by() method or partition_cols parameter (in this case) and if that produces a change in the physical output file. Will I make use of partition pruning this way? The partitions are correctly recognized on Athena&lt;/p&gt;\n\n&lt;p&gt;If someone could provide some resources/book to read it&amp;#39;ll be great, from what i searched there is no real difference between using a partition method and generating the folder structure myself. Adding partitions manually was the only alternative I found on this &lt;a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\"&gt;Athena doc page&lt;/a&gt; (Scenario 2).&lt;/p&gt;\n\n&lt;p&gt;I also read about bucketing, i think it doesn&amp;#39;t suits me for this use case but it will in the future maybe if I need to improve performance for a high cardinality field. Does bucketing affect the final .parquet file?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145kz4n", "is_robot_indexable": true, "report_reasons": null, "author": "_unwin", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145kz4n/correct_way_of_partitioning_parquet_files_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145kz4n/correct_way_of_partitioning_parquet_files_on_aws/", "subreddit_subscribers": 109982, "created_utc": 1686355155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[See here for the original r/dataengineering thread on this issue.](https://www.reddit.com/r/dataengineering/comments/140xtxu/does_the_de_community_want_to_join_the_reddit/)\n\n# What's going on?\n\nA recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.\n\nOn May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).\n\nEven if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface.\n\nThis isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.\n\n# What's the plan?\n\nOn June 12th, [many subreddits](https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/) will be going dark to protest this policy. Some will return after 48 hours: others will go away *permanently* unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because *we love Reddit*, and we truly believe this change will make it impossible to keep doing what we love.\n\nThe two-day blackout isn't the *goal*, and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action.\n\nWhat can *you* do?\n\n1. **Complain.** Message the mods of [r/reddit](https://www.reddit.com/r/reddit/).com, who are the admins of the site: message [/u/reddit](https://www.reddit.com/u/reddit/): submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on [r/reddit](https://www.reddit.com/r/reddit/), such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/), leave a negative review on their official iOS or Android app- and sign your username in support to this post.\n2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at [r/ModCoord](https://www.reddit.com/r/ModCoord/) \\- but please don't pester mods you *don't* know by simply spamming their modmail.\n3. **Boycott** ***and*** **spread the word...to Reddit's competition!** Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite *non*\\-Reddit platform of choice and make some noise in support!\n4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.\n\n&amp;#x200B;\n\n*Any communication during the blackout will be made via* [*our official mailing list*](https://dataengineeringcommunity.substack.com/)*. Please sign up if you wish to receive updates.*", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "r/dataengineering will be joining the blackout from June 12-14 to protest the proposed API changes which will end 3rd party apps.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14663ur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686418444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/140xtxu/does_the_de_community_want_to_join_the_reddit/\"&gt;See here for the original r/dataengineering thread on this issue.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;What&amp;#39;s going on?&lt;/h1&gt;\n\n&lt;p&gt;A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app &lt;strong&gt;permanently inaccessible&lt;/strong&gt; to users.&lt;/p&gt;\n\n&lt;p&gt;On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from &lt;a href=\"https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/\"&gt;Apollo&lt;/a&gt; to &lt;a href=\"https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/\"&gt;Reddit is Fun&lt;/a&gt; to &lt;a href=\"https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/\"&gt;Narwhal&lt;/a&gt; to &lt;a href=\"https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/\"&gt;BaconReader&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Even if you&amp;#39;re not a mobile user and don&amp;#39;t use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface.&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.&lt;/p&gt;\n\n&lt;h1&gt;What&amp;#39;s the plan?&lt;/h1&gt;\n\n&lt;p&gt;On June 12th, &lt;a href=\"https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/\"&gt;many subreddits&lt;/a&gt; will be going dark to protest this policy. Some will return after 48 hours: others will go away &lt;em&gt;permanently&lt;/em&gt; unless the issue is adequately addressed, since many moderators aren&amp;#39;t able to put in the work they do with the poor tools available through the official app. This isn&amp;#39;t something any of us do lightly: we do what we do because &lt;em&gt;we love Reddit&lt;/em&gt;, and we truly believe this change will make it impossible to keep doing what we love.&lt;/p&gt;\n\n&lt;p&gt;The two-day blackout isn&amp;#39;t the &lt;em&gt;goal&lt;/em&gt;, and it isn&amp;#39;t the end. Should things reach the 14th with no sign of Reddit choosing to fix what they&amp;#39;ve broken, we&amp;#39;ll use the community and buzz we&amp;#39;ve built between then and now as a tool for further action.&lt;/p&gt;\n\n&lt;p&gt;What can &lt;em&gt;you&lt;/em&gt; do?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Complain.&lt;/strong&gt; Message the mods of &lt;a href=\"https://www.reddit.com/r/reddit/\"&gt;r/reddit&lt;/a&gt;.com, who are the admins of the site: message &lt;a href=\"https://www.reddit.com/u/reddit/\"&gt;/u/reddit&lt;/a&gt;: submit a &lt;a href=\"https://support.reddithelp.com/hc/en-us/requests/new\"&gt;support request&lt;/a&gt;: comment in relevant threads on &lt;a href=\"https://www.reddit.com/r/reddit/\"&gt;r/reddit&lt;/a&gt;, such as &lt;a href=\"https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/\"&gt;this one&lt;/a&gt;, leave a negative review on their official iOS or Android app- and sign your username in support to this post.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Spread the word.&lt;/strong&gt; Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at &lt;a href=\"https://www.reddit.com/r/ModCoord/\"&gt;r/ModCoord&lt;/a&gt; - but please don&amp;#39;t pester mods you &lt;em&gt;don&amp;#39;t&lt;/em&gt; know by simply spamming their modmail.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Boycott&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;spread the word...to Reddit&amp;#39;s competition!&lt;/strong&gt; Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite &lt;em&gt;non&lt;/em&gt;-Reddit platform of choice and make some noise in support!&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Don&amp;#39;t be a jerk.&lt;/strong&gt; As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Any communication during the blackout will be made via&lt;/em&gt; &lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;&lt;em&gt;our official mailing list&lt;/em&gt;&lt;/a&gt;&lt;em&gt;. Please sign up if you wish to receive updates.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "14663ur", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14663ur/rdataengineering_will_be_joining_the_blackout/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/14663ur/rdataengineering_will_be_joining_the_blackout/", "subreddit_subscribers": 109982, "created_utc": 1686418444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it just because of popularity ? Or does spark has essence that MPI doesn't have that allows distributed cluster to work more efficiently ?", "author_fullname": "t2_qj15embb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do we use Spark instead of MPI ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145vdzk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686388318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just because of popularity ? Or does spark has essence that MPI doesn&amp;#39;t have that allows distributed cluster to work more efficiently ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "145vdzk", "is_robot_indexable": true, "report_reasons": null, "author": "Grouchy_Document7786", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145vdzk/why_do_we_use_spark_instead_of_mpi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145vdzk/why_do_we_use_spark_instead_of_mpi/", "subreddit_subscribers": 109982, "created_utc": 1686388318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gotchas of Streaming Pipelines: Profiling &amp; Performance Improvements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_1460gbr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ysrQaDgzOZR3YU82UYjgPVOequmJNTMBrhFu1yNw7IY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686404110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "eng.lyft.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://eng.lyft.com/gotchas-of-streaming-pipelines-profiling-performance-improvements-301439f46412", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3Qb9N_pSU-zoDLVLPQ0AHkOIFsqQZxMC7sQwK7nMdbM.jpg?auto=webp&amp;v=enabled&amp;s=5d1cba4b50fb41daf4ab3500f9eca59a161eb007", "width": 800, "height": 427}, "resolutions": [{"url": "https://external-preview.redd.it/3Qb9N_pSU-zoDLVLPQ0AHkOIFsqQZxMC7sQwK7nMdbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1c5956e53e24117e135d786b3ca661190113921", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/3Qb9N_pSU-zoDLVLPQ0AHkOIFsqQZxMC7sQwK7nMdbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b7c82e652ea5844b93bf48d917be789bd2327b0", "width": 216, "height": 115}, {"url": "https://external-preview.redd.it/3Qb9N_pSU-zoDLVLPQ0AHkOIFsqQZxMC7sQwK7nMdbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c114e3be0a4d3e14ab20eed17f7e994aa79c0161", "width": 320, "height": 170}, {"url": "https://external-preview.redd.it/3Qb9N_pSU-zoDLVLPQ0AHkOIFsqQZxMC7sQwK7nMdbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8101fef3773a831ab0106e830f61808c8d06460", "width": 640, "height": 341}], "variants": {}, "id": "z_BGRLC2r0ZeTiZOOuXcgz5cY7H9YGwXr_IYBqUcw3I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1460gbr", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1460gbr/gotchas_of_streaming_pipelines_profiling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://eng.lyft.com/gotchas-of-streaming-pipelines-profiling-performance-improvements-301439f46412", "subreddit_subscribers": 109982, "created_utc": 1686404110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone! I hope you are well. I have a problem with one scraper in my job, is too slow. It was made with selenium, and the scraper download files too. I was searching for a technology that helps me to run multiples instances of the same scraper (for example 8). I have access to GCP and I was thinking to use cloud jobs, but I'm not sure if is the best option, because this scraper run all the time, I mean 24/7, I can edit them so they can stop every 5 minutes and then continue. I'm not sure which is the best solution or technology. Sorry if my English is bad!\nHave a nice day:D\nThanks.", "author_fullname": "t2_cwa2139f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run multiples instances of a scraper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145eln2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686339430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I hope you are well. I have a problem with one scraper in my job, is too slow. It was made with selenium, and the scraper download files too. I was searching for a technology that helps me to run multiples instances of the same scraper (for example 8). I have access to GCP and I was thinking to use cloud jobs, but I&amp;#39;m not sure if is the best option, because this scraper run all the time, I mean 24/7, I can edit them so they can stop every 5 minutes and then continue. I&amp;#39;m not sure which is the best solution or technology. Sorry if my English is bad!\nHave a nice day:D\nThanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145eln2", "is_robot_indexable": true, "report_reasons": null, "author": "Ramcer2", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145eln2/how_to_run_multiples_instances_of_a_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145eln2/how_to_run_multiples_instances_of_a_scraper/", "subreddit_subscribers": 109982, "created_utc": 1686339430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I haven't taken the course yet in school, I'm an upcoming senior undergrad\n\nStarted learning dsa slowly so I can start leetcoding python easy's for a technical interview. I have a score of 60 but it is all SQL, can do mediums and easys.\n\nI'm just wondering if I'll be able to pass an interview for winter internships -\n\nand if I should use this month to cover as much as I can with DSA. I realize that's not the major focus of a DE internship, but I'm afraid it will weed me out of nice prospects if I can't pass any technical python questions.\n\n\\*would delay me organizing my github\n\n\\*would rather work on third project\n\n&amp;#x200B;\n\n* 2 pipeline projects\n* Databricks and Azure Data Factory project\n* Dimensional Modeling knowledge\n\nPySpark, Scala, SQL\n\n&amp;#x200B;\n\n(0 experience, absolutely no experience with internships)", "author_fullname": "t2_pwk2f3iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I continue DSA before applying to internships?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145lxvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686357787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t taken the course yet in school, I&amp;#39;m an upcoming senior undergrad&lt;/p&gt;\n\n&lt;p&gt;Started learning dsa slowly so I can start leetcoding python easy&amp;#39;s for a technical interview. I have a score of 60 but it is all SQL, can do mediums and easys.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just wondering if I&amp;#39;ll be able to pass an interview for winter internships -&lt;/p&gt;\n\n&lt;p&gt;and if I should use this month to cover as much as I can with DSA. I realize that&amp;#39;s not the major focus of a DE internship, but I&amp;#39;m afraid it will weed me out of nice prospects if I can&amp;#39;t pass any technical python questions.&lt;/p&gt;\n\n&lt;p&gt;*would delay me organizing my github&lt;/p&gt;\n\n&lt;p&gt;*would rather work on third project&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2 pipeline projects&lt;/li&gt;\n&lt;li&gt;Databricks and Azure Data Factory project&lt;/li&gt;\n&lt;li&gt;Dimensional Modeling knowledge&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PySpark, Scala, SQL&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(0 experience, absolutely no experience with internships)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145lxvb", "is_robot_indexable": true, "report_reasons": null, "author": "CowUnfair4318", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145lxvb/should_i_continue_dsa_before_applying_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145lxvb/should_i_continue_dsa_before_applying_to/", "subreddit_subscribers": 109982, "created_utc": 1686357787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the challenges u faced on azure adf and databricks from your experience", "author_fullname": "t2_5nx7csx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mention azure adf and azure darabricks challenges u faced from your experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145kwll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686354961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the challenges u faced on azure adf and databricks from your experience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "145kwll", "is_robot_indexable": true, "report_reasons": null, "author": "pavan449", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145kwll/mention_azure_adf_and_azure_darabricks_challenges/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145kwll/mention_azure_adf_and_azure_darabricks_challenges/", "subreddit_subscribers": 109982, "created_utc": 1686354961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_k6yhi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IceDB v2 \ud83e\uddca - A dirt-cheap OLAP/data lake hybrid (PoC)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1463rqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pxreED4Ow1kAMCKv98b9bDgtrJKv6g9jlgCrm1wspdU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686412624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.danthegoodman.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.danthegoodman.com/icedb-v2", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?auto=webp&amp;v=enabled&amp;s=bc135890a4b3e86dfae20d6e035e91dc2300c5c1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09910e0b0a55a1f16d1ae5e4aef040d0fefe5cce", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c00478eb2122acbd86ee37f81e3ef1854ed597d9", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47b8995ee7f917c0cd039fc41b166abfb3866f71", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99458dd5fa715d9724fda556f8f1e7d6ea6bd090", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=113465c40018ef500188c16e577bcc3deea9e830", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/LZ3yDj2-ccvGMMzdEoYRlZooMAro9wyUDpgKtUNMI6w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b693b48ab5dd4cd479e6b9046d26f917dcfb7f3", "width": 1080, "height": 567}], "variants": {}, "id": "DpoauKT_mTprty_xkNxG_mHBJK9kf8MCPhOQRMH44uU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1463rqq", "is_robot_indexable": true, "report_reasons": null, "author": "DanTheGoodman_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1463rqq/icedb_v2_a_dirtcheap_olapdata_lake_hybrid_poc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.danthegoodman.com/icedb-v2", "subreddit_subscribers": 109982, "created_utc": 1686412624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I'm posting here asking for guides developing a feature that combining decision tree &amp; data workflow. In brief, I want to build a feature that:\n\n1. Allow user to drag-and-drop to make a decision tree that split their data based on conditions: I think that I should use some 3rd plugins that help in drawing this tree, but couldn't found any. Currently I'm making a simple form instead\n2. After having that decision tree, the application should build an according workflow that runs SQL statements to produce desired output:\n\n\\+ At first, I supposed that if I was given a tree, I could myself make a single SQL statement for each branch of the tree to fetch data. However I realized that it is more difficult than I thought; it is not the normal SQL parse tree that I used to worked with.\n\n\\+ Their condition set includes OR condition, which makes me confuse to represent it on the decision tree. E.g: IF a.A=x OR a.B=y THEN a is categorized as M\n\n\\+ It the boss desire to make each condition evaluated on one step, and the output is then pipelined to the next select statement. I don't know if it is more efficient than the straightforward way of combining all features into one command only, but he rejected this idea and required us to do the workflow\n\n3. After having the output, transfer it to the target database, or emailing the result (in form of .csv/.xlsx file) to target user: I've done this task.\n\nHow should I do this? Apache Airflow and Apache Dolphin are suitable for building workflow by drag-and-drop, but not for making a decision tree. My users are not familiar with programming and writing SQL so it is impossible to tell them to write statements and put them in Airflow/Dolphin.\n\nThe source database lies on ClickHouse, and the target database is MariaDB.\n\nAny suggestion is appreciated.", "author_fullname": "t2_1qn84k08", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for advice on a feature combining decision tree &amp; data workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1461v68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686408536.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686407710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;m posting here asking for guides developing a feature that combining decision tree &amp;amp; data workflow. In brief, I want to build a feature that:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Allow user to drag-and-drop to make a decision tree that split their data based on conditions: I think that I should use some 3rd plugins that help in drawing this tree, but couldn&amp;#39;t found any. Currently I&amp;#39;m making a simple form instead&lt;/li&gt;\n&lt;li&gt;After having that decision tree, the application should build an according workflow that runs SQL statements to produce desired output:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;+ At first, I supposed that if I was given a tree, I could myself make a single SQL statement for each branch of the tree to fetch data. However I realized that it is more difficult than I thought; it is not the normal SQL parse tree that I used to worked with.&lt;/p&gt;\n\n&lt;p&gt;+ Their condition set includes OR condition, which makes me confuse to represent it on the decision tree. E.g: IF a.A=x OR a.B=y THEN a is categorized as M&lt;/p&gt;\n\n&lt;p&gt;+ It the boss desire to make each condition evaluated on one step, and the output is then pipelined to the next select statement. I don&amp;#39;t know if it is more efficient than the straightforward way of combining all features into one command only, but he rejected this idea and required us to do the workflow&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;After having the output, transfer it to the target database, or emailing the result (in form of .csv/.xlsx file) to target user: I&amp;#39;ve done this task.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How should I do this? Apache Airflow and Apache Dolphin are suitable for building workflow by drag-and-drop, but not for making a decision tree. My users are not familiar with programming and writing SQL so it is impossible to tell them to write statements and put them in Airflow/Dolphin.&lt;/p&gt;\n\n&lt;p&gt;The source database lies on ClickHouse, and the target database is MariaDB.&lt;/p&gt;\n\n&lt;p&gt;Any suggestion is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1461v68", "is_robot_indexable": true, "report_reasons": null, "author": "saklovesyao", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1461v68/asking_for_advice_on_a_feature_combining_decision/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1461v68/asking_for_advice_on_a_feature_combining_decision/", "subreddit_subscribers": 109982, "created_utc": 1686407710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I\u2019ve been tasked with recommending improvements to the current stack that\u2019s struggling with performance but I need some help. \n\nThe setup is RDS Postgres as a data warehouse and glue for transformations. \n\nWe have a team of only two mid level data engineering who are familiar with PySpark but not familiar with optimising spark for performance. \n\nI\u2019m considering recommending using DBT (which I\u2019m familiar with) and Athena or Snowflake (the company is considering redshift but I\u2019m not seeing good things about it). \n\nWe ingest about 2 - 3 million records per day and it\u2019s currently not in a compressed format.  I can\u2019t quite remember the size but it\u2019s in gigabytes, and will compress to megabytes in parquet. That being said, this is an MVP and is expected to grow to terabytes within a year. \n\nWe\u2019re likely to ingest more data types from more sources in the near future. \n\nMy current thinking is that we could stick with glue and S3 and build a pure data lake, or introduce DBT and Snowflake which I think can handle small-ish file sizes and will scale nicely. \n\nI don\u2019t have enough experience to know what considerations I might be missing so any advice would be amazing!", "author_fullname": "t2_brgs3nc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Selecting tools for a small team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145hpo2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686346893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I\u2019ve been tasked with recommending improvements to the current stack that\u2019s struggling with performance but I need some help. &lt;/p&gt;\n\n&lt;p&gt;The setup is RDS Postgres as a data warehouse and glue for transformations. &lt;/p&gt;\n\n&lt;p&gt;We have a team of only two mid level data engineering who are familiar with PySpark but not familiar with optimising spark for performance. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m considering recommending using DBT (which I\u2019m familiar with) and Athena or Snowflake (the company is considering redshift but I\u2019m not seeing good things about it). &lt;/p&gt;\n\n&lt;p&gt;We ingest about 2 - 3 million records per day and it\u2019s currently not in a compressed format.  I can\u2019t quite remember the size but it\u2019s in gigabytes, and will compress to megabytes in parquet. That being said, this is an MVP and is expected to grow to terabytes within a year. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re likely to ingest more data types from more sources in the near future. &lt;/p&gt;\n\n&lt;p&gt;My current thinking is that we could stick with glue and S3 and build a pure data lake, or introduce DBT and Snowflake which I think can handle small-ish file sizes and will scale nicely. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have enough experience to know what considerations I might be missing so any advice would be amazing!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145hpo2", "is_robot_indexable": true, "report_reasons": null, "author": "hippiecampus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145hpo2/selecting_tools_for_a_small_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145hpo2/selecting_tools_for_a_small_team/", "subreddit_subscribers": 109982, "created_utc": 1686346893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How can we improve our current process of exporting tables from our schema to CSV by considering alternative file formats like Apache Parquet? We currently use a C# script with Entity Framework to retrieve records as a list of objects and employ system reflection to build the CSV file dynamically. However, we face issues such as delimiter collision and a lack of type information in CSV. If anyone has experience with a similar process or any suggestions, we would greatly appreciate your insights.", "author_fullname": "t2_uqd1hmks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring Alternative File Formats and Process Improvements for Exporting Schema Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145vnlh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686389200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How can we improve our current process of exporting tables from our schema to CSV by considering alternative file formats like Apache Parquet? We currently use a C# script with Entity Framework to retrieve records as a list of objects and employ system reflection to build the CSV file dynamically. However, we face issues such as delimiter collision and a lack of type information in CSV. If anyone has experience with a similar process or any suggestions, we would greatly appreciate your insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145vnlh", "is_robot_indexable": true, "report_reasons": null, "author": "the-belfastian", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145vnlh/exploring_alternative_file_formats_and_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145vnlh/exploring_alternative_file_formats_and_process/", "subreddit_subscribers": 109982, "created_utc": 1686389200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have our main tables stored in hudi datalake and for real time dashboard we have sql statements running against the hudi db and pumping data to postgresql tables. The sql statements are composed of many joins operations. Since we would need to get data in less than a second, we are deleting the tables in postgres and we run our sql statements to regenerate the tables there. So, we delete, recreate every 2-3 minutes.\n\nAre there any other ways to prevent the deletion/recreation of tables. Doris could replace postgresql but the issue is not with postgresql nor doris but with the recreation of the tables that the dashboards connect to directly.\n\nDo you have any ideas on how to achieve the realtime analytics for dashboards that require seconds to be loaded when we have our main dbs in datalake and have to update the tables through join queries to another db.\n\n&amp;#x200B;", "author_fullname": "t2_a9ji1nkf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse &amp; postgresql - how to enable real time analytics queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145tbq6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686381044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have our main tables stored in hudi datalake and for real time dashboard we have sql statements running against the hudi db and pumping data to postgresql tables. The sql statements are composed of many joins operations. Since we would need to get data in less than a second, we are deleting the tables in postgres and we run our sql statements to regenerate the tables there. So, we delete, recreate every 2-3 minutes.&lt;/p&gt;\n\n&lt;p&gt;Are there any other ways to prevent the deletion/recreation of tables. Doris could replace postgresql but the issue is not with postgresql nor doris but with the recreation of the tables that the dashboards connect to directly.&lt;/p&gt;\n\n&lt;p&gt;Do you have any ideas on how to achieve the realtime analytics for dashboards that require seconds to be loaded when we have our main dbs in datalake and have to update the tables through join queries to another db.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "145tbq6", "is_robot_indexable": true, "report_reasons": null, "author": "CarefulScientist8498", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/145tbq6/lakehouse_postgresql_how_to_enable_real_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/145tbq6/lakehouse_postgresql_how_to_enable_real_time/", "subreddit_subscribers": 109982, "created_utc": 1686381044.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}