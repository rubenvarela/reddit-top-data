{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n79165dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stack Overflow Will Charge AI Giants for Training Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ckwyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 95, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 95, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1687096020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wired.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14ckwyq", "is_robot_indexable": true, "report_reasons": null, "author": "wagfrydue", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ckwyq/stack_overflow_will_charge_ai_giants_for_training/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "subreddit_subscribers": 111088, "created_utc": 1687096020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This post was mistakenly removed as it was thought to be a res ume review but actually it\u2019s a question of when is it considered reasonable to include a technology on your resume? \n\nBackground(last paragraph is main point):\n\nI\u2019ve been working on a data pipeline project for a little over 2 months now and I\u2019ve been learning and using tools like Spark, Nifi, and Airflow for it. \n\nI typically read up on the fundamentals of each too before figuring out how and why to use them in the project. Each tool plays a pretty major role in the project itself. Looking to include Kafka too but doing Kafka with python isn\u2019t as easy\n\nWhat I wonder is if once you\u2019ve gotten to this point, using the tool in your project, if you are okay to put said tool on your res ume, even if you aren\u2019t necessarily an expert, but rather you have experience using it in your own work. How do you typically get it on your res ume in a way that doesn\u2019t imply you\u2019re an expert?", "author_fullname": "t2_55fytx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do you \u201cknow\u201d a technology enough to mention it when you apply to a job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14c68nt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687047704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post was mistakenly removed as it was thought to be a res ume review but actually it\u2019s a question of when is it considered reasonable to include a technology on your resume? &lt;/p&gt;\n\n&lt;p&gt;Background(last paragraph is main point):&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working on a data pipeline project for a little over 2 months now and I\u2019ve been learning and using tools like Spark, Nifi, and Airflow for it. &lt;/p&gt;\n\n&lt;p&gt;I typically read up on the fundamentals of each too before figuring out how and why to use them in the project. Each tool plays a pretty major role in the project itself. Looking to include Kafka too but doing Kafka with python isn\u2019t as easy&lt;/p&gt;\n\n&lt;p&gt;What I wonder is if once you\u2019ve gotten to this point, using the tool in your project, if you are okay to put said tool on your res ume, even if you aren\u2019t necessarily an expert, but rather you have experience using it in your own work. How do you typically get it on your res ume in a way that doesn\u2019t imply you\u2019re an expert?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14c68nt", "is_robot_indexable": true, "report_reasons": null, "author": "ToothPickLegs", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14c68nt/when_do_you_know_a_technology_enough_to_mention/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14c68nt/when_do_you_know_a_technology_enough_to_mention/", "subreddit_subscribers": 111088, "created_utc": 1687047704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI'm currently working on developing a data quality strategy for my organization and would love to hear your opinions and insights on this topic.  \n\n\nWhat data quality strategies have you implemented in your organization?\n\nHow do you identify and handle data anomalies or discrepancies?\n\nHow do you involve stakeholders in data quality initiatives?\n\nHow do you measure and monitor data quality over time?\n\n Please share your experiences, challenges, and successes related to data quality.  \n\nTechstack , AWS , Glue , DBT.", "author_fullname": "t2_3sqs3uub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's Your Data Quality Strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cdr4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687072020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on developing a data quality strategy for my organization and would love to hear your opinions and insights on this topic.  &lt;/p&gt;\n\n&lt;p&gt;What data quality strategies have you implemented in your organization?&lt;/p&gt;\n\n&lt;p&gt;How do you identify and handle data anomalies or discrepancies?&lt;/p&gt;\n\n&lt;p&gt;How do you involve stakeholders in data quality initiatives?&lt;/p&gt;\n\n&lt;p&gt;How do you measure and monitor data quality over time?&lt;/p&gt;\n\n&lt;p&gt;Please share your experiences, challenges, and successes related to data quality.  &lt;/p&gt;\n\n&lt;p&gt;Techstack , AWS , Glue , DBT.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14cdr4i", "is_robot_indexable": true, "report_reasons": null, "author": "priyasweety1", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cdr4i/whats_your_data_quality_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cdr4i/whats_your_data_quality_strategy/", "subreddit_subscribers": 111088, "created_utc": 1687072020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**TL;DR an engineer on our team is proposing to move our EL infrastructure to MuleSoft, but it doesn\u2019t seem like the best approach from a DE perspective. Seems like writing pure Python would be preferred in our case.**\n\nTo make a long story short, our company got stuck with a contract for MuleSoft as part of our Salesforce implementation. Our warehouse will be feeding analytics and master data to Salesforce via reverse ETL.\n\nBecause of this, one of our engineers is pushing to build out all of our API endpoints as Mule app wrappers around our source/target APIs. He has discussed taking a microservices approach to the extract and load workloads of our ELT pipelines. For example, having a single application that handles the ingestion of any flat file into our warehouse.\n\nI am a bit apprehensive about this approach. MuleSoft seems great for IT systems integration and an approach geared more toward software development, but I feel like we are quickly going to be caught up in over-engineering for our pipelines. To be honest, they are not all that complex. Workloads are pretty small, and in terms of querying APIs, there really only seems to be a need for one pipeline per source as we just need to get the data into the warehouse for transform workloads.\n\nSome of the source APIs also seem to be purpose-built for record-to-record integration, as opposed to batch jobs suitable for ETL/ELT. There are other options I don\u2019t think the team is exploring because APIs are seen as preferential even when an ODBC driver might be more suitable for data engineering (as opposed to SWE).\n\nFurthermore, entirety of our data engineering team really only writes in Python, while MuleSoft really only has extensibility in Java. It\u2019s making me a bit apprehensive about our future and I talked to my manager about having a discussion with our VP about broader strategy around our infrastructure.\n\nI was hoping some fellow engineers here could lend some advice on general architectural setup in this scenario.", "author_fullname": "t2_ehi5h2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your data org handle API management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14c4z0s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687044098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR an engineer on our team is proposing to move our EL infrastructure to MuleSoft, but it doesn\u2019t seem like the best approach from a DE perspective. Seems like writing pure Python would be preferred in our case.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To make a long story short, our company got stuck with a contract for MuleSoft as part of our Salesforce implementation. Our warehouse will be feeding analytics and master data to Salesforce via reverse ETL.&lt;/p&gt;\n\n&lt;p&gt;Because of this, one of our engineers is pushing to build out all of our API endpoints as Mule app wrappers around our source/target APIs. He has discussed taking a microservices approach to the extract and load workloads of our ELT pipelines. For example, having a single application that handles the ingestion of any flat file into our warehouse.&lt;/p&gt;\n\n&lt;p&gt;I am a bit apprehensive about this approach. MuleSoft seems great for IT systems integration and an approach geared more toward software development, but I feel like we are quickly going to be caught up in over-engineering for our pipelines. To be honest, they are not all that complex. Workloads are pretty small, and in terms of querying APIs, there really only seems to be a need for one pipeline per source as we just need to get the data into the warehouse for transform workloads.&lt;/p&gt;\n\n&lt;p&gt;Some of the source APIs also seem to be purpose-built for record-to-record integration, as opposed to batch jobs suitable for ETL/ELT. There are other options I don\u2019t think the team is exploring because APIs are seen as preferential even when an ODBC driver might be more suitable for data engineering (as opposed to SWE).&lt;/p&gt;\n\n&lt;p&gt;Furthermore, entirety of our data engineering team really only writes in Python, while MuleSoft really only has extensibility in Java. It\u2019s making me a bit apprehensive about our future and I talked to my manager about having a discussion with our VP about broader strategy around our infrastructure.&lt;/p&gt;\n\n&lt;p&gt;I was hoping some fellow engineers here could lend some advice on general architectural setup in this scenario.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data &amp; Analytics Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14c4z0s", "is_robot_indexable": true, "report_reasons": null, "author": "BlurryEcho", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14c4z0s/how_does_your_data_org_handle_api_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14c4z0s/how_does_your_data_org_handle_api_management/", "subreddit_subscribers": 111088, "created_utc": 1687044098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am used to seeing targets exceeded or met as a good thing. But what happens when exceeding target is a negative thing? Would you use a different term instead of target?\n\nFor some context, the data I'm dealing with is event types and target values are currently set at Prior Year count of events less X%.\n\nPrior year count less X% can be exceeded, but if it does it's not a good thing.", "author_fullname": "t2_sie27959", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What term to use instead of \"target\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cbmb5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687074870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687064830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am used to seeing targets exceeded or met as a good thing. But what happens when exceeding target is a negative thing? Would you use a different term instead of target?&lt;/p&gt;\n\n&lt;p&gt;For some context, the data I&amp;#39;m dealing with is event types and target values are currently set at Prior Year count of events less X%.&lt;/p&gt;\n\n&lt;p&gt;Prior year count less X% can be exceeded, but if it does it&amp;#39;s not a good thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14cbmb5", "is_robot_indexable": true, "report_reasons": null, "author": "Acrobatic-Chapter959", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cbmb5/what_term_to_use_instead_of_target/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cbmb5/what_term_to_use_instead_of_target/", "subreddit_subscribers": 111088, "created_utc": 1687064830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why should you use external tables ? instead of Datawarehouse", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of External Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ckvut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687095934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why should you use external tables ? instead of Datawarehouse&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14ckvut", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ckvut/benefits_of_external_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ckvut/benefits_of_external_tables/", "subreddit_subscribers": 111088, "created_utc": 1687095934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I've aged, Data Engineering has been the last bastion of why I stick around anyway.\n\nSo I ask, which other communities do you guys follow that fosters high quality data engineering discussions?", "author_fullname": "t2_9suj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What other communities do you follow for DE discussion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cs98f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687115127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I&amp;#39;ve aged, Data Engineering has been the last bastion of why I stick around anyway.&lt;/p&gt;\n\n&lt;p&gt;So I ask, which other communities do you guys follow that fosters high quality data engineering discussions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14cs98f", "is_robot_indexable": true, "report_reasons": null, "author": "Drekalo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/", "subreddit_subscribers": 111088, "created_utc": 1687115127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n79165dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The State of Data Engineering 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 120, "top_awarded_type": null, "hide_score": false, "name": "t3_14ch7sl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.54, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LH5YJQKRpKS-jACYzeroLPz77UqjXCRAw6kijl83z_w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687084498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "lakefs.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://lakefs.io/blog/the-state-of-data-engineering-2023/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3LxL4Ls4C_fSycZBEFjfiqbvqkspUuiUa_MBUuXM5jg.jpg?auto=webp&amp;v=enabled&amp;s=202c273d84870d3622f9c7a09efcd8cfc9de556e", "width": 525, "height": 450}, "resolutions": [{"url": "https://external-preview.redd.it/3LxL4Ls4C_fSycZBEFjfiqbvqkspUuiUa_MBUuXM5jg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc32e81c50b463082a20dafd9c29fe660248b71e", "width": 108, "height": 92}, {"url": "https://external-preview.redd.it/3LxL4Ls4C_fSycZBEFjfiqbvqkspUuiUa_MBUuXM5jg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4276d0af8312a85fc0f2af59faaef3c63019d5f8", "width": 216, "height": 185}, {"url": "https://external-preview.redd.it/3LxL4Ls4C_fSycZBEFjfiqbvqkspUuiUa_MBUuXM5jg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f5a86c2ef15c1b5371c45ba13378fee3c168fcb", "width": 320, "height": 274}], "variants": {}, "id": "xpTW-m5_MtaimMuBNvYrgMV4lZGUc19QGYCIc3hzvUw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14ch7sl", "is_robot_indexable": true, "report_reasons": null, "author": "wagfrydue", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ch7sl/the_state_of_data_engineering_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://lakefs.io/blog/the-state-of-data-engineering-2023/", "subreddit_subscribers": 111088, "created_utc": 1687084498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, i have  questions related to parquet files and AWS Glue, they're maybe basic questions but i hope someone can help me understand more clearly.\n\nCurrently i'm working on a project that i use a ETL tool to load data from internal databases to AWS S3 in parquet format and then i use AWS Glue to do some transformation by using SparkSQL on these parquet files and write them back to S3 (parquet files as well)\n\n1.How Date and DateTime columns are stored in parquet file ?.\n\nAs i read here [https://github.com/apache/parquet-format/blob/master/LogicalTypes.md](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md) , they are store in Integer formats and these integers represent the number of days (for Date) or number of  milliseconds, microseconds or nanoseconds (for DateTime)  since 1970-01-01. This works as expected with the parquet file that written by our ETL tool from internal database --&gt; S3, all Data/DateTime columns are Integers, means that in Glue Job, i have to convert these Integers back to Date/Datetime value to do some transformation on them. But when parquet files are written by Spark, they are Date/DateTime (or TimeStamp to be more concise) format not Integers (i checked by read these files again into Dataframe and print schema of that dataframe) and that make me confused. Of course before writing the database table (by using ETL tool)  or dataframe (by using Spark) i leave all Date/DateTime columns as Date/DateTime format\n\n&amp;#x200B;\n\n2. Our parquet files are stored in S3 with Date partitions as &lt;base\\_path&gt;/&lt;table\\_name&gt;/&lt;year=yyyy&gt;/&lt;month=mm&gt;/&lt;day=dd&gt;/parquet\\_files . Each Glue Job will write to 1 date partition for 1 table in S3. Is it best practice to limit the number of files before writing to S3 by calling dataframe.repartition() because if i don't do that the number of files in S3 will be unexpectedly many for some Glue Jobs ?\n\n&amp;#x200B;\n\nThank you very much", "author_fullname": "t2_7fyrjq8ff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have question related to Parquet files and AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14ctdeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687120014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687117943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i have  questions related to parquet files and AWS Glue, they&amp;#39;re maybe basic questions but i hope someone can help me understand more clearly.&lt;/p&gt;\n\n&lt;p&gt;Currently i&amp;#39;m working on a project that i use a ETL tool to load data from internal databases to AWS S3 in parquet format and then i use AWS Glue to do some transformation by using SparkSQL on these parquet files and write them back to S3 (parquet files as well)&lt;/p&gt;\n\n&lt;p&gt;1.How Date and DateTime columns are stored in parquet file ?.&lt;/p&gt;\n\n&lt;p&gt;As i read here &lt;a href=\"https://github.com/apache/parquet-format/blob/master/LogicalTypes.md\"&gt;https://github.com/apache/parquet-format/blob/master/LogicalTypes.md&lt;/a&gt; , they are store in Integer formats and these integers represent the number of days (for Date) or number of  milliseconds, microseconds or nanoseconds (for DateTime)  since 1970-01-01. This works as expected with the parquet file that written by our ETL tool from internal database --&amp;gt; S3, all Data/DateTime columns are Integers, means that in Glue Job, i have to convert these Integers back to Date/Datetime value to do some transformation on them. But when parquet files are written by Spark, they are Date/DateTime (or TimeStamp to be more concise) format not Integers (i checked by read these files again into Dataframe and print schema of that dataframe) and that make me confused. Of course before writing the database table (by using ETL tool)  or dataframe (by using Spark) i leave all Date/DateTime columns as Date/DateTime format&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Our parquet files are stored in S3 with Date partitions as &amp;lt;base\\_path&amp;gt;/&amp;lt;table\\_name&amp;gt;/&amp;lt;year=yyyy&amp;gt;/&amp;lt;month=mm&amp;gt;/&amp;lt;day=dd&amp;gt;/parquet_files . Each Glue Job will write to 1 date partition for 1 table in S3. Is it best practice to limit the number of files before writing to S3 by calling dataframe.repartition() because if i don&amp;#39;t do that the number of files in S3 will be unexpectedly many for some Glue Jobs ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?auto=webp&amp;v=enabled&amp;s=c51f45b6e8d1f15543e6991d2157f0454e12bd64", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3093470252c899130e2f27fb7d76cb3aa0b468ce", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a796c7394ac14f2fbd22c352695b4223c19a7bd9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53e88e600c9e3bcacca62d52d489114ec0f4b07f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5631ac15c09241153588e02ad9cc6550235f2e8a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ccba4040a49d102a8d22fe4787f52aede31b390", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3546db03dc04a15ffd4d3ec0af7cda056364520a", "width": 1080, "height": 540}], "variants": {}, "id": "SKNeXyLZaZn_HdQEQLJJaJjNsN0LoHMwnXsUK53ewyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ctdeq", "is_robot_indexable": true, "report_reasons": null, "author": "random_name_362", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ctdeq/i_have_question_related_to_parquet_files_and_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ctdeq/i_have_question_related_to_parquet_files_and_aws/", "subreddit_subscribers": 111088, "created_utc": 1687117943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking into reverse ETL to send data from Snowflake to a CRM\u2026 any recommendations?  Any bear traps I should avoid?", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reverse ETL recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14c8m1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687055016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking into reverse ETL to send data from Snowflake to a CRM\u2026 any recommendations?  Any bear traps I should avoid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14c8m1k", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14c8m1k/reverse_etl_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14c8m1k/reverse_etl_recommendations/", "subreddit_subscribers": 111088, "created_utc": 1687055016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a spark data frame that has some reconciliation data and this data frame only gets created if reconciliation fails. \n\nWhen this spark data frame does get created in data bricks, I need to send the data in it as logs to azure application insights from data bricks. \n\nWhat\u2019s confusing me is that there seems to be two libraries, \u201copen census\u201d and \u201copen telemetry\u201d, Open Telemetry is supposed to be the newest one but seems it\u2019s still on Beta? The Microsoft documentation isn\u2019t clear on that.\n\nSo my question is which library do i need to use? Also some python code examples of how to do this would be much appreciated.\n\nThank You.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which library to use to write Data from a Spark Data Frame to Azure Application Insights from data bricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14c3tei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687041109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a spark data frame that has some reconciliation data and this data frame only gets created if reconciliation fails. &lt;/p&gt;\n\n&lt;p&gt;When this spark data frame does get created in data bricks, I need to send the data in it as logs to azure application insights from data bricks. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s confusing me is that there seems to be two libraries, \u201copen census\u201d and \u201copen telemetry\u201d, Open Telemetry is supposed to be the newest one but seems it\u2019s still on Beta? The Microsoft documentation isn\u2019t clear on that.&lt;/p&gt;\n\n&lt;p&gt;So my question is which library do i need to use? Also some python code examples of how to do this would be much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank You.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14c3tei", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14c3tei/which_library_to_use_to_write_data_from_a_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14c3tei/which_library_to_use_to_write_data_from_a_spark/", "subreddit_subscribers": 111088, "created_utc": 1687041109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I use Azure DataFactorys and Synapse Workspaces for data processing. \n\nWhat is the best way to set up jobs for this?\n\nThe triggers for the ADFs are not adequate as I have jobs that are allowed to run in parallel and others that are dependent on each other. \n\nRecently Apache Airflow can be used, but that is not a solution. \n\nI use currently the ProcFWK ([https://mrpaulandrew.github.io/procfwk/](https://mrpaulandrew.github.io/procfwk/)), but this is not a long-term solution for me. \n\nHow do you guys do something like this?  \n\n\nThanks in advance. ", "author_fullname": "t2_3x5koiy5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to orchestrate Azure ADFs and Synapse Pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cd5rp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687070002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I use Azure DataFactorys and Synapse Workspaces for data processing. &lt;/p&gt;\n\n&lt;p&gt;What is the best way to set up jobs for this?&lt;/p&gt;\n\n&lt;p&gt;The triggers for the ADFs are not adequate as I have jobs that are allowed to run in parallel and others that are dependent on each other. &lt;/p&gt;\n\n&lt;p&gt;Recently Apache Airflow can be used, but that is not a solution. &lt;/p&gt;\n\n&lt;p&gt;I use currently the ProcFWK (&lt;a href=\"https://mrpaulandrew.github.io/procfwk/\"&gt;https://mrpaulandrew.github.io/procfwk/&lt;/a&gt;), but this is not a long-term solution for me. &lt;/p&gt;\n\n&lt;p&gt;How do you guys do something like this?  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14cd5rp", "is_robot_indexable": true, "report_reasons": null, "author": "RedHatBerry", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cd5rp/how_to_orchestrate_azure_adfs_and_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cd5rp/how_to_orchestrate_azure_adfs_and_synapse/", "subreddit_subscribers": 111088, "created_utc": 1687070002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We recently spoke with a consultant to help us with data engineering work and we were quoted 200/hr. This is using mostly a Databricks/ADF stack. I\u2019m curious what the market rate would be for consulting help.", "author_fullname": "t2_a0qsnkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Market rate for consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14chc3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687084917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We recently spoke with a consultant to help us with data engineering work and we were quoted 200/hr. This is using mostly a Databricks/ADF stack. I\u2019m curious what the market rate would be for consulting help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14chc3i", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Membership-8", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14chc3i/market_rate_for_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14chc3i/market_rate_for_consulting/", "subreddit_subscribers": 111088, "created_utc": 1687084917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking Analysis: Snowflake Summit will reveal the future of data apps...here's our take - Wikibon Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_14ctv72", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": "#46d160", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gd5R1FhyhMD14yHItXNhYHiMlU4isIkg7y2Pa6yTXDg.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687119130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wikibon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wikibon.com/breaking-analysis-snowflake-summit-will-reveal-the-future-of-data-apps-heres-our-take/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?auto=webp&amp;v=enabled&amp;s=14a22b899b69c954e6ba0b2574d9c61942d6b06e", "width": 2186, "height": 1460}, "resolutions": [{"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88db018c94827d6de94eefeebf3f32752d431adf", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=663505ffc8c3f746a53a33b56c7556400ad7f584", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9716e6f1a6d3289c3073e444d6a5b33bbf03c492", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94a8376d12c1241b232c848294fdc619ccc4d3c3", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c74bf5c3d9be4dbb30a0698191289709d13ad1fc", "width": 960, "height": 641}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ddcd12bfc17b6cd6c8f383157939e302207aa1f", "width": 1080, "height": 721}], "variants": {}, "id": "QCR_IWoAnrPfuZkd102UPZltv6M_a-V9FqwQSLZwats"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14ctv72", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/14ctv72/breaking_analysis_snowflake_summit_will_reveal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wikibon.com/breaking-analysis-snowflake-summit-will-reveal-the-future-of-data-apps-heres-our-take/", "subreddit_subscribers": 111088, "created_utc": 1687119130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently started my first job .\n\nFor now my role is to mapping company database from varies system which include excel , on prem sql server , own , design, , and maintained by different departments, some database not normalised/cleaned. \n\n\nTrying to have a physical ER diagram but stuck by  different structures (multi attribute structures vs flattened structure which i posted another post in r/SQL) \n\nSeeking for advices for the tools(what diagram/type of documentation/tools would be good for the job?)\n\nER models (lost of database information)?\nConceptual ERD?\nPhysical ERD?(cannot due with structure differences with not normalised database)", "author_fullname": "t2_1wtbi6tw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Look for advice ]for choice of tools/diagram for mapping database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cf4tq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687076932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently started my first job .&lt;/p&gt;\n\n&lt;p&gt;For now my role is to mapping company database from varies system which include excel , on prem sql server , own , design, , and maintained by different departments, some database not normalised/cleaned. &lt;/p&gt;\n\n&lt;p&gt;Trying to have a physical ER diagram but stuck by  different structures (multi attribute structures vs flattened structure which i posted another post in &lt;a href=\"/r/SQL\"&gt;r/SQL&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;Seeking for advices for the tools(what diagram/type of documentation/tools would be good for the job?)&lt;/p&gt;\n\n&lt;p&gt;ER models (lost of database information)?\nConceptual ERD?\nPhysical ERD?(cannot due with structure differences with not normalised database)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14cf4tq", "is_robot_indexable": true, "report_reasons": null, "author": "gffyhgffh45655", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cf4tq/look_for_advice_for_choice_of_toolsdiagram_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cf4tq/look_for_advice_for_choice_of_toolsdiagram_for/", "subreddit_subscribers": 111088, "created_utc": 1687076932.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}