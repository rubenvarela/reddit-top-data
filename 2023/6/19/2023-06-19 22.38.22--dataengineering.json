{"kind": "Listing", "data": {"after": "t3_14dpem9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Data Engineers,\n\nThere was a great [discussion yesterday](https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&amp;utm_medium=web2x&amp;context=3) about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.\n\nSummary of potential community:\n\n* Verified data engineers (i.e. no bots, spammers)\n* Networking and in-person events\n* Advanced technical topics and industry news\n* Familiar Reddit-style feed\n\nIt's not meant to replace this community but it could in theory act as a backup. We don't have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.\n\nIf you're interested, please [**join the waitlist here**](https://tally.so/r/nGK7pe). If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.\n\n\\---\n\nWe are looking for 1-2 new mods to join our team!\n\nr/dataengineering has grown tremendously over the past few years and could use an extra set of hands and ideas. It's a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it's a great way to give back to the community. If you're interested, [please apply here](https://tally.so/r/3xj669).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New DE Community &amp; Looking for Mods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dgupv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687186533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;There was a great &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;discussion yesterday&lt;/a&gt; about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.&lt;/p&gt;\n\n&lt;p&gt;Summary of potential community:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Verified data engineers (i.e. no bots, spammers)&lt;/li&gt;\n&lt;li&gt;Networking and in-person events&lt;/li&gt;\n&lt;li&gt;Advanced technical topics and industry news&lt;/li&gt;\n&lt;li&gt;Familiar Reddit-style feed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s not meant to replace this community but it could in theory act as a backup. We don&amp;#39;t have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested, please &lt;a href=\"https://tally.so/r/nGK7pe\"&gt;&lt;strong&gt;join the waitlist here&lt;/strong&gt;&lt;/a&gt;. If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;We are looking for 1-2 new mods to join our team!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; has grown tremendously over the past few years and could use an extra set of hands and ideas. It&amp;#39;s a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it&amp;#39;s a great way to give back to the community. If you&amp;#39;re interested, &lt;a href=\"https://tally.so/r/3xj669\"&gt;please apply here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?auto=webp&amp;v=enabled&amp;s=959740544d8390056f25237218ceb90ba637127a", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48ecdd359f78a24667ad4816581834a9a92ccb16", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d95e85536dc5b64b82d2b4b91889b3b5541d82bf", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b0af19c61cfc7012ec5db942890f51a3543276", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2e4c6be4163d6167fcfb44cb1248af11df51667", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7be3948c6b699c70515b1b403a4688f2647f5ba7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75c55874f00d7e849f4f0b2e46ccb96488c843a3", "width": 1080, "height": 567}], "variants": {}, "id": "vXOF8G9GBUU_-_vM38jf2S1-5UiTZqBcFWecpk4eHS4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "14dgupv", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/", "subreddit_subscribers": 111231, "created_utc": 1687186533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bonus: for junior/entry level roles with little or no previous experience in the field\n\nI had some reddit comment in my obsidian notes that discussed just that but I can't find it, it was something among the lines of:\n\ncv: needs to show what you can do without being too meaty  \nproject readme: no hr or hiring manager will go through your extensive documentation, you need to get the point across in like 10 seconds.\n\n these seem to be good points in theory, but hard to apply in practice.\n\nSo, tell us the secrets, how do we get ourself considered?\n\nExamples for notable projects/cvs just to get a sense for the structure would be amazing too. ", "author_fullname": "t2_85fin9nj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hiring Managers, how should we structure our cvs and projects readme ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d52mz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687151102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bonus: for junior/entry level roles with little or no previous experience in the field&lt;/p&gt;\n\n&lt;p&gt;I had some reddit comment in my obsidian notes that discussed just that but I can&amp;#39;t find it, it was something among the lines of:&lt;/p&gt;\n\n&lt;p&gt;cv: needs to show what you can do without being too meaty&lt;br/&gt;\nproject readme: no hr or hiring manager will go through your extensive documentation, you need to get the point across in like 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;these seem to be good points in theory, but hard to apply in practice.&lt;/p&gt;\n\n&lt;p&gt;So, tell us the secrets, how do we get ourself considered?&lt;/p&gt;\n\n&lt;p&gt;Examples for notable projects/cvs just to get a sense for the structure would be amazing too. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14d52mz", "is_robot_indexable": true, "report_reasons": null, "author": "DimensionOne9851", "discussion_type": null, "num_comments": 13, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d52mz/hiring_managers_how_should_we_structure_our_cvs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d52mz/hiring_managers_how_should_we_structure_our_cvs/", "subreddit_subscribers": 111231, "created_utc": 1687151102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's the job market like right now? (June 2023)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dg280", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687184628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dg280", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dg280/hows_the_job_market_like_right_now_june_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dg280/hows_the_job_market_like_right_now_june_2023/", "subreddit_subscribers": 111231, "created_utc": 1687184628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering, I am a FinOps analyst for an aerospace cloud technologies team, and part of my job is to ensure things in our cloud environment are as optimized for cost. Our company has several ERP systems and there is a big push this year to get all of that data centralized in AWS for consumption into Quicksight. \n\n&amp;#x200B;\n\nRight now we have a working production environment with a single ERP database currently being replicated into S3 and plans to include more soon. The database is running on an old version of IBM's DB2, so we use a Qlikreplicate job to extract changes from the tables and then upload them to S3 for processing in an AWS Glue job that leverages pyspark and hudi to load the files into an S3 datalake with all the metadata being stored in DynamoDB and the data itself queried with Athena and Quicksight.\n\n  \nThis Glue job is the main offender of cost, as the data is pushed once every hour so we have a Glue job running for 3-4 minutes for every table (about 140) and that cost starts to adds up. At first the business wanted these jobs to be ran every 15 minutes, I told them it would probably cost around $150k annually but I'm not sure they believed me because they pushed through with that frequency but then quickly backed the frequency to 1 hour when I showed them the AWS projections of the first month's bill. Right now we pay about $40k annually for just this one database to be replicated into AWS and I'm wondering is that really the most cost effective way to do things? Again, I'm not a data engineer myself (although its a field I have recently started learning in and would love to move that direction one day), and have only been working in Cloud for the past 2.5 years (I do have 7 AWS certifications though so I have a strong grasp on the basics how each service works), so my experience is might be limited, AWS themselves helped with consulting on this project so it very may well be this is the best way to do things, I just wanted to get some outside perspective and see if the collective internet had any thoughts.", "author_fullname": "t2_kaic0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this pipeline financially optimized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dj4l0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687191733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;, I am a FinOps analyst for an aerospace cloud technologies team, and part of my job is to ensure things in our cloud environment are as optimized for cost. Our company has several ERP systems and there is a big push this year to get all of that data centralized in AWS for consumption into Quicksight. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Right now we have a working production environment with a single ERP database currently being replicated into S3 and plans to include more soon. The database is running on an old version of IBM&amp;#39;s DB2, so we use a Qlikreplicate job to extract changes from the tables and then upload them to S3 for processing in an AWS Glue job that leverages pyspark and hudi to load the files into an S3 datalake with all the metadata being stored in DynamoDB and the data itself queried with Athena and Quicksight.&lt;/p&gt;\n\n&lt;p&gt;This Glue job is the main offender of cost, as the data is pushed once every hour so we have a Glue job running for 3-4 minutes for every table (about 140) and that cost starts to adds up. At first the business wanted these jobs to be ran every 15 minutes, I told them it would probably cost around $150k annually but I&amp;#39;m not sure they believed me because they pushed through with that frequency but then quickly backed the frequency to 1 hour when I showed them the AWS projections of the first month&amp;#39;s bill. Right now we pay about $40k annually for just this one database to be replicated into AWS and I&amp;#39;m wondering is that really the most cost effective way to do things? Again, I&amp;#39;m not a data engineer myself (although its a field I have recently started learning in and would love to move that direction one day), and have only been working in Cloud for the past 2.5 years (I do have 7 AWS certifications though so I have a strong grasp on the basics how each service works), so my experience is might be limited, AWS themselves helped with consulting on this project so it very may well be this is the best way to do things, I just wanted to get some outside perspective and see if the collective internet had any thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14dj4l0", "is_robot_indexable": true, "report_reasons": null, "author": "Jaggedfel2142", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dj4l0/is_this_pipeline_financially_optimized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dj4l0/is_this_pipeline_financially_optimized/", "subreddit_subscribers": 111231, "created_utc": 1687191733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new and currently working on designing a pipeline to bring data from multiple tables sitting in S3(CDC) to Databricks. \nMy question is should I build multiple data pipelines per table or just have one pipeline that populates each table. Please note that all these tables are sitting in multiple schemas.", "author_fullname": "t2_67a9tfq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d12ae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687138604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new and currently working on designing a pipeline to bring data from multiple tables sitting in S3(CDC) to Databricks. \nMy question is should I build multiple data pipelines per table or just have one pipeline that populates each table. Please note that all these tables are sitting in multiple schemas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d12ae", "is_robot_indexable": true, "report_reasons": null, "author": "TroubleOver1378", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d12ae/data_pipeline_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d12ae/data_pipeline_advice/", "subreddit_subscribers": 111231, "created_utc": 1687138604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are renewing our very much outdated systems. Now the debate has started if we should use push or pull architecture. Simplified there are multiple source system and a \"data warehouse\" where data should land for searching, analysis and reporting.\n\nA certain delay from data creation to search availability is acceptable, say 30 min. Also data volumes are tiny! This is strictly intranet system. There are also consideration to be made about the state of the companies IT policies and IT capabilities (complex and very limited as in can't deploy a docker container, sic!).\n\n\"chief architect\" (external!) insists on going push with very complex setup. Sources should push to Kafka where sink picks up the data. I was wondering what happens when kakfa is down, all source systems should fail to ensure data consistency. It was said no, there will be a local cache in redis to prevent that. \n\nAll overly complex in my opinion.\n\nif the system itself \"pushed\" a change to redis, then there needs to be a background service that pulls it from redis and sends it to kafka. So it's just a \"hidden\" pull system right?\nIt also means we need a kafka (or insert similar tech here) setup and redis on all systems which your IT or better said external service provider is clueless about.\n\nI know its kind of boring but a \"pull passed system\" using airflow or similar tool that just pulls on last modified date or similar metadata, does processing and pushes it to sink seems to be a lot simpler. Data amounts are small, a delay is acceptable and the source system can be oblivious to what happens with the data. Seems to much simpler. \n\nWhat am I missing? How can I convince the team this is the preferred approach?", "author_fullname": "t2_p8sy28ma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "for a complete new development (including source systems and \"data warehouse) would you use push or pull architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d6ijt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687155854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are renewing our very much outdated systems. Now the debate has started if we should use push or pull architecture. Simplified there are multiple source system and a &amp;quot;data warehouse&amp;quot; where data should land for searching, analysis and reporting.&lt;/p&gt;\n\n&lt;p&gt;A certain delay from data creation to search availability is acceptable, say 30 min. Also data volumes are tiny! This is strictly intranet system. There are also consideration to be made about the state of the companies IT policies and IT capabilities (complex and very limited as in can&amp;#39;t deploy a docker container, sic!).&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;chief architect&amp;quot; (external!) insists on going push with very complex setup. Sources should push to Kafka where sink picks up the data. I was wondering what happens when kakfa is down, all source systems should fail to ensure data consistency. It was said no, there will be a local cache in redis to prevent that. &lt;/p&gt;\n\n&lt;p&gt;All overly complex in my opinion.&lt;/p&gt;\n\n&lt;p&gt;if the system itself &amp;quot;pushed&amp;quot; a change to redis, then there needs to be a background service that pulls it from redis and sends it to kafka. So it&amp;#39;s just a &amp;quot;hidden&amp;quot; pull system right?\nIt also means we need a kafka (or insert similar tech here) setup and redis on all systems which your IT or better said external service provider is clueless about.&lt;/p&gt;\n\n&lt;p&gt;I know its kind of boring but a &amp;quot;pull passed system&amp;quot; using airflow or similar tool that just pulls on last modified date or similar metadata, does processing and pushes it to sink seems to be a lot simpler. Data amounts are small, a delay is acceptable and the source system can be oblivious to what happens with the data. Seems to much simpler. &lt;/p&gt;\n\n&lt;p&gt;What am I missing? How can I convince the team this is the preferred approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14d6ijt", "is_robot_indexable": true, "report_reasons": null, "author": "RationalDialog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d6ijt/for_a_complete_new_development_including_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d6ijt/for_a_complete_new_development_including_source/", "subreddit_subscribers": 111231, "created_utc": 1687155854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does transactions in Spark work? \n\nI'd like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn't seem to follow how transactions work in normal RDBMSs. \n\nIt would be great if someone could shed some light on this.", "author_fullname": "t2_vcdigx7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transactions in Spark / Delta lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d9tfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687167266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does transactions in Spark work? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn&amp;#39;t seem to follow how transactions work in normal RDBMSs. &lt;/p&gt;\n\n&lt;p&gt;It would be great if someone could shed some light on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d9tfc", "is_robot_indexable": true, "report_reasons": null, "author": "loudandclear11", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "subreddit_subscribers": 111231, "created_utc": 1687167266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm writing a series of articles for begginers about manipulating apache Spark using docker.  \nThis describes multiple ways of creating and deploying spark images in local.  \nThis will help understanding the local development environment, by using several local deploy techniques such as 'docker-compose' , 'Kubernetes cluster' and using Helm charts.  \nWhich will cover in 3 parts, all the needed steps to deploy a cluster and runing a job in local.  \nI wish this could help  \n[https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222](https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222)", "author_fullname": "t2_ae4tw1pnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark images, docker-compose and kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dogw9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687203717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m writing a series of articles for begginers about manipulating apache Spark using docker.&lt;br/&gt;\nThis describes multiple ways of creating and deploying spark images in local.&lt;br/&gt;\nThis will help understanding the local development environment, by using several local deploy techniques such as &amp;#39;docker-compose&amp;#39; , &amp;#39;Kubernetes cluster&amp;#39; and using Helm charts.&lt;br/&gt;\nWhich will cover in 3 parts, all the needed steps to deploy a cluster and runing a job in local.&lt;br/&gt;\nI wish this could help&lt;br/&gt;\n&lt;a href=\"https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222\"&gt;https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14dogw9", "is_robot_indexable": true, "report_reasons": null, "author": "PhysicalTomorrow2098", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dogw9/spark_images_dockercompose_and_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dogw9/spark_images_dockercompose_and_kubernetes/", "subreddit_subscribers": 111231, "created_utc": 1687203717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I want to practice my Kafka, Spark Streaming, Docker knowledge on a real project and also learn Kubernetes (k8s). So, I asked ChatGPT for a project, and it provided me with the following project description:   \n\" This project is a data pipeline that combines Kafka, Spark Streaming, Docker, AWS S3, and EKS (Elastic Kubernetes Service) for processing and analyzing real-time data. It uses a self-managed Kafka cluster for data ingestion and distribution, and a Spark cluster for real-time analytics. Docker ensures consistent deployment, while AWS S3 serves as a data repository. EKS manages the Spark cluster, optimizing resource utilization for efficient handling of high data volumes. \"\n\n Do you think this is a good project to undertake and include on my resume if I want to pursue a job in a major tech company like IBM, Oracle, or Amazon? Additionally, if you have any ideas to improve the project or suggestions on elements that need to be removed, please let me know.   \n", "author_fullname": "t2_feara4tb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Data Pipeline with Kafka, Spark Streaming, Docker, AWS S3, and EKS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dn7tk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687200962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to practice my Kafka, Spark Streaming, Docker knowledge on a real project and also learn Kubernetes (k8s). So, I asked ChatGPT for a project, and it provided me with the following project description:&lt;br/&gt;\n&amp;quot; This project is a data pipeline that combines Kafka, Spark Streaming, Docker, AWS S3, and EKS (Elastic Kubernetes Service) for processing and analyzing real-time data. It uses a self-managed Kafka cluster for data ingestion and distribution, and a Spark cluster for real-time analytics. Docker ensures consistent deployment, while AWS S3 serves as a data repository. EKS manages the Spark cluster, optimizing resource utilization for efficient handling of high data volumes. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Do you think this is a good project to undertake and include on my resume if I want to pursue a job in a major tech company like IBM, Oracle, or Amazon? Additionally, if you have any ideas to improve the project or suggestions on elements that need to be removed, please let me know.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dn7tk", "is_robot_indexable": true, "report_reasons": null, "author": "Kratos_1412", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dn7tk/building_a_data_pipeline_with_kafka_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dn7tk/building_a_data_pipeline_with_kafka_spark/", "subreddit_subscribers": 111231, "created_utc": 1687200962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am investigating building tools to help data engineers build data pipelines for machine learning.   \n\n\nI was wondering what are the three biggest problems you encounter on a day-to-day basis.   \n\n\nFor example, is it extracting unstructured data, merging data streams, meeting throughput or latency requirements, keeping upstream and downstream schemas in sync, managing a large number of components in the pipeline, etc. or something else that gives you headaches? Curious to hear!", "author_fullname": "t2_9grwmp0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discussion: What are your biggest problems when building data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dlx8v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687198034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am investigating building tools to help data engineers build data pipelines for machine learning.   &lt;/p&gt;\n\n&lt;p&gt;I was wondering what are the three biggest problems you encounter on a day-to-day basis.   &lt;/p&gt;\n\n&lt;p&gt;For example, is it extracting unstructured data, merging data streams, meeting throughput or latency requirements, keeping upstream and downstream schemas in sync, managing a large number of components in the pipeline, etc. or something else that gives you headaches? Curious to hear!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14dlx8v", "is_robot_indexable": true, "report_reasons": null, "author": "Tricky_Drawer_2917", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dlx8v/discussion_what_are_your_biggest_problems_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dlx8v/discussion_what_are_your_biggest_problems_when/", "subreddit_subscribers": 111231, "created_utc": 1687198034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been seeing a lot of VCs looking for Data Engineers lately, curious if the work is all that interesting or how different is it from working at a typical startup that builds products.", "author_fullname": "t2_5w2yqiczk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have experience working for VCs as Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dou8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687204548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been seeing a lot of VCs looking for Data Engineers lately, curious if the work is all that interesting or how different is it from working at a typical startup that builds products.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dou8c", "is_robot_indexable": true, "report_reasons": null, "author": "ReliefCreepy8397", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dou8c/anyone_have_experience_working_for_vcs_as_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dou8c/anyone_have_experience_working_for_vcs_as_data/", "subreddit_subscribers": 111231, "created_utc": 1687204548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_gpjiq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a functional, effectful Distributed System from scratch in Scala 3, just to avoid Leetcode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14do59q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1687203006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chollinger.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://chollinger.com/blog/2023/06/building-a-functional-effectful-distributed-system-from-scratch-in-scala-3-just-to-avoid-leetcode-part-1/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "14do59q", "is_robot_indexable": true, "report_reasons": null, "author": "otter-in-a-suit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14do59q/building_a_functional_effectful_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://chollinger.com/blog/2023/06/building-a-functional-effectful-distributed-system-from-scratch-in-scala-3-just-to-avoid-leetcode-part-1/", "subreddit_subscribers": 111231, "created_utc": 1687203006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Couldn\u2019t find on LinkedIn Learning and only 1-2 blog posts. I read this recently and thought it was more clear than many others so trying to find more that are possibly less sales-e .  \n\nhttps://estuary.dev/chatgpt-custom-data/", "author_fullname": "t2_a6gn9tzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any could tutorials on building knowledgeable/ vector store data pipeline to enrich llm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dh3iv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687187083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn\u2019t find on LinkedIn Learning and only 1-2 blog posts. I read this recently and thought it was more clear than many others so trying to find more that are possibly less sales-e .  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://estuary.dev/chatgpt-custom-data/\"&gt;https://estuary.dev/chatgpt-custom-data/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dh3iv", "is_robot_indexable": true, "report_reasons": null, "author": "HovercraftGold980", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dh3iv/any_could_tutorials_on_building_knowledgeable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dh3iv/any_could_tutorials_on_building_knowledgeable/", "subreddit_subscribers": 111231, "created_utc": 1687187083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a software engineer, working on implementing the following.\n- Create an API that can be triggered when necessary.\n- When triggered, fetch some records from a read-only datastore.\n- Current estimate is that 1-2 triggers will be received in a week.\n- Each trigger will result in fetching [50 - 100] million records from the datastore for processing.\n- Apply some simple business logic on each record.\n- Based on the output of the previous step, make API calls to other internal systems.\n- Reports need to be generated after the processing is done. The way this will be consumed is not yet decided.\n- Speed of completing the processing is a factor.\n\nThe high level solution that I have for this now:\n- A dockerized web application(referred to as ZZ from here on) which authenticates user and exposes the required API.\n- The internal APIs that are being called, can handle 10K TPS from ZZ\n- ZZ will fetch the records for a job from the datastore, and write it to a Kafka topic.\n- ZZ has a background service which is polling the kafka topic and processing records.\n- Initial testing shows one ZZ container is able to handle 500 TPS of record processing.\n- ZZ will be scaled to 20, so that it can fully utilize the internal API capacity of 10K TPS.\n- ZZ will have some internal controls to only send 500 TPS at a time.\n- If a record processing fails it is retried once.\n- Reports are generated and written to Postgres for retrieval/query.\n- The reports from a job are expected to be available for 1 week after its run.\n- Before a job is initiated, ZZ will be scaled up. And if needed the other internal APIs.\n\nWorking through this, I feel this might be a problem that is common in the data engineering world.\nAny solution/tool that already solves this?\nAppreciate any advice you can provide.", "author_fullname": "t2_1l8inv31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I reinventing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dilon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687190542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a software engineer, working on implementing the following.\n- Create an API that can be triggered when necessary.\n- When triggered, fetch some records from a read-only datastore.\n- Current estimate is that 1-2 triggers will be received in a week.\n- Each trigger will result in fetching [50 - 100] million records from the datastore for processing.\n- Apply some simple business logic on each record.\n- Based on the output of the previous step, make API calls to other internal systems.\n- Reports need to be generated after the processing is done. The way this will be consumed is not yet decided.\n- Speed of completing the processing is a factor.&lt;/p&gt;\n\n&lt;p&gt;The high level solution that I have for this now:\n- A dockerized web application(referred to as ZZ from here on) which authenticates user and exposes the required API.\n- The internal APIs that are being called, can handle 10K TPS from ZZ\n- ZZ will fetch the records for a job from the datastore, and write it to a Kafka topic.\n- ZZ has a background service which is polling the kafka topic and processing records.\n- Initial testing shows one ZZ container is able to handle 500 TPS of record processing.\n- ZZ will be scaled to 20, so that it can fully utilize the internal API capacity of 10K TPS.\n- ZZ will have some internal controls to only send 500 TPS at a time.\n- If a record processing fails it is retried once.\n- Reports are generated and written to Postgres for retrieval/query.\n- The reports from a job are expected to be available for 1 week after its run.\n- Before a job is initiated, ZZ will be scaled up. And if needed the other internal APIs.&lt;/p&gt;\n\n&lt;p&gt;Working through this, I feel this might be a problem that is common in the data engineering world.\nAny solution/tool that already solves this?\nAppreciate any advice you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dilon", "is_robot_indexable": true, "report_reasons": null, "author": "find_my_path01", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dilon/am_i_reinventing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dilon/am_i_reinventing/", "subreddit_subscribers": 111231, "created_utc": 1687190542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have implemented a Python script that validates the load of 3 critical tables every morning. What it does is a simple row count check.\n\nThe script is **very rudimentary**. I have one method per table (3). If the check fails a pre-determined criteria I send a slack message to a dedicated channel with a message containing the table name. In short:\n\n    def send_slack_message()\n    def check_table1()\n    def check_table2()\n    try:\n    check_table1()\n    check_table2()\n    else:\n    finally:\n\nI want to improve it though.  I am working on adding a check to compare today's date to a \"last update\" attribute in each of the 3 tables.\n\nWhat other kind of checks would you suggest I implement?", "author_fullname": "t2_8hos5mrg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic Load Check", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14diaq7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687189864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have implemented a Python script that validates the load of 3 critical tables every morning. What it does is a simple row count check.&lt;/p&gt;\n\n&lt;p&gt;The script is &lt;strong&gt;very rudimentary&lt;/strong&gt;. I have one method per table (3). If the check fails a pre-determined criteria I send a slack message to a dedicated channel with a message containing the table name. In short:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def send_slack_message()\ndef check_table1()\ndef check_table2()\ntry:\ncheck_table1()\ncheck_table2()\nelse:\nfinally:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I want to improve it though.  I am working on adding a check to compare today&amp;#39;s date to a &amp;quot;last update&amp;quot; attribute in each of the 3 tables.&lt;/p&gt;\n\n&lt;p&gt;What other kind of checks would you suggest I implement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14diaq7", "is_robot_indexable": true, "report_reasons": null, "author": "BatCommercial7523", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14diaq7/basic_load_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14diaq7/basic_load_check/", "subreddit_subscribers": 111231, "created_utc": 1687189864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have recently taken the GCP Professional Data Engineer certificate and have any advice on question topics you remember or websites with great practice questions please leave a comment, TIA!", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good practice question banks for CP Professional Data Engineer Cert? question topics from anyone who has taken it recently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14df68i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687182476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have recently taken the GCP Professional Data Engineer certificate and have any advice on question topics you remember or websites with great practice questions please leave a comment, TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14df68i", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14df68i/good_practice_question_banks_for_cp_professional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14df68i/good_practice_question_banks_for_cp_professional/", "subreddit_subscribers": 111231, "created_utc": 1687182476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Keboola is hosting a webinar this Wednesday together with Snowflake. \n\n&amp;#x200B;\n\nJoin us to discover Snowpark\u2019s power that helps you bridge the gap between data engineers and data scientists. We\u2019re talking about a unified environment, enabled by an end-to-end data platform where these two teams collaborate, combining their expertise to streamline complex data operations.\n\n&amp;#x200B;\n\nReserve your spot: [https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola](https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola)", "author_fullname": "t2_opyjpm1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Webinar: Supercharge Your Data with Snowflake Snowpark &amp; Keboola", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d6yhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687157400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Keboola is hosting a webinar this Wednesday together with Snowflake. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Join us to discover Snowpark\u2019s power that helps you bridge the gap between data engineers and data scientists. We\u2019re talking about a unified environment, enabled by an end-to-end data platform where these two teams collaborate, combining their expertise to streamline complex data operations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reserve your spot: &lt;a href=\"https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola\"&gt;https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?auto=webp&amp;v=enabled&amp;s=e6acb740776cb94fd793469fa13585e02943e3bb", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193414485aef2c3437d97175f5bfd80ad0e5cb32", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7aeda51779888a5aa671649c876888023df7a81", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c8bfc11425dc55368dc4941f9c184756d8097cc", "width": 320, "height": 320}], "variants": {}, "id": "J7yfwyLNBmr8hnc17qkzr-hk8MGj6eeLMDAMKBhlHio"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14d6yhf", "is_robot_indexable": true, "report_reasons": null, "author": "CalleKeboola", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d6yhf/webinar_supercharge_your_data_with_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d6yhf/webinar_supercharge_your_data_with_snowflake/", "subreddit_subscribers": 111231, "created_utc": 1687157400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! My team is giving a try to the Azure Managed Airflow. I wonder if anyone has successfully implemented the Azure Managed Airflow?  How did you implement CI/CD, are there any tips to know before implementing it? Is it better to deploy it on AKS?\n\nI'm using Astro CLI for local dev and hoping to establish a CI/CD pipeline to deploy it on Azure.", "author_fullname": "t2_7zv6bhar", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying on Azure Managed Airflow vs. AKS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dpg2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687205922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! My team is giving a try to the Azure Managed Airflow. I wonder if anyone has successfully implemented the Azure Managed Airflow?  How did you implement CI/CD, are there any tips to know before implementing it? Is it better to deploy it on AKS?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Astro CLI for local dev and hoping to establish a CI/CD pipeline to deploy it on Azure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dpg2o", "is_robot_indexable": true, "report_reasons": null, "author": "Resident-Income-5222", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dpg2o/deploying_on_azure_managed_airflow_vs_aks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dpg2o/deploying_on_azure_managed_airflow_vs_aks/", "subreddit_subscribers": 111231, "created_utc": 1687205922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been Big data engineer ( spark, hadoop, python, pipeline buidling on cloud as well on premise) with 7 plus years of experience and have been java developer for 2 years before that. I have worked independently with out any supervision for some part of my career and even lead one major project for my team where co-ordination needed across 7 teams but never as an official lead in the project.\n\nI do agree i never got to work as an architect but i built pipelines independently all by my myself using diff tech. \n\nI have been putting away taking next step in my career so far but rite now i wanna pursue it and really wanna take it seriously. Any suggestions on the process and key areas i need to focus and ways to improve would be nice. Thank you!", "author_fullname": "t2_17b2qof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do one become Staff engineer in bigdata ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dmvxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687200230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been Big data engineer ( spark, hadoop, python, pipeline buidling on cloud as well on premise) with 7 plus years of experience and have been java developer for 2 years before that. I have worked independently with out any supervision for some part of my career and even lead one major project for my team where co-ordination needed across 7 teams but never as an official lead in the project.&lt;/p&gt;\n\n&lt;p&gt;I do agree i never got to work as an architect but i built pipelines independently all by my myself using diff tech. &lt;/p&gt;\n\n&lt;p&gt;I have been putting away taking next step in my career so far but rite now i wanna pursue it and really wanna take it seriously. Any suggestions on the process and key areas i need to focus and ways to improve would be nice. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dmvxf", "is_robot_indexable": true, "report_reasons": null, "author": "I_like_salads", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dmvxf/how_do_one_become_staff_engineer_in_bigdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dmvxf/how_do_one_become_staff_engineer_in_bigdata/", "subreddit_subscribers": 111231, "created_utc": 1687200230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "people, I am data engineer with moderate background in ML. Working on ML/NLP tasks.\n\nMy question is, can we add more context or enrich text to have better embeddings...?\n\nContext:\n\nI am dealing with bunch of PDF files, which are in various structure but all of them are very high level updates, and mostly in bullet or numbered lists with little text.\n\nSo created embeddings out of it, and connected with LLM. Problem is, (imagine we have \"heading blah blah\" and some statements following...) when I ask what are the points from heading blah blah... (due to data is chunked... and embedded) I only get first few items (that are in same chunk as the heading) and rest of them are lost... I think it has to do with the context, rest of the embeddings do not have similarity with the heading so vector database is missing them (basically first vector db pulls the doc and share it with LLM...).\n\nSo I was thinking about either to enrich the data or adding more context (like, identifying heading and then adding something like \"below are points related to this topic...\") or creating nested dict (something like {\"heading 1\": {\"contents\": \\[\"aaaa\", \"bbbb\", \\], \"childs\"; {}}}\n\nI dont know whether these stupid ideas or not... need some help here... or please let me know if I am missing something?\n\n(btw, I really hope my question make sense...)", "author_fullname": "t2_4v8di1j8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with preprocessing unstructured data for better embeddings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14dmcws", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687199047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;people, I am data engineer with moderate background in ML. Working on ML/NLP tasks.&lt;/p&gt;\n\n&lt;p&gt;My question is, can we add more context or enrich text to have better embeddings...?&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;I am dealing with bunch of PDF files, which are in various structure but all of them are very high level updates, and mostly in bullet or numbered lists with little text.&lt;/p&gt;\n\n&lt;p&gt;So created embeddings out of it, and connected with LLM. Problem is, (imagine we have &amp;quot;heading blah blah&amp;quot; and some statements following...) when I ask what are the points from heading blah blah... (due to data is chunked... and embedded) I only get first few items (that are in same chunk as the heading) and rest of them are lost... I think it has to do with the context, rest of the embeddings do not have similarity with the heading so vector database is missing them (basically first vector db pulls the doc and share it with LLM...).&lt;/p&gt;\n\n&lt;p&gt;So I was thinking about either to enrich the data or adding more context (like, identifying heading and then adding something like &amp;quot;below are points related to this topic...&amp;quot;) or creating nested dict (something like {&amp;quot;heading 1&amp;quot;: {&amp;quot;contents&amp;quot;: [&amp;quot;aaaa&amp;quot;, &amp;quot;bbbb&amp;quot;, ], &amp;quot;childs&amp;quot;; {}}}&lt;/p&gt;\n\n&lt;p&gt;I dont know whether these stupid ideas or not... need some help here... or please let me know if I am missing something?&lt;/p&gt;\n\n&lt;p&gt;(btw, I really hope my question make sense...)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dmcws", "is_robot_indexable": true, "report_reasons": null, "author": "Tumbleweed-Afraid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dmcws/need_help_with_preprocessing_unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dmcws/need_help_with_preprocessing_unstructured_data/", "subreddit_subscribers": 111231, "created_utc": 1687199047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bhpulfr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping YouTube with a \u2018headful\u2019 remote web scraping browser", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14dh6ao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oKotqwuhj_5AVbJP6-J8Rjj3o3v3SA7TNMUfKn3O7Xk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687187253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "javascript.plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://javascript.plainenglish.io/scraping-youtube-with-this-headful-remote-web-scraping-browser-5e2448c8b54", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?auto=webp&amp;v=enabled&amp;s=8c13b1c763de2c934316f77d50e6c5d73b4e8237", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e59a735b8e9f2095047fcf8792394b59edf25b7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=041ea2d038b8013519e39592f82616f51c54ebfd", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac558e53d86a39df9b5183261d11c48f7713f44d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c73e1d1cddbc29cdb12a08ad18affac80f59f53", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef0ac3f06df27112846d1e02b6043d3a8d4194aa", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/ZrxculCbW_t1bfaL5btEuoCDsEAMQgW0W7AL3bZqoCI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e96b5c1c93d1fe105e8d9bf0083190df216c185b", "width": 1080, "height": 1080}], "variants": {}, "id": "diypAG39iE4Ttu6JoQMdgU7NeBEcawfz-sLDY0LDWcw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14dh6ao", "is_robot_indexable": true, "report_reasons": null, "author": "9millionrainydays_91", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dh6ao/scraping_youtube_with_a_headful_remote_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://javascript.plainenglish.io/scraping-youtube-with-this-headful-remote-web-scraping-browser-5e2448c8b54", "subreddit_subscribers": 111231, "created_utc": 1687187253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a telecom company with an on-premise Cloudera hadoop environment, we have a project that has A LOT of data, the last daily partition i checked had 6 trillion rows. It's having some performance issues, and I want some suggestions from you.\n\nFirst, my role is in the operations field, i don't develop projects, I just need to make sure they are running.\n\nThe problem: It's intended to run daily, but sometimes, thanks to Hbase hotspotting, the process takes a long time to run, even reaching 140hours to finish.\n\nWe have an Oracle Golden Gate that feeds an Hive external table pointing to hbase table with trillions of rows. (The OGG is not on behalf of my team)\n\nThen, we use Hive to full scan the stage table and populate the final fact table for the user area. There is no transformation process.\n\nI know the project is wrong, but it was already running when I got into the company.\n\nPlease, if you have some suggestion to improve its performance or need more details to help me, just let me know!\n\nI would also be glad if you have some suggestion in how this project should have been designed.", "author_fullname": "t2_9fkl3gdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help in a work project (OGG -&gt; Hbase -&gt; Hive)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14drjd4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687210692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a telecom company with an on-premise Cloudera hadoop environment, we have a project that has A LOT of data, the last daily partition i checked had 6 trillion rows. It&amp;#39;s having some performance issues, and I want some suggestions from you.&lt;/p&gt;\n\n&lt;p&gt;First, my role is in the operations field, i don&amp;#39;t develop projects, I just need to make sure they are running.&lt;/p&gt;\n\n&lt;p&gt;The problem: It&amp;#39;s intended to run daily, but sometimes, thanks to Hbase hotspotting, the process takes a long time to run, even reaching 140hours to finish.&lt;/p&gt;\n\n&lt;p&gt;We have an Oracle Golden Gate that feeds an Hive external table pointing to hbase table with trillions of rows. (The OGG is not on behalf of my team)&lt;/p&gt;\n\n&lt;p&gt;Then, we use Hive to full scan the stage table and populate the final fact table for the user area. There is no transformation process.&lt;/p&gt;\n\n&lt;p&gt;I know the project is wrong, but it was already running when I got into the company.&lt;/p&gt;\n\n&lt;p&gt;Please, if you have some suggestion to improve its performance or need more details to help me, just let me know!&lt;/p&gt;\n\n&lt;p&gt;I would also be glad if you have some suggestion in how this project should have been designed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14drjd4", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished_Emu9409", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14drjd4/need_some_help_in_a_work_project_ogg_hbase_hive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14drjd4/need_some_help_in_a_work_project_ogg_hbase_hive/", "subreddit_subscribers": 111231, "created_utc": 1687210692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn the process of standing up databricks as a sole data practioner and rapidly grinding my way through the academy courses but a bit unclear on naming conventions. So I understand unity has a catalog.schema.table naming convention which sounds great and Id like to implement a medallion architecture moving forwards. I have some webhook event data I want to load directly into my bronze delta table as insert only and some batch files that will come from another adls gen 2 store I can add as an external location.\n\nMy understanding is for our org, one workspace with a catalog for dev and prod would be best although I haven't got to the cicd bit of the academy yet so not too sure exactly how that work flow happens in databricks.\n\nSo that gives me prod.schema.table to work with.\n\nShould I do prod.bronze.salesforce_orders? Or should the schema be the system and use the medallion terminology in the table name...?\n\nSorry if this is a really simple question but don't want to make a stupid mistake I'll have to undo down the line\n\nThanks!", "author_fullname": "t2_t12o9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Naming conventions in Unity Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14dqq70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687208873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In the process of standing up databricks as a sole data practioner and rapidly grinding my way through the academy courses but a bit unclear on naming conventions. So I understand unity has a catalog.schema.table naming convention which sounds great and Id like to implement a medallion architecture moving forwards. I have some webhook event data I want to load directly into my bronze delta table as insert only and some batch files that will come from another adls gen 2 store I can add as an external location.&lt;/p&gt;\n\n&lt;p&gt;My understanding is for our org, one workspace with a catalog for dev and prod would be best although I haven&amp;#39;t got to the cicd bit of the academy yet so not too sure exactly how that work flow happens in databricks.&lt;/p&gt;\n\n&lt;p&gt;So that gives me prod.schema.table to work with.&lt;/p&gt;\n\n&lt;p&gt;Should I do prod.bronze.salesforce_orders? Or should the schema be the system and use the medallion terminology in the table name...?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is a really simple question but don&amp;#39;t want to make a stupid mistake I&amp;#39;ll have to undo down the line&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14dqq70", "is_robot_indexable": true, "report_reasons": null, "author": "KingslyLear", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dqq70/naming_conventions_in_unity_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dqq70/naming_conventions_in_unity_databricks/", "subreddit_subscribers": 111231, "created_utc": 1687208873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3eoui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to tackle large number of records using Python dataframes in Odoo?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_14dq4ho", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-k5PIentL2KFscuYHVtbldbvje8m00fnHZszSCN4kGw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687207476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "numla.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://numla.com/blog/odoo-development-18/tackling-large-number-of-records-using-python-dataframes-in-odoo-15", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?auto=webp&amp;v=enabled&amp;s=ab57dc7b45caa8381ba7b3eb4c68cee4d764b70d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa920fe0b8c4fd6e00b7f21670f9d45aa9f30be6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da1b15c56c23792d0165d5ce625aa6b302710e75", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=755c00afd8c9c7dec350a8abee985aa1b4074c9d", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59ca959ccab5b6048f197d26a0355250985a8038", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05c25914db7fe381f3dec2a3d0154bdbd9b08196", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/Iflve4eHu4CjC5ePZsJm3NLl-nQSeDSZpDhi-n_W8jA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3581d029ead86de69f3095ab29fce288518c459f", "width": 1080, "height": 564}], "variants": {}, "id": "Gf4gZApmr8wXV7MLyP2lcoKRa_i4IqPCsIwJojVdY2E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14dq4ho", "is_robot_indexable": true, "report_reasons": null, "author": "waqararif", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dq4ho/how_to_tackle_large_number_of_records_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://numla.com/blog/odoo-development-18/tackling-large-number-of-records-using-python-dataframes-in-odoo-15", "subreddit_subscribers": 111231, "created_utc": 1687207476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jcps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lessons learned building on Snowpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 135, "top_awarded_type": null, "hide_score": false, "name": "t3_14dpem9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4qYXfKh3_D3Wl-bRfoCMSFNWe1QMO2nHpAyaBK0dVR4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687205828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "learningfromdata.zingg.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.learningfromdata.zingg.ai/p/building-identity-resolution-on-snowflake", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?auto=webp&amp;v=enabled&amp;s=6e38b5b421a3789acd233ff97777a777ca9f77aa", "width": 1080, "height": 1049}, "resolutions": [{"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d0e172ac0d5797c63702045e4ef8b787abcfea3", "width": 108, "height": 104}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=491f0bc79555df73f58e098b002b947ef9fd9900", "width": 216, "height": 209}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f29ba29c5a021fcee815ccfea5ccb9106fefea8e", "width": 320, "height": 310}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4c8acbcf15768d003cc90f6ed881c7540c556c8", "width": 640, "height": 621}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b04658cff1bd7378f14965a8285a649a3944a2a", "width": 960, "height": 932}, {"url": "https://external-preview.redd.it/V_E9EpfnPdflktvW0V8xiPoD_4QWy39T5HdmgT7u9-I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ac48361806694da49c91f4b9578affa4d4bfa6d", "width": 1080, "height": 1049}], "variants": {}, "id": "nGS0oBLgu-qRM_n4RrQEgFL_itbpg1CW56SmE9hYCeQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14dpem9", "is_robot_indexable": true, "report_reasons": null, "author": "sonalg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dpem9/lessons_learned_building_on_snowpark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.learningfromdata.zingg.ai/p/building-identity-resolution-on-snowflake", "subreddit_subscribers": 111231, "created_utc": 1687205828.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}