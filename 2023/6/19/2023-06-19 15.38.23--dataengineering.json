{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I've aged, Data Engineering has been the last bastion of why I stick around anyway.\n\nSo I ask, which other communities do you guys follow that fosters high quality data engineering discussions?", "author_fullname": "t2_9suj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What other communities do you follow for DE discussion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cs98f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687115127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I&amp;#39;ve aged, Data Engineering has been the last bastion of why I stick around anyway.&lt;/p&gt;\n\n&lt;p&gt;So I ask, which other communities do you guys follow that fosters high quality data engineering discussions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14cs98f", "is_robot_indexable": true, "report_reasons": null, "author": "Drekalo", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/", "subreddit_subscribers": 111190, "created_utc": 1687115127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually learn from YouTube videos but I don't see a reputed playlist on Apache Airflow. Any recommendations from where I can learn the same?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resource to learn Apache Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cve1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687122872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually learn from YouTube videos but I don&amp;#39;t see a reputed playlist on Apache Airflow. Any recommendations from where I can learn the same?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14cve1u", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cve1u/best_resource_to_learn_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cve1u/best_resource_to_learn_apache_airflow/", "subreddit_subscribers": 111190, "created_utc": 1687122872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bonus: for junior/entry level roles with little or no previous experience in the field\n\nI had some reddit comment in my obsidian notes that discussed just that but I can't find it, it was something among the lines of:\n\ncv: needs to show what you can do without being too meaty  \nproject readme: no hr or hiring manager will go through your extensive documentation, you need to get the point across in like 10 seconds.\n\n these seem to be good points in theory, but hard to apply in practice.\n\nSo, tell us the secrets, how do we get ourself considered?\n\nExamples for notable projects/cvs just to get a sense for the structure would be amazing too. ", "author_fullname": "t2_85fin9nj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hiring Managers, how should we structure our cvs and projects readme ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d52mz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687151102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bonus: for junior/entry level roles with little or no previous experience in the field&lt;/p&gt;\n\n&lt;p&gt;I had some reddit comment in my obsidian notes that discussed just that but I can&amp;#39;t find it, it was something among the lines of:&lt;/p&gt;\n\n&lt;p&gt;cv: needs to show what you can do without being too meaty&lt;br/&gt;\nproject readme: no hr or hiring manager will go through your extensive documentation, you need to get the point across in like 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;these seem to be good points in theory, but hard to apply in practice.&lt;/p&gt;\n\n&lt;p&gt;So, tell us the secrets, how do we get ourself considered?&lt;/p&gt;\n\n&lt;p&gt;Examples for notable projects/cvs just to get a sense for the structure would be amazing too. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14d52mz", "is_robot_indexable": true, "report_reasons": null, "author": "DimensionOne9851", "discussion_type": null, "num_comments": 12, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d52mz/hiring_managers_how_should_we_structure_our_cvs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d52mz/hiring_managers_how_should_we_structure_our_cvs/", "subreddit_subscribers": 111190, "created_utc": 1687151102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new and currently working on designing a pipeline to bring data from multiple tables sitting in S3(CDC) to Databricks. \nMy question is should I build multiple data pipelines per table or just have one pipeline that populates each table. Please note that all these tables are sitting in multiple schemas.", "author_fullname": "t2_67a9tfq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d12ae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687138604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new and currently working on designing a pipeline to bring data from multiple tables sitting in S3(CDC) to Databricks. \nMy question is should I build multiple data pipelines per table or just have one pipeline that populates each table. Please note that all these tables are sitting in multiple schemas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d12ae", "is_robot_indexable": true, "report_reasons": null, "author": "TroubleOver1378", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d12ae/data_pipeline_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d12ae/data_pipeline_advice/", "subreddit_subscribers": 111190, "created_utc": 1687138604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does transactions in Spark work? \n\nI'd like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn't seem to follow how transactions work in normal RDBMSs. \n\nIt would be great if someone could shed some light on this.", "author_fullname": "t2_vcdigx7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transactions in Spark / Delta lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d9tfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687167266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does transactions in Spark work? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn&amp;#39;t seem to follow how transactions work in normal RDBMSs. &lt;/p&gt;\n\n&lt;p&gt;It would be great if someone could shed some light on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d9tfc", "is_robot_indexable": true, "report_reasons": null, "author": "loudandclear11", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "subreddit_subscribers": 111190, "created_utc": 1687167266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are renewing our very much outdated systems. Now the debate has started if we should use push or pull architecture. Simplified there are multiple source system and a \"data warehouse\" where data should land for searching, analysis and reporting.\n\nA certain delay from data creation to search availability is acceptable, say 30 min. Also data volumes are tiny! This is strictly intranet system. There are also consideration to be made about the state of the companies IT policies and IT capabilities (complex and very limited as in can't deploy a docker container, sic!).\n\n\"chief architect\" (external!) insists on going push with very complex setup. Sources should push to Kafka where sink picks up the data. I was wondering what happens when kakfa is down, all source systems should fail to ensure data consistency. It was said no, there will be a local cache in redis to prevent that. \n\nAll overly complex in my opinion.\n\nif the system itself \"pushed\" a change to redis, then there needs to be a background service that pulls it from redis and sends it to kafka. So it's just a \"hidden\" pull system right?\nIt also means we need a kafka (or insert similar tech here) setup and redis on all systems which your IT or better said external service provider is clueless about.\n\nI know its kind of boring but a \"pull passed system\" using airflow or similar tool that just pulls on last modified date or similar metadata, does processing and pushes it to sink seems to be a lot simpler. Data amounts are small, a delay is acceptable and the source system can be oblivious to what happens with the data. Seems to much simpler. \n\nWhat am I missing? How can I convince the team this is the preferred approach?", "author_fullname": "t2_p8sy28ma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "for a complete new development (including source systems and \"data warehouse) would you use push or pull architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d6ijt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687155854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are renewing our very much outdated systems. Now the debate has started if we should use push or pull architecture. Simplified there are multiple source system and a &amp;quot;data warehouse&amp;quot; where data should land for searching, analysis and reporting.&lt;/p&gt;\n\n&lt;p&gt;A certain delay from data creation to search availability is acceptable, say 30 min. Also data volumes are tiny! This is strictly intranet system. There are also consideration to be made about the state of the companies IT policies and IT capabilities (complex and very limited as in can&amp;#39;t deploy a docker container, sic!).&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;chief architect&amp;quot; (external!) insists on going push with very complex setup. Sources should push to Kafka where sink picks up the data. I was wondering what happens when kakfa is down, all source systems should fail to ensure data consistency. It was said no, there will be a local cache in redis to prevent that. &lt;/p&gt;\n\n&lt;p&gt;All overly complex in my opinion.&lt;/p&gt;\n\n&lt;p&gt;if the system itself &amp;quot;pushed&amp;quot; a change to redis, then there needs to be a background service that pulls it from redis and sends it to kafka. So it&amp;#39;s just a &amp;quot;hidden&amp;quot; pull system right?\nIt also means we need a kafka (or insert similar tech here) setup and redis on all systems which your IT or better said external service provider is clueless about.&lt;/p&gt;\n\n&lt;p&gt;I know its kind of boring but a &amp;quot;pull passed system&amp;quot; using airflow or similar tool that just pulls on last modified date or similar metadata, does processing and pushes it to sink seems to be a lot simpler. Data amounts are small, a delay is acceptable and the source system can be oblivious to what happens with the data. Seems to much simpler. &lt;/p&gt;\n\n&lt;p&gt;What am I missing? How can I convince the team this is the preferred approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14d6ijt", "is_robot_indexable": true, "report_reasons": null, "author": "RationalDialog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d6ijt/for_a_complete_new_development_including_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d6ijt/for_a_complete_new_development_including_source/", "subreddit_subscribers": 111190, "created_utc": 1687155854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, i have  questions related to parquet files and AWS Glue, they're maybe basic questions but i hope someone can help me understand more clearly.\n\nCurrently i'm working on a project that i use a ETL tool to load data from internal databases to AWS S3 in parquet format and then i use AWS Glue to do some transformation by using SparkSQL on these parquet files and write them back to S3 (parquet files as well)\n\n1.How Date and DateTime columns are stored in parquet file ?.\n\nAs i read here [https://github.com/apache/parquet-format/blob/master/LogicalTypes.md](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md) , they are store in Integer formats and these integers represent the number of days (for Date) or number of  milliseconds, microseconds or nanoseconds (for DateTime)  since 1970-01-01. This works as expected with the parquet file that written by our ETL tool from internal database --&gt; S3, all Data/DateTime columns are Integers, means that in Glue Job, i have to convert these Integers back to Date/Datetime value to do some transformation on them. But when parquet files are written by Spark, they are Date/DateTime (or TimeStamp to be more concise) format not Integers (i checked by read these files again into Dataframe and print schema of that dataframe) and that make me confused. Of course before writing the database table (by using ETL tool)  or dataframe (by using Spark) i leave all Date/DateTime columns as Date/DateTime format\n\n&amp;#x200B;\n\n2. Our parquet files are stored in S3 with Date partitions as &lt;base\\_path&gt;/&lt;table\\_name&gt;/&lt;year=yyyy&gt;/&lt;month=mm&gt;/&lt;day=dd&gt;/parquet\\_files . Each Glue Job will write to 1 date partition for 1 table in S3. Is it best practice to limit the number of files before writing to S3 by calling dataframe.repartition() because if i don't do that the number of files in S3 will be unexpectedly many for some Glue Jobs ?\n\n&amp;#x200B;\n\nThank you very much", "author_fullname": "t2_7fyrjq8ff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have question related to Parquet files and AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ctdeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687120014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687117943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i have  questions related to parquet files and AWS Glue, they&amp;#39;re maybe basic questions but i hope someone can help me understand more clearly.&lt;/p&gt;\n\n&lt;p&gt;Currently i&amp;#39;m working on a project that i use a ETL tool to load data from internal databases to AWS S3 in parquet format and then i use AWS Glue to do some transformation by using SparkSQL on these parquet files and write them back to S3 (parquet files as well)&lt;/p&gt;\n\n&lt;p&gt;1.How Date and DateTime columns are stored in parquet file ?.&lt;/p&gt;\n\n&lt;p&gt;As i read here &lt;a href=\"https://github.com/apache/parquet-format/blob/master/LogicalTypes.md\"&gt;https://github.com/apache/parquet-format/blob/master/LogicalTypes.md&lt;/a&gt; , they are store in Integer formats and these integers represent the number of days (for Date) or number of  milliseconds, microseconds or nanoseconds (for DateTime)  since 1970-01-01. This works as expected with the parquet file that written by our ETL tool from internal database --&amp;gt; S3, all Data/DateTime columns are Integers, means that in Glue Job, i have to convert these Integers back to Date/Datetime value to do some transformation on them. But when parquet files are written by Spark, they are Date/DateTime (or TimeStamp to be more concise) format not Integers (i checked by read these files again into Dataframe and print schema of that dataframe) and that make me confused. Of course before writing the database table (by using ETL tool)  or dataframe (by using Spark) i leave all Date/DateTime columns as Date/DateTime format&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Our parquet files are stored in S3 with Date partitions as &amp;lt;base\\_path&amp;gt;/&amp;lt;table\\_name&amp;gt;/&amp;lt;year=yyyy&amp;gt;/&amp;lt;month=mm&amp;gt;/&amp;lt;day=dd&amp;gt;/parquet_files . Each Glue Job will write to 1 date partition for 1 table in S3. Is it best practice to limit the number of files before writing to S3 by calling dataframe.repartition() because if i don&amp;#39;t do that the number of files in S3 will be unexpectedly many for some Glue Jobs ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?auto=webp&amp;v=enabled&amp;s=c51f45b6e8d1f15543e6991d2157f0454e12bd64", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3093470252c899130e2f27fb7d76cb3aa0b468ce", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a796c7394ac14f2fbd22c352695b4223c19a7bd9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53e88e600c9e3bcacca62d52d489114ec0f4b07f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5631ac15c09241153588e02ad9cc6550235f2e8a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ccba4040a49d102a8d22fe4787f52aede31b390", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3546db03dc04a15ffd4d3ec0af7cda056364520a", "width": 1080, "height": 540}], "variants": {}, "id": "SKNeXyLZaZn_HdQEQLJJaJjNsN0LoHMwnXsUK53ewyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ctdeq", "is_robot_indexable": true, "report_reasons": null, "author": "random_name_362", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ctdeq/i_have_question_related_to_parquet_files_and_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ctdeq/i_have_question_related_to_parquet_files_and_aws/", "subreddit_subscribers": 111190, "created_utc": 1687117943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Keboola is hosting a webinar this Wednesday together with Snowflake. \n\n&amp;#x200B;\n\nJoin us to discover Snowpark\u2019s power that helps you bridge the gap between data engineers and data scientists. We\u2019re talking about a unified environment, enabled by an end-to-end data platform where these two teams collaborate, combining their expertise to streamline complex data operations.\n\n&amp;#x200B;\n\nReserve your spot: [https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola](https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola)", "author_fullname": "t2_opyjpm1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Webinar: Supercharge Your Data with Snowflake Snowpark &amp; Keboola", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d6yhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687157400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Keboola is hosting a webinar this Wednesday together with Snowflake. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Join us to discover Snowpark\u2019s power that helps you bridge the gap between data engineers and data scientists. We\u2019re talking about a unified environment, enabled by an end-to-end data platform where these two teams collaborate, combining their expertise to streamline complex data operations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reserve your spot: &lt;a href=\"https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola\"&gt;https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?auto=webp&amp;v=enabled&amp;s=e6acb740776cb94fd793469fa13585e02943e3bb", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193414485aef2c3437d97175f5bfd80ad0e5cb32", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7aeda51779888a5aa671649c876888023df7a81", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c8bfc11425dc55368dc4941f9c184756d8097cc", "width": 320, "height": 320}], "variants": {}, "id": "J7yfwyLNBmr8hnc17qkzr-hk8MGj6eeLMDAMKBhlHio"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14d6yhf", "is_robot_indexable": true, "report_reasons": null, "author": "CalleKeboola", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d6yhf/webinar_supercharge_your_data_with_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d6yhf/webinar_supercharge_your_data_with_snowflake/", "subreddit_subscribers": 111190, "created_utc": 1687157400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Data Engineers,\n\nThere was a great [discussion yesterday](https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&amp;utm_medium=web2x&amp;context=3) about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.\n\nSummary of potential community:\n\n* Verified data engineers (i.e. no bots, spammers)\n* Networking and in-person events\n* Advanced technical topics and industry news\n* Familiar Reddit-style feed\n\nIt's not meant to replace this community but it could in theory act as a backup. We don't have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.\n\nIf you're interested, please [**join the waitlist here**](https://tally.so/r/nGK7pe). If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.\n\n\\---\n\nWe are looking for 1-2 new mods to join our team!\n\nr/dataengineering has grown tremendously over the past few years and could use an extra set of hands and ideas. It's a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it's a great way to give back to the community. If you're interested, [please apply here](https://tally.so/r/3xj669).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New DE Community &amp; Looking for Mods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14dgupv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687186533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;There was a great &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;discussion yesterday&lt;/a&gt; about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.&lt;/p&gt;\n\n&lt;p&gt;Summary of potential community:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Verified data engineers (i.e. no bots, spammers)&lt;/li&gt;\n&lt;li&gt;Networking and in-person events&lt;/li&gt;\n&lt;li&gt;Advanced technical topics and industry news&lt;/li&gt;\n&lt;li&gt;Familiar Reddit-style feed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s not meant to replace this community but it could in theory act as a backup. We don&amp;#39;t have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested, please &lt;a href=\"https://tally.so/r/nGK7pe\"&gt;&lt;strong&gt;join the waitlist here&lt;/strong&gt;&lt;/a&gt;. If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;We are looking for 1-2 new mods to join our team!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; has grown tremendously over the past few years and could use an extra set of hands and ideas. It&amp;#39;s a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it&amp;#39;s a great way to give back to the community. If you&amp;#39;re interested, &lt;a href=\"https://tally.so/r/3xj669\"&gt;please apply here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?auto=webp&amp;v=enabled&amp;s=959740544d8390056f25237218ceb90ba637127a", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48ecdd359f78a24667ad4816581834a9a92ccb16", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d95e85536dc5b64b82d2b4b91889b3b5541d82bf", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4b0af19c61cfc7012ec5db942890f51a3543276", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2e4c6be4163d6167fcfb44cb1248af11df51667", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7be3948c6b699c70515b1b403a4688f2647f5ba7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75c55874f00d7e849f4f0b2e46ccb96488c843a3", "width": 1080, "height": 567}], "variants": {}, "id": "vXOF8G9GBUU_-_vM38jf2S1-5UiTZqBcFWecpk4eHS4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "14dgupv", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/", "subreddit_subscribers": 111190, "created_utc": 1687186533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have recently taken the GCP Professional Data Engineer certificate and have any advice on question topics you remember or websites with great practice questions please leave a comment, TIA!", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good practice question banks for CP Professional Data Engineer Cert? question topics from anyone who has taken it recently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14df68i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687182476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have recently taken the GCP Professional Data Engineer certificate and have any advice on question topics you remember or websites with great practice questions please leave a comment, TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14df68i", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14df68i/good_practice_question_banks_for_cp_professional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14df68i/good_practice_question_banks_for_cp_professional/", "subreddit_subscribers": 111190, "created_utc": 1687182476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am currently trying to parse and store a UDP stream. For my use case, I have very fast arriving data (about 20 million in 1 day). I tried several databases to store parsed data. Main ones are MongoDB, Cassandra and HBASE. My question is \"Why does the same data took considerably more space in disk when I use HBASE?\"\n\nI have test data that consists about 10 minutes of stream. Both MongoDB and HBASE writes about 170k entries. (172,003 to be precise) However, when I check MongoDB the same data takes about 1MB of space in disk, while HBASE takes 140 MBs. So, why is the huge difference?\n\n&amp;#x200B;\n\nThanks in advance :)", "author_fullname": "t2_liz90", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why HBASE takes more space in disk than MongoDB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14de0lk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687179582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am currently trying to parse and store a UDP stream. For my use case, I have very fast arriving data (about 20 million in 1 day). I tried several databases to store parsed data. Main ones are MongoDB, Cassandra and HBASE. My question is &amp;quot;Why does the same data took considerably more space in disk when I use HBASE?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I have test data that consists about 10 minutes of stream. Both MongoDB and HBASE writes about 170k entries. (172,003 to be precise) However, when I check MongoDB the same data takes about 1MB of space in disk, while HBASE takes 140 MBs. So, why is the huge difference?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14de0lk", "is_robot_indexable": true, "report_reasons": null, "author": "theoziyu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14de0lk/why_hbase_takes_more_space_in_disk_than_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14de0lk/why_hbase_takes_more_space_in_disk_than_mongodb/", "subreddit_subscribers": 111190, "created_utc": 1687179582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking Analysis: Snowflake Summit will reveal the future of data apps...here's our take - Wikibon Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_14ctv72", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": "#46d160", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gd5R1FhyhMD14yHItXNhYHiMlU4isIkg7y2Pa6yTXDg.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687119130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wikibon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wikibon.com/breaking-analysis-snowflake-summit-will-reveal-the-future-of-data-apps-heres-our-take/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?auto=webp&amp;v=enabled&amp;s=14a22b899b69c954e6ba0b2574d9c61942d6b06e", "width": 2186, "height": 1460}, "resolutions": [{"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88db018c94827d6de94eefeebf3f32752d431adf", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=663505ffc8c3f746a53a33b56c7556400ad7f584", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9716e6f1a6d3289c3073e444d6a5b33bbf03c492", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94a8376d12c1241b232c848294fdc619ccc4d3c3", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c74bf5c3d9be4dbb30a0698191289709d13ad1fc", "width": 960, "height": 641}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ddcd12bfc17b6cd6c8f383157939e302207aa1f", "width": 1080, "height": 721}], "variants": {}, "id": "QCR_IWoAnrPfuZkd102UPZltv6M_a-V9FqwQSLZwats"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14ctv72", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/14ctv72/breaking_analysis_snowflake_summit_will_reveal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wikibon.com/breaking-analysis-snowflake-summit-will-reveal-the-future-of-data-apps-heres-our-take/", "subreddit_subscribers": 111190, "created_utc": 1687119130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How's the job market like right now? (June 2023)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14dg280", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687184628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14dg280", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14dg280/hows_the_job_market_like_right_now_june_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14dg280/hows_the_job_market_like_right_now_june_2023/", "subreddit_subscribers": 111190, "created_utc": 1687184628.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}