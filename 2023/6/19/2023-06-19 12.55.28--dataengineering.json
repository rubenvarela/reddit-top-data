{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n79165dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stack Overflow Will Charge AI Giants for Training Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ckwyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 172, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 172, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1687096020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wired.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14ckwyq", "is_robot_indexable": true, "report_reasons": null, "author": "wagfrydue", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ckwyq/stack_overflow_will_charge_ai_giants_for_training/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "subreddit_subscribers": 111174, "created_utc": 1687096020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I've aged, Data Engineering has been the last bastion of why I stick around anyway.\n\nSo I ask, which other communities do you guys follow that fosters high quality data engineering discussions?", "author_fullname": "t2_9suj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What other communities do you follow for DE discussion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cs98f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687115127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I&amp;#39;ve aged, Data Engineering has been the last bastion of why I stick around anyway.&lt;/p&gt;\n\n&lt;p&gt;So I ask, which other communities do you guys follow that fosters high quality data engineering discussions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14cs98f", "is_robot_indexable": true, "report_reasons": null, "author": "Drekalo", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/", "subreddit_subscribers": 111174, "created_utc": 1687115127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually learn from YouTube videos but I don't see a reputed playlist on Apache Airflow. Any recommendations from where I can learn the same?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resource to learn Apache Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14cve1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687122872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually learn from YouTube videos but I don&amp;#39;t see a reputed playlist on Apache Airflow. Any recommendations from where I can learn the same?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14cve1u", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14cve1u/best_resource_to_learn_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14cve1u/best_resource_to_learn_apache_airflow/", "subreddit_subscribers": 111174, "created_utc": 1687122872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why should you use external tables ? instead of Datawarehouse", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of External Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ckvut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687095934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why should you use external tables ? instead of Datawarehouse&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14ckvut", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ckvut/benefits_of_external_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ckvut/benefits_of_external_tables/", "subreddit_subscribers": 111174, "created_utc": 1687095934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new and currently working on designing a pipeline to bring data from multiple tables sitting in S3(CDC) to Databricks. \nMy question is should I build multiple data pipelines per table or just have one pipeline that populates each table. Please note that all these tables are sitting in multiple schemas.", "author_fullname": "t2_67a9tfq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d12ae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687138604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new and currently working on designing a pipeline to bring data from multiple tables sitting in S3(CDC) to Databricks. \nMy question is should I build multiple data pipelines per table or just have one pipeline that populates each table. Please note that all these tables are sitting in multiple schemas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d12ae", "is_robot_indexable": true, "report_reasons": null, "author": "TroubleOver1378", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d12ae/data_pipeline_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d12ae/data_pipeline_advice/", "subreddit_subscribers": 111174, "created_utc": 1687138604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, i have  questions related to parquet files and AWS Glue, they're maybe basic questions but i hope someone can help me understand more clearly.\n\nCurrently i'm working on a project that i use a ETL tool to load data from internal databases to AWS S3 in parquet format and then i use AWS Glue to do some transformation by using SparkSQL on these parquet files and write them back to S3 (parquet files as well)\n\n1.How Date and DateTime columns are stored in parquet file ?.\n\nAs i read here [https://github.com/apache/parquet-format/blob/master/LogicalTypes.md](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md) , they are store in Integer formats and these integers represent the number of days (for Date) or number of  milliseconds, microseconds or nanoseconds (for DateTime)  since 1970-01-01. This works as expected with the parquet file that written by our ETL tool from internal database --&gt; S3, all Data/DateTime columns are Integers, means that in Glue Job, i have to convert these Integers back to Date/Datetime value to do some transformation on them. But when parquet files are written by Spark, they are Date/DateTime (or TimeStamp to be more concise) format not Integers (i checked by read these files again into Dataframe and print schema of that dataframe) and that make me confused. Of course before writing the database table (by using ETL tool)  or dataframe (by using Spark) i leave all Date/DateTime columns as Date/DateTime format\n\n&amp;#x200B;\n\n2. Our parquet files are stored in S3 with Date partitions as &lt;base\\_path&gt;/&lt;table\\_name&gt;/&lt;year=yyyy&gt;/&lt;month=mm&gt;/&lt;day=dd&gt;/parquet\\_files . Each Glue Job will write to 1 date partition for 1 table in S3. Is it best practice to limit the number of files before writing to S3 by calling dataframe.repartition() because if i don't do that the number of files in S3 will be unexpectedly many for some Glue Jobs ?\n\n&amp;#x200B;\n\nThank you very much", "author_fullname": "t2_7fyrjq8ff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have question related to Parquet files and AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ctdeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1687120014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687117943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i have  questions related to parquet files and AWS Glue, they&amp;#39;re maybe basic questions but i hope someone can help me understand more clearly.&lt;/p&gt;\n\n&lt;p&gt;Currently i&amp;#39;m working on a project that i use a ETL tool to load data from internal databases to AWS S3 in parquet format and then i use AWS Glue to do some transformation by using SparkSQL on these parquet files and write them back to S3 (parquet files as well)&lt;/p&gt;\n\n&lt;p&gt;1.How Date and DateTime columns are stored in parquet file ?.&lt;/p&gt;\n\n&lt;p&gt;As i read here &lt;a href=\"https://github.com/apache/parquet-format/blob/master/LogicalTypes.md\"&gt;https://github.com/apache/parquet-format/blob/master/LogicalTypes.md&lt;/a&gt; , they are store in Integer formats and these integers represent the number of days (for Date) or number of  milliseconds, microseconds or nanoseconds (for DateTime)  since 1970-01-01. This works as expected with the parquet file that written by our ETL tool from internal database --&amp;gt; S3, all Data/DateTime columns are Integers, means that in Glue Job, i have to convert these Integers back to Date/Datetime value to do some transformation on them. But when parquet files are written by Spark, they are Date/DateTime (or TimeStamp to be more concise) format not Integers (i checked by read these files again into Dataframe and print schema of that dataframe) and that make me confused. Of course before writing the database table (by using ETL tool)  or dataframe (by using Spark) i leave all Date/DateTime columns as Date/DateTime format&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Our parquet files are stored in S3 with Date partitions as &amp;lt;base\\_path&amp;gt;/&amp;lt;table\\_name&amp;gt;/&amp;lt;year=yyyy&amp;gt;/&amp;lt;month=mm&amp;gt;/&amp;lt;day=dd&amp;gt;/parquet_files . Each Glue Job will write to 1 date partition for 1 table in S3. Is it best practice to limit the number of files before writing to S3 by calling dataframe.repartition() because if i don&amp;#39;t do that the number of files in S3 will be unexpectedly many for some Glue Jobs ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?auto=webp&amp;v=enabled&amp;s=c51f45b6e8d1f15543e6991d2157f0454e12bd64", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3093470252c899130e2f27fb7d76cb3aa0b468ce", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a796c7394ac14f2fbd22c352695b4223c19a7bd9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53e88e600c9e3bcacca62d52d489114ec0f4b07f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5631ac15c09241153588e02ad9cc6550235f2e8a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ccba4040a49d102a8d22fe4787f52aede31b390", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/L_noUJsdf1kelYTYwu43nlIkaT6YtUXoul27TImVf9Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3546db03dc04a15ffd4d3ec0af7cda056364520a", "width": 1080, "height": 540}], "variants": {}, "id": "SKNeXyLZaZn_HdQEQLJJaJjNsN0LoHMwnXsUK53ewyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ctdeq", "is_robot_indexable": true, "report_reasons": null, "author": "random_name_362", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ctdeq/i_have_question_related_to_parquet_files_and_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ctdeq/i_have_question_related_to_parquet_files_and_aws/", "subreddit_subscribers": 111174, "created_utc": 1687117943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bonus: for junior/entry level roles with little or no previous experience in the field\n\nI had some reddit comment in my obsidian notes that discussed just that but I can't find it, it was something among the lines of:\n\ncv: needs to show what you can do without being too meaty  \nproject readme: no hr or hiring manager will go through your extensive documentation, you need to get the point across in like 10 seconds.\n\n these seem to be good points in theory, but hard to apply in practice.\n\nSo, tell us the secrets, how do we get ourself considered?\n\nExamples for notable projects/cvs just to get a sense for the structure would be amazing too. ", "author_fullname": "t2_85fin9nj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hiring Managers, how should we structure our cvs and projects readme ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d52mz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687151102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bonus: for junior/entry level roles with little or no previous experience in the field&lt;/p&gt;\n\n&lt;p&gt;I had some reddit comment in my obsidian notes that discussed just that but I can&amp;#39;t find it, it was something among the lines of:&lt;/p&gt;\n\n&lt;p&gt;cv: needs to show what you can do without being too meaty&lt;br/&gt;\nproject readme: no hr or hiring manager will go through your extensive documentation, you need to get the point across in like 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;these seem to be good points in theory, but hard to apply in practice.&lt;/p&gt;\n\n&lt;p&gt;So, tell us the secrets, how do we get ourself considered?&lt;/p&gt;\n\n&lt;p&gt;Examples for notable projects/cvs just to get a sense for the structure would be amazing too. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14d52mz", "is_robot_indexable": true, "report_reasons": null, "author": "DimensionOne9851", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d52mz/hiring_managers_how_should_we_structure_our_cvs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d52mz/hiring_managers_how_should_we_structure_our_cvs/", "subreddit_subscribers": 111174, "created_utc": 1687151102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does transactions in Spark work? \n\nI'd like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn't seem to follow how transactions work in normal RDBMSs. \n\nIt would be great if someone could shed some light on this.", "author_fullname": "t2_vcdigx7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transactions in Spark / Delta lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d9tfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687167266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does transactions in Spark work? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to SELECT and UPDATE in one transaction but getting confused by the documentation. It doesn&amp;#39;t seem to follow how transactions work in normal RDBMSs. &lt;/p&gt;\n\n&lt;p&gt;It would be great if someone could shed some light on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14d9tfc", "is_robot_indexable": true, "report_reasons": null, "author": "loudandclear11", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d9tfc/transactions_in_spark_delta_lake/", "subreddit_subscribers": 111174, "created_utc": 1687167266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Keboola is hosting a webinar this Wednesday together with Snowflake. \n\n&amp;#x200B;\n\nJoin us to discover Snowpark\u2019s power that helps you bridge the gap between data engineers and data scientists. We\u2019re talking about a unified environment, enabled by an end-to-end data platform where these two teams collaborate, combining their expertise to streamline complex data operations.\n\n&amp;#x200B;\n\nReserve your spot: [https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola](https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola)", "author_fullname": "t2_opyjpm1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Webinar: Supercharge Your Data with Snowflake Snowpark &amp; Keboola", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d6yhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1687157400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Keboola is hosting a webinar this Wednesday together with Snowflake. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Join us to discover Snowpark\u2019s power that helps you bridge the gap between data engineers and data scientists. We\u2019re talking about a unified environment, enabled by an end-to-end data platform where these two teams collaborate, combining their expertise to streamline complex data operations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reserve your spot: &lt;a href=\"https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola\"&gt;https://www.keboola.com/webinars/supercharge-your-data-with-snowflake-snowpark-keboola&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?auto=webp&amp;v=enabled&amp;s=e6acb740776cb94fd793469fa13585e02943e3bb", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193414485aef2c3437d97175f5bfd80ad0e5cb32", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7aeda51779888a5aa671649c876888023df7a81", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YvGzBZyC0udR0DXZQ9W5elrmLHCcZooRv4qKKwpxaeg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c8bfc11425dc55368dc4941f9c184756d8097cc", "width": 320, "height": 320}], "variants": {}, "id": "J7yfwyLNBmr8hnc17qkzr-hk8MGj6eeLMDAMKBhlHio"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14d6yhf", "is_robot_indexable": true, "report_reasons": null, "author": "CalleKeboola", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d6yhf/webinar_supercharge_your_data_with_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d6yhf/webinar_supercharge_your_data_with_snowflake/", "subreddit_subscribers": 111174, "created_utc": 1687157400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are renewing our very much outdated systems. Now the debate has started if we should use push or pull architecture. Simplified there are multiple source system and a \"data warehouse\" where data should land for searching, analysis and reporting.\n\nA certain delay from data creation to search availability is acceptable, say 30 min. Also data volumes are tiny! This is strictly intranet system. There are also consideration to be made about the state of the companies IT policies and IT capabilities (complex and very limited as in can't deploy a docker container, sic!).\n\n\"chief architect\" (external!) insists on going push with very complex setup. Sources should push to Kafka where sink picks up the data. I was wondering what happens when kakfa is down, all source systems should fail to ensure data consistency. It was said no, there will be a local cache in redis to prevent that. \n\nAll overly complex in my opinion.\n\nif the system itself \"pushed\" a change to redis, then there needs to be a background service that pulls it from redis and sends it to kafka. So it's just a \"hidden\" pull system right?\nIt also means we need a kafka (or insert similar tech here) setup and redis on all systems which your IT or better said external service provider is clueless about.\n\nI know its kind of boring but a \"pull passed system\" using airflow or similar tool that just pulls on last modified date or similar metadata, does processing and pushes it to sink seems to be a lot simpler. Data amounts are small, a delay is acceptable and the source system can be oblivious to what happens with the data. Seems to much simpler. \n\nWhat am I missing? How can I convince the team this is the preferred approach?", "author_fullname": "t2_p8sy28ma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "for a complete new development (including source systems and \"data warehouse) would you use push or pull architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14d6ijt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687155854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are renewing our very much outdated systems. Now the debate has started if we should use push or pull architecture. Simplified there are multiple source system and a &amp;quot;data warehouse&amp;quot; where data should land for searching, analysis and reporting.&lt;/p&gt;\n\n&lt;p&gt;A certain delay from data creation to search availability is acceptable, say 30 min. Also data volumes are tiny! This is strictly intranet system. There are also consideration to be made about the state of the companies IT policies and IT capabilities (complex and very limited as in can&amp;#39;t deploy a docker container, sic!).&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;chief architect&amp;quot; (external!) insists on going push with very complex setup. Sources should push to Kafka where sink picks up the data. I was wondering what happens when kakfa is down, all source systems should fail to ensure data consistency. It was said no, there will be a local cache in redis to prevent that. &lt;/p&gt;\n\n&lt;p&gt;All overly complex in my opinion.&lt;/p&gt;\n\n&lt;p&gt;if the system itself &amp;quot;pushed&amp;quot; a change to redis, then there needs to be a background service that pulls it from redis and sends it to kafka. So it&amp;#39;s just a &amp;quot;hidden&amp;quot; pull system right?\nIt also means we need a kafka (or insert similar tech here) setup and redis on all systems which your IT or better said external service provider is clueless about.&lt;/p&gt;\n\n&lt;p&gt;I know its kind of boring but a &amp;quot;pull passed system&amp;quot; using airflow or similar tool that just pulls on last modified date or similar metadata, does processing and pushes it to sink seems to be a lot simpler. Data amounts are small, a delay is acceptable and the source system can be oblivious to what happens with the data. Seems to much simpler. &lt;/p&gt;\n\n&lt;p&gt;What am I missing? How can I convince the team this is the preferred approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14d6ijt", "is_robot_indexable": true, "report_reasons": null, "author": "RationalDialog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14d6ijt/for_a_complete_new_development_including_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14d6ijt/for_a_complete_new_development_including_source/", "subreddit_subscribers": 111174, "created_utc": 1687155854.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking Analysis: Snowflake Summit will reveal the future of data apps...here's our take - Wikibon Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_14ctv72", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": "#46d160", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gd5R1FhyhMD14yHItXNhYHiMlU4isIkg7y2Pa6yTXDg.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1687119130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wikibon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://wikibon.com/breaking-analysis-snowflake-summit-will-reveal-the-future-of-data-apps-heres-our-take/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?auto=webp&amp;v=enabled&amp;s=14a22b899b69c954e6ba0b2574d9c61942d6b06e", "width": 2186, "height": 1460}, "resolutions": [{"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88db018c94827d6de94eefeebf3f32752d431adf", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=663505ffc8c3f746a53a33b56c7556400ad7f584", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9716e6f1a6d3289c3073e444d6a5b33bbf03c492", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94a8376d12c1241b232c848294fdc619ccc4d3c3", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c74bf5c3d9be4dbb30a0698191289709d13ad1fc", "width": 960, "height": 641}, {"url": "https://external-preview.redd.it/2Nq45hz2by2R0Fo5nKwTc-OiwHmZVmIitjMccvy0Rtg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ddcd12bfc17b6cd6c8f383157939e302207aa1f", "width": 1080, "height": 721}], "variants": {}, "id": "QCR_IWoAnrPfuZkd102UPZltv6M_a-V9FqwQSLZwats"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14ctv72", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/14ctv72/breaking_analysis_snowflake_summit_will_reveal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wikibon.com/breaking-analysis-snowflake-summit-will-reveal-the-future-of-data-apps-heres-our-take/", "subreddit_subscribers": 111174, "created_utc": 1687119130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1 year studying data with great professional experience in Electrical Engineering and Project Management. Started studying for Data Analysis and now Data Engineering, it's so much more fun! I've been aplying for DE roles but had no return, now I'm applying for DA and getting nice returns. I've heard people saying that indeed it's better to start at more entry level jobs such as DA and then go to DE. These careers do have some similarities but it's just SO MUCH to study. Getting back to statistics, machine learning and dashboards is starting to become a burden for someone that has focused so much on cloud (even got my AWS Solutions Architect associate certificate), Apache suite and more.\n\nWhat should I do?\n\n1. Keep grinding for DE with some knowledge in DA.\n2. Push for DA now to get a former professional experience to then grind for DE", "author_fullname": "t2_6c3dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning carrers: Should I start as Data Analyst then Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14db8pb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1687171818.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1 year studying data with great professional experience in Electrical Engineering and Project Management. Started studying for Data Analysis and now Data Engineering, it&amp;#39;s so much more fun! I&amp;#39;ve been aplying for DE roles but had no return, now I&amp;#39;m applying for DA and getting nice returns. I&amp;#39;ve heard people saying that indeed it&amp;#39;s better to start at more entry level jobs such as DA and then go to DE. These careers do have some similarities but it&amp;#39;s just SO MUCH to study. Getting back to statistics, machine learning and dashboards is starting to become a burden for someone that has focused so much on cloud (even got my AWS Solutions Architect associate certificate), Apache suite and more.&lt;/p&gt;\n\n&lt;p&gt;What should I do?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Keep grinding for DE with some knowledge in DA.&lt;/li&gt;\n&lt;li&gt;Push for DA now to get a former professional experience to then grind for DE&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14db8pb", "is_robot_indexable": true, "report_reasons": null, "author": "leolmx", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14db8pb/transitioning_carrers_should_i_start_as_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14db8pb/transitioning_carrers_should_i_start_as_data/", "subreddit_subscribers": 111174, "created_utc": 1687171818.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}