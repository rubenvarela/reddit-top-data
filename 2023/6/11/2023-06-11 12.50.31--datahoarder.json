{"kind": "Listing", "data": {"after": "t3_146jleb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Welcome to the [Post-API](https://socialmediaandpolitics.org/53-digital-methods-post-api-era-deen-freelon/) dystopia! So unless you have been living under a rock, Reddit has decided to begin pay-tiering its API following the footsteps of Facebook, Google and very recently Twitter. And people are [MAD](https://9to5mac.com/2023/05/31/reddit-may-force-apollo-and-third-party-clients-to-shut-down)!\n\nGiven that here at Reddit we are a more tech-competent audience, protest has been very _interesting_. We have seen Subreddit black-outs, user mass-deletions.. I think the funniest suggestion I heard came from u/IkePAnderson who suggested overwriting posts with gibberish instead.\n\nExcept there's a problem: I think this general attitude will not only fail to bring change, it will give the company exactly what it wants. I mean, is there any form of dissent better than self-destruction? All the complaints being filed and the rage and vitriol are cleaning after themselves. Once the new pay-tiers come into effect, the evidence of people not welcoming the change will vanish as has already happened in the case of Facebook and Twitter whose API changes failed to attract much attention from the press.\n\nReddit, for better or worse, is a company that derives its revenue from band-waggoning trends. The top subreddits on this site include r/funny , r/AskReddit , r/worldnews ; things that capture the here and now and are not so much concerned with posteriority. Might I remind you that just until a few months ago, threads older than 6 months would be locked not allowing further edits or comments. Reddit's revenue stream does not benefit from retaining history beyond a certain point and is only retained as a gesture for brand-loyalty. So if everyone who now despises Reddit removes their history, that's okay, those who are indifferent will get to keep the same benefits and it won't cost Reddit any more or less.\n\nI'm saying all of this to make a point that mass-deletion only hurts individuals. It hurts you, it hurts me; it hurts the dissent towards Reddit because the community becomes invisible.. Your content is yours. It's not property of Reddit. And therefore, if you so wish, you can move it to another platform. As a dissenter of the API overhaul, I think it is in our interest to do so.\n\nThe fact that our content is portable in this way is a thing that scares companies, because it is dangerous. Just look at YouTube and Twitch to see how they force their big streamers into exclusivity contracts. I might be u/themadprogramer on Reddit, and my words might be attributed to that name. But I can also exist as @madpro on other platforms; whether on YouTube or Discord, or something fediversy like Mastodon or Pleroma.\n\nSo I believe the best way we can petition our redress is not through mass-deletion, but rather mass-action. You're a data hoarder, just download a bulk of your comments and post to a blog. If you're not camera shy record yourself talking about the API changes and why you left Reddit and put it on YouTube or TikTok. Do you want to know the best part? Reddit can't do anything about it, even the skeptics who have suggested the possibility of the company to revert changes must concede that the company cannot suppress what is happening outside of their platform.\n\nIf nothing else, I just think it's good practice to cross-post because redundancy means retention. Every one of us has a personal history and that is **personal** not **Redditorial**. That personal history is split across mediums, as it should be, because we move in the world. Reddit is merely the context, it is neither the object nor subject.\n\nThe best form of protest **can only be** reclaiming our content instead of destroying it!", "author_fullname": "t2_2emtczf", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Your content belongs to you, not Reddit: A thread.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_145yk1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1475, "total_awards_received": 2, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1475, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1686398898.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686398692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to the &lt;a href=\"https://socialmediaandpolitics.org/53-digital-methods-post-api-era-deen-freelon/\"&gt;Post-API&lt;/a&gt; dystopia! So unless you have been living under a rock, Reddit has decided to begin pay-tiering its API following the footsteps of Facebook, Google and very recently Twitter. And people are &lt;a href=\"https://9to5mac.com/2023/05/31/reddit-may-force-apollo-and-third-party-clients-to-shut-down\"&gt;MAD&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;Given that here at Reddit we are a more tech-competent audience, protest has been very &lt;em&gt;interesting&lt;/em&gt;. We have seen Subreddit black-outs, user mass-deletions.. I think the funniest suggestion I heard came from &lt;a href=\"/u/IkePAnderson\"&gt;u/IkePAnderson&lt;/a&gt; who suggested overwriting posts with gibberish instead.&lt;/p&gt;\n\n&lt;p&gt;Except there&amp;#39;s a problem: I think this general attitude will not only fail to bring change, it will give the company exactly what it wants. I mean, is there any form of dissent better than self-destruction? All the complaints being filed and the rage and vitriol are cleaning after themselves. Once the new pay-tiers come into effect, the evidence of people not welcoming the change will vanish as has already happened in the case of Facebook and Twitter whose API changes failed to attract much attention from the press.&lt;/p&gt;\n\n&lt;p&gt;Reddit, for better or worse, is a company that derives its revenue from band-waggoning trends. The top subreddits on this site include &lt;a href=\"/r/funny\"&gt;r/funny&lt;/a&gt; , &lt;a href=\"/r/AskReddit\"&gt;r/AskReddit&lt;/a&gt; , &lt;a href=\"/r/worldnews\"&gt;r/worldnews&lt;/a&gt; ; things that capture the here and now and are not so much concerned with posteriority. Might I remind you that just until a few months ago, threads older than 6 months would be locked not allowing further edits or comments. Reddit&amp;#39;s revenue stream does not benefit from retaining history beyond a certain point and is only retained as a gesture for brand-loyalty. So if everyone who now despises Reddit removes their history, that&amp;#39;s okay, those who are indifferent will get to keep the same benefits and it won&amp;#39;t cost Reddit any more or less.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m saying all of this to make a point that mass-deletion only hurts individuals. It hurts you, it hurts me; it hurts the dissent towards Reddit because the community becomes invisible.. Your content is yours. It&amp;#39;s not property of Reddit. And therefore, if you so wish, you can move it to another platform. As a dissenter of the API overhaul, I think it is in our interest to do so.&lt;/p&gt;\n\n&lt;p&gt;The fact that our content is portable in this way is a thing that scares companies, because it is dangerous. Just look at YouTube and Twitch to see how they force their big streamers into exclusivity contracts. I might be &lt;a href=\"/u/themadprogramer\"&gt;u/themadprogramer&lt;/a&gt; on Reddit, and my words might be attributed to that name. But I can also exist as @madpro on other platforms; whether on YouTube or Discord, or something fediversy like Mastodon or Pleroma.&lt;/p&gt;\n\n&lt;p&gt;So I believe the best way we can petition our redress is not through mass-deletion, but rather mass-action. You&amp;#39;re a data hoarder, just download a bulk of your comments and post to a blog. If you&amp;#39;re not camera shy record yourself talking about the API changes and why you left Reddit and put it on YouTube or TikTok. Do you want to know the best part? Reddit can&amp;#39;t do anything about it, even the skeptics who have suggested the possibility of the company to revert changes must concede that the company cannot suppress what is happening outside of their platform.&lt;/p&gt;\n\n&lt;p&gt;If nothing else, I just think it&amp;#39;s good practice to cross-post because redundancy means retention. Every one of us has a personal history and that is &lt;strong&gt;personal&lt;/strong&gt; not &lt;strong&gt;Redditorial&lt;/strong&gt;. That personal history is split across mediums, as it should be, because we move in the world. Reddit is merely the context, it is neither the object nor subject.&lt;/p&gt;\n\n&lt;p&gt;The best form of protest &lt;strong&gt;can only be&lt;/strong&gt; reclaiming our content instead of destroying it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VNVhQOFUs7gWgbbJT6R07bG5IrtEeaVap37OBzAGJkw.jpg?auto=webp&amp;v=enabled&amp;s=da698b3fd0dd383d43fa7d818b7d8b3eef7d7ebc", "width": 473, "height": 491}, "resolutions": [{"url": "https://external-preview.redd.it/VNVhQOFUs7gWgbbJT6R07bG5IrtEeaVap37OBzAGJkw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3924714adb5aa47cf5b609b73b6ec57388783c2", "width": 108, "height": 112}, {"url": "https://external-preview.redd.it/VNVhQOFUs7gWgbbJT6R07bG5IrtEeaVap37OBzAGJkw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1a3ada896229de76cb50a03b5fe704f7030789a", "width": 216, "height": 224}, {"url": "https://external-preview.redd.it/VNVhQOFUs7gWgbbJT6R07bG5IrtEeaVap37OBzAGJkw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb652a8fbff90893830ba839ba8f5e3ce099f66b", "width": 320, "height": 332}], "variants": {}, "id": "nzq6ngQG_UpfBblkPQ2t17vLE9PqxwVVyolQxazK8rk"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_b4ff447e-05a5-42dc-9002-63568807cfe6", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A glowing commendation for all to see", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "All-Seeing Upvote", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=25880d00e4283ff6a3851b64c83fea465c3fac48", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=67fae0c2af26488b9d70cb7afe877086707cf1d1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=3033c6583809a27b998883b7c56c158102cb0420", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=cccb48d7518eaba72894a7ac4214bb70c7120793", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=4101e0eff424bbd818195853387db358bec74ed0", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/am40b8b08l581_All-SeeingUpvote2.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "145yk1y", "is_robot_indexable": true, "report_reasons": null, "author": "themadprogramer", "discussion_type": null, "num_comments": 154, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/145yk1y/your_content_belongs_to_you_not_reddit_a_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/145yk1y/your_content_belongs_to_you_not_reddit_a_thread/", "subreddit_subscribers": 687274, "created_utc": 1686398692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_63o77fvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Czkawka 6.0 - File cleaner, now finds similar audio files by content, files by size and name and fix and speedup similar images search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_146nmeh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 199, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/jqa60612ec5b1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jqa60612ec5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/jqa60612ec5b1/DASHPlaylist.mpd?a=1689079831%2CYmExM2Y2NDBjMTI1YjI5Zjk0N2Y4YzBlODAzZWI5Njc0MGFlNjA4MzU5OTQwNjBlNGE2ODNmYzQwYzBhOWVjYQ%3D%3D&amp;v=1&amp;f=sd", "duration": 158, "hls_url": "https://v.redd.it/jqa60612ec5b1/HLSPlaylist.m3u8?a=1689079831%2CZjI1ZDk0NzgxZDVhNDAzNTBhOGI2NzIwMDJmODA4OTk1ZDBjNzJkYjRiMzUzMjJiNDE0ZDJhY2ZlOGJkMWQ5Yw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 199, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YffpmEmamU3bVQhoCxDNiQoWL69enwqyhjR7iugRr1A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686468974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/jqa60612ec5b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ff9f20bb6c013191dd6f44ee5792537685fe6f4b", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0c744882dc338b5c5fbcb402edcebdf67e4a670f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=180c425dfb145e0f6457f6ae610f360b86158e83", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=84d9c8eaa1077d43e24fbdf61c28d1f03d1a8ddd", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=362d6c8c4197b79e42f26cb076b614d15f9666e5", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b095a49eb39b899a9ddbbbff698ec0b3255f4c3a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ba9808773e1886acbc0446dc9bdf4ee65703f1b8", "width": 1080, "height": 607}], "variants": {}, "id": "__Kb4xeAQKPSI4h5sy9Y2K2G339SeeDekM62Clq58cU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146nmeh", "is_robot_indexable": true, "report_reasons": null, "author": "krutkrutrar", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146nmeh/czkawka_60_file_cleaner_now_finds_similar_audio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/jqa60612ec5b1", "subreddit_subscribers": 687274, "created_utc": 1686468974.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/jqa60612ec5b1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jqa60612ec5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/jqa60612ec5b1/DASHPlaylist.mpd?a=1689079831%2CYmExM2Y2NDBjMTI1YjI5Zjk0N2Y4YzBlODAzZWI5Njc0MGFlNjA4MzU5OTQwNjBlNGE2ODNmYzQwYzBhOWVjYQ%3D%3D&amp;v=1&amp;f=sd", "duration": 158, "hls_url": "https://v.redd.it/jqa60612ec5b1/HLSPlaylist.m3u8?a=1689079831%2CZjI1ZDk0NzgxZDVhNDAzNTBhOGI2NzIwMDJmODA4OTk1ZDBjNzJkYjRiMzUzMjJiNDE0ZDJhY2ZlOGJkMWQ5Yw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "50 CDs I purchased today to rip sort and store", "author_fullname": "t2_rvu33ehg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a new 50 pack from V Stock today. Gonna have some fun with this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_146dg3k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6xCDl4TQ7uOpgYSxwrL-axDxm-ISrQc0xvK0zNry_-I.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686437056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;50 CDs I purchased today to rip sort and store&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/a2k960t6r95b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?auto=webp&amp;v=enabled&amp;s=4ce2ae70b296901519c8125f8abf7504dfca76e0", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8cc0485e89a7f876e007a5aa705ec0fb4b4a5201", "width": 108, "height": 81}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df77a220510e21bb6b382453a0d6d5eb9c582fa6", "width": 216, "height": 162}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88a0e76ffdad4e2291b5c8781ad8196bf7b83970", "width": 320, "height": 240}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da2820eb163c0b4111bcf24d4c49f697e983947a", "width": 640, "height": 480}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8f9f21f93381229f22f54a185c1a614efc33e38", "width": 960, "height": 720}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95c2bcc85e2ab8fa8cf84f22d32fcb65b4e32c12", "width": 1080, "height": 810}], "variants": {}, "id": "7oMa0YivGPYmtGzpuiVhPYUeNzIKZeFa4Xt4VXdzeFo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "856GB and counting", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146dg3k", "is_robot_indexable": true, "report_reasons": null, "author": "TheMagicFolf331", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/146dg3k/got_a_new_50_pack_from_v_stock_today_gonna_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/a2k960t6r95b1.jpg", "subreddit_subscribers": 687274, "created_utc": 1686437056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Though I haven't been on reddit for as long as many of you this was one of my favorite communities. It's a bummer how things are looking with the recent events with reddit leadership. \n\nWould people be interested a mastodon server as a potential replacement? I know there's the discord, I'm on there. But I'd much prefer to have our own space operated on our own hardware (aka rented by us) by people we trust rather than some company. \n\nThoughts?", "author_fullname": "t2_fsm385o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interest in DH Mastodon instance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1463zn5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686413137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Though I haven&amp;#39;t been on reddit for as long as many of you this was one of my favorite communities. It&amp;#39;s a bummer how things are looking with the recent events with reddit leadership. &lt;/p&gt;\n\n&lt;p&gt;Would people be interested a mastodon server as a potential replacement? I know there&amp;#39;s the discord, I&amp;#39;m on there. But I&amp;#39;d much prefer to have our own space operated on our own hardware (aka rented by us) by people we trust rather than some company. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "32TB + Cloud", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1463zn5", "is_robot_indexable": true, "report_reasons": null, "author": "Specktr", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1463zn5/interest_in_dh_mastodon_instance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1463zn5/interest_in_dh_mastodon_instance/", "subreddit_subscribers": 687274, "created_utc": 1686413137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " **TL;DR**\n\n**I've asked and been given hard drive manufacturing answers by someone within the industry.**\n\n**I can't and won't ask or disclose anything that may lead to this person's identity, employer or position.**\n\n**I have no agenda in sharing the following other than keeping with my general goal here of helping and learning from others, as shown by the majority of my posts.**\n\nBegin Q&amp;A\n\nI've been very fortunate to have been contacted by someone within the hard drive manufacturing industry because of some of my posts. I can't disclose whom this person is, but from my layman's understanding of additional undisclosable detailed information, I'm fully confident this person is whom/what they claim to be.\n\nI was able to ask some questions and was given some very interesting info that confirms what I've posted and others have speculated about.\n\nI hope we'll be able to ask more questions in the future, but understand that both I and my source may refuse to ask or answer any question publicly for confidentiality reasons.\n\nQ: A CMR drive be changed to DM-SMR or only HM or HA-SMR? There's a conspiracy theory that the manufacturers may try to submarine SMR into their drives in the future. IMO, it would be market suicide!\n\n*A: In general, SMR drives use the same hardware (heads and platters) as CMR drives. SMR just has the tracks closer so you get more capacity. DM-SMR is the consumer level version that was created to reduce manufacturing cost on lowest capacity drives. It was predicted that SSD was going to take over low capacity HDD\u2019s years ago. The goal of DM-SMR is to reduce manufacturing costs while maintaining acceptable performance in intended applications (light duty consumer applications).*\n\n*On the opposite end of the market, for Cloud, they want as much capacity as possible, so that\u2019s why we use SMR there as well. They demand consistent performance though, for their customers in turn. So HM-SMR is sold to that market. You get the benefit of the closer tracks (so more physical tracks on the same platter) at the cost of writing a whole 256 MiB zone at once, because every 256 MiB there\u2019s a \u201cgap\u201d in the SMR tracks. HM-SMR requires a file system and storage driver that are aware of the HM-SMR rules. If there\u2019s data already in one of those SMR zones, you can\u2019t just write in the middle of one of those zones, without resetting that zone first. Otherwise it would be way too easy to overwrite data in that zone. The HDD firmware keeps track of where we are allowed to write within zones and on HM-SMR drives, you can ask the drive for those values, called \u201cwrite pointers\u201d. HA-SMR isn\u2019t popular anymore and I don\u2019t know if anyone is still making those. Hybrid SMR is also out there in the world, where you can convert any individual 256 MiB zones between SMR and CMR. Those require special kernel and HBA/controller firmware and OS and file systems to work.*\n\n*Also, for HM-SMR, there\u2019s a beta version of BTRFS that mostly works. Might be worth mentioning. You can Google \u201cbtrfs hmsmr\u201d for tutorials. I probably wouldn\u2019t use it for production data but if you end up with one of those HM-SMR drives, it works well enough for Chia or something.*\n\nMy notes: I posted about HM-SMR in this thread: [https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets\\_discuss\\_dmsmr\\_hmsmr\\_hasmr\\_and\\_dropbox/](https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets_discuss_dmsmr_hmsmr_hasmr_and_dropbox/)\n\nQ: Is it true that the drives in externals can be: overstock, overruns, binned (out of spec drives), from cancelled orders.\n\n*A: Yes to all of it. Externals are the lowest bins above the \\[redated\\] (Edit: binned rives} we sell to third parties. It\u2019s whatever is leftover. They have less warranty because they aren\u2019t expected to last as long.*\n\nMy notes: The first part is supported by what I posted in this thread, [https://www.reddit.com/r/DataHoarder/comments/11jmot5/to\\_those\\_asking\\_what\\_drive\\_is\\_inside\\_my\\_wd/](https://www.reddit.com/r/DataHoarder/comments/11jmot5/to_those_asking_what_drive_is_inside_my_wd/) which has a link to WD's disclosure about this.\n\nIt's been confirmed by another source that the binned drives, are drives that are Out Of Spec, flashed with special firmware that can't be updated and is no longer supported by the manufacturer. This is source of SOME of the unbranded drives from certain resellers.\n\nQ: Is it true that in a given generation of HDD, when reduced capacities are released at the same time, you can sometimes tell from the model number that it\u2019s the same hardware inside as a full capacity drive\u201d To be used in externals or sold to resellers?\n\n*A: Yes, see above. The \\[redated\\] (My edit: XX drive size) were reconfigured for 12 and 14TB. The \\[redacted\\] went all the way down to 10TB to my knowledge. We just disable specific bad heads in the factory and rewrite the tracks. It\u2019s an automated process obviously, but we can internally look up all that history on any serial number.*\n\nQ: Is it true that some or all drives of a particular size come from the same hardware line and only the firmware determines which line, e.g. Home, Survelliance, NAS, Datacenter, Enterprise it's labeled as. Or are there separate lines for each type?\n\n*A: Yes, kind of. In the spirit of the question, yes, especially in recent years. The firmware is different for those markets, and specific hardware might be binned for those markets too. For example, surveillance firmware is tuned to be performing writes for the vast majority of the time and to handle data streaming better, as in from cameras. Some of the caching and performance features are disabled, and the reliability features are tweaked to be less likely to interrupt that incoming data stream.*\n\nMy note: To my layman's understanding, this doesn't mean that ALL drives of a given size are the same. Just that SOME drives of a given size MAY be from the same line.\n\nAlso note that \"binned\" as used in the answer doesn't necessarily mean lower or better performing or drive specs. Just different for different uses.\n\nQ: Is it true that drives that don't meet the full specs can be binned and their firmware permanently changed to a lower spec drive, then sold to resellers?\n\n*A: Yes. The firmware doesn\u2019t change the drive to a lower quality drive, it is the manufacturing imperfections. Head fly height is less than 5 nanometers, but if any of those heads touch the disk, the head suffers damage. And there are a lot of heads in high capacity hard drives now. Temperature and humidity affect the aerodynamics of the heads at that level so that is adjusted constantly within the firmware. Usually it\u2019s 20 heads per drive, and even if we bin the parts before assembly, sometimes we figure out in the factory that one or more heads can\u2019t meet the reliability requirements, so we permanently disable them in the firmware.*", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Information about CMR to SMR, Manufacturer externals and Binned drives from a confidential source.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146hb9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686447909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;ve asked and been given hard drive manufacturing answers by someone within the industry.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I can&amp;#39;t and won&amp;#39;t ask or disclose anything that may lead to this person&amp;#39;s identity, employer or position.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have no agenda in sharing the following other than keeping with my general goal here of helping and learning from others, as shown by the majority of my posts.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Begin Q&amp;amp;A&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been very fortunate to have been contacted by someone within the hard drive manufacturing industry because of some of my posts. I can&amp;#39;t disclose whom this person is, but from my layman&amp;#39;s understanding of additional undisclosable detailed information, I&amp;#39;m fully confident this person is whom/what they claim to be.&lt;/p&gt;\n\n&lt;p&gt;I was able to ask some questions and was given some very interesting info that confirms what I&amp;#39;ve posted and others have speculated about.&lt;/p&gt;\n\n&lt;p&gt;I hope we&amp;#39;ll be able to ask more questions in the future, but understand that both I and my source may refuse to ask or answer any question publicly for confidentiality reasons.&lt;/p&gt;\n\n&lt;p&gt;Q: A CMR drive be changed to DM-SMR or only HM or HA-SMR? There&amp;#39;s a conspiracy theory that the manufacturers may try to submarine SMR into their drives in the future. IMO, it would be market suicide!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: In general, SMR drives use the same hardware (heads and platters) as CMR drives. SMR just has the tracks closer so you get more capacity. DM-SMR is the consumer level version that was created to reduce manufacturing cost on lowest capacity drives. It was predicted that SSD was going to take over low capacity HDD\u2019s years ago. The goal of DM-SMR is to reduce manufacturing costs while maintaining acceptable performance in intended applications (light duty consumer applications).&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;On the opposite end of the market, for Cloud, they want as much capacity as possible, so that\u2019s why we use SMR there as well. They demand consistent performance though, for their customers in turn. So HM-SMR is sold to that market. You get the benefit of the closer tracks (so more physical tracks on the same platter) at the cost of writing a whole 256 MiB zone at once, because every 256 MiB there\u2019s a \u201cgap\u201d in the SMR tracks. HM-SMR requires a file system and storage driver that are aware of the HM-SMR rules. If there\u2019s data already in one of those SMR zones, you can\u2019t just write in the middle of one of those zones, without resetting that zone first. Otherwise it would be way too easy to overwrite data in that zone. The HDD firmware keeps track of where we are allowed to write within zones and on HM-SMR drives, you can ask the drive for those values, called \u201cwrite pointers\u201d. HA-SMR isn\u2019t popular anymore and I don\u2019t know if anyone is still making those. Hybrid SMR is also out there in the world, where you can convert any individual 256 MiB zones between SMR and CMR. Those require special kernel and HBA/controller firmware and OS and file systems to work.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Also, for HM-SMR, there\u2019s a beta version of BTRFS that mostly works. Might be worth mentioning. You can Google \u201cbtrfs hmsmr\u201d for tutorials. I probably wouldn\u2019t use it for production data but if you end up with one of those HM-SMR drives, it works well enough for Chia or something.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;My notes: I posted about HM-SMR in this thread: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets_discuss_dmsmr_hmsmr_hasmr_and_dropbox/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets_discuss_dmsmr_hmsmr_hasmr_and_dropbox/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that the drives in externals can be: overstock, overruns, binned (out of spec drives), from cancelled orders.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes to all of it. Externals are the lowest bins above the [redated] (Edit: binned rives} we sell to third parties. It\u2019s whatever is leftover. They have less warranty because they aren\u2019t expected to last as long.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;My notes: The first part is supported by what I posted in this thread, &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/11jmot5/to_those_asking_what_drive_is_inside_my_wd/\"&gt;https://www.reddit.com/r/DataHoarder/comments/11jmot5/to_those_asking_what_drive_is_inside_my_wd/&lt;/a&gt; which has a link to WD&amp;#39;s disclosure about this.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been confirmed by another source that the binned drives, are drives that are Out Of Spec, flashed with special firmware that can&amp;#39;t be updated and is no longer supported by the manufacturer. This is source of SOME of the unbranded drives from certain resellers.&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that in a given generation of HDD, when reduced capacities are released at the same time, you can sometimes tell from the model number that it\u2019s the same hardware inside as a full capacity drive\u201d To be used in externals or sold to resellers?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes, see above. The [redated] (My edit: XX drive size) were reconfigured for 12 and 14TB. The [redacted] went all the way down to 10TB to my knowledge. We just disable specific bad heads in the factory and rewrite the tracks. It\u2019s an automated process obviously, but we can internally look up all that history on any serial number.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that some or all drives of a particular size come from the same hardware line and only the firmware determines which line, e.g. Home, Survelliance, NAS, Datacenter, Enterprise it&amp;#39;s labeled as. Or are there separate lines for each type?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes, kind of. In the spirit of the question, yes, especially in recent years. The firmware is different for those markets, and specific hardware might be binned for those markets too. For example, surveillance firmware is tuned to be performing writes for the vast majority of the time and to handle data streaming better, as in from cameras. Some of the caching and performance features are disabled, and the reliability features are tweaked to be less likely to interrupt that incoming data stream.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;My note: To my layman&amp;#39;s understanding, this doesn&amp;#39;t mean that ALL drives of a given size are the same. Just that SOME drives of a given size MAY be from the same line.&lt;/p&gt;\n\n&lt;p&gt;Also note that &amp;quot;binned&amp;quot; as used in the answer doesn&amp;#39;t necessarily mean lower or better performing or drive specs. Just different for different uses.&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that drives that don&amp;#39;t meet the full specs can be binned and their firmware permanently changed to a lower spec drive, then sold to resellers?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes. The firmware doesn\u2019t change the drive to a lower quality drive, it is the manufacturing imperfections. Head fly height is less than 5 nanometers, but if any of those heads touch the disk, the head suffers damage. And there are a lot of heads in high capacity hard drives now. Temperature and humidity affect the aerodynamics of the heads at that level so that is adjusted constantly within the firmware. Usually it\u2019s 20 heads per drive, and even if we bin the parts before assembly, sometimes we figure out in the factory that one or more heads can\u2019t meet the reliability requirements, so we permanently disable them in the firmware.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146hb9k", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146hb9k/information_about_cmr_to_smr_manufacturer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146hb9k/information_about_cmr_to_smr_manufacturer/", "subreddit_subscribers": 687274, "created_utc": 1686447909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I mean is something that I can throw, say, \"hgst datahoarder reddit\" and get a bunch of results from this sub on reddit about hgst.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to search the wayback machine like google?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1462okw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686409828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I mean is something that I can throw, say, &amp;quot;hgst datahoarder reddit&amp;quot; and get a bunch of results from this sub on reddit about hgst.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "76/108 TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1462okw", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1462okw/is_there_a_way_to_search_the_wayback_machine_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1462okw/is_there_a_way_to_search_the_wayback_machine_like/", "subreddit_subscribers": 687274, "created_utc": 1686409828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know there is a 1000 post limit on the reddit API, but could I get a list of IDs of every post in a subreddit then download the posts by IDs using the API?  I guess I would need to scrape reddit to do this. Does such a tool exist? Thank you.", "author_fullname": "t2_jzkm8as0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Completely archiving subreddits not on pushshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146dazo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686436685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there is a 1000 post limit on the reddit API, but could I get a list of IDs of every post in a subreddit then download the posts by IDs using the API?  I guess I would need to scrape reddit to do this. Does such a tool exist? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146dazo", "is_robot_indexable": true, "report_reasons": null, "author": "itszedfsnotzeefs", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146dazo/completely_archiving_subreddits_not_on_pushshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146dazo/completely_archiving_subreddits_not_on_pushshift/", "subreddit_subscribers": 687274, "created_utc": 1686436685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a list of about 10,000 URLs of various Reddit comment threads. I would like to archive the contents locally. These are not linked to any particular Reddit account or subreddit.\n\nIs there a way to automatically download the entire comment threads (including the original post / link) from each of those URLs?\n\nI'm a barely-computer-literate Windows user. I don't know how to use CLIs, Python scripts, Docker images, or web servers, but I can follow a step-by-step dummy's guide. I can install Linux Mint or Raspbian on a fresh machine if needed.\n\nI've seen this tool by u/Ailothaen: https://github.com/Ailothaen/RedditArchiver\nThis looks like it would generate the output in a format that I need. However, from the screenshots it appears to work on one URL at a time.\n\nSome of the posts I want are not on archive.org in their entirety. I'm aware of the ArchiveTeam project, but I don't know how to access their data, or pull a particular post using an URL.\n\nI'd really appreciate any help, or links to resources. Thank you!", "author_fullname": "t2_gq40u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass-archiving Reddit comment threads from a list of URLs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146b4yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686431246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a list of about 10,000 URLs of various Reddit comment threads. I would like to archive the contents locally. These are not linked to any particular Reddit account or subreddit.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to automatically download the entire comment threads (including the original post / link) from each of those URLs?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a barely-computer-literate Windows user. I don&amp;#39;t know how to use CLIs, Python scripts, Docker images, or web servers, but I can follow a step-by-step dummy&amp;#39;s guide. I can install Linux Mint or Raspbian on a fresh machine if needed.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen this tool by &lt;a href=\"/u/Ailothaen\"&gt;u/Ailothaen&lt;/a&gt;: &lt;a href=\"https://github.com/Ailothaen/RedditArchiver\"&gt;https://github.com/Ailothaen/RedditArchiver&lt;/a&gt;\nThis looks like it would generate the output in a format that I need. However, from the screenshots it appears to work on one URL at a time.&lt;/p&gt;\n\n&lt;p&gt;Some of the posts I want are not on archive.org in their entirety. I&amp;#39;m aware of the ArchiveTeam project, but I don&amp;#39;t know how to access their data, or pull a particular post using an URL.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate any help, or links to resources. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?auto=webp&amp;v=enabled&amp;s=5dd738ddecc2863758b8b05832d2111c2e0bbfee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19662ab0e7825663460336b4a3414fcf9c02f2ca", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=507d1fcd4cd6af027159a95c5f96cf82530c796e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b688a2ca61c9b369c2d5213e3f45e1b91d1766ac", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ff8a787c16b93e0058c0a30b88a28b0f78e8394", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb0c029a818b71f33f024cfd449168c11cd588bd", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9790f2065419b1328d96ce3fd0b0356e44419b73", "width": 1080, "height": 540}], "variants": {}, "id": "xzWIieX_uPh5GVSnThUvDs4kSPVFuTepLV5dCuu01qQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146b4yd", "is_robot_indexable": true, "report_reasons": null, "author": "adpj3fns", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146b4yd/massarchiving_reddit_comment_threads_from_a_list/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146b4yd/massarchiving_reddit_comment_threads_from_a_list/", "subreddit_subscribers": 687274, "created_utc": 1686431246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\ncould anyone recommend me an affordable technology that'd allow me to reliably cold store few hundreds of GB (of my serial stash) for a long time (30 years plus)? I'd like to store it without power on long-term, hence why internal HDDs nor NASes are not desired.\n\nI'm thinking of M-Disc, but I don't currently have any burner on hand.", "author_fullname": "t2_6m008lir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long-term cold data archival", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1464ukp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686417656.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686415298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;could anyone recommend me an affordable technology that&amp;#39;d allow me to reliably cold store few hundreds of GB (of my serial stash) for a long time (30 years plus)? I&amp;#39;d like to store it without power on long-term, hence why internal HDDs nor NASes are not desired.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of M-Disc, but I don&amp;#39;t currently have any burner on hand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1464ukp", "is_robot_indexable": true, "report_reasons": null, "author": "creeper6530", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1464ukp/longterm_cold_data_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1464ukp/longterm_cold_data_archival/", "subreddit_subscribers": 687274, "created_utc": 1686415298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 100GB of family photos, videos, and documents. I plan to use 7zip to encrypt and compress this. I am wondering if anyone knows a free website or offsite location to upload my encrypted file or files as a offsite backup. I\u2019m willing to split my files into like part 1, 2, etc in like 5G files each.", "author_fullname": "t2_aiwum8a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Cloud Data Backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146953y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686426152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 100GB of family photos, videos, and documents. I plan to use 7zip to encrypt and compress this. I am wondering if anyone knows a free website or offsite location to upload my encrypted file or files as a offsite backup. I\u2019m willing to split my files into like part 1, 2, etc in like 5G files each.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16.1 TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146953y", "is_robot_indexable": true, "report_reasons": null, "author": "BradenTheLegend", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/146953y/free_cloud_data_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146953y/free_cloud_data_backup/", "subreddit_subscribers": 687274, "created_utc": 1686426152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd ideally like to save the content of the post / comment text that I bookmarked instead of just a URL. Many communities are going dark and content may not become public again.  I'm wondering if there is some API-based service that saves to PDF or something similar. \n\nSame for my user posts &amp; comments. \n\nIDK what the future of this platform holds, but I'd like to preserve the content from here that has been helpful, transformative, or just the most excellent pictures of cats.", "author_fullname": "t2_1qrsw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to save / download reddit bookmarks to offsite/offline before all APIs are nuked?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1468h5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686424427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d ideally like to save the content of the post / comment text that I bookmarked instead of just a URL. Many communities are going dark and content may not become public again.  I&amp;#39;m wondering if there is some API-based service that saves to PDF or something similar. &lt;/p&gt;\n\n&lt;p&gt;Same for my user posts &amp;amp; comments. &lt;/p&gt;\n\n&lt;p&gt;IDK what the future of this platform holds, but I&amp;#39;d like to preserve the content from here that has been helpful, transformative, or just the most excellent pictures of cats.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1468h5z", "is_robot_indexable": true, "report_reasons": null, "author": "astaramence", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1468h5z/whats_the_best_way_to_save_download_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1468h5z/whats_the_best_way_to_save_download_reddit/", "subreddit_subscribers": 687274, "created_utc": 1686424427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Before the 30th I'm looking to backup all the posts I have, so comments, submissions, etc, in a way that's more human readable.\n\nAnyone know of some good tools for that? I know of bulk-reddit-exporter which is good for saved posts, but stuff you submit yourself and the comments within those I still haven't found much on. \n\nPowerDeleteSuite does export into a .csv, it's not the most easily readable but if that's all I got then I guess I can go with that, but I'm open to suggestions first.\n\nEDIT: I also know of reddit's own data exporter method via gdpr, which I did submit for but I'm still waiting on that", "author_fullname": "t2_73kyrax1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have any good suggestions for backing up personal submissions in a human readable manner?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1463u2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686459511.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686412793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before the 30th I&amp;#39;m looking to backup all the posts I have, so comments, submissions, etc, in a way that&amp;#39;s more human readable.&lt;/p&gt;\n\n&lt;p&gt;Anyone know of some good tools for that? I know of bulk-reddit-exporter which is good for saved posts, but stuff you submit yourself and the comments within those I still haven&amp;#39;t found much on. &lt;/p&gt;\n\n&lt;p&gt;PowerDeleteSuite does export into a .csv, it&amp;#39;s not the most easily readable but if that&amp;#39;s all I got then I guess I can go with that, but I&amp;#39;m open to suggestions first.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I also know of reddit&amp;#39;s own data exporter method via gdpr, which I did submit for but I&amp;#39;m still waiting on that&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1463u2r", "is_robot_indexable": true, "report_reasons": null, "author": "BenefitAffectionate", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1463u2r/anyone_have_any_good_suggestions_for_backing_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1463u2r/anyone_have_any_good_suggestions_for_backing_up/", "subreddit_subscribers": 687274, "created_utc": 1686412793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been saying for years that the only remaining 1TB 2.5\" CMR drive was the WD Red. Then learned a while back that Seagate has a 1.2TB (2x 600GB platter) CMR drive in their Exos line. There's also a 2.4TB CMR drive in the line.\n\nThen today, I checked the Toshiba site because of another thread and to my surprise see that Toshiba has a 1TB 2.5\" CMR drive! But it's OEM only. \n\nHere's the spec sheet for Toshiba 2.5\" drives: [https://storage.toshiba.com/docs/support-docs/L200-SalesSheet\\_English\\_Web\\_r2.pdf](https://storage.toshiba.com/docs/support-docs/L200-SalesSheet_English_Web_r2.pdf)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae\n\nhttps://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=45df751fc6e6943291c29683e415f3a273ed1858\n\nNote that the 1TB CMR drive is bulk only and is 9.5mm. Which makes sense since it would have to be two 500GB platters.\n\nThe 1TB SMR drive is 7mm because it only has a single platter. ", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TIL - There is a 2.5\" 1TB CMR drive from Toshiba. But it's OEM/Bare only.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"y5xjvqn1j85b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f29936595eed553cef72331e38c8118778e0c06"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ce1682b60c0da3728d3048c6d11c88b6dd12a31"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81e08e8c4aa2b1f5f6172c596b1162680f122b42"}, {"y": 329, "x": 640, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a68e7dbbe764f0752429a807ee7558b321f06fa"}, {"y": 493, "x": 960, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da0598a542adaaca8f85c4c4a67549870ebc6d9c"}, {"y": 555, "x": 1080, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61ecc44c0aadc5aaec642c7507137758aa9e64d7"}], "s": {"y": 652, "x": 1268, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae"}, "id": "y5xjvqn1j85b1"}, "u5q5eu0hj85b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 17, "x": 108, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fe3afcc02fa460694945508edcedea26a54766e"}, {"y": 34, "x": 216, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a55bfcea14ef0dedb150c940288079eb428f1a6e"}, {"y": 50, "x": 320, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18fd29ca9b941a075db2fa57e0d9869f9b6c8f27"}, {"y": 101, "x": 640, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35e9618c710a9ef706bcdf20addf063c65970e46"}, {"y": 151, "x": 960, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c81f80c2692faa5d48bd2ebe9c7881192402f10d"}, {"y": 170, "x": 1080, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92e3c189cb34ec8567da5b02c9d0a310ac1a4c2c"}], "s": {"y": 187, "x": 1183, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=45df751fc6e6943291c29683e415f3a273ed1858"}, "id": "u5q5eu0hj85b1"}}, "name": "t3_1467pi0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FX4KJ2aexDxyAyytzprdqvWFbMSNmhSvtW_25-2Ns7w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686422486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been saying for years that the only remaining 1TB 2.5&amp;quot; CMR drive was the WD Red. Then learned a while back that Seagate has a 1.2TB (2x 600GB platter) CMR drive in their Exos line. There&amp;#39;s also a 2.4TB CMR drive in the line.&lt;/p&gt;\n\n&lt;p&gt;Then today, I checked the Toshiba site because of another thread and to my surprise see that Toshiba has a 1TB 2.5&amp;quot; CMR drive! But it&amp;#39;s OEM only. &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the spec sheet for Toshiba 2.5&amp;quot; drives: &lt;a href=\"https://storage.toshiba.com/docs/support-docs/L200-SalesSheet_English_Web_r2.pdf\"&gt;https://storage.toshiba.com/docs/support-docs/L200-SalesSheet_English_Web_r2.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae\"&gt;https://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=45df751fc6e6943291c29683e415f3a273ed1858\"&gt;https://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=45df751fc6e6943291c29683e415f3a273ed1858&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note that the 1TB CMR drive is bulk only and is 9.5mm. Which makes sense since it would have to be two 500GB platters.&lt;/p&gt;\n\n&lt;p&gt;The 1TB SMR drive is 7mm because it only has a single platter. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1467pi0", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1467pi0/til_there_is_a_25_1tb_cmr_drive_from_toshiba_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1467pi0/til_there_is_a_25_1tb_cmr_drive_from_toshiba_but/", "subreddit_subscribers": 687274, "created_utc": 1686422486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seeing as many subreddits will be going dark in a few days (some for only a few days, some supposedly permanently), I am looking for a way to back up my saved reddit posts and threads. Some threads I want to save in their entirety (or as much as I can get, stuff like AskReddit threads with lots of funny responses or useful information) and some I just want the video/picture/gif that's the main post. There have been a lot of posts about backing up your reddit account that I've found, but they've either seemed too limited (requesting GDPR data, which isn't really what I want) or too technical (programming scripts and stuff). While I'm not necessarily against the technical stuff, I don't have any background in using it and worry about messing it up. Does anyone have an idiot-proof guide on how to download saved posts? I don't mind doing it manually per post - honestly I'd prefer it for full control - but I just don't know where to get started.", "author_fullname": "t2_g2nib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Request: Idiot-friendly method for backing up saved reddit threads/posts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146dfry", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686437035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeing as many subreddits will be going dark in a few days (some for only a few days, some supposedly permanently), I am looking for a way to back up my saved reddit posts and threads. Some threads I want to save in their entirety (or as much as I can get, stuff like AskReddit threads with lots of funny responses or useful information) and some I just want the video/picture/gif that&amp;#39;s the main post. There have been a lot of posts about backing up your reddit account that I&amp;#39;ve found, but they&amp;#39;ve either seemed too limited (requesting GDPR data, which isn&amp;#39;t really what I want) or too technical (programming scripts and stuff). While I&amp;#39;m not necessarily against the technical stuff, I don&amp;#39;t have any background in using it and worry about messing it up. Does anyone have an idiot-proof guide on how to download saved posts? I don&amp;#39;t mind doing it manually per post - honestly I&amp;#39;d prefer it for full control - but I just don&amp;#39;t know where to get started.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146dfry", "is_robot_indexable": true, "report_reasons": null, "author": "KnightPlutonian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146dfry/request_idiotfriendly_method_for_backing_up_saved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146dfry/request_idiotfriendly_method_for_backing_up_saved/", "subreddit_subscribers": 687274, "created_utc": 1686437035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Given the API protests going on I fear some subreddits may be deleted or lost. Do we have any full backups of reddit?", "author_fullname": "t2_l9loi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a complete backup of reddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146msr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686465997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given the API protests going on I fear some subreddits may be deleted or lost. Do we have any full backups of reddit?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146msr8", "is_robot_indexable": true, "report_reasons": null, "author": "yogopig", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146msr8/is_there_a_complete_backup_of_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146msr8/is_there_a_complete_backup_of_reddit/", "subreddit_subscribers": 687274, "created_utc": 1686465997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have bought two Toshiba MG08ACA16TE 16TB internal HDDs a few weeks ago.\n\nI have noticed that one of them makes clicking sounds after spinning up at bootup.\nIs it dying already!?\n\nSMART says it's fine, also it behaves normally when I use it.\n\nI have linked a sound of the bootup, it's at about second 12.", "author_fullname": "t2_cuqg6y5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New HDD failing? Getting clicking sounds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146jczn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1686454322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "on.soundcloud.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have bought two Toshiba MG08ACA16TE 16TB internal HDDs a few weeks ago.&lt;/p&gt;\n\n&lt;p&gt;I have noticed that one of them makes clicking sounds after spinning up at bootup.\nIs it dying already!?&lt;/p&gt;\n\n&lt;p&gt;SMART says it&amp;#39;s fine, also it behaves normally when I use it.&lt;/p&gt;\n\n&lt;p&gt;I have linked a sound of the bootup, it&amp;#39;s at about second 12.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://on.soundcloud.com/c4z57", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146jczn", "is_robot_indexable": true, "report_reasons": null, "author": "ACrossingTroll", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146jczn/new_hdd_failing_getting_clicking_sounds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://on.soundcloud.com/c4z57", "subreddit_subscribers": 687274, "created_utc": 1686454322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m on a upgrade kick at the moment and recently upgraded my laptop to a ssd hard drive and looking to do the same to my wife\u2019s. I downloaded and used Acronis cloning software for my transfer and everything worked great, so I\u2019m partial to using the same for my wife\u2019s. But her laptop is so bogged down that I couldn\u2019t imagine trying to download the program to her laptop without crashing it. Would it be possible to connect her old hdd and new ssd to my computer and just use acronis to clone it. From my basic understanding, I can safely plug in another hdd while my computer is running and then thru acronis, I choose it as my \u201cdisk to be cloned\u201d from the list and then choose the new ssd as the \u201cdestination disk\u201d. Seems logical to me, but computers are far from my expertise", "author_fullname": "t2_o0shmlnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hdd to ssd clone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146j8tc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686453963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m on a upgrade kick at the moment and recently upgraded my laptop to a ssd hard drive and looking to do the same to my wife\u2019s. I downloaded and used Acronis cloning software for my transfer and everything worked great, so I\u2019m partial to using the same for my wife\u2019s. But her laptop is so bogged down that I couldn\u2019t imagine trying to download the program to her laptop without crashing it. Would it be possible to connect her old hdd and new ssd to my computer and just use acronis to clone it. From my basic understanding, I can safely plug in another hdd while my computer is running and then thru acronis, I choose it as my \u201cdisk to be cloned\u201d from the list and then choose the new ssd as the \u201cdestination disk\u201d. Seems logical to me, but computers are far from my expertise&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146j8tc", "is_robot_indexable": true, "report_reasons": null, "author": "Tx_0618", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146j8tc/hdd_to_ssd_clone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146j8tc/hdd_to_ssd_clone/", "subreddit_subscribers": 687274, "created_utc": 1686453963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I upload 1TB/day on my 2TB M.2 drive, and so far I have 115TB read on it.. I don't know if that affects the lifespan of my drive, and I'm thinking of doing the uploading using an external hard drive, but I also don't know how long those last for constant reading.\n\nWhich is the more long-term budget friendly option? I thought SSDs will last longer but are pretty expensive while HDDs won't last as long (I've had 6 broken down in less than a year or extremely slow, mostly WD drives) but I'm not well informed\n\nThanks in advance", "author_fullname": "t2_bmlf894u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does total host reads affect SSD lifetime just like TBW? Should I upload with an HDD instead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146hjub", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686448661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I upload 1TB/day on my 2TB M.2 drive, and so far I have 115TB read on it.. I don&amp;#39;t know if that affects the lifespan of my drive, and I&amp;#39;m thinking of doing the uploading using an external hard drive, but I also don&amp;#39;t know how long those last for constant reading.&lt;/p&gt;\n\n&lt;p&gt;Which is the more long-term budget friendly option? I thought SSDs will last longer but are pretty expensive while HDDs won&amp;#39;t last as long (I&amp;#39;ve had 6 broken down in less than a year or extremely slow, mostly WD drives) but I&amp;#39;m not well informed&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146hjub", "is_robot_indexable": true, "report_reasons": null, "author": "colt45keyboard", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146hjub/does_total_host_reads_affect_ssd_lifetime_just/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146hjub/does_total_host_reads_affect_ssd_lifetime_just/", "subreddit_subscribers": 687274, "created_utc": 1686448661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there such thing that I can throw in a docker that would search multiple home servers for relevant results? \n\nI was looking at the Google search appliance while I was waiting for a few different searches from multiple storage servers to complete...and not find what I was looking for....so I thought may be someone made something like what I'm looking for.\n\nEta: like if I search a part number or phrase, it can find it in a document too, like google, but self hosted and not with all the Google...negatives.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Search engine for multiple storage servers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1462x1z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686410973.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686410428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there such thing that I can throw in a docker that would search multiple home servers for relevant results? &lt;/p&gt;\n\n&lt;p&gt;I was looking at the Google search appliance while I was waiting for a few different searches from multiple storage servers to complete...and not find what I was looking for....so I thought may be someone made something like what I&amp;#39;m looking for.&lt;/p&gt;\n\n&lt;p&gt;Eta: like if I search a part number or phrase, it can find it in a document too, like google, but self hosted and not with all the Google...negatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "76/108 TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1462x1z", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1462x1z/search_engine_for_multiple_storage_servers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1462x1z/search_engine_for_multiple_storage_servers/", "subreddit_subscribers": 687274, "created_utc": 1686410428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9xph37h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this noise common/acceptable in a 4TB WD Blue or should I be worried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_146s48s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 70, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 250, "fallback_url": "https://v.redd.it/9fyumyckod5b1/DASH_220.mp4?source=fallback", "has_audio": true, "height": 128, "width": 128, "scrubber_media_url": "https://v.redd.it/9fyumyckod5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/9fyumyckod5b1/DASHPlaylist.mpd?a=1689079831%2CZWVmZmNlOWE4NGUxZWZjNzI0MzBjNjA5MGE2ZmY4NDg3NWY0NDU5YWYwYTAxMjFhMWQyNTBlN2Q1ZDBkNjBkMA%3D%3D&amp;v=1&amp;f=sd", "duration": 50, "hls_url": "https://v.redd.it/9fyumyckod5b1/HLSPlaylist.m3u8?a=1689079831%2CMWMyOGYyODY3YjcwMWRhNjAyMjk2OTcyMzE0ZGQzYWU1YzYxOTY5MGIyYTEyNDFkOTY3ODg4NmM0YWMyNjQ5MA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PGbLHTi0pMhgKDPJuTDwkZkm_AbbP2lQKkk-CZF7AKY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686484683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/9fyumyckod5b1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OkbLolmi-RzKJPM1lE46T2Y_L3KuxRctjMMF_kHFSSo.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=afcd3182778f63919201091fcb9a40019d1daf33", "width": 128, "height": 128}, "resolutions": [{"url": "https://external-preview.redd.it/OkbLolmi-RzKJPM1lE46T2Y_L3KuxRctjMMF_kHFSSo.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ef6b1ea6cf440f4db5022d6f7290a397f89d3f4e", "width": 108, "height": 108}], "variants": {}, "id": "klcCEAU3X-YZwSkBoHESxXlG3yu_1dxLdfsqU9E0UtA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146s48s", "is_robot_indexable": true, "report_reasons": null, "author": "Nuklr", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146s48s/is_this_noise_commonacceptable_in_a_4tb_wd_blue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/9fyumyckod5b1", "subreddit_subscribers": 687274, "created_utc": 1686484683.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 250, "fallback_url": "https://v.redd.it/9fyumyckod5b1/DASH_220.mp4?source=fallback", "has_audio": true, "height": 128, "width": 128, "scrubber_media_url": "https://v.redd.it/9fyumyckod5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/9fyumyckod5b1/DASHPlaylist.mpd?a=1689079831%2CZWVmZmNlOWE4NGUxZWZjNzI0MzBjNjA5MGE2ZmY4NDg3NWY0NDU5YWYwYTAxMjFhMWQyNTBlN2Q1ZDBkNjBkMA%3D%3D&amp;v=1&amp;f=sd", "duration": 50, "hls_url": "https://v.redd.it/9fyumyckod5b1/HLSPlaylist.m3u8?a=1689079831%2CMWMyOGYyODY3YjcwMWRhNjAyMjk2OTcyMzE0ZGQzYWU1YzYxOTY5MGIyYTEyNDFkOTY3ODg4NmM0YWMyNjQ5MA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks, I'm stuck and could really use some assistance here. \n\nI've been trying to save a website using HTTrack, but I'm running into a roadblock with the third-party authorization feature on Substack. the website runs on its own domain, but when I authorize, it redirects me to Substack and then back to the original site upon success.  \n\nNo matter what I've tried with HTTrack so far\u2014using site-specific cookies from Chrome or attempting all the cookies saved in Chrome\u2014I still can't access the pages that require authorization. Has anyone out there successfully tackled this issue before? If you have any advice or tips to share, I'd greatly appreciate it. Thanks a bunch in advance!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_3tsqz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with HTTrack and Substack website authorization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_146ruhk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686483794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I&amp;#39;m stuck and could really use some assistance here. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to save a website using HTTrack, but I&amp;#39;m running into a roadblock with the third-party authorization feature on Substack. the website runs on its own domain, but when I authorize, it redirects me to Substack and then back to the original site upon success.  &lt;/p&gt;\n\n&lt;p&gt;No matter what I&amp;#39;ve tried with HTTrack so far\u2014using site-specific cookies from Chrome or attempting all the cookies saved in Chrome\u2014I still can&amp;#39;t access the pages that require authorization. Has anyone out there successfully tackled this issue before? If you have any advice or tips to share, I&amp;#39;d greatly appreciate it. Thanks a bunch in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146ruhk", "is_robot_indexable": true, "report_reasons": null, "author": "getry", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146ruhk/need_help_with_httrack_and_substack_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146ruhk/need_help_with_httrack_and_substack_website/", "subreddit_subscribers": 687274, "created_utc": 1686483794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Follow up to [this](https://www.reddit.com/r/DataHoarder/comments/10fg8f3/issuu_making_changes_that_will_make_a_lot_of/) (I'm not OP).\n\nStarting from June 19, another limitation is going to be applied to free Basic plan:\n\n&gt;**Publishing limit:** Basic users will be limited to 5 published documents.  \n&gt;  \n&gt;Starting on June 19, 2023, we will begin a phased rollout to all Basic accounts limiting the number of published documents. Any existing uploads on a Basic account that exceed the above limitation will continue to be hosted on Issuu but not accessible to readers.\u00a0\n\nThey're basically killing the free plan, and making a lot of content unavaliable forever (the premium plan is at least $19 per month, IMHO too expensive for most publishers, without counting the \"zoombie publishers\" that are not even going to see what's happening)", "author_fullname": "t2_81fvh23e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issuu making changes that will make a lot of content unavaliable (yes, again)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146pmrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686476305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Follow up to &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10fg8f3/issuu_making_changes_that_will_make_a_lot_of/\"&gt;this&lt;/a&gt; (I&amp;#39;m not OP).&lt;/p&gt;\n\n&lt;p&gt;Starting from June 19, another limitation is going to be applied to free Basic plan:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Publishing limit:&lt;/strong&gt; Basic users will be limited to 5 published documents.  &lt;/p&gt;\n\n&lt;p&gt;Starting on June 19, 2023, we will begin a phased rollout to all Basic accounts limiting the number of published documents. Any existing uploads on a Basic account that exceed the above limitation will continue to be hosted on Issuu but not accessible to readers.\u00a0&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;They&amp;#39;re basically killing the free plan, and making a lot of content unavaliable forever (the premium plan is at least $19 per month, IMHO too expensive for most publishers, without counting the &amp;quot;zoombie publishers&amp;quot; that are not even going to see what&amp;#39;s happening)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146pmrp", "is_robot_indexable": true, "report_reasons": null, "author": "CheesecakeNo7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146pmrp/issuu_making_changes_that_will_make_a_lot_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146pmrp/issuu_making_changes_that_will_make_a_lot_of/", "subreddit_subscribers": 687274, "created_utc": 1686476305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nPerhaps ya'll can help. I recently picked up a Quantum LTO-6 external tape drive for my Ubuntu setup here at home. Got it all setup, LTFS working, all that good stuff.\n\nNow I need to begin the process of actually moving data onto there (thanks Google, re GSuite Business changes).\n\nWorst case scenario I'll just script something up, but I can't imagine there isn't something out there that probably does exactly what I'm looking for and most likely more.\n\nBasically, I'm looking to do two things:\n\n1.) Copy &amp; verify data written to the tape cartridge. On Windows, I'd use something like TeraCopy for this. IIRC rsync can do this.\n\n2.) Catalog the data that is written to the tape cartridge so I know exactly what files (names, size, directories, etc) are on the cartridge. For example, after copying a bunch of files, I'd run \"catalog\" or something, and it would basically scan the entire tape and output a browsable, offline (no tape cartridge in the drive required), webpage or some crap I could open and see a directory/file tree/etc.\n\nMy requirements are very broad/relaxed and like I said, if I can't really find anything I'll probably just script something up.\n\nI must be using the wrong terminology or something (and IIRC reading the documentation, cataloging/indexing is already terminology used for other tape related functions) because I can't find a lot of information on tools that do this type of stuff - primarily the cataloging I mentioned above.\n\nThis would be for use on a Ubuntu 23.04 Desktop install. GUI would be preferred, but not at all married to that.\n\nDoes anyone here do anything like this, or know of tools to do so?\n\nThanks much!", "author_fullname": "t2_lm6eu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux tools for tape backup management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146lkfo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686461706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Perhaps ya&amp;#39;ll can help. I recently picked up a Quantum LTO-6 external tape drive for my Ubuntu setup here at home. Got it all setup, LTFS working, all that good stuff.&lt;/p&gt;\n\n&lt;p&gt;Now I need to begin the process of actually moving data onto there (thanks Google, re GSuite Business changes).&lt;/p&gt;\n\n&lt;p&gt;Worst case scenario I&amp;#39;ll just script something up, but I can&amp;#39;t imagine there isn&amp;#39;t something out there that probably does exactly what I&amp;#39;m looking for and most likely more.&lt;/p&gt;\n\n&lt;p&gt;Basically, I&amp;#39;m looking to do two things:&lt;/p&gt;\n\n&lt;p&gt;1.) Copy &amp;amp; verify data written to the tape cartridge. On Windows, I&amp;#39;d use something like TeraCopy for this. IIRC rsync can do this.&lt;/p&gt;\n\n&lt;p&gt;2.) Catalog the data that is written to the tape cartridge so I know exactly what files (names, size, directories, etc) are on the cartridge. For example, after copying a bunch of files, I&amp;#39;d run &amp;quot;catalog&amp;quot; or something, and it would basically scan the entire tape and output a browsable, offline (no tape cartridge in the drive required), webpage or some crap I could open and see a directory/file tree/etc.&lt;/p&gt;\n\n&lt;p&gt;My requirements are very broad/relaxed and like I said, if I can&amp;#39;t really find anything I&amp;#39;ll probably just script something up.&lt;/p&gt;\n\n&lt;p&gt;I must be using the wrong terminology or something (and IIRC reading the documentation, cataloging/indexing is already terminology used for other tape related functions) because I can&amp;#39;t find a lot of information on tools that do this type of stuff - primarily the cataloging I mentioned above.&lt;/p&gt;\n\n&lt;p&gt;This would be for use on a Ubuntu 23.04 Desktop install. GUI would be preferred, but not at all married to that.&lt;/p&gt;\n\n&lt;p&gt;Does anyone here do anything like this, or know of tools to do so?&lt;/p&gt;\n\n&lt;p&gt;Thanks much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146lkfo", "is_robot_indexable": true, "report_reasons": null, "author": "xydrine", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/146lkfo/linux_tools_for_tape_backup_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146lkfo/linux_tools_for_tape_backup_management/", "subreddit_subscribers": 687274, "created_utc": 1686461706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am having a problem where as time goes on my mp3 files start to give problems. When I first play them back they give me no problems, however eventually they will start to cut off then play a random different song. I've attached an example of what I'm talking about [here](https://youtu.be/9-IFlXY08mc) (happens at around the 1:36 mark). \n\nIn words, imagine a 2 minute song plays normally for 1 minute with no issues, then at the 1 minute mark it somehow transforms into a completely different song as if someone edited the last minute of the song to replace its audio. Keep in mind that this is not a problem when the file is first downloaded.\n\nIs this a form of data corruption? Is something that you guys have experienced before? If so, is there someway to prevent this? Thank you.", "author_fullname": "t2_ao92sjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help regarding corrupted MP3 files.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146k6mb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686457027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am having a problem where as time goes on my mp3 files start to give problems. When I first play them back they give me no problems, however eventually they will start to cut off then play a random different song. I&amp;#39;ve attached an example of what I&amp;#39;m talking about &lt;a href=\"https://youtu.be/9-IFlXY08mc\"&gt;here&lt;/a&gt; (happens at around the 1:36 mark). &lt;/p&gt;\n\n&lt;p&gt;In words, imagine a 2 minute song plays normally for 1 minute with no issues, then at the 1 minute mark it somehow transforms into a completely different song as if someone edited the last minute of the song to replace its audio. Keep in mind that this is not a problem when the file is first downloaded.&lt;/p&gt;\n\n&lt;p&gt;Is this a form of data corruption? Is something that you guys have experienced before? If so, is there someway to prevent this? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?auto=webp&amp;v=enabled&amp;s=10227d881699acd2135996ff15f3f1a04eb5387f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=100875830905dc76337a1d35fcf1030ead2155b2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9abdfea6d90681f590300c6d7f16c6dc5394ae2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54b27712e4ccfbfed69d7087a698835aa4939798", "width": 320, "height": 240}], "variants": {}, "id": "7YHre2ibQTlOBomewwhG_5hh-BKv1_FZQgLZwTTF8uE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146k6mb", "is_robot_indexable": true, "report_reasons": null, "author": "freshcokecola", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146k6mb/help_regarding_corrupted_mp3_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146k6mb/help_regarding_corrupted_mp3_files/", "subreddit_subscribers": 687274, "created_utc": 1686457027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After more than a day's work swapping boards and updating firmware:\n\n* With an LSI 9211-8i (SAS2008) in IT/JBOD mode, the Z230 won't even boot: gives five beeps for invalid memory configuration.\n\n* With an LSI 9207-8i (SAS2308) in IT/JBOD mode, the Z230 doesn't see half the memory.\n\n* Only with an LSI 9261 (SAS2108) in IR/RAID mode does it work with all the memory.\n\nI don't have any SAS3008 boards, so I couldn't test those, but I suspect they would only work in IR/RAID mode.\n\nBTW, you have update the LSI firmware on a different machine, since the Z230 UEFI firmware doesn't play well with LSI's utilities either. \n\nTo configure the drives, you have to change the bios settings to run the option roms for *all* your cards to legacy, configure the drives, then change the settings back to UEFI.\n\nWith 14 drives (11 SSDs, 1 M.2, and 2 7200 HDDs) and 16GB memory it idles at 60 watts with a E3-1246v3 using the integrated graphics. I added a USB fan because the SSDs are packed (Icy Dock MB994+MB996), but even then its tolerably quiet.\n\nIf you're thinking of a low-power server, it's not a bad option, but be prepared for some work.\n\nWill work with three NetApp DS4243's? Some day I'll find out. Not today.\n\nBTW, the magic commands to get smart data from an LSI raid card in linux are:\n\n# storcli /c0 /eall /sall show\n252:0    20 Onln   1 475.905 GB SATA SSD N   N  512B \n...\n\nsmartctl -x -d megaraid,20 /dev/sda\n\nwhere 20 is whatever the device id storcli reports. The /dev/sda is arbitrary and not connected with the output. There's also no obvious connection between the SAS addresses storcli reports and those from lsscsi --wwn.", "author_fullname": "t2_cc4k2ojsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP Z230 does not play well with others (LSI cards in particular)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146jleb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686455089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After more than a day&amp;#39;s work swapping boards and updating firmware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;With an LSI 9211-8i (SAS2008) in IT/JBOD mode, the Z230 won&amp;#39;t even boot: gives five beeps for invalid memory configuration.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;With an LSI 9207-8i (SAS2308) in IT/JBOD mode, the Z230 doesn&amp;#39;t see half the memory.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Only with an LSI 9261 (SAS2108) in IR/RAID mode does it work with all the memory.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I don&amp;#39;t have any SAS3008 boards, so I couldn&amp;#39;t test those, but I suspect they would only work in IR/RAID mode.&lt;/p&gt;\n\n&lt;p&gt;BTW, you have update the LSI firmware on a different machine, since the Z230 UEFI firmware doesn&amp;#39;t play well with LSI&amp;#39;s utilities either. &lt;/p&gt;\n\n&lt;p&gt;To configure the drives, you have to change the bios settings to run the option roms for &lt;em&gt;all&lt;/em&gt; your cards to legacy, configure the drives, then change the settings back to UEFI.&lt;/p&gt;\n\n&lt;p&gt;With 14 drives (11 SSDs, 1 M.2, and 2 7200 HDDs) and 16GB memory it idles at 60 watts with a E3-1246v3 using the integrated graphics. I added a USB fan because the SSDs are packed (Icy Dock MB994+MB996), but even then its tolerably quiet.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re thinking of a low-power server, it&amp;#39;s not a bad option, but be prepared for some work.&lt;/p&gt;\n\n&lt;p&gt;Will work with three NetApp DS4243&amp;#39;s? Some day I&amp;#39;ll find out. Not today.&lt;/p&gt;\n\n&lt;p&gt;BTW, the magic commands to get smart data from an LSI raid card in linux are:&lt;/p&gt;\n\n&lt;h1&gt;storcli /c0 /eall /sall show&lt;/h1&gt;\n\n&lt;p&gt;252:0    20 Onln   1 475.905 GB SATA SSD N   N  512B \n...&lt;/p&gt;\n\n&lt;p&gt;smartctl -x -d megaraid,20 /dev/sda&lt;/p&gt;\n\n&lt;p&gt;where 20 is whatever the device id storcli reports. The /dev/sda is arbitrary and not connected with the output. There&amp;#39;s also no obvious connection between the SAS addresses storcli reports and those from lsscsi --wwn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146jleb", "is_robot_indexable": true, "report_reasons": null, "author": "iscsi-root", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146jleb/hp_z230_does_not_play_well_with_others_lsi_cards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146jleb/hp_z230_does_not_play_well_with_others_lsi_cards/", "subreddit_subscribers": 687274, "created_utc": 1686455089.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}