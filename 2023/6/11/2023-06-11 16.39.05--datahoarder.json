{"kind": "Listing", "data": {"after": "t3_146hjub", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_63o77fvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Czkawka 6.0 - File cleaner, now finds similar audio files by content, files by size and name and fix and speedup similar images search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_146nmeh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 414, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/jqa60612ec5b1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jqa60612ec5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/jqa60612ec5b1/DASHPlaylist.mpd?a=1689093545%2CNDA0ZDBjNjI0YWE1MDk0MTVjY2MwMzI1YjcxYTIxYTM5NDZjZDc4NjlkZWE1MmFmODM2Y2QxYzIxMDUyOWVlNg%3D%3D&amp;v=1&amp;f=sd", "duration": 158, "hls_url": "https://v.redd.it/jqa60612ec5b1/HLSPlaylist.m3u8?a=1689093545%2CZTkwNGRlYjc0MmFjY2FlNjMzOWU5MzQ4MWVlZGMzYWRiYWMwZTk5ZGFmNjQ4NmU0ODE3ZjhkZTE1OWI5NWNhNQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 414, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YffpmEmamU3bVQhoCxDNiQoWL69enwqyhjR7iugRr1A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686468974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/jqa60612ec5b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ff9f20bb6c013191dd6f44ee5792537685fe6f4b", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0c744882dc338b5c5fbcb402edcebdf67e4a670f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=180c425dfb145e0f6457f6ae610f360b86158e83", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=84d9c8eaa1077d43e24fbdf61c28d1f03d1a8ddd", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=362d6c8c4197b79e42f26cb076b614d15f9666e5", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b095a49eb39b899a9ddbbbff698ec0b3255f4c3a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/35F8Gr3-AfNa2eKf_PXFgmiV6onSFUGrxZGLFbFOKd4.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ba9808773e1886acbc0446dc9bdf4ee65703f1b8", "width": 1080, "height": 607}], "variants": {}, "id": "__Kb4xeAQKPSI4h5sy9Y2K2G339SeeDekM62Clq58cU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146nmeh", "is_robot_indexable": true, "report_reasons": null, "author": "krutkrutrar", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146nmeh/czkawka_60_file_cleaner_now_finds_similar_audio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/jqa60612ec5b1", "subreddit_subscribers": 687337, "created_utc": 1686468974.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/jqa60612ec5b1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jqa60612ec5b1/DASH_96.mp4", "dash_url": "https://v.redd.it/jqa60612ec5b1/DASHPlaylist.mpd?a=1689093545%2CNDA0ZDBjNjI0YWE1MDk0MTVjY2MwMzI1YjcxYTIxYTM5NDZjZDc4NjlkZWE1MmFmODM2Y2QxYzIxMDUyOWVlNg%3D%3D&amp;v=1&amp;f=sd", "duration": 158, "hls_url": "https://v.redd.it/jqa60612ec5b1/HLSPlaylist.m3u8?a=1689093545%2CZTkwNGRlYjc0MmFjY2FlNjMzOWU5MzQ4MWVlZGMzYWRiYWMwZTk5ZGFmNjQ4NmU0ODE3ZjhkZTE1OWI5NWNhNQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "50 CDs I purchased today to rip sort and store", "author_fullname": "t2_rvu33ehg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a new 50 pack from V Stock today. Gonna have some fun with this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_146dg3k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6xCDl4TQ7uOpgYSxwrL-axDxm-ISrQc0xvK0zNry_-I.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686437056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;50 CDs I purchased today to rip sort and store&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/a2k960t6r95b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?auto=webp&amp;v=enabled&amp;s=4ce2ae70b296901519c8125f8abf7504dfca76e0", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8cc0485e89a7f876e007a5aa705ec0fb4b4a5201", "width": 108, "height": 81}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df77a220510e21bb6b382453a0d6d5eb9c582fa6", "width": 216, "height": 162}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88a0e76ffdad4e2291b5c8781ad8196bf7b83970", "width": 320, "height": 240}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da2820eb163c0b4111bcf24d4c49f697e983947a", "width": 640, "height": 480}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8f9f21f93381229f22f54a185c1a614efc33e38", "width": 960, "height": 720}, {"url": "https://preview.redd.it/a2k960t6r95b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95c2bcc85e2ab8fa8cf84f22d32fcb65b4e32c12", "width": 1080, "height": 810}], "variants": {}, "id": "7oMa0YivGPYmtGzpuiVhPYUeNzIKZeFa4Xt4VXdzeFo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "856GB and counting", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146dg3k", "is_robot_indexable": true, "report_reasons": null, "author": "TheMagicFolf331", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/146dg3k/got_a_new_50_pack_from_v_stock_today_gonna_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/a2k960t6r95b1.jpg", "subreddit_subscribers": 687337, "created_utc": 1686437056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " **TL;DR**\n\n**I've asked and been given hard drive manufacturing answers by someone within the industry.**\n\n**I can't and won't ask or disclose anything that may lead to this person's identity, employer or position.**\n\n**I have no agenda in sharing the following other than keeping with my general goal here of helping and learning from others, as shown by the majority of my posts.**\n\nBegin Q&amp;A\n\nI've been very fortunate to have been contacted by someone within the hard drive manufacturing industry because of some of my posts. I can't disclose whom this person is, but from my layman's understanding of additional undisclosable detailed information, I'm fully confident this person is whom/what they claim to be.\n\nI was able to ask some questions and was given some very interesting info that confirms what I've posted and others have speculated about.\n\nI hope we'll be able to ask more questions in the future, but understand that both I and my source may refuse to ask or answer any question publicly for confidentiality reasons.\n\nQ: A CMR drive be changed to DM-SMR or only HM or HA-SMR? There's a conspiracy theory that the manufacturers may try to submarine SMR into their drives in the future. IMO, it would be market suicide!\n\n*A: In general, SMR drives use the same hardware (heads and platters) as CMR drives. SMR just has the tracks closer so you get more capacity. DM-SMR is the consumer level version that was created to reduce manufacturing cost on lowest capacity drives. It was predicted that SSD was going to take over low capacity HDD\u2019s years ago. The goal of DM-SMR is to reduce manufacturing costs while maintaining acceptable performance in intended applications (light duty consumer applications).*\n\n*On the opposite end of the market, for Cloud, they want as much capacity as possible, so that\u2019s why we use SMR there as well. They demand consistent performance though, for their customers in turn. So HM-SMR is sold to that market. You get the benefit of the closer tracks (so more physical tracks on the same platter) at the cost of writing a whole 256 MiB zone at once, because every 256 MiB there\u2019s a \u201cgap\u201d in the SMR tracks. HM-SMR requires a file system and storage driver that are aware of the HM-SMR rules. If there\u2019s data already in one of those SMR zones, you can\u2019t just write in the middle of one of those zones, without resetting that zone first. Otherwise it would be way too easy to overwrite data in that zone. The HDD firmware keeps track of where we are allowed to write within zones and on HM-SMR drives, you can ask the drive for those values, called \u201cwrite pointers\u201d. HA-SMR isn\u2019t popular anymore and I don\u2019t know if anyone is still making those. Hybrid SMR is also out there in the world, where you can convert any individual 256 MiB zones between SMR and CMR. Those require special kernel and HBA/controller firmware and OS and file systems to work.*\n\n*Also, for HM-SMR, there\u2019s a beta version of BTRFS that mostly works. Might be worth mentioning. You can Google \u201cbtrfs hmsmr\u201d for tutorials. I probably wouldn\u2019t use it for production data but if you end up with one of those HM-SMR drives, it works well enough for Chia or something.*\n\nMy notes: I posted about HM-SMR in this thread: [https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets\\_discuss\\_dmsmr\\_hmsmr\\_hasmr\\_and\\_dropbox/](https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets_discuss_dmsmr_hmsmr_hasmr_and_dropbox/)\n\nQ: Is it true that the drives in externals can be: overstock, overruns, binned (out of spec drives), from cancelled orders.\n\n*A: Yes to all of it. Externals are the lowest bins above the \\[redated\\] (Edit: binned rives} we sell to third parties. It\u2019s whatever is leftover. They have less warranty because they aren\u2019t expected to last as long.*\n\nMy notes: The first part is supported by what I posted in this thread, [https://www.reddit.com/r/DataHoarder/comments/11jmot5/to\\_those\\_asking\\_what\\_drive\\_is\\_inside\\_my\\_wd/](https://www.reddit.com/r/DataHoarder/comments/11jmot5/to_those_asking_what_drive_is_inside_my_wd/) which has a link to WD's disclosure about this.\n\nIt's been confirmed by another source that the binned drives, are drives that are Out Of Spec, flashed with special firmware that can't be updated and is no longer supported by the manufacturer. This is source of SOME of the unbranded drives from certain resellers.\n\nQ: Is it true that in a given generation of HDD, when reduced capacities are released at the same time, you can sometimes tell from the model number that it\u2019s the same hardware inside as a full capacity drive\u201d To be used in externals or sold to resellers?\n\n*A: Yes, see above. The \\[redated\\] (My edit: XX drive size) were reconfigured for 12 and 14TB. The \\[redacted\\] went all the way down to 10TB to my knowledge. We just disable specific bad heads in the factory and rewrite the tracks. It\u2019s an automated process obviously, but we can internally look up all that history on any serial number.*\n\nQ: Is it true that some or all drives of a particular size come from the same hardware line and only the firmware determines which line, e.g. Home, Survelliance, NAS, Datacenter, Enterprise it's labeled as. Or are there separate lines for each type?\n\n*A: Yes, kind of. In the spirit of the question, yes, especially in recent years. The firmware is different for those markets, and specific hardware might be binned for those markets too. For example, surveillance firmware is tuned to be performing writes for the vast majority of the time and to handle data streaming better, as in from cameras. Some of the caching and performance features are disabled, and the reliability features are tweaked to be less likely to interrupt that incoming data stream.*\n\nMy note: To my layman's understanding, this doesn't mean that ALL drives of a given size are the same. Just that SOME drives of a given size MAY be from the same line.\n\nAlso note that \"binned\" as used in the answer doesn't necessarily mean lower or better performing or drive specs. Just different for different uses.\n\nQ: Is it true that drives that don't meet the full specs can be binned and their firmware permanently changed to a lower spec drive, then sold to resellers?\n\n*A: Yes. The firmware doesn\u2019t change the drive to a lower quality drive, it is the manufacturing imperfections. Head fly height is less than 5 nanometers, but if any of those heads touch the disk, the head suffers damage. And there are a lot of heads in high capacity hard drives now. Temperature and humidity affect the aerodynamics of the heads at that level so that is adjusted constantly within the firmware. Usually it\u2019s 20 heads per drive, and even if we bin the parts before assembly, sometimes we figure out in the factory that one or more heads can\u2019t meet the reliability requirements, so we permanently disable them in the firmware.*", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Information about CMR to SMR, Manufacturer externals and Binned drives from a confidential source.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146hb9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686447909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;ve asked and been given hard drive manufacturing answers by someone within the industry.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I can&amp;#39;t and won&amp;#39;t ask or disclose anything that may lead to this person&amp;#39;s identity, employer or position.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have no agenda in sharing the following other than keeping with my general goal here of helping and learning from others, as shown by the majority of my posts.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Begin Q&amp;amp;A&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been very fortunate to have been contacted by someone within the hard drive manufacturing industry because of some of my posts. I can&amp;#39;t disclose whom this person is, but from my layman&amp;#39;s understanding of additional undisclosable detailed information, I&amp;#39;m fully confident this person is whom/what they claim to be.&lt;/p&gt;\n\n&lt;p&gt;I was able to ask some questions and was given some very interesting info that confirms what I&amp;#39;ve posted and others have speculated about.&lt;/p&gt;\n\n&lt;p&gt;I hope we&amp;#39;ll be able to ask more questions in the future, but understand that both I and my source may refuse to ask or answer any question publicly for confidentiality reasons.&lt;/p&gt;\n\n&lt;p&gt;Q: A CMR drive be changed to DM-SMR or only HM or HA-SMR? There&amp;#39;s a conspiracy theory that the manufacturers may try to submarine SMR into their drives in the future. IMO, it would be market suicide!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: In general, SMR drives use the same hardware (heads and platters) as CMR drives. SMR just has the tracks closer so you get more capacity. DM-SMR is the consumer level version that was created to reduce manufacturing cost on lowest capacity drives. It was predicted that SSD was going to take over low capacity HDD\u2019s years ago. The goal of DM-SMR is to reduce manufacturing costs while maintaining acceptable performance in intended applications (light duty consumer applications).&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;On the opposite end of the market, for Cloud, they want as much capacity as possible, so that\u2019s why we use SMR there as well. They demand consistent performance though, for their customers in turn. So HM-SMR is sold to that market. You get the benefit of the closer tracks (so more physical tracks on the same platter) at the cost of writing a whole 256 MiB zone at once, because every 256 MiB there\u2019s a \u201cgap\u201d in the SMR tracks. HM-SMR requires a file system and storage driver that are aware of the HM-SMR rules. If there\u2019s data already in one of those SMR zones, you can\u2019t just write in the middle of one of those zones, without resetting that zone first. Otherwise it would be way too easy to overwrite data in that zone. The HDD firmware keeps track of where we are allowed to write within zones and on HM-SMR drives, you can ask the drive for those values, called \u201cwrite pointers\u201d. HA-SMR isn\u2019t popular anymore and I don\u2019t know if anyone is still making those. Hybrid SMR is also out there in the world, where you can convert any individual 256 MiB zones between SMR and CMR. Those require special kernel and HBA/controller firmware and OS and file systems to work.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Also, for HM-SMR, there\u2019s a beta version of BTRFS that mostly works. Might be worth mentioning. You can Google \u201cbtrfs hmsmr\u201d for tutorials. I probably wouldn\u2019t use it for production data but if you end up with one of those HM-SMR drives, it works well enough for Chia or something.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;My notes: I posted about HM-SMR in this thread: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets_discuss_dmsmr_hmsmr_hasmr_and_dropbox/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13z7w96/lets_discuss_dmsmr_hmsmr_hasmr_and_dropbox/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that the drives in externals can be: overstock, overruns, binned (out of spec drives), from cancelled orders.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes to all of it. Externals are the lowest bins above the [redated] (Edit: binned rives} we sell to third parties. It\u2019s whatever is leftover. They have less warranty because they aren\u2019t expected to last as long.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;My notes: The first part is supported by what I posted in this thread, &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/11jmot5/to_those_asking_what_drive_is_inside_my_wd/\"&gt;https://www.reddit.com/r/DataHoarder/comments/11jmot5/to_those_asking_what_drive_is_inside_my_wd/&lt;/a&gt; which has a link to WD&amp;#39;s disclosure about this.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been confirmed by another source that the binned drives, are drives that are Out Of Spec, flashed with special firmware that can&amp;#39;t be updated and is no longer supported by the manufacturer. This is source of SOME of the unbranded drives from certain resellers.&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that in a given generation of HDD, when reduced capacities are released at the same time, you can sometimes tell from the model number that it\u2019s the same hardware inside as a full capacity drive\u201d To be used in externals or sold to resellers?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes, see above. The [redated] (My edit: XX drive size) were reconfigured for 12 and 14TB. The [redacted] went all the way down to 10TB to my knowledge. We just disable specific bad heads in the factory and rewrite the tracks. It\u2019s an automated process obviously, but we can internally look up all that history on any serial number.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that some or all drives of a particular size come from the same hardware line and only the firmware determines which line, e.g. Home, Survelliance, NAS, Datacenter, Enterprise it&amp;#39;s labeled as. Or are there separate lines for each type?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes, kind of. In the spirit of the question, yes, especially in recent years. The firmware is different for those markets, and specific hardware might be binned for those markets too. For example, surveillance firmware is tuned to be performing writes for the vast majority of the time and to handle data streaming better, as in from cameras. Some of the caching and performance features are disabled, and the reliability features are tweaked to be less likely to interrupt that incoming data stream.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;My note: To my layman&amp;#39;s understanding, this doesn&amp;#39;t mean that ALL drives of a given size are the same. Just that SOME drives of a given size MAY be from the same line.&lt;/p&gt;\n\n&lt;p&gt;Also note that &amp;quot;binned&amp;quot; as used in the answer doesn&amp;#39;t necessarily mean lower or better performing or drive specs. Just different for different uses.&lt;/p&gt;\n\n&lt;p&gt;Q: Is it true that drives that don&amp;#39;t meet the full specs can be binned and their firmware permanently changed to a lower spec drive, then sold to resellers?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A: Yes. The firmware doesn\u2019t change the drive to a lower quality drive, it is the manufacturing imperfections. Head fly height is less than 5 nanometers, but if any of those heads touch the disk, the head suffers damage. And there are a lot of heads in high capacity hard drives now. Temperature and humidity affect the aerodynamics of the heads at that level so that is adjusted constantly within the firmware. Usually it\u2019s 20 heads per drive, and even if we bin the parts before assembly, sometimes we figure out in the factory that one or more heads can\u2019t meet the reliability requirements, so we permanently disable them in the firmware.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146hb9k", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146hb9k/information_about_cmr_to_smr_manufacturer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146hb9k/information_about_cmr_to_smr_manufacturer/", "subreddit_subscribers": 687337, "created_utc": 1686447909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone! I figured I'd share a [product](https://www.3dfilmworks.com/store/p/ssd-holder-for-editing-stations-v2) I designed to help eliminate clutter on creative's desks. If this is something you'd find useful, I'm going to leave a link here to order one! Thanks in advance for any support. [https://www.3dfilmworks.com/store/p/ssd-holder-for-editing-stations-v2](https://www.3dfilmworks.com/store/p/ssd-holder-for-editing-stations-v2)", "author_fullname": "t2_aprzp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD &amp; Cable Organizer - Cleaning Creative\u2019s Clutter!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_146vn1i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sVvPZ84fah900iHJCagEjb5R84-Idh_o5nG0MEggaak.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686494478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I figured I&amp;#39;d share a &lt;a href=\"https://www.3dfilmworks.com/store/p/ssd-holder-for-editing-stations-v2\"&gt;product&lt;/a&gt; I designed to help eliminate clutter on creative&amp;#39;s desks. If this is something you&amp;#39;d find useful, I&amp;#39;m going to leave a link here to order one! Thanks in advance for any support. &lt;a href=\"https://www.3dfilmworks.com/store/p/ssd-holder-for-editing-stations-v2\"&gt;https://www.3dfilmworks.com/store/p/ssd-holder-for-editing-stations-v2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/xmbz5fzxhe5b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?auto=webp&amp;v=enabled&amp;s=7b576ebd4dde5365fe0ca06d626ca57397415622", "width": 4536, "height": 8064}, "resolutions": [{"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37ea162b77be522b559df97c89b3e44f9626b8e9", "width": 108, "height": 192}, {"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f3f1750b8064b0b72b80b8fd1f5e363413113c8", "width": 216, "height": 384}, {"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=835d76f4f38edeb47ff28bca40ebbdadd06313e2", "width": 320, "height": 568}, {"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81ba4026cee56485f534dac1a8e832190e94a0c9", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02958f2d6cb259242ac20f1f6b3c729671ea164d", "width": 960, "height": 1706}, {"url": "https://preview.redd.it/xmbz5fzxhe5b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=541319a47b0717cafaa5a1e4478d2d624a3057ea", "width": 1080, "height": 1920}], "variants": {}, "id": "tbasTksNOwPbEEqbVfFWp4ZVaKfvd84LQdjq_6hd218"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146vn1i", "is_robot_indexable": true, "report_reasons": null, "author": "erichernandez91", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146vn1i/ssd_cable_organizer_cleaning_creatives_clutter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/xmbz5fzxhe5b1.jpg", "subreddit_subscribers": 687337, "created_utc": 1686494478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Though I haven't been on reddit for as long as many of you this was one of my favorite communities. It's a bummer how things are looking with the recent events with reddit leadership. \n\nWould people be interested a mastodon server as a potential replacement? I know there's the discord, I'm on there. But I'd much prefer to have our own space operated on our own hardware (aka rented by us) by people we trust rather than some company. \n\nThoughts?", "author_fullname": "t2_fsm385o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interest in DH Mastodon instance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1463zn5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686413137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Though I haven&amp;#39;t been on reddit for as long as many of you this was one of my favorite communities. It&amp;#39;s a bummer how things are looking with the recent events with reddit leadership. &lt;/p&gt;\n\n&lt;p&gt;Would people be interested a mastodon server as a potential replacement? I know there&amp;#39;s the discord, I&amp;#39;m on there. But I&amp;#39;d much prefer to have our own space operated on our own hardware (aka rented by us) by people we trust rather than some company. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "32TB + Cloud", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1463zn5", "is_robot_indexable": true, "report_reasons": null, "author": "Specktr", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1463zn5/interest_in_dh_mastodon_instance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1463zn5/interest_in_dh_mastodon_instance/", "subreddit_subscribers": 687337, "created_utc": 1686413137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's always been a concern of mine that stuff that exists on the Internet may one day just disappear. This is a problem, because I use the Internet as my extended memory: instead of remembering stuff I just remember how to find stuff. Imagine how horrible having an amnesia would be. Well, deleting stuff off the Internet (or gatekeeping access to public domain info) is like causing our collective super-intelligence to have an amnesia.\n\nKeep hoarding, and thank you for your service! You're doing a service to humanity!", "author_fullname": "t2_a3a159dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just found this sub, I am thankful that people like you exist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146v425", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Thankyou", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686493139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s always been a concern of mine that stuff that exists on the Internet may one day just disappear. This is a problem, because I use the Internet as my extended memory: instead of remembering stuff I just remember how to find stuff. Imagine how horrible having an amnesia would be. Well, deleting stuff off the Internet (or gatekeeping access to public domain info) is like causing our collective super-intelligence to have an amnesia.&lt;/p&gt;\n\n&lt;p&gt;Keep hoarding, and thank you for your service! You&amp;#39;re doing a service to humanity!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146v425", "is_robot_indexable": true, "report_reasons": null, "author": "bitcoincashautist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146v425/just_found_this_sub_i_am_thankful_that_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146v425/just_found_this_sub_i_am_thankful_that_people/", "subreddit_subscribers": 687337, "created_utc": 1686493139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know there is a 1000 post limit on the reddit API, but could I get a list of IDs of every post in a subreddit then download the posts by IDs using the API?  I guess I would need to scrape reddit to do this. Does such a tool exist? Thank you.", "author_fullname": "t2_jzkm8as0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Completely archiving subreddits not on pushshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146dazo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686436685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there is a 1000 post limit on the reddit API, but could I get a list of IDs of every post in a subreddit then download the posts by IDs using the API?  I guess I would need to scrape reddit to do this. Does such a tool exist? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146dazo", "is_robot_indexable": true, "report_reasons": null, "author": "itszedfsnotzeefs", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146dazo/completely_archiving_subreddits_not_on_pushshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146dazo/completely_archiving_subreddits_not_on_pushshift/", "subreddit_subscribers": 687337, "created_utc": 1686436685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a list of about 10,000 URLs of various Reddit comment threads. I would like to archive the contents locally. These are not linked to any particular Reddit account or subreddit.\n\nIs there a way to automatically download the entire comment threads (including the original post / link) from each of those URLs?\n\nI'm a barely-computer-literate Windows user. I don't know how to use CLIs, Python scripts, Docker images, or web servers, but I can follow a step-by-step dummy's guide. I can install Linux Mint or Raspbian on a fresh machine if needed.\n\nI've seen this tool by u/Ailothaen: https://github.com/Ailothaen/RedditArchiver\nThis looks like it would generate the output in a format that I need. However, from the screenshots it appears to work on one URL at a time.\n\nSome of the posts I want are not on archive.org in their entirety. I'm aware of the ArchiveTeam project, but I don't know how to access their data, or pull a particular post using an URL.\n\nI'd really appreciate any help, or links to resources. Thank you!", "author_fullname": "t2_gq40u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass-archiving Reddit comment threads from a list of URLs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146b4yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686431246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a list of about 10,000 URLs of various Reddit comment threads. I would like to archive the contents locally. These are not linked to any particular Reddit account or subreddit.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to automatically download the entire comment threads (including the original post / link) from each of those URLs?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a barely-computer-literate Windows user. I don&amp;#39;t know how to use CLIs, Python scripts, Docker images, or web servers, but I can follow a step-by-step dummy&amp;#39;s guide. I can install Linux Mint or Raspbian on a fresh machine if needed.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen this tool by &lt;a href=\"/u/Ailothaen\"&gt;u/Ailothaen&lt;/a&gt;: &lt;a href=\"https://github.com/Ailothaen/RedditArchiver\"&gt;https://github.com/Ailothaen/RedditArchiver&lt;/a&gt;\nThis looks like it would generate the output in a format that I need. However, from the screenshots it appears to work on one URL at a time.&lt;/p&gt;\n\n&lt;p&gt;Some of the posts I want are not on archive.org in their entirety. I&amp;#39;m aware of the ArchiveTeam project, but I don&amp;#39;t know how to access their data, or pull a particular post using an URL.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate any help, or links to resources. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?auto=webp&amp;v=enabled&amp;s=5dd738ddecc2863758b8b05832d2111c2e0bbfee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=19662ab0e7825663460336b4a3414fcf9c02f2ca", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=507d1fcd4cd6af027159a95c5f96cf82530c796e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b688a2ca61c9b369c2d5213e3f45e1b91d1766ac", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ff8a787c16b93e0058c0a30b88a28b0f78e8394", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb0c029a818b71f33f024cfd449168c11cd588bd", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/68-Hq7BnHna2i3vGBtgUPVmwrQ44thXGsramb-oF_K8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9790f2065419b1328d96ce3fd0b0356e44419b73", "width": 1080, "height": 540}], "variants": {}, "id": "xzWIieX_uPh5GVSnThUvDs4kSPVFuTepLV5dCuu01qQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146b4yd", "is_robot_indexable": true, "report_reasons": null, "author": "adpj3fns", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146b4yd/massarchiving_reddit_comment_threads_from_a_list/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146b4yd/massarchiving_reddit_comment_threads_from_a_list/", "subreddit_subscribers": 687337, "created_utc": 1686431246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\ncould anyone recommend me an affordable technology that'd allow me to reliably cold store few hundreds of GB (of my serial stash) for a long time (30 years plus)? I'd like to store it without power on long-term, hence why internal HDDs nor NASes are not desired.\n\nI'm thinking of M-Disc, but I don't currently have any burner on hand.", "author_fullname": "t2_6m008lir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long-term cold data archival", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1464ukp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686417656.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686415298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;could anyone recommend me an affordable technology that&amp;#39;d allow me to reliably cold store few hundreds of GB (of my serial stash) for a long time (30 years plus)? I&amp;#39;d like to store it without power on long-term, hence why internal HDDs nor NASes are not desired.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of M-Disc, but I don&amp;#39;t currently have any burner on hand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1464ukp", "is_robot_indexable": true, "report_reasons": null, "author": "creeper6530", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1464ukp/longterm_cold_data_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1464ukp/longterm_cold_data_archival/", "subreddit_subscribers": 687337, "created_utc": 1686415298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd ideally like to save the content of the post / comment text that I bookmarked instead of just a URL. Many communities are going dark and content may not become public again.  I'm wondering if there is some API-based service that saves to PDF or something similar. \n\nSame for my user posts &amp; comments. \n\nIDK what the future of this platform holds, but I'd like to preserve the content from here that has been helpful, transformative, or just the most excellent pictures of cats.", "author_fullname": "t2_1qrsw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to save / download reddit bookmarks to offsite/offline before all APIs are nuked?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1468h5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686424427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d ideally like to save the content of the post / comment text that I bookmarked instead of just a URL. Many communities are going dark and content may not become public again.  I&amp;#39;m wondering if there is some API-based service that saves to PDF or something similar. &lt;/p&gt;\n\n&lt;p&gt;Same for my user posts &amp;amp; comments. &lt;/p&gt;\n\n&lt;p&gt;IDK what the future of this platform holds, but I&amp;#39;d like to preserve the content from here that has been helpful, transformative, or just the most excellent pictures of cats.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1468h5z", "is_robot_indexable": true, "report_reasons": null, "author": "astaramence", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1468h5z/whats_the_best_way_to_save_download_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1468h5z/whats_the_best_way_to_save_download_reddit/", "subreddit_subscribers": 687337, "created_utc": 1686424427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I am not a very techie person, but in light of recent Reddit events, i want to save wikis of the subbredits I am subscribed to. How would you recommend going about this for someone who is absolutely not a tech guru? I am looking for something that won't require me learning python etc. and can be done in a day. Thanks", "author_fullname": "t2_nkzqvk4f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to easily and quickly save all my subbreddit's wikis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146t26z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686487548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I am not a very techie person, but in light of recent Reddit events, i want to save wikis of the subbredits I am subscribed to. How would you recommend going about this for someone who is absolutely not a tech guru? I am looking for something that won&amp;#39;t require me learning python etc. and can be done in a day. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146t26z", "is_robot_indexable": true, "report_reasons": null, "author": "kitsuneterminator400", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146t26z/how_to_easily_and_quickly_save_all_my_subbreddits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146t26z/how_to_easily_and_quickly_save_all_my_subbreddits/", "subreddit_subscribers": 687337, "created_utc": 1686487548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Given the API protests going on I fear some subreddits may be deleted or lost. Do we have any full backups of reddit?", "author_fullname": "t2_l9loi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a complete backup of reddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146msr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686465997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given the API protests going on I fear some subreddits may be deleted or lost. Do we have any full backups of reddit?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146msr8", "is_robot_indexable": true, "report_reasons": null, "author": "yogopig", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146msr8/is_there_a_complete_backup_of_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146msr8/is_there_a_complete_backup_of_reddit/", "subreddit_subscribers": 687337, "created_utc": 1686465997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 100GB of family photos, videos, and documents. I plan to use 7zip to encrypt and compress this. I am wondering if anyone knows a free website or offsite location to upload my encrypted file or files as a offsite backup. I\u2019m willing to split my files into like part 1, 2, etc in like 5G files each.", "author_fullname": "t2_aiwum8a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Cloud Data Backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146953y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686426152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 100GB of family photos, videos, and documents. I plan to use 7zip to encrypt and compress this. I am wondering if anyone knows a free website or offsite location to upload my encrypted file or files as a offsite backup. I\u2019m willing to split my files into like part 1, 2, etc in like 5G files each.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16.1 TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146953y", "is_robot_indexable": true, "report_reasons": null, "author": "BradenTheLegend", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/146953y/free_cloud_data_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146953y/free_cloud_data_backup/", "subreddit_subscribers": 687337, "created_utc": 1686426152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have bought two Toshiba MG08ACA16TE 16TB internal HDDs a few weeks ago.\n\nI have noticed that one of them makes clicking sounds after spinning up at bootup.\nIs it dying already!?\n\nSMART says it's fine, also it behaves normally when I use it.\n\nI have linked a sound of the bootup, it's at about second 12.", "author_fullname": "t2_cuqg6y5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New HDD failing? Getting clicking sounds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146jczn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1686454322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "on.soundcloud.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have bought two Toshiba MG08ACA16TE 16TB internal HDDs a few weeks ago.&lt;/p&gt;\n\n&lt;p&gt;I have noticed that one of them makes clicking sounds after spinning up at bootup.\nIs it dying already!?&lt;/p&gt;\n\n&lt;p&gt;SMART says it&amp;#39;s fine, also it behaves normally when I use it.&lt;/p&gt;\n\n&lt;p&gt;I have linked a sound of the bootup, it&amp;#39;s at about second 12.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://on.soundcloud.com/c4z57", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146jczn", "is_robot_indexable": true, "report_reasons": null, "author": "ACrossingTroll", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146jczn/new_hdd_failing_getting_clicking_sounds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://on.soundcloud.com/c4z57", "subreddit_subscribers": 687337, "created_utc": 1686454322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been saying for years that the only remaining 1TB 2.5\" CMR drive was the WD Red. Then learned a while back that Seagate has a 1.2TB (2x 600GB platter) CMR drive in their Exos line. There's also a 2.4TB CMR drive in the line.\n\nThen today, I checked the Toshiba site because of another thread and to my surprise see that Toshiba has a 1TB 2.5\" CMR drive! But it's OEM only. \n\nHere's the spec sheet for Toshiba 2.5\" drives: [https://storage.toshiba.com/docs/support-docs/L200-SalesSheet\\_English\\_Web\\_r2.pdf](https://storage.toshiba.com/docs/support-docs/L200-SalesSheet_English_Web_r2.pdf)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae\n\nhttps://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=45df751fc6e6943291c29683e415f3a273ed1858\n\nNote that the 1TB CMR drive is bulk only and is 9.5mm. Which makes sense since it would have to be two 500GB platters.\n\nThe 1TB SMR drive is 7mm because it only has a single platter. ", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TIL - There is a 2.5\" 1TB CMR drive from Toshiba. But it's OEM/Bare only.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"y5xjvqn1j85b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f29936595eed553cef72331e38c8118778e0c06"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ce1682b60c0da3728d3048c6d11c88b6dd12a31"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81e08e8c4aa2b1f5f6172c596b1162680f122b42"}, {"y": 329, "x": 640, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a68e7dbbe764f0752429a807ee7558b321f06fa"}, {"y": 493, "x": 960, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da0598a542adaaca8f85c4c4a67549870ebc6d9c"}, {"y": 555, "x": 1080, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61ecc44c0aadc5aaec642c7507137758aa9e64d7"}], "s": {"y": 652, "x": 1268, "u": "https://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae"}, "id": "y5xjvqn1j85b1"}, "u5q5eu0hj85b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 17, "x": 108, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fe3afcc02fa460694945508edcedea26a54766e"}, {"y": 34, "x": 216, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a55bfcea14ef0dedb150c940288079eb428f1a6e"}, {"y": 50, "x": 320, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=18fd29ca9b941a075db2fa57e0d9869f9b6c8f27"}, {"y": 101, "x": 640, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35e9618c710a9ef706bcdf20addf063c65970e46"}, {"y": 151, "x": 960, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c81f80c2692faa5d48bd2ebe9c7881192402f10d"}, {"y": 170, "x": 1080, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92e3c189cb34ec8567da5b02c9d0a310ac1a4c2c"}], "s": {"y": 187, "x": 1183, "u": "https://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=45df751fc6e6943291c29683e415f3a273ed1858"}, "id": "u5q5eu0hj85b1"}}, "name": "t3_1467pi0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FX4KJ2aexDxyAyytzprdqvWFbMSNmhSvtW_25-2Ns7w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686422486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been saying for years that the only remaining 1TB 2.5&amp;quot; CMR drive was the WD Red. Then learned a while back that Seagate has a 1.2TB (2x 600GB platter) CMR drive in their Exos line. There&amp;#39;s also a 2.4TB CMR drive in the line.&lt;/p&gt;\n\n&lt;p&gt;Then today, I checked the Toshiba site because of another thread and to my surprise see that Toshiba has a 1TB 2.5&amp;quot; CMR drive! But it&amp;#39;s OEM only. &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the spec sheet for Toshiba 2.5&amp;quot; drives: &lt;a href=\"https://storage.toshiba.com/docs/support-docs/L200-SalesSheet_English_Web_r2.pdf\"&gt;https://storage.toshiba.com/docs/support-docs/L200-SalesSheet_English_Web_r2.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae\"&gt;https://preview.redd.it/y5xjvqn1j85b1.png?width=1268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b344efc7a43bf62c760e0fa10acd7763b9fd5aae&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=45df751fc6e6943291c29683e415f3a273ed1858\"&gt;https://preview.redd.it/u5q5eu0hj85b1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=45df751fc6e6943291c29683e415f3a273ed1858&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note that the 1TB CMR drive is bulk only and is 9.5mm. Which makes sense since it would have to be two 500GB platters.&lt;/p&gt;\n\n&lt;p&gt;The 1TB SMR drive is 7mm because it only has a single platter. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1467pi0", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1467pi0/til_there_is_a_25_1tb_cmr_drive_from_toshiba_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1467pi0/til_there_is_a_25_1tb_cmr_drive_from_toshiba_but/", "subreddit_subscribers": 687337, "created_utc": 1686422486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Follow up to [this](https://www.reddit.com/r/DataHoarder/comments/10fg8f3/issuu_making_changes_that_will_make_a_lot_of/) (I'm not OP).\n\nStarting from June 19, another limitation is going to be applied to free Basic plan:\n\n&gt;**Publishing limit:** Basic users will be limited to 5 published documents.  \n&gt;  \n&gt;Starting on June 19, 2023, we will begin a phased rollout to all Basic accounts limiting the number of published documents. Any existing uploads on a Basic account that exceed the above limitation will continue to be hosted on Issuu but not accessible to readers.\u00a0\n\nThey're basically killing the free plan, and making a lot of content unavaliable forever (the premium plan is at least $19 per month, IMHO too expensive for most publishers, without counting the \"zoombie publishers\" that are not even going to see what's happening)", "author_fullname": "t2_81fvh23e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issuu making changes that will make a lot of content unavaliable (yes, again)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146pmrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686476305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Follow up to &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10fg8f3/issuu_making_changes_that_will_make_a_lot_of/\"&gt;this&lt;/a&gt; (I&amp;#39;m not OP).&lt;/p&gt;\n\n&lt;p&gt;Starting from June 19, another limitation is going to be applied to free Basic plan:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Publishing limit:&lt;/strong&gt; Basic users will be limited to 5 published documents.  &lt;/p&gt;\n\n&lt;p&gt;Starting on June 19, 2023, we will begin a phased rollout to all Basic accounts limiting the number of published documents. Any existing uploads on a Basic account that exceed the above limitation will continue to be hosted on Issuu but not accessible to readers.\u00a0&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;They&amp;#39;re basically killing the free plan, and making a lot of content unavaliable forever (the premium plan is at least $19 per month, IMHO too expensive for most publishers, without counting the &amp;quot;zoombie publishers&amp;quot; that are not even going to see what&amp;#39;s happening)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146pmrp", "is_robot_indexable": true, "report_reasons": null, "author": "CheesecakeNo7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146pmrp/issuu_making_changes_that_will_make_a_lot_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146pmrp/issuu_making_changes_that_will_make_a_lot_of/", "subreddit_subscribers": 687337, "created_utc": 1686476305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m on a upgrade kick at the moment and recently upgraded my laptop to a ssd hard drive and looking to do the same to my wife\u2019s. I downloaded and used Acronis cloning software for my transfer and everything worked great, so I\u2019m partial to using the same for my wife\u2019s. But her laptop is so bogged down that I couldn\u2019t imagine trying to download the program to her laptop without crashing it. Would it be possible to connect her old hdd and new ssd to my computer and just use acronis to clone it. From my basic understanding, I can safely plug in another hdd while my computer is running and then thru acronis, I choose it as my \u201cdisk to be cloned\u201d from the list and then choose the new ssd as the \u201cdestination disk\u201d. Seems logical to me, but computers are far from my expertise", "author_fullname": "t2_o0shmlnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hdd to ssd clone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146j8tc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686453963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m on a upgrade kick at the moment and recently upgraded my laptop to a ssd hard drive and looking to do the same to my wife\u2019s. I downloaded and used Acronis cloning software for my transfer and everything worked great, so I\u2019m partial to using the same for my wife\u2019s. But her laptop is so bogged down that I couldn\u2019t imagine trying to download the program to her laptop without crashing it. Would it be possible to connect her old hdd and new ssd to my computer and just use acronis to clone it. From my basic understanding, I can safely plug in another hdd while my computer is running and then thru acronis, I choose it as my \u201cdisk to be cloned\u201d from the list and then choose the new ssd as the \u201cdestination disk\u201d. Seems logical to me, but computers are far from my expertise&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146j8tc", "is_robot_indexable": true, "report_reasons": null, "author": "Tx_0618", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146j8tc/hdd_to_ssd_clone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146j8tc/hdd_to_ssd_clone/", "subreddit_subscribers": 687337, "created_utc": 1686453963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seeing as many subreddits will be going dark in a few days (some for only a few days, some supposedly permanently), I am looking for a way to back up my saved reddit posts and threads. Some threads I want to save in their entirety (or as much as I can get, stuff like AskReddit threads with lots of funny responses or useful information) and some I just want the video/picture/gif that's the main post. There have been a lot of posts about backing up your reddit account that I've found, but they've either seemed too limited (requesting GDPR data, which isn't really what I want) or too technical (programming scripts and stuff). While I'm not necessarily against the technical stuff, I don't have any background in using it and worry about messing it up. Does anyone have an idiot-proof guide on how to download saved posts? I don't mind doing it manually per post - honestly I'd prefer it for full control - but I just don't know where to get started.", "author_fullname": "t2_g2nib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Request: Idiot-friendly method for backing up saved reddit threads/posts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146dfry", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686437035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeing as many subreddits will be going dark in a few days (some for only a few days, some supposedly permanently), I am looking for a way to back up my saved reddit posts and threads. Some threads I want to save in their entirety (or as much as I can get, stuff like AskReddit threads with lots of funny responses or useful information) and some I just want the video/picture/gif that&amp;#39;s the main post. There have been a lot of posts about backing up your reddit account that I&amp;#39;ve found, but they&amp;#39;ve either seemed too limited (requesting GDPR data, which isn&amp;#39;t really what I want) or too technical (programming scripts and stuff). While I&amp;#39;m not necessarily against the technical stuff, I don&amp;#39;t have any background in using it and worry about messing it up. Does anyone have an idiot-proof guide on how to download saved posts? I don&amp;#39;t mind doing it manually per post - honestly I&amp;#39;d prefer it for full control - but I just don&amp;#39;t know where to get started.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146dfry", "is_robot_indexable": true, "report_reasons": null, "author": "KnightPlutonian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146dfry/request_idiotfriendly_method_for_backing_up_saved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146dfry/request_idiotfriendly_method_for_backing_up_saved/", "subreddit_subscribers": 687337, "created_utc": 1686437035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to watch a seminar that I can\u2019t attend live today.  Is there a way to record that seminar and have it go directly to a cloud service?  From either my phone or laptop?\n\n(So that it won\u2019t be stored in my device)\n\nThank you.", "author_fullname": "t2_vfugp2oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to store data directly from screen recording on a lap top?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_146wms3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686496901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to watch a seminar that I can\u2019t attend live today.  Is there a way to record that seminar and have it go directly to a cloud service?  From either my phone or laptop?&lt;/p&gt;\n\n&lt;p&gt;(So that it won\u2019t be stored in my device)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146wms3", "is_robot_indexable": true, "report_reasons": null, "author": "Afraid_Bus_5756", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146wms3/is_there_a_way_to_store_data_directly_from_screen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146wms3/is_there_a_way_to_store_data_directly_from_screen/", "subreddit_subscribers": 687337, "created_utc": 1686496901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to search for a YouTube video based on whether the video contains a portion of a song? Essentially I'm trying to find a video whose title or creator I can't remember because it's been so long, but I remember vividly a song that was used in it. When I try using search terms for actions or \"catch phrases\" from what I remember in the video, it is too general, is totally misleading, and I can't get any good results. Honestly, I'm not even sure if this video exists on youtube anymore, but I know it contained a pretty unique song so I figured that could be a good indicator, so I figured this was worth a shot. Anyone know how to do this search or is it too convoluted? If anyone knew how to do such a thing, I'd imagine someone in this community can help. Thanks in advance!!", "author_fullname": "t2_74i2uwce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you search a youtube video based on whether it contains a particular song?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_146wex0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686496360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to search for a YouTube video based on whether the video contains a portion of a song? Essentially I&amp;#39;m trying to find a video whose title or creator I can&amp;#39;t remember because it&amp;#39;s been so long, but I remember vividly a song that was used in it. When I try using search terms for actions or &amp;quot;catch phrases&amp;quot; from what I remember in the video, it is too general, is totally misleading, and I can&amp;#39;t get any good results. Honestly, I&amp;#39;m not even sure if this video exists on youtube anymore, but I know it contained a pretty unique song so I figured that could be a good indicator, so I figured this was worth a shot. Anyone know how to do this search or is it too convoluted? If anyone knew how to do such a thing, I&amp;#39;d imagine someone in this community can help. Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146wex0", "is_robot_indexable": true, "report_reasons": null, "author": "humelectra2000", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146wex0/can_you_search_a_youtube_video_based_on_whether/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146wex0/can_you_search_a_youtube_video_based_on_whether/", "subreddit_subscribers": 687337, "created_utc": 1686496360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://jff.jpf.go.jp/watch/independent-cinema/\n\nThe JAPANESE FILM FESTIVAL ONLINE's program, a new event part of the Japanese government's Japan Foundation arts and cultural promotion, will all be taken down in a couple days. I'm hoping they'll be able to be archived/preserved somehow \u2013 and I fear these films might be too niche for usual piracy release groups to do WEB-DLs of the content.\n\nThey're free to watch online once you register with an email/password, and it looks like the content is using Microsoft PlayReady DRM, with the .mp4s and .mpd (MPEG DASH streaming setup) viewable in the browser. You just need to get a key (probably using Widevine L3 Decryptor or the like &amp; the PSSH keys from the .mpd) and then Bento4's [mp4decrypt](https://www.bento4.com/documentation/mp4decrypt/) to decode the mp4 files, and ffmpeg to mux together video and audio.\n\nI've never done this before and am running into some issues but hopefully someone else is familiar with this all, or the above info helps people (there's a good amount of similar information in forum posts on videohelp.com as well).", "author_fullname": "t2_dyqly0md", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Japanese Film Festival's free-to-view films will go offline on Wednesday", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146vcbc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "PSA/Request", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686493730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://jff.jpf.go.jp/watch/independent-cinema/\"&gt;https://jff.jpf.go.jp/watch/independent-cinema/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The JAPANESE FILM FESTIVAL ONLINE&amp;#39;s program, a new event part of the Japanese government&amp;#39;s Japan Foundation arts and cultural promotion, will all be taken down in a couple days. I&amp;#39;m hoping they&amp;#39;ll be able to be archived/preserved somehow \u2013 and I fear these films might be too niche for usual piracy release groups to do WEB-DLs of the content.&lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re free to watch online once you register with an email/password, and it looks like the content is using Microsoft PlayReady DRM, with the .mp4s and .mpd (MPEG DASH streaming setup) viewable in the browser. You just need to get a key (probably using Widevine L3 Decryptor or the like &amp;amp; the PSSH keys from the .mpd) and then Bento4&amp;#39;s &lt;a href=\"https://www.bento4.com/documentation/mp4decrypt/\"&gt;mp4decrypt&lt;/a&gt; to decode the mp4 files, and ffmpeg to mux together video and audio.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never done this before and am running into some issues but hopefully someone else is familiar with this all, or the above info helps people (there&amp;#39;s a good amount of similar information in forum posts on videohelp.com as well).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?auto=webp&amp;v=enabled&amp;s=e1e388d2e637e5d108fd213ddebe1eae6cbba128", "width": 1920, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ae3a7ca8c003fff0f752d6be0b4e418301e31a7", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=706dbab0e15a60ce49b024f0981efbe34aa1d7a6", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=293fd240912c8e302e91ba92221668eb0cc32160", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d631f82d051ea4e1e9ae7e6bfd560a5fdb0577b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ca301e6c23e5279332de7556d8551d3fc8df62c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/F7eFNIyeLmAF3ezJMnuQMWVZBUJSlq8VsD7K40UyoK4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3694df1b3976505ff0aa3bab3b05d7584a31c2b7", "width": 1080, "height": 720}], "variants": {}, "id": "8iZAZJFbhCewb5MZAStsY8NC8PwXm4N1I8uSJnnWvtI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "146vcbc", "is_robot_indexable": true, "report_reasons": null, "author": "throwra8138", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146vcbc/japanese_film_festivals_freetoview_films_will_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146vcbc/japanese_film_festivals_freetoview_films_will_go/", "subreddit_subscribers": 687337, "created_utc": 1686493730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI'd like to know if there's a tool that can automatically download all media and all tweets from a GDPR data request, like \"saving\" everything offline: tweets, images, videos that are linked to my account (that I liked, retweeted, bookmarked and posted).", "author_fullname": "t2_sufg5mng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Retrieve all media from Twitter GDPR data request?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146sxe0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686487155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if there&amp;#39;s a tool that can automatically download all media and all tweets from a GDPR data request, like &amp;quot;saving&amp;quot; everything offline: tweets, images, videos that are linked to my account (that I liked, retweeted, bookmarked and posted).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146sxe0", "is_robot_indexable": true, "report_reasons": null, "author": "calmingcroco", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146sxe0/retrieve_all_media_from_twitter_gdpr_data_request/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146sxe0/retrieve_all_media_from_twitter_gdpr_data_request/", "subreddit_subscribers": 687337, "created_utc": 1686487155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nPerhaps ya'll can help. I recently picked up a Quantum LTO-6 external tape drive for my Ubuntu setup here at home. Got it all setup, LTFS working, all that good stuff.\n\nNow I need to begin the process of actually moving data onto there (thanks Google, re GSuite Business changes).\n\nWorst case scenario I'll just script something up, but I can't imagine there isn't something out there that probably does exactly what I'm looking for and most likely more.\n\nBasically, I'm looking to do two things:\n\n1.) Copy &amp; verify data written to the tape cartridge. On Windows, I'd use something like TeraCopy for this. IIRC rsync can do this.\n\n2.) Catalog the data that is written to the tape cartridge so I know exactly what files (names, size, directories, etc) are on the cartridge. For example, after copying a bunch of files, I'd run \"catalog\" or something, and it would basically scan the entire tape and output a browsable, offline (no tape cartridge in the drive required), webpage or some crap I could open and see a directory/file tree/etc.\n\nMy requirements are very broad/relaxed and like I said, if I can't really find anything I'll probably just script something up.\n\nI must be using the wrong terminology or something (and IIRC reading the documentation, cataloging/indexing is already terminology used for other tape related functions) because I can't find a lot of information on tools that do this type of stuff - primarily the cataloging I mentioned above.\n\nThis would be for use on a Ubuntu 23.04 Desktop install. GUI would be preferred, but not at all married to that.\n\nDoes anyone here do anything like this, or know of tools to do so?\n\nThanks much!", "author_fullname": "t2_lm6eu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux tools for tape backup management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146lkfo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686461706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Perhaps ya&amp;#39;ll can help. I recently picked up a Quantum LTO-6 external tape drive for my Ubuntu setup here at home. Got it all setup, LTFS working, all that good stuff.&lt;/p&gt;\n\n&lt;p&gt;Now I need to begin the process of actually moving data onto there (thanks Google, re GSuite Business changes).&lt;/p&gt;\n\n&lt;p&gt;Worst case scenario I&amp;#39;ll just script something up, but I can&amp;#39;t imagine there isn&amp;#39;t something out there that probably does exactly what I&amp;#39;m looking for and most likely more.&lt;/p&gt;\n\n&lt;p&gt;Basically, I&amp;#39;m looking to do two things:&lt;/p&gt;\n\n&lt;p&gt;1.) Copy &amp;amp; verify data written to the tape cartridge. On Windows, I&amp;#39;d use something like TeraCopy for this. IIRC rsync can do this.&lt;/p&gt;\n\n&lt;p&gt;2.) Catalog the data that is written to the tape cartridge so I know exactly what files (names, size, directories, etc) are on the cartridge. For example, after copying a bunch of files, I&amp;#39;d run &amp;quot;catalog&amp;quot; or something, and it would basically scan the entire tape and output a browsable, offline (no tape cartridge in the drive required), webpage or some crap I could open and see a directory/file tree/etc.&lt;/p&gt;\n\n&lt;p&gt;My requirements are very broad/relaxed and like I said, if I can&amp;#39;t really find anything I&amp;#39;ll probably just script something up.&lt;/p&gt;\n\n&lt;p&gt;I must be using the wrong terminology or something (and IIRC reading the documentation, cataloging/indexing is already terminology used for other tape related functions) because I can&amp;#39;t find a lot of information on tools that do this type of stuff - primarily the cataloging I mentioned above.&lt;/p&gt;\n\n&lt;p&gt;This would be for use on a Ubuntu 23.04 Desktop install. GUI would be preferred, but not at all married to that.&lt;/p&gt;\n\n&lt;p&gt;Does anyone here do anything like this, or know of tools to do so?&lt;/p&gt;\n\n&lt;p&gt;Thanks much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146lkfo", "is_robot_indexable": true, "report_reasons": null, "author": "xydrine", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/146lkfo/linux_tools_for_tape_backup_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146lkfo/linux_tools_for_tape_backup_management/", "subreddit_subscribers": 687337, "created_utc": 1686461706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am having a problem where as time goes on my mp3 files start to give problems. When I first play them back they give me no problems, however eventually they will start to cut off then play a random different song. I've attached an example of what I'm talking about [here](https://youtu.be/9-IFlXY08mc) (happens at around the 1:36 mark). \n\nIn words, imagine a 2 minute song plays normally for 1 minute with no issues, then at the 1 minute mark it somehow transforms into a completely different song as if someone edited the last minute of the song to replace its audio. Keep in mind that this is not a problem when the file is first downloaded.\n\nIs this a form of data corruption? Is something that you guys have experienced before? If so, is there someway to prevent this? Thank you.", "author_fullname": "t2_ao92sjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help regarding corrupted MP3 files.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146k6mb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686457027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am having a problem where as time goes on my mp3 files start to give problems. When I first play them back they give me no problems, however eventually they will start to cut off then play a random different song. I&amp;#39;ve attached an example of what I&amp;#39;m talking about &lt;a href=\"https://youtu.be/9-IFlXY08mc\"&gt;here&lt;/a&gt; (happens at around the 1:36 mark). &lt;/p&gt;\n\n&lt;p&gt;In words, imagine a 2 minute song plays normally for 1 minute with no issues, then at the 1 minute mark it somehow transforms into a completely different song as if someone edited the last minute of the song to replace its audio. Keep in mind that this is not a problem when the file is first downloaded.&lt;/p&gt;\n\n&lt;p&gt;Is this a form of data corruption? Is something that you guys have experienced before? If so, is there someway to prevent this? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?auto=webp&amp;v=enabled&amp;s=10227d881699acd2135996ff15f3f1a04eb5387f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=100875830905dc76337a1d35fcf1030ead2155b2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9abdfea6d90681f590300c6d7f16c6dc5394ae2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/OXUVC534R-X5gkvaHcE47G4e46ffeCorj8EOqL-xwJo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54b27712e4ccfbfed69d7087a698835aa4939798", "width": 320, "height": 240}], "variants": {}, "id": "7YHre2ibQTlOBomewwhG_5hh-BKv1_FZQgLZwTTF8uE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146k6mb", "is_robot_indexable": true, "report_reasons": null, "author": "freshcokecola", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146k6mb/help_regarding_corrupted_mp3_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146k6mb/help_regarding_corrupted_mp3_files/", "subreddit_subscribers": 687337, "created_utc": 1686457027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I upload 1TB/day on my 2TB M.2 drive, and so far I have 115TB read on it.. I don't know if that affects the lifespan of my drive, and I'm thinking of doing the uploading using an external hard drive, but I also don't know how long those last for constant reading.\n\nWhich is the more long-term budget friendly option? I thought SSDs will last longer but are pretty expensive while HDDs won't last as long (I've had 6 broken down in less than a year or extremely slow, mostly WD drives) but I'm not well informed\n\nThanks in advance", "author_fullname": "t2_bmlf894u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does total host reads affect SSD lifetime just like TBW? Should I upload with an HDD instead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_146hjub", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686448661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I upload 1TB/day on my 2TB M.2 drive, and so far I have 115TB read on it.. I don&amp;#39;t know if that affects the lifespan of my drive, and I&amp;#39;m thinking of doing the uploading using an external hard drive, but I also don&amp;#39;t know how long those last for constant reading.&lt;/p&gt;\n\n&lt;p&gt;Which is the more long-term budget friendly option? I thought SSDs will last longer but are pretty expensive while HDDs won&amp;#39;t last as long (I&amp;#39;ve had 6 broken down in less than a year or extremely slow, mostly WD drives) but I&amp;#39;m not well informed&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "146hjub", "is_robot_indexable": true, "report_reasons": null, "author": "colt45keyboard", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/146hjub/does_total_host_reads_affect_ssd_lifetime_just/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/146hjub/does_total_host_reads_affect_ssd_lifetime_just/", "subreddit_subscribers": 687337, "created_utc": 1686448661.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}