{"kind": "Listing", "data": {"after": "t3_142isd1", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hey\n\nI know paperless-ngx seems to be the default recommendation for document management systems, but given that's not the most exciting of topics I guess most often overlook it - but seriously, paperless has pretty much revolutionized my administrative life. \n\nI live between 4 countries so trust me when I say life is CHAOS. I scan EVERYTHING. Going from a zero automation flat dir structure in onedrive to paperless is just wow! \n\nIf you are even remotely busy and own a scanner, 11/10 would dedicate a couple hours to giving it a go. \n\nTo be clear, I am not at all associated with paperless in anyway, just a very happy end user \n\nIf you are a paperless developer - hi - feature request, please please please add rotation and document splitting. I often shove 50 pages through my scanners document feeder thinking \"Oh, ill sort that later\" - and its always a nightmare...", "author_fullname": "t2_njteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A note of appreciation for paperless ngx", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "textstorage", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142lntb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 327, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Text Storage", "can_mod_post": false, "score": 327, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686069302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;/p&gt;\n\n&lt;p&gt;I know paperless-ngx seems to be the default recommendation for document management systems, but given that&amp;#39;s not the most exciting of topics I guess most often overlook it - but seriously, paperless has pretty much revolutionized my administrative life. &lt;/p&gt;\n\n&lt;p&gt;I live between 4 countries so trust me when I say life is CHAOS. I scan EVERYTHING. Going from a zero automation flat dir structure in onedrive to paperless is just wow! &lt;/p&gt;\n\n&lt;p&gt;If you are even remotely busy and own a scanner, 11/10 would dedicate a couple hours to giving it a go. &lt;/p&gt;\n\n&lt;p&gt;To be clear, I am not at all associated with paperless in anyway, just a very happy end user &lt;/p&gt;\n\n&lt;p&gt;If you are a paperless developer - hi - feature request, please please please add rotation and document splitting. I often shove 50 pages through my scanners document feeder thinking &amp;quot;Oh, ill sort that later&amp;quot; - and its always a nightmare...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "38c528e4-7e68-11e9-baff-0ec5f304b28c", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142lntb", "is_robot_indexable": true, "report_reasons": null, "author": "InfaSyn", "discussion_type": null, "num_comments": 113, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142lntb/a_note_of_appreciation_for_paperless_ngx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142lntb/a_note_of_appreciation_for_paperless_ngx/", "subreddit_subscribers": 252939, "created_utc": 1686069302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Link: [github.com/azukaar/cosmos-Server/](https://github.com/azukaar/cosmos-Server/)\n\nHello everyone!!\n\nI'm super excited to announce that since my last update here a lot have happened for Cosmos. As a reminder, Cosmos is an all-in-one solution completely dedicated to self-hosting, that includes:\n\n&amp;#x200B;\n\n* **Reverse-Proxy** \ud83d\udd04\ud83d\udd17 Targeting containers, other servers, or serving static folders / SPA with **automatic HTTPS**, and a **nice UI**\n* **Authentication Server** \ud83d\udc66\ud83d\udc69 With strong security, **multi-factor authentication** and multiple strategies (**OpenId**, forward headers, HTML)\n* **Container manager** \ud83d\udc0b\ud83d\udd27  To easily manage your containers and their settings, keep them up to  date as well as audit their security. Includes docker-compose support!\n* **Identity Provider** \ud83d\udc66\ud83d\udc69 To easily manage your users, **invite your friends and family**  to your applications without awkardly sharing credentials. Let them  request a password change with an email rather than having you unlock  their account manually!\n* **SmartShield technology** \ud83e\udde0\ud83d\udee1  Automatically secure your applications without manual adjustments (see  below for more details). Includes anti-bot and anti-DDOS strategies.\n\n&amp;#x200B;\n\n[Some screenshot of URL management, and container management, as well as the login page. It is a modern UI, fully responsive for mobile and tablet](https://preview.redd.it/hezaemyxof4b1.png?width=1560&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6a7133cce2dce4a4d07694b21277658dee7875a5)\n\nThe new version released today just added experimental OpenID support, which allows you to login to apps such as Gitea, Nextcloud, etc.. using the user accounts managed in Cosmos directly.\n\n&amp;#x200B;\n\n[Example with Gitea](https://preview.redd.it/s3v63dg0pf4b1.png?width=1498&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=987c00445d99cd2a688fc0ca8e71e7f733bae9e1)\n\nLooking forward to receiving feedback on this new feature, and please check out the rest of the demo, I'm always open to hearing about people's opinion!\n\nThanks, happy hosting!", "author_fullname": "t2_1p1hpmlp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83c\udd95 Cosmos 0.6.0 - All in one secure Reverse-proxy, container manager and authentication provider now supports OpenID! Guides available in the documentation on how to setup Nextcloud, Minio and Gitea easily from the UI.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "productannouncement", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hezaemyxof4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 103, "x": 108, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f27fa0a3359f91ebc38b2315aa0fe4d63eb92c8"}, {"y": 207, "x": 216, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4864ddafffd48975bba8d70042fa2bb10429d89d"}, {"y": 307, "x": 320, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c8e05f560120f65a5340e8a5c417285f4a871a5"}, {"y": 615, "x": 640, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97832505bb7a032c9fc2fc0871fb3f8f75589877"}, {"y": 923, "x": 960, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a93f3e5f600b1c1f9964a9d144c8fc8ce8553f98"}, {"y": 1038, "x": 1080, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55efc6a48469584e4dfcfc6a45adb15dc0daaf88"}], "s": {"y": 1500, "x": 1560, "u": "https://preview.redd.it/hezaemyxof4b1.png?width=1560&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6a7133cce2dce4a4d07694b21277658dee7875a5"}, "id": "hezaemyxof4b1"}, "s3v63dg0pf4b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 91, "x": 108, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6554753a8664310e931742b9d4505a91331dfebb"}, {"y": 183, "x": 216, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08fd90654d2995b7cab9511a8421f777ed054776"}, {"y": 271, "x": 320, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a96e0beae75902e56997a0c90709ba98c7a1cb1"}, {"y": 543, "x": 640, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce452dd01d9d3a30a5d5436cf0099c77a33dd492"}, {"y": 815, "x": 960, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c203e80d68b80f427e07b6676c30b2bc157e5640"}, {"y": 917, "x": 1080, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d9d6d279468cf5f6c93e696a8a42abb7475740"}], "s": {"y": 1272, "x": 1498, "u": "https://preview.redd.it/s3v63dg0pf4b1.png?width=1498&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=987c00445d99cd2a688fc0ca8e71e7f733bae9e1"}, "id": "s3v63dg0pf4b1"}}, "name": "t3_142nfck", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 236, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Product Announcement", "can_mod_post": false, "score": 236, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kI1eZTbzUT8B47negdEVZhR6Q381jSHwaLKF0HIixrg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1686073147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Link: &lt;a href=\"https://github.com/azukaar/cosmos-Server/\"&gt;github.com/azukaar/cosmos-Server/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hello everyone!!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super excited to announce that since my last update here a lot have happened for Cosmos. As a reminder, Cosmos is an all-in-one solution completely dedicated to self-hosting, that includes:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Reverse-Proxy&lt;/strong&gt; \ud83d\udd04\ud83d\udd17 Targeting containers, other servers, or serving static folders / SPA with &lt;strong&gt;automatic HTTPS&lt;/strong&gt;, and a &lt;strong&gt;nice UI&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Authentication Server&lt;/strong&gt; \ud83d\udc66\ud83d\udc69 With strong security, &lt;strong&gt;multi-factor authentication&lt;/strong&gt; and multiple strategies (&lt;strong&gt;OpenId&lt;/strong&gt;, forward headers, HTML)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Container manager&lt;/strong&gt; \ud83d\udc0b\ud83d\udd27  To easily manage your containers and their settings, keep them up to  date as well as audit their security. Includes docker-compose support!&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Identity Provider&lt;/strong&gt; \ud83d\udc66\ud83d\udc69 To easily manage your users, &lt;strong&gt;invite your friends and family&lt;/strong&gt;  to your applications without awkardly sharing credentials. Let them  request a password change with an email rather than having you unlock  their account manually!&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;SmartShield technology&lt;/strong&gt; \ud83e\udde0\ud83d\udee1  Automatically secure your applications without manual adjustments (see  below for more details). Includes anti-bot and anti-DDOS strategies.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hezaemyxof4b1.png?width=1560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6a7133cce2dce4a4d07694b21277658dee7875a5\"&gt;Some screenshot of URL management, and container management, as well as the login page. It is a modern UI, fully responsive for mobile and tablet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The new version released today just added experimental OpenID support, which allows you to login to apps such as Gitea, Nextcloud, etc.. using the user accounts managed in Cosmos directly.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s3v63dg0pf4b1.png?width=1498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=987c00445d99cd2a688fc0ca8e71e7f733bae9e1\"&gt;Example with Gitea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Looking forward to receiving feedback on this new feature, and please check out the rest of the demo, I&amp;#39;m always open to hearing about people&amp;#39;s opinion!&lt;/p&gt;\n\n&lt;p&gt;Thanks, happy hosting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?auto=webp&amp;v=enabled&amp;s=62c072c4afced8052b89e0cf403e7a1d60f7c3ca", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d155e007977a258de17698ebfcde13250b670722", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5ca956f818d033bfbaaed5f4b524c170a303c9f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cec247ddebda5dce5d68c6bf869d9b386dc82217", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=980991f166f7341b3b5412c38815914ea07ac875", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39e33b01a13438c0e59ea29ee2eb7ebb8e1cc33c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tVZ1Zk_aqbREWiJ99EAmbj9KshvTTbmhiJfJcvRdk2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0cf45df9ac79e5cbed18946c1f114299ec4df47", "width": 1080, "height": 540}], "variants": {}, "id": "H73x7vsQxn13811Ol4uvOObTjFR29Q1d47oUADhsmJg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c7fc47c4-7e6b-11e9-b04d-0e70893a61b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "142nfck", "is_robot_indexable": true, "report_reasons": null, "author": "azukaar", "discussion_type": null, "num_comments": 104, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142nfck/cosmos_060_all_in_one_secure_reverseproxy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142nfck/cosmos_060_all_in_one_secure_reverseproxy/", "subreddit_subscribers": 252939, "created_utc": 1686073147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "[https://github.com/go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) Updates!  \n\n\n\ud83d\ude80\ud83d\udd25 Exciting news! LocalAI v1.18.0 is here with a stellar release packed full of new features, bug fixes, and updates! \ud83c\udf89\ud83d\udd25\n\nA huge shoutout to the amazing community for their invaluable help in making this a fantastic community-driven release! Thank you for your support and make the community grow! \ud83d\ude4c\n\n# What is LocalAI?\n\nLocalAI is the OpenAI compatible API that lets you run AI models locally on your own CPU! \ud83d\udcbb Data never leaves your machine! No need for  expensive cloud services or GPUs, LocalAI uses llama.cpp and ggml to power your AI projects! \ud83e\udd99\n\n# What's new?\n\nThis LocalAI release is plenty of new features, bugfixes and updates!  Thanks to the community for the help, this was a great community  release!\n\nWe now support a vast variety of models, while being backward  compatible with prior quantization formats, this new release allows  still to load older formats and new [k-quants](https://github.com/ggerganov/llama.cpp/pull/1684)!\n\n# New features\n\n* \u2728 Added support for falcon\\-based model families (7b)  ( [mudler](https://github.com/mudler) )\n* \u2728 Experimental support for Metal Apple Silicon GPU - ( [mudler](https://github.com/mudler) and thanks to u/Soleblaze for testing! ). See the [build section](https://localai.io/basics/build/#Acceleration).\n* \u2728 Support for token stream in the /v1/completions endpoint ( [samm81](https://github.com/samm81) )\n* \u2728 Added huggingface backend ( [Evilfreelancer](https://github.com/EvilFreelancer) )\n* \ud83d\udcf7 Stablediffusion now can output 2048x2048 images size with esrgan! ( [mudler](https://github.com/mudler) )\n\n# Container images\n\n* \ud83d\udc0b CUDA container images (arm64, x86\\_64) ( [sebastien-prudhomme](https://github.com/sebastien-prudhomme) )\n* \ud83d\udc0b FFmpeg container images (arm64, x86\\_64) ( [mudler](https://github.com/mudler) )\n\n# Dependencies updates\n\n* \ud83c\udd99 Bloomz has been updated to the latest ggml changes, including new quantization format ( [mudler](https://github.com/mudler) )\n* \ud83c\udd99 RWKV has been updated to the new quantization format( [mudler](https://github.com/mudler) )\n* \ud83c\udd99 [k-quants](https://github.com/ggerganov/llama.cpp/pull/1684) format support for the llama  \n models ( [mudler](https://github.com/mudler) )\n* \ud83c\udd99 gpt4all has been updated, incorporating upstream changes allowing  to load older models, and with different CPU instruction set (AVX only,  AVX2) from the same binary! ( [mudler](https://github.com/mudler) )\n\n# Generic\n\n* \ud83d\udc27 Fully Linux static binary releases ( [mudler](https://github.com/mudler) )\n* \ud83d\udcf7 Stablediffusion has been enabled on container images by default ( [mudler](https://github.com/mudler) ) Note: You can disable container image rebuilds with REBUILD=false\n\n# Examples\n\n* \ud83d\udca1 [AutoGPT](https://github.com/go-skynet/LocalAI/tree/master/examples/autoGPT) example ( [mudler](https://github.com/mudler) )\n* \ud83d\udca1 [PrivateGPT](https://github.com/go-skynet/LocalAI/tree/master/examples/privateGPT) example ( [mudler](https://github.com/mudler) )\n* \ud83d\udca1 [Flowise](https://github.com/go-skynet/LocalAI/tree/master/examples/flowise) example ( [mudler](https://github.com/mudler) )\n\nTwo new projects offer now direct integration with LocalAI!\n\n* [Flowise](https://github.com/FlowiseAI/Flowise/pull/123)\n* [Mods](https://github.com/charmbracelet/mods)\n\n[Full release changelog](https://github.com/go-skynet/LocalAI/releases/tag/v1.18.0)\n\n&amp;#x200B;\n\nThank you for your support, and happy hacking!", "author_fullname": "t2_g0k1wu3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LocalAI v1.18.0 release!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142uqn4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 199, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 199, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686088947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/go-skynet/LocalAI\"&gt;https://github.com/go-skynet/LocalAI&lt;/a&gt; Updates!  &lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80\ud83d\udd25 Exciting news! LocalAI v1.18.0 is here with a stellar release packed full of new features, bug fixes, and updates! \ud83c\udf89\ud83d\udd25&lt;/p&gt;\n\n&lt;p&gt;A huge shoutout to the amazing community for their invaluable help in making this a fantastic community-driven release! Thank you for your support and make the community grow! \ud83d\ude4c&lt;/p&gt;\n\n&lt;h1&gt;What is LocalAI?&lt;/h1&gt;\n\n&lt;p&gt;LocalAI is the OpenAI compatible API that lets you run AI models locally on your own CPU! \ud83d\udcbb Data never leaves your machine! No need for  expensive cloud services or GPUs, LocalAI uses llama.cpp and ggml to power your AI projects! \ud83e\udd99&lt;/p&gt;\n\n&lt;h1&gt;What&amp;#39;s new?&lt;/h1&gt;\n\n&lt;p&gt;This LocalAI release is plenty of new features, bugfixes and updates!  Thanks to the community for the help, this was a great community  release!&lt;/p&gt;\n\n&lt;p&gt;We now support a vast variety of models, while being backward  compatible with prior quantization formats, this new release allows  still to load older formats and new &lt;a href=\"https://github.com/ggerganov/llama.cpp/pull/1684\"&gt;k-quants&lt;/a&gt;!&lt;/p&gt;\n\n&lt;h1&gt;New features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\u2728 Added support for falcon-based model families (7b)  ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\u2728 Experimental support for Metal Apple Silicon GPU - ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; and thanks to &lt;a href=\"/u/Soleblaze\"&gt;u/Soleblaze&lt;/a&gt; for testing! ). See the &lt;a href=\"https://localai.io/basics/build/#Acceleration\"&gt;build section&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;\u2728 Support for token stream in the /v1/completions endpoint ( &lt;a href=\"https://github.com/samm81\"&gt;samm81&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\u2728 Added huggingface backend ( &lt;a href=\"https://github.com/EvilFreelancer\"&gt;Evilfreelancer&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83d\udcf7 Stablediffusion now can output 2048x2048 images size with esrgan! ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Container images&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\ud83d\udc0b CUDA container images (arm64, x86_64) ( &lt;a href=\"https://github.com/sebastien-prudhomme\"&gt;sebastien-prudhomme&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83d\udc0b FFmpeg container images (arm64, x86_64) ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Dependencies updates&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\ud83c\udd99 Bloomz has been updated to the latest ggml changes, including new quantization format ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83c\udd99 RWKV has been updated to the new quantization format( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83c\udd99 &lt;a href=\"https://github.com/ggerganov/llama.cpp/pull/1684\"&gt;k-quants&lt;/a&gt; format support for the llama&lt;br/&gt;\nmodels ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83c\udd99 gpt4all has been updated, incorporating upstream changes allowing  to load older models, and with different CPU instruction set (AVX only,  AVX2) from the same binary! ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Generic&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\ud83d\udc27 Fully Linux static binary releases ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83d\udcf7 Stablediffusion has been enabled on container images by default ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; ) Note: You can disable container image rebuilds with REBUILD=false&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Examples&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\ud83d\udca1 &lt;a href=\"https://github.com/go-skynet/LocalAI/tree/master/examples/autoGPT\"&gt;AutoGPT&lt;/a&gt; example ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83d\udca1 &lt;a href=\"https://github.com/go-skynet/LocalAI/tree/master/examples/privateGPT\"&gt;PrivateGPT&lt;/a&gt; example ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;li&gt;\ud83d\udca1 &lt;a href=\"https://github.com/go-skynet/LocalAI/tree/master/examples/flowise\"&gt;Flowise&lt;/a&gt; example ( &lt;a href=\"https://github.com/mudler\"&gt;mudler&lt;/a&gt; )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Two new projects offer now direct integration with LocalAI!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/FlowiseAI/Flowise/pull/123\"&gt;Flowise&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/charmbracelet/mods\"&gt;Mods&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/go-skynet/LocalAI/releases/tag/v1.18.0\"&gt;Full release changelog&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your support, and happy hacking!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?auto=webp&amp;v=enabled&amp;s=bcd7bfb38ec1002e85e94f43bdde96894e011ee0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e74c80851be11fbaac70d5ef4c7c041cb072cfe", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50a79e603293a769cf09363306b8016912b06d99", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=312657372ed13380b99edf291142005c14d26fd7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26146a544279201554cdb69b8d83e35c44198250", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4fe2e79ab45dbc8bf28f94f51badca13d204890", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QmsDfC5XJiVbqIq9J1VFfmejf_JVzPR_bB62-qUPGzo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ca227fe8d3f49d8578394b9e54561190ad7c8cc", "width": 1080, "height": 540}], "variants": {}, "id": "s-3Py4QBMcmGZhK82kMFFBqMbm--Iv0cn5Z_tIUNgqM"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 325, "id": "award_2bc47247-b107-44a8-a78c-613da21869ff", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Rocket_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Rocket_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Rocket_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Rocket_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Rocket_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Rocket_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Boldly go where we haven't been in a long, long time.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "To The Stars", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/1sof6d93g9e51_ToTheStars.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=06f2d039483185440c5e566f72737a920237569b", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/1sof6d93g9e51_ToTheStars.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=d0ecc136d90892f44d543d59e829c16e5fb88e47", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/1sof6d93g9e51_ToTheStars.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=62b08d35d1a520218ef92eba8201ac610dce24d7", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/1sof6d93g9e51_ToTheStars.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=380d1eeb5fe919116bbafe61b1aa5a82e83d59c2", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/1sof6d93g9e51_ToTheStars.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=28263127cf796e9c53e91fb0a750106e59ac67c0", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/1sof6d93g9e51_ToTheStars.png"}], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142uqn4", "is_robot_indexable": true, "report_reasons": null, "author": "mudler_it", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142uqn4/localai_v1180_release/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142uqn4/localai_v1180_release/", "subreddit_subscribers": 252939, "created_utc": 1686088947.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hey folks,\n\nToday we are launching OpenObserve. An open source Elasticsearch/Splunk/Datadog alternative written in rust and vue that is super easy to get started with and has 140x lower storage cost. It offers logs, metrics, traces, dashboards, alerts, functions (run aws lambda like functions during ingestion and query to enrich, redact, transform, normalize and whatever else you want to do. Think redacting email IDs from logs, adding geolocation based on IP address, etc). You can do all of this from the UI; no messing up with configuration files.\n\nOpenObserve can use local disk for storage in single node mode or s3/gc/minio/azure blob or any s3 compatible store in HA mode.\n\nWe found that setting up observability often involved setting up 4 different tools (grafana for dashboarding, elasticsearch/loki/etc for logs, jaeger for tracing, thanos, cortex etc for metics) and its not simple to do these things.\n\nHere is a blog on why we built OpenObserve - [https://openobserve.ai/blog/launching-openobserve](https://openobserve.ai/blog/launching-openobserve).\n\nWe are in early days and would love to get feedback and suggestions.\n\nHere is the github page. [https://github.com/openobserve/openobserve](https://github.com/openobserve/openobserve)\n\nYou can run it in your raspberry pi and in a 300 node cluster ingesting a petabyte of data per day.", "author_fullname": "t2_7bsgws6q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenObserve: Open source Elasticsearch alternative in Rust for logs. 140x lower storage cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1435zxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 144, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 144, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686121257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;Today we are launching OpenObserve. An open source Elasticsearch/Splunk/Datadog alternative written in rust and vue that is super easy to get started with and has 140x lower storage cost. It offers logs, metrics, traces, dashboards, alerts, functions (run aws lambda like functions during ingestion and query to enrich, redact, transform, normalize and whatever else you want to do. Think redacting email IDs from logs, adding geolocation based on IP address, etc). You can do all of this from the UI; no messing up with configuration files.&lt;/p&gt;\n\n&lt;p&gt;OpenObserve can use local disk for storage in single node mode or s3/gc/minio/azure blob or any s3 compatible store in HA mode.&lt;/p&gt;\n\n&lt;p&gt;We found that setting up observability often involved setting up 4 different tools (grafana for dashboarding, elasticsearch/loki/etc for logs, jaeger for tracing, thanos, cortex etc for metics) and its not simple to do these things.&lt;/p&gt;\n\n&lt;p&gt;Here is a blog on why we built OpenObserve - &lt;a href=\"https://openobserve.ai/blog/launching-openobserve\"&gt;https://openobserve.ai/blog/launching-openobserve&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;We are in early days and would love to get feedback and suggestions.&lt;/p&gt;\n\n&lt;p&gt;Here is the github page. &lt;a href=\"https://github.com/openobserve/openobserve\"&gt;https://github.com/openobserve/openobserve&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can run it in your raspberry pi and in a 300 node cluster ingesting a petabyte of data per day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?auto=webp&amp;v=enabled&amp;s=7af5d4a7c0c23b43d6468eed6a9865232ffa57c1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86d33bedec1eedc64b24099befee2610e36c1a80", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=586642648cb9af20ea78deb9cde7d79077e2e0d0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cdf2de17fd1043b683e41e84fe5fb28c2f0659c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1383d36d9feeafc549b1450e47c1f89b407d8bd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488da2967b97a31c9c5be251dbb1a1075bb3d933", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Adh2OFRbF0Mpp2vtvO-d6SqX0hIHPqWjaucS2EG-16w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=baa4f68a31fa7a7db0ae4647c7ea0236f6f85d6f", "width": 1080, "height": 567}], "variants": {}, "id": "4otMKjAedw7YCHVFlFfFjck6QDT82noeIjZFvL9idEA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1435zxl", "is_robot_indexable": true, "report_reasons": null, "author": "the_ml_guy", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1435zxl/openobserve_open_source_elasticsearch_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1435zxl/openobserve_open_source_elasticsearch_alternative/", "subreddit_subscribers": 252939, "created_utc": 1686121257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I don't know if this where already posted or if this is the right subreddit to post but here is my idea:\n\nReddit 2.0 because reddit changing its API Policies and becoming more interested in their investors then in thier community.\n\n**Reddit 2.0**\n\n1. Use the Reddit Archive of Archiv Team from r/DataHoarder to create an easily accessible database where everyone can go select a subreddit and download the files required to spin up a Lemmy instance to host this subreddit on their server.\n2. Maybe create some sort of community funding system where people get paid when hosting subreddits and where the payout gets calculated on how many users this instance has. And if the server that hosts a high-demand subreddit (on lemmy) goes down a hosting fund gets created that pays the people re-spinning up this subreddit to get it re-online faster\n3. Some other communities like r/webdev could be asked to create a better Reddit-style Lemmy frontpage and r/Frontend could be asked to create a more normie appealing frontend (design) for Lemmy.\n\n&amp;#x200B;\n\n**How to maybe moderate the platform**\n\nReddit already moderates its platform only through bots and moderators working in their free time. It would need a system to convert the already existing moderators from Reddit to Lemmy.\n\n&amp;#x200B;\n\n**community fund**\n\n* Some sort of premium subscription like pay $2,99/Month to use the full platform.\n* A Advertisement System where Advertises pay directly into the fund to place ads on the platform (without tracking of course)\n* For the Crypto Bros: LemmyCoin or Reddit2.0Coin where you mine it by hosting an instance.\n\n&amp;#x200B;\n\nI know this idea has its problems like Lemmy having its problems, would this find a community-wide adaptation, and and and. And I know its hard to get the community to work together but maybe a group of people could be found making this a reality. Or is it just me and the idea of creating a social media page like Reddit but it is more decentralized and community driven would be great.\n\nWould this be a good Idea would appreciate feedback for this.", "author_fullname": "t2_dvw71m6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Community Effort to Create Reddit 2.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142t8tw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686085545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if this where already posted or if this is the right subreddit to post but here is my idea:&lt;/p&gt;\n\n&lt;p&gt;Reddit 2.0 because reddit changing its API Policies and becoming more interested in their investors then in thier community.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Reddit 2.0&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use the Reddit Archive of Archiv Team from &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt; to create an easily accessible database where everyone can go select a subreddit and download the files required to spin up a Lemmy instance to host this subreddit on their server.&lt;/li&gt;\n&lt;li&gt;Maybe create some sort of community funding system where people get paid when hosting subreddits and where the payout gets calculated on how many users this instance has. And if the server that hosts a high-demand subreddit (on lemmy) goes down a hosting fund gets created that pays the people re-spinning up this subreddit to get it re-online faster&lt;/li&gt;\n&lt;li&gt;Some other communities like &lt;a href=\"/r/webdev\"&gt;r/webdev&lt;/a&gt; could be asked to create a better Reddit-style Lemmy frontpage and &lt;a href=\"/r/Frontend\"&gt;r/Frontend&lt;/a&gt; could be asked to create a more normie appealing frontend (design) for Lemmy.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to maybe moderate the platform&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Reddit already moderates its platform only through bots and moderators working in their free time. It would need a system to convert the already existing moderators from Reddit to Lemmy.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;community fund&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some sort of premium subscription like pay $2,99/Month to use the full platform.&lt;/li&gt;\n&lt;li&gt;A Advertisement System where Advertises pay directly into the fund to place ads on the platform (without tracking of course)&lt;/li&gt;\n&lt;li&gt;For the Crypto Bros: LemmyCoin or Reddit2.0Coin where you mine it by hosting an instance.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know this idea has its problems like Lemmy having its problems, would this find a community-wide adaptation, and and and. And I know its hard to get the community to work together but maybe a group of people could be found making this a reality. Or is it just me and the idea of creating a social media page like Reddit but it is more decentralized and community driven would be great.&lt;/p&gt;\n\n&lt;p&gt;Would this be a good Idea would appreciate feedback for this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142t8tw", "is_robot_indexable": true, "report_reasons": null, "author": "BikaenDerAndereAcc", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142t8tw/community_effort_to_create_reddit_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142t8tw/community_effort_to_create_reddit_20/", "subreddit_subscribers": 252939, "created_utc": 1686085545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I'm currently using trello to keep a bunch of stuff organised. But I'm wanting to organise a little more and I'm having a hard time choosing the best application for my use case.\n\nWhat I'm looking for:\nA lot of similar features to trello.\nUnlimited boards and unlimited workspaces.\nAllows the collaboration between a few members.\nKanban.\n\nBasically I'm looking for the closest thing to trello that allows me to have more boards within my workspaces without paying etc.\n\nI've looked at leantime and focalboard but I'm having trouble knowing if they have the features I want. I forgot to also mention I'm looking to run this on a raspberry pi 4. I've had leantime run in there before with no issues, I was looking for another feature at the time though and couldn't find it.\n\nAny help would be greatly appreciated. I'm honestly really stuck and lost with this.\n~Blood", "author_fullname": "t2_atzwj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trello Alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142vwm6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686091709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently using trello to keep a bunch of stuff organised. But I&amp;#39;m wanting to organise a little more and I&amp;#39;m having a hard time choosing the best application for my use case.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for:\nA lot of similar features to trello.\nUnlimited boards and unlimited workspaces.\nAllows the collaboration between a few members.\nKanban.&lt;/p&gt;\n\n&lt;p&gt;Basically I&amp;#39;m looking for the closest thing to trello that allows me to have more boards within my workspaces without paying etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at leantime and focalboard but I&amp;#39;m having trouble knowing if they have the features I want. I forgot to also mention I&amp;#39;m looking to run this on a raspberry pi 4. I&amp;#39;ve had leantime run in there before with no issues, I was looking for another feature at the time though and couldn&amp;#39;t find it.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated. I&amp;#39;m honestly really stuck and lost with this.\n~Blood&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142vwm6", "is_robot_indexable": true, "report_reasons": null, "author": "bloodshotpico", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142vwm6/trello_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142vwm6/trello_alternative/", "subreddit_subscribers": 252939, "created_utc": 1686091709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": " Reddit user [/u/TheArstaInventor](https://www.reddit.com/u/TheArstaInventor/) was recently banned from Reddit, alongside a subreddit they created [r/LemmyMigration](https://www.reddit.com/r/LemmyMigration/) which was promoting Lemmy.   \n\n\nLemmy is a self-hosted social link sharing and discussion platform, offering an alternative experience to Reddit. Considering recent issues with Reddit API changes, and the impending hemorrhage to Reddit's userbase, this is a sign they're panicking.\n\nThe account and subreddit have since been reinstated, but this doesn't look good for Reddit.\n\n[Full Story Here](https://www.videogamer.com/news/reddit-ban-subreddit-user-for-alternative-platforms/)", "author_fullname": "t2_323wrsms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit temporarily ban subreddit and user advertising rival self-hosted platform (Lemmy)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_143diuj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686145007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reddit user &lt;a href=\"https://www.reddit.com/u/TheArstaInventor/\"&gt;/u/TheArstaInventor&lt;/a&gt; was recently banned from Reddit, alongside a subreddit they created &lt;a href=\"https://www.reddit.com/r/LemmyMigration/\"&gt;r/LemmyMigration&lt;/a&gt; which was promoting Lemmy.   &lt;/p&gt;\n\n&lt;p&gt;Lemmy is a self-hosted social link sharing and discussion platform, offering an alternative experience to Reddit. Considering recent issues with Reddit API changes, and the impending hemorrhage to Reddit&amp;#39;s userbase, this is a sign they&amp;#39;re panicking.&lt;/p&gt;\n\n&lt;p&gt;The account and subreddit have since been reinstated, but this doesn&amp;#39;t look good for Reddit.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.videogamer.com/news/reddit-ban-subreddit-user-for-alternative-platforms/\"&gt;Full Story Here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?auto=webp&amp;v=enabled&amp;s=b8eefb80e051ba5398a798c3a229f32db754c076", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0561d12bc98eb5f1b8d0d944a5c060c864ab8623", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9df6672753cabfd8d6415b81932b4e9cdb47ddd8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef98ae9ad8c8655be51f274feff1b59ef96d0243", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a0d880e3969f6248f2c83387e65a8f196c8c24f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef106070d31ec92bfaac7be0f6ef9be04a758ff8", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/PzuqbMn9alFK81jaXqH63dvBafiuAetLc-MBF0oPzoo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3baf5840512a29988e474a7940626d2606c072fd", "width": 1080, "height": 607}], "variants": {}, "id": "Gm61LbO2xeYFv46HtomUc2iJNR9HfwuX54_1zHApFg0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143diuj", "is_robot_indexable": true, "report_reasons": null, "author": "aDogWithoutABone", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143diuj/reddit_temporarily_ban_subreddit_and_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143diuj/reddit_temporarily_ban_subreddit_and_user/", "subreddit_subscribers": 252939, "created_utc": 1686145007.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "A month ago I made a [blogpost](https://tarneo.fr/posts/minimal_web/) where I wrote about the idea of making a new and federated search engine independent from others which would have the same idea as [marginalia](https://marginalia.nu) or [wiby](https://wiby.me).\n\nThe main question is if this would actually get hosted by at least some people because web crawlers are quite heavy things to host. There would of course be an option to limit these easily.\n\nIf you are ready to host it, could you also share how much resources you'd give it?\n\n[View Poll](https://www.reddit.com/poll/142h79i)", "author_fullname": "t2_763cfwty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you be ready to self-host a federated search engine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "productannouncement", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142h79i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Product Announcement", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686060350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A month ago I made a &lt;a href=\"https://tarneo.fr/posts/minimal_web/\"&gt;blogpost&lt;/a&gt; where I wrote about the idea of making a new and federated search engine independent from others which would have the same idea as &lt;a href=\"https://marginalia.nu\"&gt;marginalia&lt;/a&gt; or &lt;a href=\"https://wiby.me\"&gt;wiby&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The main question is if this would actually get hosted by at least some people because web crawlers are quite heavy things to host. There would of course be an option to limit these easily.&lt;/p&gt;\n\n&lt;p&gt;If you are ready to host it, could you also share how much resources you&amp;#39;d give it?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/142h79i\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c7fc47c4-7e6b-11e9-b04d-0e70893a61b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "142h79i", "is_robot_indexable": true, "report_reasons": null, "author": "tarneaux", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1686665150259, "options": [{"text": "Yes", "id": "23362367"}, {"text": "No", "id": "23362368"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 235, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142h79i/would_you_be_ready_to_selfhost_a_federated_search/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/selfhosted/comments/142h79i/would_you_be_ready_to_selfhost_a_federated_search/", "subreddit_subscribers": 252939, "created_utc": 1686060350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I hope reddit can forgive the click-baity title. I am trying something new\n\nOntime is an application for managing rundowns and stage timers. The first version was released slightly over a year ago.\n\nSince, we have learned about the application and how users interact with Ontime and have prepared a new version\n\nV2 is a great technical achievement, mainly focused on preparing architecture to enable us to expand. Unfortunately, technical achievements do not present themselves to regular users, but we still have a good list of features and improvements:\n\n* Complete **redesign of editors** and views focusing on adding more power and friendlier UX\n* New, **cleaner styling** system with a focus on dark schemes\n* New **integrations engine** for enabling Ontime to share its data with other software (focusing on OSC)\n* Friendly **view configurations**\n* Comprehensive **companion module**\n* **Improved playback** mode\n* Timer **automations**\n* ... Several small UX and UI improvements requested by users\n* ... and lots more\n\nv2 has been beta testing for nearly half a year and is now a stable release.\n\n&amp;#x200B;\n\nOntime is [available for Mac, Windows, Linux](https://github.com/cpvalente/ontime/releases/tag/v2.0.0), and a [docker image](https://hub.docker.com/r/getontime/ontime).\n\nPlease visit [the website](https://www.getontime.no/) or see the [documentation in GitBook](https://ontime.gitbook.io/v2/). You are also welcome to check in on our development and participate with bug reports and feature requests on [Github](https://github.com/cpvalente/ontime)\n\nOntime always was and will remain free.", "author_fullname": "t2_dny19tii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ontime v2: rundown and time manager is now free", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ro1f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686081900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope reddit can forgive the click-baity title. I am trying something new&lt;/p&gt;\n\n&lt;p&gt;Ontime is an application for managing rundowns and stage timers. The first version was released slightly over a year ago.&lt;/p&gt;\n\n&lt;p&gt;Since, we have learned about the application and how users interact with Ontime and have prepared a new version&lt;/p&gt;\n\n&lt;p&gt;V2 is a great technical achievement, mainly focused on preparing architecture to enable us to expand. Unfortunately, technical achievements do not present themselves to regular users, but we still have a good list of features and improvements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Complete &lt;strong&gt;redesign of editors&lt;/strong&gt; and views focusing on adding more power and friendlier UX&lt;/li&gt;\n&lt;li&gt;New, &lt;strong&gt;cleaner styling&lt;/strong&gt; system with a focus on dark schemes&lt;/li&gt;\n&lt;li&gt;New &lt;strong&gt;integrations engine&lt;/strong&gt; for enabling Ontime to share its data with other software (focusing on OSC)&lt;/li&gt;\n&lt;li&gt;Friendly &lt;strong&gt;view configurations&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Comprehensive &lt;strong&gt;companion module&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Improved playback&lt;/strong&gt; mode&lt;/li&gt;\n&lt;li&gt;Timer &lt;strong&gt;automations&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;... Several small UX and UI improvements requested by users&lt;/li&gt;\n&lt;li&gt;... and lots more&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;v2 has been beta testing for nearly half a year and is now a stable release.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ontime is &lt;a href=\"https://github.com/cpvalente/ontime/releases/tag/v2.0.0\"&gt;available for Mac, Windows, Linux&lt;/a&gt;, and a &lt;a href=\"https://hub.docker.com/r/getontime/ontime\"&gt;docker image&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Please visit &lt;a href=\"https://www.getontime.no/\"&gt;the website&lt;/a&gt; or see the &lt;a href=\"https://ontime.gitbook.io/v2/\"&gt;documentation in GitBook&lt;/a&gt;. You are also welcome to check in on our development and participate with bug reports and feature requests on &lt;a href=\"https://github.com/cpvalente/ontime\"&gt;Github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ontime always was and will remain free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?auto=webp&amp;v=enabled&amp;s=ced5d1c828969cbc11f300d34e2be4d5f1fdebfc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a256953e3907e070d84b8bc7d5772ec0958b92c2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=707a48d69a3b1c68b99b7109fa49b318761b761f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61fd1afcdb3c99899a60253f574bb9ab23202ff7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe8e3efa89f29acd962d4a45d87cc4f5ebde8a0e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99a02f32d41b8215076d6997d6679858b13bf9bc", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_PAdhr0f3c6lQ0xzAI3xLu7qI1vDsBKpJR03JRv_0-0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eadb251364569404b6999af9dc2954a790922955", "width": 1080, "height": 540}], "variants": {}, "id": "h1-JFeAGzTqAb7X3Mrkg-Sh26vLs7v1vF5qUL-3F9w0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142ro1f", "is_robot_indexable": true, "report_reasons": null, "author": "somedevstuff", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142ro1f/ontime_v2_rundown_and_time_manager_is_now_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142ro1f/ontime_v2_rundown_and_time_manager_is_now_free/", "subreddit_subscribers": 252939, "created_utc": 1686081900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "So in lieu of Reddit's recent API changes, it seems people will want to have ways to dump their data and move elsewhere if the announced pricing plan isn't adjusted. Since I wanted to dump my own Reddit messages, I came up with a script that makes this possible.\n\nReddit's new chat infrastructure is based on Matrix, allowing us to use standard Matrix clients to access the message history.\n\nAs I used Golang for this, I used the Mautrix client, and came up with the following:\n\n    func FetchMessages(client *mautrix.Client, roomID id.RoomID, callback func(messages []*event.Event)) error {\n    \tr, err := client.CreateFilter(mautrix.NewDefaultSyncer().FilterJSON)\n    \n    \tif err != nil {\n    \t\treturn err\n    \t}\n    \n    \tresp, err := client.SyncRequest(0, \"\", r.FilterID, true, event.PresenceOnline, context.TODO())\n    \n    \tif err != nil {\n    \t\treturn err\n    \t}\n    \n    \tvar room *mautrix.SyncJoinedRoom\n    \n    \tfor id, r := range resp.Rooms.Join {\n    \t\tif id == roomID {\n    \t\t\troom = r\n    \t\t\tbreak\n    \t\t}\n    \t}\n    \n    \tvar messages []*event.Event\n    \n    \tfor _, m := range room.Timeline.Events {\n    \t\tif m.Type == event.EventMessage {\n    \t\t\tmessages = append(messages, m)\n    \t\t}\n    \t}\n    \n    \tcallback(messages)\n    \n    \tend := room.Timeline.PrevBatch\n    \n    \tfor {\n    \t\tif end == \"\" {\n    \t\t\tbreak\n    \t\t}\n    \n    \t\tvar messages []*event.Event\n    \n    \t\tmsgs, err := client.Messages(roomID, end, \"\", mautrix.DirectionBackward, &amp;mautrix.FilterPart{}, 100)\n    \n    \t\tif err != nil {\n    \t\t\tlog.Fatalf(err.Error())\n    \t\t}\n    \n    \t\tmessages = append(messages, msgs.Chunk...)\n    \t\tcallback(messages)\n    \n    \t\tend = msgs.End\n    \n    \t\tif len(messages) == 0 {\n    \t\t\tcontinue\n    \t\t}\n    \t}\n    \n    \treturn nil\n    }\n\nThis method will fetch all the messages from a given room ID, and call the `callback()` function in batches. From there you can use the events to dump as JSON, store in a DB, or anything else. \n\nTo create the Mautrix client and `roomID` argument, the following snippet can be used:\n\n    client, err := mautrix.NewClient(\"https://matrix.redditspace.com/\", id.NewUserID(\"t2_&lt;userID&gt;\", \"reddit.com\"), \"&lt;redditAccessToken\"\")\n    roomID := id.RoomID(\"&lt;roomID&gt;\")\n\nTo fill out the above variables, you'll need to use your browser's network tab to inspect requests and get the IDs and access token. For that head to Reddit's chat at [https://chat.reddit.com](https://chat.reddit.com) and reload the window with the network tab open. \n\n**User ID**\n\nYour user ID is visible in the request to [https://matrix.redditspace.com/\\_matrix/client/r0/login](https://matrix.redditspace.com/_matrix/client/r0/login). It will be part of the response as `user_id`.\n\n**Room ID**\n\nThe room ID will be part of the URL when you select a chat room. Simply copy the entire path after [https://chat.reddit.com/room](https://chat.reddit.com/room) and URL decode it.\n\n**Access Token**\n\nYour access token will be included in all requests after the login. I used the request to /filter and copy the value from the `Authorization` header without \"Bearer \".\n\nNow, depending on what you want to do with the messages you'll want to write your own parsing and mapping logic, as well as saving, but a fairly straightforward `main()` method to save all the messages in JSON can look like this:\n\n    package main\n    \n    type Message struct {\n        Source      string    `bson:\"source\"`\n        ChatID      string    `bson:\"chat_id\"`\n        Author      string    `bson:\"author\"`\n        Timestamp   time.Time `bson:\"timestamp\"`\n        SourceID    string    `bson:\"source_id\"`\n        Body        string    `bson:\"body\"`\n        Attachments []string  `bson:\"attachments\"`\n    }\n    \n    func parseMsg(message *event.Event, roomId id.RoomID) *model.Message {\n    \tts := time.Unix(message.Timestamp, 0)\n    \n    \tmsg := &amp;model.Message{\n    \t\tSource:    \"reddit\",\n    \t\tChatID:    roomId.String(),\n    \t\tAuthor:    message.Sender.String(),\n    \t\tTimestamp: ts,\n    \t\tSourceID:  message.ID.String(),\n    \t}\n    \n    \tswitch message.Content.Raw[\"msgtype\"] {\n    \tcase \"m.text\":\n    \t\tif message.Content.Raw[\"body\"] == nil {\n    \t\t\tfmt.Println(\"Empty message body:\", message.Content.Raw)\n    \t\t\treturn nil\n    \t\t} else {\n    \t\t\tmsg.Body = message.Content.Raw[\"body\"].(string)\n    \t\t}\n    \tcase \"m.image\":\n    \t\tmsg.Attachments = []string{\n    \t\t\tmessage.Content.Raw[\"url\"].(string),\n    \t\t}\n    \tcase nil:\n    \t\tif message.Content.Raw[\"m.relates_to\"] != nil &amp;&amp; message.Content.Raw[\"m.relates_to\"].(map[string]interface{})[\"rel_type\"] == \"com.reddit.potentially_toxic\" {\n    \t\t} else {\n    \t\t\tfmt.Println(\"No message type:\", message.Content.Raw)\n    \t\t}\n    \t\treturn nil\n    \tdefault:\n    \t\tfmt.Println(\"Unknown message type:\", message.Content.Raw)\n    \t}\n    \n    \treturn msg\n    }\n    \n    func main() {\n        var allMessages []*Message\n        \n        err = reddit.FetchMessages(client, roomId, func(messages []*event.Event) {\n            for _, msg := range messages {\n                m := parseMsg(msg, roomId)\n                if m == nil {\n                    continue\n                }\n                messages = append(messages, m)\n            }\n        }\n    \n        if err != nil {\n            log.Fatalf(err.Error())\n        }\n    \n        file, _ := json.MarshalIndent(allMessages, \"\", \" \")\n        _ = os.WriteFile(\"events.json\", file, 0644)\n    }\n\nHappy dumping!", "author_fullname": "t2_9w1ryk11p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created some Go scripts to dump Reddit chats!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "chatsystem", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143bl09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Chat System", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686140052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So in lieu of Reddit&amp;#39;s recent API changes, it seems people will want to have ways to dump their data and move elsewhere if the announced pricing plan isn&amp;#39;t adjusted. Since I wanted to dump my own Reddit messages, I came up with a script that makes this possible.&lt;/p&gt;\n\n&lt;p&gt;Reddit&amp;#39;s new chat infrastructure is based on Matrix, allowing us to use standard Matrix clients to access the message history.&lt;/p&gt;\n\n&lt;p&gt;As I used Golang for this, I used the Mautrix client, and came up with the following:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;func FetchMessages(client *mautrix.Client, roomID id.RoomID, callback func(messages []*event.Event)) error {\n    r, err := client.CreateFilter(mautrix.NewDefaultSyncer().FilterJSON)\n\n    if err != nil {\n        return err\n    }\n\n    resp, err := client.SyncRequest(0, &amp;quot;&amp;quot;, r.FilterID, true, event.PresenceOnline, context.TODO())\n\n    if err != nil {\n        return err\n    }\n\n    var room *mautrix.SyncJoinedRoom\n\n    for id, r := range resp.Rooms.Join {\n        if id == roomID {\n            room = r\n            break\n        }\n    }\n\n    var messages []*event.Event\n\n    for _, m := range room.Timeline.Events {\n        if m.Type == event.EventMessage {\n            messages = append(messages, m)\n        }\n    }\n\n    callback(messages)\n\n    end := room.Timeline.PrevBatch\n\n    for {\n        if end == &amp;quot;&amp;quot; {\n            break\n        }\n\n        var messages []*event.Event\n\n        msgs, err := client.Messages(roomID, end, &amp;quot;&amp;quot;, mautrix.DirectionBackward, &amp;amp;mautrix.FilterPart{}, 100)\n\n        if err != nil {\n            log.Fatalf(err.Error())\n        }\n\n        messages = append(messages, msgs.Chunk...)\n        callback(messages)\n\n        end = msgs.End\n\n        if len(messages) == 0 {\n            continue\n        }\n    }\n\n    return nil\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This method will fetch all the messages from a given room ID, and call the &lt;code&gt;callback()&lt;/code&gt; function in batches. From there you can use the events to dump as JSON, store in a DB, or anything else. &lt;/p&gt;\n\n&lt;p&gt;To create the Mautrix client and &lt;code&gt;roomID&lt;/code&gt; argument, the following snippet can be used:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;client, err := mautrix.NewClient(&amp;quot;https://matrix.redditspace.com/&amp;quot;, id.NewUserID(&amp;quot;t2_&amp;lt;userID&amp;gt;&amp;quot;, &amp;quot;reddit.com&amp;quot;), &amp;quot;&amp;lt;redditAccessToken&amp;quot;&amp;quot;)\nroomID := id.RoomID(&amp;quot;&amp;lt;roomID&amp;gt;&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To fill out the above variables, you&amp;#39;ll need to use your browser&amp;#39;s network tab to inspect requests and get the IDs and access token. For that head to Reddit&amp;#39;s chat at &lt;a href=\"https://chat.reddit.com\"&gt;https://chat.reddit.com&lt;/a&gt; and reload the window with the network tab open. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;User ID&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Your user ID is visible in the request to &lt;a href=\"https://matrix.redditspace.com/_matrix/client/r0/login\"&gt;https://matrix.redditspace.com/_matrix/client/r0/login&lt;/a&gt;. It will be part of the response as &lt;code&gt;user_id&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Room ID&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The room ID will be part of the URL when you select a chat room. Simply copy the entire path after &lt;a href=\"https://chat.reddit.com/room\"&gt;https://chat.reddit.com/room&lt;/a&gt; and URL decode it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Access Token&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Your access token will be included in all requests after the login. I used the request to /filter and copy the value from the &lt;code&gt;Authorization&lt;/code&gt; header without &amp;quot;Bearer &amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Now, depending on what you want to do with the messages you&amp;#39;ll want to write your own parsing and mapping logic, as well as saving, but a fairly straightforward &lt;code&gt;main()&lt;/code&gt; method to save all the messages in JSON can look like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;package main\n\ntype Message struct {\n    Source      string    `bson:&amp;quot;source&amp;quot;`\n    ChatID      string    `bson:&amp;quot;chat_id&amp;quot;`\n    Author      string    `bson:&amp;quot;author&amp;quot;`\n    Timestamp   time.Time `bson:&amp;quot;timestamp&amp;quot;`\n    SourceID    string    `bson:&amp;quot;source_id&amp;quot;`\n    Body        string    `bson:&amp;quot;body&amp;quot;`\n    Attachments []string  `bson:&amp;quot;attachments&amp;quot;`\n}\n\nfunc parseMsg(message *event.Event, roomId id.RoomID) *model.Message {\n    ts := time.Unix(message.Timestamp, 0)\n\n    msg := &amp;amp;model.Message{\n        Source:    &amp;quot;reddit&amp;quot;,\n        ChatID:    roomId.String(),\n        Author:    message.Sender.String(),\n        Timestamp: ts,\n        SourceID:  message.ID.String(),\n    }\n\n    switch message.Content.Raw[&amp;quot;msgtype&amp;quot;] {\n    case &amp;quot;m.text&amp;quot;:\n        if message.Content.Raw[&amp;quot;body&amp;quot;] == nil {\n            fmt.Println(&amp;quot;Empty message body:&amp;quot;, message.Content.Raw)\n            return nil\n        } else {\n            msg.Body = message.Content.Raw[&amp;quot;body&amp;quot;].(string)\n        }\n    case &amp;quot;m.image&amp;quot;:\n        msg.Attachments = []string{\n            message.Content.Raw[&amp;quot;url&amp;quot;].(string),\n        }\n    case nil:\n        if message.Content.Raw[&amp;quot;m.relates_to&amp;quot;] != nil &amp;amp;&amp;amp; message.Content.Raw[&amp;quot;m.relates_to&amp;quot;].(map[string]interface{})[&amp;quot;rel_type&amp;quot;] == &amp;quot;com.reddit.potentially_toxic&amp;quot; {\n        } else {\n            fmt.Println(&amp;quot;No message type:&amp;quot;, message.Content.Raw)\n        }\n        return nil\n    default:\n        fmt.Println(&amp;quot;Unknown message type:&amp;quot;, message.Content.Raw)\n    }\n\n    return msg\n}\n\nfunc main() {\n    var allMessages []*Message\n\n    err = reddit.FetchMessages(client, roomId, func(messages []*event.Event) {\n        for _, msg := range messages {\n            m := parseMsg(msg, roomId)\n            if m == nil {\n                continue\n            }\n            messages = append(messages, m)\n        }\n    }\n\n    if err != nil {\n        log.Fatalf(err.Error())\n    }\n\n    file, _ := json.MarshalIndent(allMessages, &amp;quot;&amp;quot;, &amp;quot; &amp;quot;)\n    _ = os.WriteFile(&amp;quot;events.json&amp;quot;, file, 0644)\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Happy dumping!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0d52d76a-7e68-11e9-977c-0eabae61418e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143bl09", "is_robot_indexable": true, "report_reasons": null, "author": "Dan6erbond2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143bl09/i_created_some_go_scripts_to_dump_reddit_chats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143bl09/i_created_some_go_scripts_to_dump_reddit_chats/", "subreddit_subscribers": 252939, "created_utc": 1686140052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I have recently started using FreshRSS as my RSS aggregator but the native app has terrible UI and I really want to use something better which has more intuitive UI. Do yall have a suggestion? What do you use?", "author_fullname": "t2_4zs5x8y7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Android clients for FreshRSS instances?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142hgyx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686062930.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686060898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently started using FreshRSS as my RSS aggregator but the native app has terrible UI and I really want to use something better which has more intuitive UI. Do yall have a suggestion? What do you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142hgyx", "is_robot_indexable": true, "report_reasons": null, "author": "fakedoorsarereal", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142hgyx/android_clients_for_freshrss_instances/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142hgyx/android_clients_for_freshrss_instances/", "subreddit_subscribers": 252939, "created_utc": 1686060898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "", "author_fullname": "t2_6p31env7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I love Apache (well, like it at least) (2014)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142j78s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1686064484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "utcc.utoronto.ca", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://utcc.utoronto.ca/~cks/space/blog/web/ApacheLove", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142j78s", "is_robot_indexable": true, "report_reasons": null, "author": "vegetaaaaaaa", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142j78s/i_love_apache_well_like_it_at_least_2014/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://utcc.utoronto.ca/~cks/space/blog/web/ApacheLove", "subreddit_subscribers": 252939, "created_utc": 1686064484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "after mullvad got rid of port forwarding, would it still be feasible to use qbittorent and prowlarr to hide my ip?", "author_fullname": "t2_4qrzevb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mullvad and qbittorent", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1432gqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686109753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;after mullvad got rid of port forwarding, would it still be feasible to use qbittorent and prowlarr to hide my ip?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1432gqu", "is_robot_indexable": true, "report_reasons": null, "author": "eldaniay", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1432gqu/mullvad_and_qbittorent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1432gqu/mullvad_and_qbittorent/", "subreddit_subscribers": 252939, "created_utc": 1686109753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Newb question\n\nIs it possible to run a spreadsheet on a homelab and access from devices not able to run the software? I use open office and would like to be able to work on some spreadsheets while not at home and from my phone or tablet. I want my data kept locally, so cloud based software isn\u2019t an option.", "author_fullname": "t2_gomeeqog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spreadsheets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142mbhu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686070843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newb question&lt;/p&gt;\n\n&lt;p&gt;Is it possible to run a spreadsheet on a homelab and access from devices not able to run the software? I use open office and would like to be able to work on some spreadsheets while not at home and from my phone or tablet. I want my data kept locally, so cloud based software isn\u2019t an option.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142mbhu", "is_robot_indexable": true, "report_reasons": null, "author": "Authentic-469", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142mbhu/spreadsheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142mbhu/spreadsheets/", "subreddit_subscribers": 252939, "created_utc": 1686070843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I'm never clear on what counts as self promotion, so I won't include the link until someone tells I can. Besides that, I haven't run it anywhere but my own server so there are probably bugs.\n\nI made a web-based photo gallery for my home photos and I would like it if some people would help test it and share feedback.\n\nKey features: \n \n * It handles 200,000+ images and videos well, showing them all in a date-sorted scrolling wall of thumbnails, which you can click to view full sized. \n * \"Rewind\" button to show all photos taken on today's date in past years\n * Jump-to-date calendar popup to navigate your large photo library\n * Shows photo locations on a map, clicking on the map takes you to the photo\n * Can toggle map filter mode so that only photos on the current map view are shown in the scrollable area. \n * Reads tags from metadata and lets you filter photos by tags\n * Generates streamable copies of your videos so you can watch them all in the browser\n * Basic username and password protection\n * Progressive Web App so you can put an icon on your home page and it will cache thumbnails and resources\n * Lastly, it should be easy to set up\n\nThe code is a bit old-school. Written in PHP and JavaScript. It needs ffmpeg for video thumbnails and either vipsthumbnail, imagemagic or gd for image thumbnails.\n\nIt's 100% open source and I don't (and won't) have a business around it. I just put a lot of work in to it, think it's kind of cool and would love to get some feedback.", "author_fullname": "t2_k2sbwvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a few testers for a self-hosted image gallery project (free &amp; open source, of course)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "mediaserving", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143ajs9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Media Serving", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686137094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m never clear on what counts as self promotion, so I won&amp;#39;t include the link until someone tells I can. Besides that, I haven&amp;#39;t run it anywhere but my own server so there are probably bugs.&lt;/p&gt;\n\n&lt;p&gt;I made a web-based photo gallery for my home photos and I would like it if some people would help test it and share feedback.&lt;/p&gt;\n\n&lt;p&gt;Key features: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It handles 200,000+ images and videos well, showing them all in a date-sorted scrolling wall of thumbnails, which you can click to view full sized. &lt;/li&gt;\n&lt;li&gt;&amp;quot;Rewind&amp;quot; button to show all photos taken on today&amp;#39;s date in past years&lt;/li&gt;\n&lt;li&gt;Jump-to-date calendar popup to navigate your large photo library&lt;/li&gt;\n&lt;li&gt;Shows photo locations on a map, clicking on the map takes you to the photo&lt;/li&gt;\n&lt;li&gt;Can toggle map filter mode so that only photos on the current map view are shown in the scrollable area. &lt;/li&gt;\n&lt;li&gt;Reads tags from metadata and lets you filter photos by tags&lt;/li&gt;\n&lt;li&gt;Generates streamable copies of your videos so you can watch them all in the browser&lt;/li&gt;\n&lt;li&gt;Basic username and password protection&lt;/li&gt;\n&lt;li&gt;Progressive Web App so you can put an icon on your home page and it will cache thumbnails and resources&lt;/li&gt;\n&lt;li&gt;Lastly, it should be easy to set up&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The code is a bit old-school. Written in PHP and JavaScript. It needs ffmpeg for video thumbnails and either vipsthumbnail, imagemagic or gd for image thumbnails.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s 100% open source and I don&amp;#39;t (and won&amp;#39;t) have a business around it. I just put a lot of work in to it, think it&amp;#39;s kind of cool and would love to get some feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "cb71ccc0-7e67-11e9-841a-0e67038620c2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "143ajs9", "is_robot_indexable": true, "report_reasons": null, "author": "cspybbq", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/143ajs9/looking_for_a_few_testers_for_a_selfhosted_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/143ajs9/looking_for_a_few_testers_for_a_selfhosted_image/", "subreddit_subscribers": 252939, "created_utc": 1686137094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi all,\n\nIn the past I've leaned heavily on Consul and DNSMasq for my service discovery and resolution, and I've also used PiHole as a stop-gap when I don't need auto-discovery, however I have a new project I'm working on and I'd love to be able to add DNS functionality \"out of the box\", but with service discovery in the same way that Traefik reads the docker socket.\n\nI've looked at CoreDNS, and that seems to do what I want for k8s and etcd, but I'm running neither of those things for this project as it is based on the [Balena.io](https://Balena.io) platform and docker-compose, meaning that I do not have access to the underlying physical host (a raspberry pi in this case).\n\nIn my head, the idea is that I feed this DNS resolver the domain (for example [`mydomain.com`](https://mydomain.com), and it then reads the docker socket and serves up records in the format &lt;container\\_name&gt;.mydomain.com based on the value it reads from Docker (in the same way that Consul does for Nomad or using [https://medium.com/@david.curran3/microservices-running-with-docker-and-consul-for-service-discovery-3c6d05b8030b](https://medium.com/@david.curran3/microservices-running-with-docker-and-consul-for-service-discovery-3c6d05b8030b))\n\nThe thing is, Consul feels like overkill for this particular application, so I'm hoping there's something else out there I can use!", "author_fullname": "t2_y0tbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lightweight DNS Server that can be run in a docker container and also allows for service discovery of other containers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142stoi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686089480.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686084569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;In the past I&amp;#39;ve leaned heavily on Consul and DNSMasq for my service discovery and resolution, and I&amp;#39;ve also used PiHole as a stop-gap when I don&amp;#39;t need auto-discovery, however I have a new project I&amp;#39;m working on and I&amp;#39;d love to be able to add DNS functionality &amp;quot;out of the box&amp;quot;, but with service discovery in the same way that Traefik reads the docker socket.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at CoreDNS, and that seems to do what I want for k8s and etcd, but I&amp;#39;m running neither of those things for this project as it is based on the &lt;a href=\"https://Balena.io\"&gt;Balena.io&lt;/a&gt; platform and docker-compose, meaning that I do not have access to the underlying physical host (a raspberry pi in this case).&lt;/p&gt;\n\n&lt;p&gt;In my head, the idea is that I feed this DNS resolver the domain (for example &lt;a href=\"https://mydomain.com\"&gt;&lt;code&gt;mydomain.com&lt;/code&gt;&lt;/a&gt;, and it then reads the docker socket and serves up records in the format &amp;lt;container\\_name&amp;gt;.mydomain.com based on the value it reads from Docker (in the same way that Consul does for Nomad or using &lt;a href=\"https://medium.com/@david.curran3/microservices-running-with-docker-and-consul-for-service-discovery-3c6d05b8030b\"&gt;https://medium.com/@david.curran3/microservices-running-with-docker-and-consul-for-service-discovery-3c6d05b8030b&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;The thing is, Consul feels like overkill for this particular application, so I&amp;#39;m hoping there&amp;#39;s something else out there I can use!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142stoi", "is_robot_indexable": true, "report_reasons": null, "author": "TheProffalken", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142stoi/lightweight_dns_server_that_can_be_run_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142stoi/lightweight_dns_server_that_can_be_run_in_a/", "subreddit_subscribers": 252939, "created_utc": 1686084569.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Are there any reasonably easy to setup monitoring solutions for getting things like webhooks/email notifications for any 'ERROR' level statement across all deployed containers?  I've briefly looked into things like Zabbix in the past and got discouraged but will try and keep at it if that's the recommendation.\n\nThanks!", "author_fullname": "t2_jg9xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for free automatic alerts from container logs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142i3ih", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686062229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any reasonably easy to setup monitoring solutions for getting things like webhooks/email notifications for any &amp;#39;ERROR&amp;#39; level statement across all deployed containers?  I&amp;#39;ve briefly looked into things like Zabbix in the past and got discouraged but will try and keep at it if that&amp;#39;s the recommendation.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142i3ih", "is_robot_indexable": true, "report_reasons": null, "author": "lollanlols", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142i3ih/recommendations_for_free_automatic_alerts_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142i3ih/recommendations_for_free_automatic_alerts_from/", "subreddit_subscribers": 252939, "created_utc": 1686062229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hey everyone,  \ni recently created my Paperless instance and so far it works \"okay\" ;)  \n\n\nI have 2 Problems:  \n1. If i scan multiple Documents with Barcodes on each document, Paperless didnt split the PDF file.  \nThe Barcodes get recognised\n\nIn the Log the Barcode with Code128 are the ones from my Sticker.  \n\n\n    [2023-05-25 14:55:03,087] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/Gewerbe/Eingangs_Rechnung/20230525_134142.pdf to the task queue.\n    [2023-05-25 14:55:03,094] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/20230525_134142.pdf to the task queue.\n    [2023-05-25 14:55:07,891] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,004] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,165] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n    [2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n    [2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,290] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n    [2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n    [2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,331] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,458] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,522] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n    [2023-05-25 14:55:08,523] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,648] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n    [2023-05-25 14:55:08,649] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,668] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,796] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,849] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n    [2023-05-25 14:55:08,850] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n    [2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,014] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,136] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,196] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,306] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,338] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,441] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n    [2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n    [2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n    [2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n    [2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,689] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,779] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n    002\n    1\n    SCT\n    \n    RG119608\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n    [2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n    002\n    1\n    SCT\n    \n    RG119608\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n    [2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,091] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,257] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,326] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n    [2023-05-25 14:55:10,456] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n    [2023-05-25 14:55:10,460] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n    [2023-05-25 14:55:10,462] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n    [2023-05-25 14:55:10,464] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n    [2023-05-25 14:55:10,531] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n    [2023-05-25 14:55:10,535] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n    [2023-05-25 14:55:10,536] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n    [2023-05-25 14:55:10,539] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n    [2023-05-25 14:55:10,573] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {'input_file': PosixPath('/tmp/paperless/paperless-ngxzbpeluif/20230525_134142.pdf'), 'output_file': PosixPath('/tmp/paperless/paperless-1wqpexro/archive.pdf'), 'use_threads': True, 'jobs': '2', 'language': 'eng', 'output_type': 'pdfa', 'progress_bar': False, 'skip_text': True, 'clean': True, 'deskew': True, 'rotate_pages': True, 'rotate_pages_threshold': 12.0, 'sidecar': PosixPath('/tmp/paperless/paperless-1wqpexro/sidecar.txt')}\n    [2023-05-25 14:55:10,627] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {'input_file': PosixPath('/tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf'), 'output_file': PosixPath('/tmp/paperless/paperless-djjj8xny/archive.pdf'), 'use_threads': True, 'jobs': '2', 'language': 'eng', 'output_type': 'pdfa', 'progress_bar': False, 'skip_text': True, 'clean': True, 'deskew': True, 'rotate_pages': True, 'rotate_pages_threshold': 12.0, 'sidecar': PosixPath('/tmp/paperless/paperless-djjj8xny/sidecar.txt')}\n    [2023-05-25 14:56:11,255] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:11,267] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:11,273] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:11,277] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,398] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,433] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,442] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:56:59,455] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:57:07,981] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n    [2023-05-25 14:57:07,982] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n    [2023-05-25 14:57:07,986] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-djjj8xny/archive.pdf[0] /tmp/paperless/paperless-djjj8xny/convert.webp\n    [2023-05-25 14:57:08,009] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n    [2023-05-25 14:57:08,011] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n    [2023-05-25 14:57:08,014] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-1wqpexro/archive.pdf[0] /tmp/paperless/paperless-1wqpexro/convert.webp\n    [2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Saving record to database\n    [2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.529478+02:00\n    [2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Saving record to database\n    [2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.457478+02:00\n    [2023-05-25 14:57:10,125] [INFO] [paperless.handlers] Assigning document type Eingangs Rechnungen to 2023-05-25 20230525_134142\n    [2023-05-25 14:57:10,143] [INFO] [paperless.handlers] Tagging \"2023-05-25 20230525_134142\" with \"Gewerbe\"\n    [2023-05-25 14:57:10,161] [INFO] [paperless.handlers] Assigning storage path Gewerbe_Eingangsrechnung to 2023-05-25 20230525_134142\n    [2023-05-25 14:57:10,223] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:57:10,236] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n    [2023-05-25 14:57:10,238] [DEBUG] [paperless.consumer] Deleting file /tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf\n    [2023-05-25 14:57:10,241] [DEBUG] [paperless.parsing.tesseract] Deleting directory /tmp/paperless/paperless-djjj8xny\n    [2023-05-25 14:57:10,242] [INFO] [paperless.consumer] Document 2023-05-25 20230525_134142 consumption finished\n\nDid you know what i am doing wrong?  \n\n\nAnd my second \"Problem\" in that case i was thinking that Paperless put the Barcode ID in the Document ID automatically?  \nOr does it need more \"Training\" data to know that it should do that?  \n\n\nThanks for your Help ;)", "author_fullname": "t2_4vwsum16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paperless-ngx (Docker) Barcode problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "business", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1436e8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Business Tools", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686122658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\ni recently created my Paperless instance and so far it works &amp;quot;okay&amp;quot; ;)  &lt;/p&gt;\n\n&lt;p&gt;I have 2 Problems:&lt;br/&gt;\n1. If i scan multiple Documents with Barcodes on each document, Paperless didnt split the PDF file.&lt;br/&gt;\nThe Barcodes get recognised&lt;/p&gt;\n\n&lt;p&gt;In the Log the Barcode with Code128 are the ones from my Sticker.  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[2023-05-25 14:55:03,087] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/Gewerbe/Eingangs_Rechnung/20230525_134142.pdf to the task queue.\n[2023-05-25 14:55:03,094] [INFO] [paperless.management.consumer] Adding /usr/src/paperless/consume/20230525_134142.pdf to the task queue.\n[2023-05-25 14:55:07,891] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,004] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,165] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n[2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n[2023-05-25 14:55:08,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,290] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: 23E00517929\n[2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00009\n[2023-05-25 14:55:08,291] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,331] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,458] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,522] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n[2023-05-25 14:55:08,523] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,648] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00010\n[2023-05-25 14:55:08,649] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,668] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,796] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,849] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n[2023-05-25 14:55:08,850] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00011\n[2023-05-25 14:55:08,989] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,014] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,136] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,196] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,306] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,338] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,441] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n[2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n[2023-05-25 14:55:09,533] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: VL0088449\n[2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00012\n[2023-05-25 14:55:09,629] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,689] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,779] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n002\n1\nSCT\n\nRG119608\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n[2023-05-25 14:55:09,941] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type QRCODE found: BCD\n002\n1\nSCT\n\nRG119608\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE39 found: RG119608\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Barcode of type CODE128 found: 00013\n[2023-05-25 14:55:10,021] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,091] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,166] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,257] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,326] [DEBUG] [paperless.barcodes] Scanning for barcodes using PYZBAR\n[2023-05-25 14:55:10,456] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n[2023-05-25 14:55:10,460] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n[2023-05-25 14:55:10,462] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n[2023-05-25 14:55:10,464] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n[2023-05-25 14:55:10,531] [INFO] [paperless.consumer] Consuming 20230525_134142.pdf\n[2023-05-25 14:55:10,535] [DEBUG] [paperless.consumer] Detected mime type: application/pdf\n[2023-05-25 14:55:10,536] [DEBUG] [paperless.consumer] Parser: RasterisedDocumentParser\n[2023-05-25 14:55:10,539] [DEBUG] [paperless.consumer] Parsing 20230525_134142.pdf...\n[2023-05-25 14:55:10,573] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {&amp;#39;input_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-ngxzbpeluif/20230525_134142.pdf&amp;#39;), &amp;#39;output_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-1wqpexro/archive.pdf&amp;#39;), &amp;#39;use_threads&amp;#39;: True, &amp;#39;jobs&amp;#39;: &amp;#39;2&amp;#39;, &amp;#39;language&amp;#39;: &amp;#39;eng&amp;#39;, &amp;#39;output_type&amp;#39;: &amp;#39;pdfa&amp;#39;, &amp;#39;progress_bar&amp;#39;: False, &amp;#39;skip_text&amp;#39;: True, &amp;#39;clean&amp;#39;: True, &amp;#39;deskew&amp;#39;: True, &amp;#39;rotate_pages&amp;#39;: True, &amp;#39;rotate_pages_threshold&amp;#39;: 12.0, &amp;#39;sidecar&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-1wqpexro/sidecar.txt&amp;#39;)}\n[2023-05-25 14:55:10,627] [DEBUG] [paperless.parsing.tesseract] Calling OCRmyPDF with args: {&amp;#39;input_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf&amp;#39;), &amp;#39;output_file&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-djjj8xny/archive.pdf&amp;#39;), &amp;#39;use_threads&amp;#39;: True, &amp;#39;jobs&amp;#39;: &amp;#39;2&amp;#39;, &amp;#39;language&amp;#39;: &amp;#39;eng&amp;#39;, &amp;#39;output_type&amp;#39;: &amp;#39;pdfa&amp;#39;, &amp;#39;progress_bar&amp;#39;: False, &amp;#39;skip_text&amp;#39;: True, &amp;#39;clean&amp;#39;: True, &amp;#39;deskew&amp;#39;: True, &amp;#39;rotate_pages&amp;#39;: True, &amp;#39;rotate_pages_threshold&amp;#39;: 12.0, &amp;#39;sidecar&amp;#39;: PosixPath(&amp;#39;/tmp/paperless/paperless-djjj8xny/sidecar.txt&amp;#39;)}\n[2023-05-25 14:56:11,255] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:11,267] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:11,273] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:11,277] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,398] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,433] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,442] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:56:59,455] [DEBUG] [paperless.filehandling] Document has storage_path 4 (Default/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:57:07,981] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n[2023-05-25 14:57:07,982] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n[2023-05-25 14:57:07,986] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&amp;gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-djjj8xny/archive.pdf[0] /tmp/paperless/paperless-djjj8xny/convert.webp\n[2023-05-25 14:57:08,009] [DEBUG] [paperless.parsing.tesseract] Using text from sidecar file\n[2023-05-25 14:57:08,011] [DEBUG] [paperless.consumer] Generating thumbnail for 20230525_134142.pdf...\n[2023-05-25 14:57:08,014] [DEBUG] [paperless.parsing] Execute: convert -density 300 -scale 500x5000&amp;gt; -alpha remove -strip -auto-orient /tmp/paperless/paperless-1wqpexro/archive.pdf[0] /tmp/paperless/paperless-1wqpexro/convert.webp\n[2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Saving record to database\n[2023-05-25 14:57:09,952] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.529478+02:00\n[2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Saving record to database\n[2023-05-25 14:57:09,968] [DEBUG] [paperless.consumer] Creation date from st_mtime: 2023-05-25 14:55:10.457478+02:00\n[2023-05-25 14:57:10,125] [INFO] [paperless.handlers] Assigning document type Eingangs Rechnungen to 2023-05-25 20230525_134142\n[2023-05-25 14:57:10,143] [INFO] [paperless.handlers] Tagging &amp;quot;2023-05-25 20230525_134142&amp;quot; with &amp;quot;Gewerbe&amp;quot;\n[2023-05-25 14:57:10,161] [INFO] [paperless.handlers] Assigning storage path Gewerbe_Eingangsrechnung to 2023-05-25 20230525_134142\n[2023-05-25 14:57:10,223] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:57:10,236] [DEBUG] [paperless.filehandling] Document has storage_path 2 (Gewerbe_Eingangsrechnung/{correspondent}/{created_year}/{asn}-{title}) set\n[2023-05-25 14:57:10,238] [DEBUG] [paperless.consumer] Deleting file /tmp/paperless/paperless-ngxu8thnhgd/20230525_134142.pdf\n[2023-05-25 14:57:10,241] [DEBUG] [paperless.parsing.tesseract] Deleting directory /tmp/paperless/paperless-djjj8xny\n[2023-05-25 14:57:10,242] [INFO] [paperless.consumer] Document 2023-05-25 20230525_134142 consumption finished\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Did you know what i am doing wrong?  &lt;/p&gt;\n\n&lt;p&gt;And my second &amp;quot;Problem&amp;quot; in that case i was thinking that Paperless put the Barcode ID in the Document ID automatically?&lt;br/&gt;\nOr does it need more &amp;quot;Training&amp;quot; data to know that it should do that?  &lt;/p&gt;\n\n&lt;p&gt;Thanks for your Help ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0ac01dca-53ce-11ed-9fce-c6cd629e2d85", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1436e8p", "is_robot_indexable": true, "report_reasons": null, "author": "Heartbeats_1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1436e8p/paperlessngx_docker_barcode_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1436e8p/paperlessngx_docker_barcode_problems/", "subreddit_subscribers": 252939, "created_utc": 1686122658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Looking for a good self hosted ticketing system that has:\n\n- Knowledge base before user can submit a ticket\n- Customizable ticketing submission page\n- And of course user friendly and reliable", "author_fullname": "t2_l4ihi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best self hosted docker ticketing systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142xkpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686096018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a good self hosted ticketing system that has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Knowledge base before user can submit a ticket&lt;/li&gt;\n&lt;li&gt;Customizable ticketing submission page&lt;/li&gt;\n&lt;li&gt;And of course user friendly and reliable&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142xkpx", "is_robot_indexable": true, "report_reasons": null, "author": "amcco1", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142xkpx/best_self_hosted_docker_ticketing_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142xkpx/best_self_hosted_docker_ticketing_systems/", "subreddit_subscribers": 252939, "created_utc": 1686096018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi, \ni have a folder called \"Backup\". \nI need a tool for windows which will automatically sync this folder to a s3 compatible service like idrive. I dont mind to buy or if the software is closed source. Function should be similair to the Google drive app for windows. \n\nThanks", "author_fullname": "t2_64q45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "software auto sync folder to s3 service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142mjhi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686071392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, \ni have a folder called &amp;quot;Backup&amp;quot;. \nI need a tool for windows which will automatically sync this folder to a s3 compatible service like idrive. I dont mind to buy or if the software is closed source. Function should be similair to the Google drive app for windows. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142mjhi", "is_robot_indexable": true, "report_reasons": null, "author": "SkapyTek", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142mjhi/software_auto_sync_folder_to_s3_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142mjhi/software_auto_sync_folder_to_s3_service/", "subreddit_subscribers": 252939, "created_utc": 1686071392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I have been going back and forth with Namecheap as I am trying to get them to help me create a SPF or DKIM record for my domain.\n\nHas anyone else successfully create these records for their Namecheap hosted domain?\n\n&amp;#x200B;\n\nEDIT: u/lolklolk's comment about Cloudflare's documentation lead me to what I think is a solution. Comparing their documentation to my records, I made some edits and was able to see my email go through without bouncing back.\n\n&amp;#x200B;", "author_fullname": "t2_d0mm8rgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Namecheap and DKIM/SPF records", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "solved", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ktuf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Solved", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686079131.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686067682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been going back and forth with Namecheap as I am trying to get them to help me create a SPF or DKIM record for my domain.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else successfully create these records for their Namecheap hosted domain?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT: &lt;a href=\"/u/lolklolk\"&gt;u/lolklolk&lt;/a&gt;&amp;#39;s comment about Cloudflare&amp;#39;s documentation lead me to what I think is a solution. Comparing their documentation to my records, I made some edits and was able to see my email go through without bouncing back.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7b9b9156-5ccb-11eb-9ab6-0e4839ba7cc3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142ktuf", "is_robot_indexable": true, "report_reasons": null, "author": "winston198451", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142ktuf/namecheap_and_dkimspf_records/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142ktuf/namecheap_and_dkimspf_records/", "subreddit_subscribers": 252939, "created_utc": 1686067682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hi, I have the following task on my desk: I want to backup my emails (with attachments) in an automated fashion. As of now I am using gmail using the web interface via browser.\n\nOfficial google backups are quite cumbersome to deal with. I read that the easiest way to keep a copy is to have a mail client (like thunderbird) and keep a backup of it. I tested thunderbird and it works fine (it can really mirror gmail via IMAP and you can import/export the backups).\n\nHowever, I want something similar that can run on my server. In particular, I am looking for:\n\n* something that can pull changes automatically every N-hours (not when opening the app)\n* something that can be self-contained in a docker that run on a headless server\n* has a web interface (like zimbra/roundcube/...) just to check if everything is working. However, this is not mandatory, but just nice to have.\n\nAny recommended workflow here?", "author_fullname": "t2_9mioauuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for suggestions: selfhosted server based mirror of gmail for backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1438gam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686130241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have the following task on my desk: I want to backup my emails (with attachments) in an automated fashion. As of now I am using gmail using the web interface via browser.&lt;/p&gt;\n\n&lt;p&gt;Official google backups are quite cumbersome to deal with. I read that the easiest way to keep a copy is to have a mail client (like thunderbird) and keep a backup of it. I tested thunderbird and it works fine (it can really mirror gmail via IMAP and you can import/export the backups).&lt;/p&gt;\n\n&lt;p&gt;However, I want something similar that can run on my server. In particular, I am looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;something that can pull changes automatically every N-hours (not when opening the app)&lt;/li&gt;\n&lt;li&gt;something that can be self-contained in a docker that run on a headless server&lt;/li&gt;\n&lt;li&gt;has a web interface (like zimbra/roundcube/...) just to check if everything is working. However, this is not mandatory, but just nice to have.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any recommended workflow here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1438gam", "is_robot_indexable": true, "report_reasons": null, "author": "-elmuz-", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1438gam/asking_for_suggestions_selfhosted_server_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1438gam/asking_for_suggestions_selfhosted_server_based/", "subreddit_subscribers": 252939, "created_utc": 1686130241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "TL;DR Latest version of Docker seems to be incompatible with LXC on Proxmox, and I need help to set up Docker, Compose, and Portainer on versions that do work in an LXC.\n\nI'm using Proxmox and I have an Ubuntu server 22.04.2 LXC. Following Docker's own install guides like [this one](https://docs.docker.com/engine/install/ubuntu/) as well as other guides like [this one](https://docs.fuga.cloud/how-to-install-portainer-docker-ui-manager-on-ubuntu-20.04-18.04-16.04) run into errors. Here is the error from trying a docker hello-world\n\n`sudo docker run hello-world`\n\n`docker: Error response from daemon: AppArmor enabled on system but the docker-default profile could not be loaded: running \\`/usr/sbin/apparmor\\_parser apparmor\\_parser -Kr /var/lib/docker/tmp/docker-default1024565780\\` failed with output: apparmor\\_parser: Unable to replace \"docker-default\".  Permission denied; attempted to load a profile while confined?\\`\n\n&amp;#x200B;\n\n`error: exit status 243.`\n\n`ERRO[0000] error waiting for container:`\n\nI have been chasing a rabbit hole of errors for a while, and bug posts like [this one](https://github.com/moby/moby/pull/44902) and [this one](https://github.com/moby/moby/issues/44900) as well as other posts [like this one](https://forum.proxmox.com/threads/priviledge-container-disabling-apparmor-does-not-work.122168/) have led me to believe that the problem I'm facing is that my LXC does not support Apparmor because as an LXC, it is using the kernel from the host OS (Proxmox) and the team behind one of the docker packages is trying to force Apparmor because of safety concerns. This seems to have come up at Docker v. 23, and was somehow sorted out for subsequent 23.x versions (apparently by making Apparmor no longer required, at least for the time being), but the latest install version is now 24.0.2 and the errors related to Apparmor are back. I'm all for safety, and if there's some way to get Apparmor actually working in Proxmox so that my LXC container can use it, that'd be great. But if not, I need a way to go back to an older version of docker, I guess.\n\nI don't know very much about installing packages that are older than the latest apt versions, so I can't figure out how to just install older versions of docker and related packages like portainer. That's why I'm asking here for help. Is there a CT Template somewhere (from a reputable source) that includes docker, compose, and portainer? Or could someone point me at a guide or even just the console command to install the working (older) versions of docker and its related packages? I can't just use latest because it seems that the problems are with the latest versions.\n\nThanks, and I hope this also helps anyone else who is banging their heads against the wall trying to get Docker to work in an LXC container in Portainer, and following guides that used to work no longer works because of these Apparmor errors.", "author_fullname": "t2_5jpmatk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for LXC-friendly Docker install instructions for use with Compose and Portainer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1436ekf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Need Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686122691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR Latest version of Docker seems to be incompatible with LXC on Proxmox, and I need help to set up Docker, Compose, and Portainer on versions that do work in an LXC.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Proxmox and I have an Ubuntu server 22.04.2 LXC. Following Docker&amp;#39;s own install guides like &lt;a href=\"https://docs.docker.com/engine/install/ubuntu/\"&gt;this one&lt;/a&gt; as well as other guides like &lt;a href=\"https://docs.fuga.cloud/how-to-install-portainer-docker-ui-manager-on-ubuntu-20.04-18.04-16.04\"&gt;this one&lt;/a&gt; run into errors. Here is the error from trying a docker hello-world&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sudo docker run hello-world&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker: Error response from daemon: AppArmor enabled on system but the docker-default profile could not be loaded: running \\&lt;/code&gt;/usr/sbin/apparmor_parser apparmor_parser -Kr /var/lib/docker/tmp/docker-default1024565780` failed with output: apparmor_parser: Unable to replace &amp;quot;docker-default&amp;quot;.  Permission denied; attempted to load a profile while confined?`&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;error: exit status 243.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ERRO[0000] error waiting for container:&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I have been chasing a rabbit hole of errors for a while, and bug posts like &lt;a href=\"https://github.com/moby/moby/pull/44902\"&gt;this one&lt;/a&gt; and &lt;a href=\"https://github.com/moby/moby/issues/44900\"&gt;this one&lt;/a&gt; as well as other posts &lt;a href=\"https://forum.proxmox.com/threads/priviledge-container-disabling-apparmor-does-not-work.122168/\"&gt;like this one&lt;/a&gt; have led me to believe that the problem I&amp;#39;m facing is that my LXC does not support Apparmor because as an LXC, it is using the kernel from the host OS (Proxmox) and the team behind one of the docker packages is trying to force Apparmor because of safety concerns. This seems to have come up at Docker v. 23, and was somehow sorted out for subsequent 23.x versions (apparently by making Apparmor no longer required, at least for the time being), but the latest install version is now 24.0.2 and the errors related to Apparmor are back. I&amp;#39;m all for safety, and if there&amp;#39;s some way to get Apparmor actually working in Proxmox so that my LXC container can use it, that&amp;#39;d be great. But if not, I need a way to go back to an older version of docker, I guess.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know very much about installing packages that are older than the latest apt versions, so I can&amp;#39;t figure out how to just install older versions of docker and related packages like portainer. That&amp;#39;s why I&amp;#39;m asking here for help. Is there a CT Template somewhere (from a reputable source) that includes docker, compose, and portainer? Or could someone point me at a guide or even just the console command to install the working (older) versions of docker and its related packages? I can&amp;#39;t just use latest because it seems that the problems are with the latest versions.&lt;/p&gt;\n\n&lt;p&gt;Thanks, and I hope this also helps anyone else who is banging their heads against the wall trying to get Docker to work in an LXC container in Portainer, and following guides that used to work no longer works because of these Apparmor errors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82508a80-05bc-11eb-ae09-0effb713859f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1436ekf", "is_robot_indexable": true, "report_reasons": null, "author": "ResearchTLDR", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/1436ekf/looking_for_lxcfriendly_docker_install/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/1436ekf/looking_for_lxcfriendly_docker_install/", "subreddit_subscribers": 252939, "created_utc": 1686122691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "I have several dozen containers and I want to establish a good backup policy. Currently, all containers that store data do so by bind mounts to directories on a BTRFS volume. The files are backed up directly from the host. For MySQL/PostgreSQL containers I've been using the fantastic  utility [tiredofit/db-backup](https://github.com/tiredofit/docker-db-backup), which does a hot dump of all the data.\n\nThe thing is that many of my containers use SQLite, and it is not recommended to make a copy of DB files without stopping the process first.  Again, I could make a copy with the \"tiredofit/db-backup\" utility, but I would need to configure it for every SQLite file (some containers even have more than one database).\n\nThis has the following drawbacks:\n\n1. I'm doing the dump of the databases  and, a few minutes later, the rest of the bind mount files (including the database files) a few minutes apart, so there could be inconsistencies between what is stored in the database (the dump) and the files the process would expect.\n\n2. Copying the files from the host sometimes gives errors because some files (mostly the SQLite database files) are in use.\n\n3. I need to manually configure the backup for each database. Its cumbersome and it's easy to forget. \n\n4. From what I have been reading, MySQL/PostgreSQL/SQLite database files are not recommended to be stored on a BTRFS volume. Some people recommend disabling CoW and compression, while others directly recommend using EXT4.\n\n5. I am copying the binary files of the databases, and also a dump of them. It's a waste of space and time.\n\nTo solve points 1,2 i 3 I think it would be best to make the copy by stopping the container first. I've been looking for utilities and I've found some like [offen/docker-volume-backup](https://github.com/offen/docker-volume-backup). Again, the disadvantage is that you have to configure everything (mount points, files, users, passwords...) manually for each container, so it is very easy to forget to create the backup of one of them. Also, the scheduling would not be centralised, so the backup wouldn't be run sequentially container after container. I've searched a lot, but I haven't found any good alternative.   How are you dealing with the backup of containers?\n\nFor point 4 and 5, I tried to bind the database files to a different volume than the rest of the files.  The problem is with SQLite files, as many times these files are generated dynamically or other files are created with extensions .shm, .wal, .journal, etc. and it is not possible to bind only these files. Is there an easy way to solve this?", "author_fullname": "t2_6q88r5er", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended container backup policy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142nvru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686073991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several dozen containers and I want to establish a good backup policy. Currently, all containers that store data do so by bind mounts to directories on a BTRFS volume. The files are backed up directly from the host. For MySQL/PostgreSQL containers I&amp;#39;ve been using the fantastic  utility &lt;a href=\"https://github.com/tiredofit/docker-db-backup\"&gt;tiredofit/db-backup&lt;/a&gt;, which does a hot dump of all the data.&lt;/p&gt;\n\n&lt;p&gt;The thing is that many of my containers use SQLite, and it is not recommended to make a copy of DB files without stopping the process first.  Again, I could make a copy with the &amp;quot;tiredofit/db-backup&amp;quot; utility, but I would need to configure it for every SQLite file (some containers even have more than one database).&lt;/p&gt;\n\n&lt;p&gt;This has the following drawbacks:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;m doing the dump of the databases  and, a few minutes later, the rest of the bind mount files (including the database files) a few minutes apart, so there could be inconsistencies between what is stored in the database (the dump) and the files the process would expect.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Copying the files from the host sometimes gives errors because some files (mostly the SQLite database files) are in use.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I need to manually configure the backup for each database. Its cumbersome and it&amp;#39;s easy to forget. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;From what I have been reading, MySQL/PostgreSQL/SQLite database files are not recommended to be stored on a BTRFS volume. Some people recommend disabling CoW and compression, while others directly recommend using EXT4.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I am copying the binary files of the databases, and also a dump of them. It&amp;#39;s a waste of space and time.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To solve points 1,2 i 3 I think it would be best to make the copy by stopping the container first. I&amp;#39;ve been looking for utilities and I&amp;#39;ve found some like &lt;a href=\"https://github.com/offen/docker-volume-backup\"&gt;offen/docker-volume-backup&lt;/a&gt;. Again, the disadvantage is that you have to configure everything (mount points, files, users, passwords...) manually for each container, so it is very easy to forget to create the backup of one of them. Also, the scheduling would not be centralised, so the backup wouldn&amp;#39;t be run sequentially container after container. I&amp;#39;ve searched a lot, but I haven&amp;#39;t found any good alternative.   How are you dealing with the backup of containers?&lt;/p&gt;\n\n&lt;p&gt;For point 4 and 5, I tried to bind the database files to a different volume than the rest of the files.  The problem is with SQLite files, as many times these files are generated dynamically or other files are created with extensions .shm, .wal, .journal, etc. and it is not possible to bind only these files. Is there an easy way to solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?auto=webp&amp;v=enabled&amp;s=77036abdba4a575cc0d0270ef6f596b772725a24", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c3c68d7dc133b978ad10a0ebb3d148f1e2599c7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d26a3144a4f346d3979b51284c2b850b0aafe5ad", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51c65ab189b6c22114010ccc8a026df898eef4a6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d42090e97c7bdc2f9981ca47b5f29a0cdda3c7e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a4e724e3777c6d4485b2c19fefde262ed1ee92", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/73HXrXdQfAY7Iq_15yL62HeyFBIiJTMnVmWONAzPecA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3a84538a60e146902ca83911ed2f83fa5aead3a", "width": 1080, "height": 540}], "variants": {}, "id": "74DkM8zpkYGSxR__DQfGL5olamLJhBfDHRR_SAtnU0M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142nvru", "is_robot_indexable": true, "report_reasons": null, "author": "KlinX79", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142nvru/recommended_container_backup_policy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142nvru/recommended_container_backup_policy/", "subreddit_subscribers": 252939, "created_utc": 1686073991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "selfhosted", "selftext": "Hello there,\n\nthere are good software like Viseron and other that support coral TPU for NVR CCTV, does anyone have bought one of this? Does it worth in terms of offload? I'm based in Italy, and it's practically impossible to find and in case it's really expensive compared to standard price,", "author_fullname": "t2_g2t6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "coral tpu for nvr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/selfhosted", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142isd1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686063661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.selfhosted", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;there are good software like Viseron and other that support coral TPU for NVR CCTV, does anyone have bought one of this? Does it worth in terms of offload? I&amp;#39;m based in Italy, and it&amp;#39;s practically impossible to find and in case it&amp;#39;s really expensive compared to standard price,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_32hch", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "142isd1", "is_robot_indexable": true, "report_reasons": null, "author": "spupuz", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/selfhosted/comments/142isd1/coral_tpu_for_nvr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/selfhosted/comments/142isd1/coral_tpu_for_nvr/", "subreddit_subscribers": 252939, "created_utc": 1686063661.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}