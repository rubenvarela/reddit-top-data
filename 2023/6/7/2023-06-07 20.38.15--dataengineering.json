{"kind": "Listing", "data": {"after": "t3_143dxhm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently in my first job with 2 years of experience. I feel lost and I'm not as confident as I probably should be in data engineering.\n\nWhat things should I be doing over the next few years to become more experienced and valuable as a Data Engineer?\n\n- What is data engineering really about? Which parts of data engineering are the most important?\n- Should I get experience with as many tools as possible, or focus on the most popular tools?\n- Are side/personal projects important or helpful? What projects could I do for data engineering?\n\nAny info would be great. There are so many things to learn that I feel paralyzed when I try to pick one.", "author_fullname": "t2_43sriaci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to become a good Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1432zk2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 117, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686111324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently in my first job with 2 years of experience. I feel lost and I&amp;#39;m not as confident as I probably should be in data engineering.&lt;/p&gt;\n\n&lt;p&gt;What things should I be doing over the next few years to become more experienced and valuable as a Data Engineer?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What is data engineering really about? Which parts of data engineering are the most important?&lt;/li&gt;\n&lt;li&gt;Should I get experience with as many tools as possible, or focus on the most popular tools?&lt;/li&gt;\n&lt;li&gt;Are side/personal projects important or helpful? What projects could I do for data engineering?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any info would be great. There are so many things to learn that I feel paralyzed when I try to pick one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1432zk2", "is_robot_indexable": true, "report_reasons": null, "author": "KP_DaBoi99", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1432zk2/how_to_become_a_good_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1432zk2/how_to_become_a_good_data_engineer/", "subreddit_subscribers": 109367, "created_utc": 1686111324.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our use cases for airflow will almost entirely involve it triggering other services that do the heavy lifting and have their own compute. I am really struggling to understand why I would need a full on cluster with separate workers, scheduler and webserver. Could I get away with deploying it on say a single ec2 instance or am I missing something obvious?", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow- am i missing something? Why does it need to be run on a large cluster with lots of workers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1438mkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686130858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our use cases for airflow will almost entirely involve it triggering other services that do the heavy lifting and have their own compute. I am really struggling to understand why I would need a full on cluster with separate workers, scheduler and webserver. Could I get away with deploying it on say a single ec2 instance or am I missing something obvious?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1438mkz", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1438mkz/airflow_am_i_missing_something_why_does_it_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1438mkz/airflow_am_i_missing_something_why_does_it_need/", "subreddit_subscribers": 109367, "created_utc": 1686130858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nNot sure if this is the right forum. I work in an industry service organisation that recently started moving its business and project data into Snowflake. The process of data ingestion is still new and being developed - a lot of the project data is Excel spreadsheets. \n\nAs a data analyst, I am finding Snowflake fast and easy to use, but a pathway to data documentation seems missing. Our organisation apparently has a plan for this but it's still in the planning stage. \n\nFor example, yesterday I came across sensor timestamp data that was stored \"No Time Zone\". I had no way to know what the time zone was. After asking around I was told the sensor generating the data uses UTC. However this information is not stored in Snowflake and I see no obvious way to store it there (even if I had permission to add documentation). Other columns also lack documentation, such as temperatures that could be in Celsius of Fahrenheit.\n\nI'm wondering how your company handles data documentation for data in something like Snowflake. Do you have a unified approach or tool? Do you store the documentation in the database itself?\n\nThanks!", "author_fullname": "t2_221frg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle data documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142si63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the right forum. I work in an industry service organisation that recently started moving its business and project data into Snowflake. The process of data ingestion is still new and being developed - a lot of the project data is Excel spreadsheets. &lt;/p&gt;\n\n&lt;p&gt;As a data analyst, I am finding Snowflake fast and easy to use, but a pathway to data documentation seems missing. Our organisation apparently has a plan for this but it&amp;#39;s still in the planning stage. &lt;/p&gt;\n\n&lt;p&gt;For example, yesterday I came across sensor timestamp data that was stored &amp;quot;No Time Zone&amp;quot;. I had no way to know what the time zone was. After asking around I was told the sensor generating the data uses UTC. However this information is not stored in Snowflake and I see no obvious way to store it there (even if I had permission to add documentation). Other columns also lack documentation, such as temperatures that could be in Celsius of Fahrenheit.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering how your company handles data documentation for data in something like Snowflake. Do you have a unified approach or tool? Do you store the documentation in the database itself?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142si63", "is_robot_indexable": true, "report_reasons": null, "author": "si_wo", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142si63/how_does_your_company_handle_data_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142si63/how_does_your_company_handle_data_documentation/", "subreddit_subscribers": 109367, "created_utc": 1686083836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Yet another data orchestration post. I'm trying to decide on which tool to introduce into our data team to standardise data pipelines. Our team is pretty small- 2 Data Engineers and 1 Data Analyst. Our main data sources are from our customer databases and systems that load files into ADLS and 3rd party SaaS tools. We have various ways of ingesting data into SQL Server (legacy) and Snowflake, stored procedures, bespoke application that runs on Azure Function App, Snowpipe from blob. \n\nWe're on Azure, with Snowflake, and are introducing DBT into the mix, along with an orchestrator. Admittedly, I don't have too much experience working with an orchestration tool. Looking for one that doesn't require too much maintenance, is easy to manage, not too expensive and has good integrations with our stack.\n\nWe're currently evaluating Airflow, Prefect, Dagster\n\n* Airflow: Easy to build pipelines, Flexible, good integrations. I've heard it can be difficult to manage and is slightly outdated and bloated.\n* Prefect: Like Airflow but easy to manage. Free Prefect Cloud option looks appealing for our small team but not sure how soon we'd run into limitations which force license upgrades. \n* Dagster: Love the concept and can see how you can build really powerful, robust pipelines. Just a bit of a      learning curve. Good integrations.\n\nAnyone have any recommendations for our case? Thanks in advance!", "author_fullname": "t2_4di18tzv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deciding on an Orchestrator: Airflow, Prefect &amp; Dagster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142trzs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686086754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another data orchestration post. I&amp;#39;m trying to decide on which tool to introduce into our data team to standardise data pipelines. Our team is pretty small- 2 Data Engineers and 1 Data Analyst. Our main data sources are from our customer databases and systems that load files into ADLS and 3rd party SaaS tools. We have various ways of ingesting data into SQL Server (legacy) and Snowflake, stored procedures, bespoke application that runs on Azure Function App, Snowpipe from blob. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re on Azure, with Snowflake, and are introducing DBT into the mix, along with an orchestrator. Admittedly, I don&amp;#39;t have too much experience working with an orchestration tool. Looking for one that doesn&amp;#39;t require too much maintenance, is easy to manage, not too expensive and has good integrations with our stack.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re currently evaluating Airflow, Prefect, Dagster&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Airflow: Easy to build pipelines, Flexible, good integrations. I&amp;#39;ve heard it can be difficult to manage and is slightly outdated and bloated.&lt;/li&gt;\n&lt;li&gt;Prefect: Like Airflow but easy to manage. Free Prefect Cloud option looks appealing for our small team but not sure how soon we&amp;#39;d run into limitations which force license upgrades. &lt;/li&gt;\n&lt;li&gt;Dagster: Love the concept and can see how you can build really powerful, robust pipelines. Just a bit of a      learning curve. Good integrations.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone have any recommendations for our case? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142trzs", "is_robot_indexable": true, "report_reasons": null, "author": "TshirtJefferson", "discussion_type": null, "num_comments": 20, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142trzs/deciding_on_an_orchestrator_airflow_prefect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142trzs/deciding_on_an_orchestrator_airflow_prefect/", "subreddit_subscribers": 109367, "created_utc": 1686086754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ELT is such a common pattern, and I just want to make sure that I'm understanding it correctly.\n\nI've been reading through Data Pipelines Pocket Reference from O'Riley, the book lays out a common ELT architecture like so (comments in parentheses my own):\n\n* Extract - Read data from some source system and land it in S3 (raw data lake?), possibly performing some minor transformations (EtLT) (processed data lake?) to enable downstream loading.\n* Load - Basically just use Redshift's COPY command to bring the landed data into staging tables in the data warehouse.\n* Transform - Use dbt to build models from the staging tables.\n\nI actually think that I have a good enough understanding of ELT, where the confusion is is with the initial S3 bucket(s) I extract my data to. The ultimate goal is to build up a single source of truth in a data warehouse, but with the Et part of my ELT taking place in S3, it *feels* like I'm halfway towards building a data lake! All I need to do is throw Athena in the mix and it'd be a functional one!\n\nIn summary, my questions would be:\n\n* Is it normal to extract all data to a bucket in an ELT pipeline?\n* Would this bucket be considered a data lake?\n* Why not build an analytical lake w/ Athena as an alternative to Redshift?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me understand ELT! Should I be moving data through both my data lake and data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142xmcx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686096128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ELT is such a common pattern, and I just want to make sure that I&amp;#39;m understanding it correctly.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reading through Data Pipelines Pocket Reference from O&amp;#39;Riley, the book lays out a common ELT architecture like so (comments in parentheses my own):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Extract - Read data from some source system and land it in S3 (raw data lake?), possibly performing some minor transformations (EtLT) (processed data lake?) to enable downstream loading.&lt;/li&gt;\n&lt;li&gt;Load - Basically just use Redshift&amp;#39;s COPY command to bring the landed data into staging tables in the data warehouse.&lt;/li&gt;\n&lt;li&gt;Transform - Use dbt to build models from the staging tables.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I actually think that I have a good enough understanding of ELT, where the confusion is is with the initial S3 bucket(s) I extract my data to. The ultimate goal is to build up a single source of truth in a data warehouse, but with the Et part of my ELT taking place in S3, it &lt;em&gt;feels&lt;/em&gt; like I&amp;#39;m halfway towards building a data lake! All I need to do is throw Athena in the mix and it&amp;#39;d be a functional one!&lt;/p&gt;\n\n&lt;p&gt;In summary, my questions would be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it normal to extract all data to a bucket in an ELT pipeline?&lt;/li&gt;\n&lt;li&gt;Would this bucket be considered a data lake?&lt;/li&gt;\n&lt;li&gt;Why not build an analytical lake w/ Athena as an alternative to Redshift?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142xmcx", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142xmcx/help_me_understand_elt_should_i_be_moving_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142xmcx/help_me_understand_elt_should_i_be_moving_data/", "subreddit_subscribers": 109367, "created_utc": 1686096128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nMy employer is offering 2k towards a class/certification.  I have extensive experience in SQL and have done data migrations the old school way of using stored procedures etc. I really want to get a better understanding of the tools and methods of running a data pipeline. I have limited experience with Python. Is there a recommendation for a good \u201call-in-one\u201d data engineering course I can take? I\u2019d really like to take up the offer of using the 2k towards this. \n\nThanks!", "author_fullname": "t2_7y30rgjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2k towards a course in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143gt1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686152909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;My employer is offering 2k towards a class/certification.  I have extensive experience in SQL and have done data migrations the old school way of using stored procedures etc. I really want to get a better understanding of the tools and methods of running a data pipeline. I have limited experience with Python. Is there a recommendation for a good \u201call-in-one\u201d data engineering course I can take? I\u2019d really like to take up the offer of using the 2k towards this. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143gt1s", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Fox940", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143gt1s/2k_towards_a_course_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143gt1s/2k_towards_a_course_in_data_engineering/", "subreddit_subscribers": 109367, "created_utc": 1686152909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I feel dirty even asking this since whenever someone posts \"How do I learn about X?\", someone inevitably posts \"Go read 'The Big Book of X' by [very important person]\". It seems to be a very popular way to learn.\n\nI have never once read any book related to data. I've read some blogs, articles, websites, watched some videos, but largely learnt from the others around me or through tackling the challenges I encounter in my job and projects on an ad hoc basis. In some cases, I've learnt through a few courses. But never once have I read a technical knowledge book.", "author_fullname": "t2_d505p8is", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone learn without reading books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143f4n0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686149000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel dirty even asking this since whenever someone posts &amp;quot;How do I learn about X?&amp;quot;, someone inevitably posts &amp;quot;Go read &amp;#39;The Big Book of X&amp;#39; by [very important person]&amp;quot;. It seems to be a very popular way to learn.&lt;/p&gt;\n\n&lt;p&gt;I have never once read any book related to data. I&amp;#39;ve read some blogs, articles, websites, watched some videos, but largely learnt from the others around me or through tackling the challenges I encounter in my job and projects on an ad hoc basis. In some cases, I&amp;#39;ve learnt through a few courses. But never once have I read a technical knowledge book.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143f4n0", "is_robot_indexable": true, "report_reasons": null, "author": "Length-Working", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143f4n0/does_anyone_learn_without_reading_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143f4n0/does_anyone_learn_without_reading_books/", "subreddit_subscribers": 109367, "created_utc": 1686149000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nIf any of you are like me, I get so sucked into what I am working on that day that I end up sitting in my chair for hours at a time. I wanted to put together a project that would help me stop that trend, and I thought I would share because I am sure there are others in the community that struggle with the same issue. \n\n[https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/](https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/)\n\nIn this project, I go from end-to-end and create a daily alert to stretch if I haven't already. Here is a brief outline of the project:\n\n* Use Python code to call the Peloton API to get my personal workout data\n* Do some basic cleaning with Python on the data\n* Upload the data to Snowflake\n* Set up an alert with SQL that will send an email to me if I haven't stretched by 1PM. \n\nThis project was completed in Shipyard (which is where I work), however it could be completed using any orchestration tool or even just scripts. \n\nLet me know what you think. If there are any improvements that you could note, feel free to pass those along as well!", "author_fullname": "t2_pvgyqb8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End-to-End Data Project to Send an Automated Reminder to Stretch Throughout the Day", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143dvll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686145910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;If any of you are like me, I get so sucked into what I am working on that day that I end up sitting in my chair for hours at a time. I wanted to put together a project that would help me stop that trend, and I thought I would share because I am sure there are others in the community that struggle with the same issue. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/\"&gt;https://www.shipyardapp.com/blog/turning-your-peloton-data-into-a-personal-fitness-reminder-a-data-driven-approach/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this project, I go from end-to-end and create a daily alert to stretch if I haven&amp;#39;t already. Here is a brief outline of the project:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use Python code to call the Peloton API to get my personal workout data&lt;/li&gt;\n&lt;li&gt;Do some basic cleaning with Python on the data&lt;/li&gt;\n&lt;li&gt;Upload the data to Snowflake&lt;/li&gt;\n&lt;li&gt;Set up an alert with SQL that will send an email to me if I haven&amp;#39;t stretched by 1PM. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This project was completed in Shipyard (which is where I work), however it could be completed using any orchestration tool or even just scripts. &lt;/p&gt;\n\n&lt;p&gt;Let me know what you think. If there are any improvements that you could note, feel free to pass those along as well!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?auto=webp&amp;v=enabled&amp;s=53e4f42d4b968cf96281d54558f49e0e2dc06c0b", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e06cb2de3fe205c2525a8f16da78d8dec24e4ad", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b77b899e5ac04903779b0da694ddcb33097a2219", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3c08fe3ca38f4d220909733f36c17ff7300b4e3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f47e3cf99f5eb182dba016c2b06d28a0e943bca", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af3d408011a6ba1cea8c307a9d16c88af30e00c9", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/qSqIfq4eB0k63x5JSQOJzXlajN_b6pcjgYKwRyAr43c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33d69a5e958572130638050edf963eabb1448572", "width": 1080, "height": 607}], "variants": {}, "id": "-EBJ3hUdCCn6i7lpzQ3kGn1v-Ye1nHFdLirmF50JK4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "143dvll", "is_robot_indexable": true, "report_reasons": null, "author": "Steven_Johnson34", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143dvll/endtoend_data_project_to_send_an_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143dvll/endtoend_data_project_to_send_an_automated/", "subreddit_subscribers": 109367, "created_utc": 1686145910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I don't understand it, on the resource group my user that's creating the access connector is the owner and I even added Databricks Resource Provider as an owner to the resource group too, I have no clue what else I can do to get the authorisation working, has anyone ran into this issue before?\n\nThanks everyone!", "author_fullname": "t2_56o0g58i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Databricks User not authorized (403) error when creating Access Connector for Azure Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142wlbp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686093393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t understand it, on the resource group my user that&amp;#39;s creating the access connector is the owner and I even added Databricks Resource Provider as an owner to the resource group too, I have no clue what else I can do to get the authorisation working, has anyone ran into this issue before?&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142wlbp", "is_robot_indexable": true, "report_reasons": null, "author": "IG-55", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142wlbp/azure_databricks_user_not_authorized_403_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142wlbp/azure_databricks_user_not_authorized_403_error/", "subreddit_subscribers": 109367, "created_utc": 1686093393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have ~3.5 years of experience as an analyst. I'm pretty good with SQL, Power Automate, and PowerBI. I have some experience with database design in Access. \n\nI work for a company that is building out a new data warehouse using the Microsoft suite. SSIS, SSAS, SSRS, tabular modeling, etc. Locally hosted server to be mirrored in Azure to ease WFM accessibility. Our department is filling a data engineering gap with external contractors and my boss wants me to find a course so I can train up to take over the role.\n\nCan anyone recommend a reputable, paid online course I could look into? Some kind of accreditation would be great.", "author_fullname": "t2_4xvmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paid course recommendations a data analyst could take to become an engineer / database architect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143epp9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686148002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have ~3.5 years of experience as an analyst. I&amp;#39;m pretty good with SQL, Power Automate, and PowerBI. I have some experience with database design in Access. &lt;/p&gt;\n\n&lt;p&gt;I work for a company that is building out a new data warehouse using the Microsoft suite. SSIS, SSAS, SSRS, tabular modeling, etc. Locally hosted server to be mirrored in Azure to ease WFM accessibility. Our department is filling a data engineering gap with external contractors and my boss wants me to find a course so I can train up to take over the role.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a reputable, paid online course I could look into? Some kind of accreditation would be great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "143epp9", "is_robot_indexable": true, "report_reasons": null, "author": "funkyman50", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143epp9/paid_course_recommendations_a_data_analyst_could/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143epp9/paid_course_recommendations_a_data_analyst_could/", "subreddit_subscribers": 109367, "created_utc": 1686148002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ok Reddit, help me figure out what I am. \n\nI\u2019ve got about 6 years of SQL experience, querying, inserting, deleting, and updating data in live production SSMS databases on a daily basis. \n\nI\u2019ve migrated data from one db to another with a matching schema with various methods, but the most streamlined of which included a Python script/program to retrieve the data from database(s) make any necessary manipulations with mapping files or utilizing pandas data frame merges, and inserting it into a new database. \n\nIt\u2019s as terrifying as it sounds. \n\nI\u2019ve been in a new role the last year that is far less technical and far more business, mostly using sql to build looker dashboards, and it\u2019s boring. I\u2019m hoping I can move into DE but my concern is my skills will fall short as I have almost no experience with big data/cloud infrastructure, and really have no clue how to bridge that gap. \n\nAny inputs on what kind of positions I could fit my current skill set into? My previous position seems fairly niche, and I can\u2019t seem to find something similar.", "author_fullname": "t2_fgr2zl76", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m a data\u2026 wannabe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142wx4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686094249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok Reddit, help me figure out what I am. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got about 6 years of SQL experience, querying, inserting, deleting, and updating data in live production SSMS databases on a daily basis. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve migrated data from one db to another with a matching schema with various methods, but the most streamlined of which included a Python script/program to retrieve the data from database(s) make any necessary manipulations with mapping files or utilizing pandas data frame merges, and inserting it into a new database. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s as terrifying as it sounds. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been in a new role the last year that is far less technical and far more business, mostly using sql to build looker dashboards, and it\u2019s boring. I\u2019m hoping I can move into DE but my concern is my skills will fall short as I have almost no experience with big data/cloud infrastructure, and really have no clue how to bridge that gap. &lt;/p&gt;\n\n&lt;p&gt;Any inputs on what kind of positions I could fit my current skill set into? My previous position seems fairly niche, and I can\u2019t seem to find something similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "142wx4e", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway682749273", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142wx4e/im_a_data_wannabe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142wx4e/im_a_data_wannabe/", "subreddit_subscribers": 109367, "created_utc": 1686094249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_zia33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Profiler 0.9.0 -- offering a massive improvement to memory usage during profiling of large datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_143dscv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sejrMas-9x1K5DtGLpcxdJ_h-N4dt9jYl13XnnKB-ss.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686145681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/capitalone/DataProfiler", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?auto=webp&amp;v=enabled&amp;s=e5536a80b6750ba08e3bf3856396b1dd19f3fbc3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc011712f24eebfe017ce880714982d8cdec6f42", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3afb78a8d1a5bb0c1baf7c3413391ae6e363e1b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff1f1a543249dd5270e2efb9385ef330e13dff79", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9097872352ace9c407ce48682cbe0acae1757d25", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93509909321fafcfde66039cafa368a0a1722127", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4DHw2iqbdi7YwXZ3KxDByStoFJEvtehodn-6yWB-4rE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17b39558456f9099164009ff3235052559b3b9dd", "width": 1080, "height": 540}], "variants": {}, "id": "WBr7WyealiMEllP6S-Ob6zF8PTPg7XX4xon-M5lsTe0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143dscv", "is_robot_indexable": true, "report_reasons": null, "author": "fitz_n_fitz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143dscv/data_profiler_090_offering_a_massive_improvement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/capitalone/DataProfiler", "subreddit_subscribers": 109367, "created_utc": 1686145681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ayc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Daft: A High-Performance Distributed Dataframe Library for Multimodal Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "name": "t3_143g5la", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jwGyLjzZbEIgBtBgbuKaWsGg1LxyH7-fX-cny7P6PQc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686151394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.getdaft.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.getdaft.io/p/introducing-daft-a-high-performance", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?auto=webp&amp;v=enabled&amp;s=af44267447f9459af1d070fc65d1957009020626", "width": 946, "height": 369}, "resolutions": [{"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a645bf20db63c3833dd03a6b7c5637d856189d42", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=266bc632d794474fd89bbdc267fdb0f9b6165094", "width": 216, "height": 84}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3da78b4472f60f2f820e4e7beb924ec94e3c53b", "width": 320, "height": 124}, {"url": "https://external-preview.redd.it/qolNplVW4KvrBvY5GaM4Y4-0qkRB9fTyU7yuBt0YYDo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8df22713f1d50180e70199fb801c3b8e2c0dd566", "width": 640, "height": 249}], "variants": {}, "id": "kEXEo96ULUyl0I7ys3DrbgOGio_zYSY3_GKj6DhNg2Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143g5la", "is_robot_indexable": true, "report_reasons": null, "author": "fridder", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143g5la/introducing_daft_a_highperformance_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.getdaft.io/p/introducing-daft-a-high-performance", "subreddit_subscribers": 109367, "created_utc": 1686151394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding Cosine Similarity in Python with Scikit-Learn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_143fs64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9oP57TbhSd9XCV1dSV6Zee29dDpjqJVqYsnkDj13m-s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686150526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/cosine-similarity-python-scikit-learn", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?auto=webp&amp;v=enabled&amp;s=3533b46296685791e839082dc4d459e9aad1a4e3", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=651a8521db318040bd7f58a8e6d13b3482a02812", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b2d7eed8f5880f5a82559056042e715d8dcd369", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2524ed207c5fa0f5cfbaf0675d5ab8084f4a450f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d48fd8fe765f0b57e12a24e238754479c9c19e92", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6d275bca85ecea83ce88d06a36f925262a2af3a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/HVJqAzISRfv2Ys-V40OalOlp7GXiReHBrr1JstHmiKg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fc8cc096d38070d7a1b0ffc49c175a69afc1aba", "width": 1080, "height": 540}], "variants": {}, "id": "UnF7YPlNeqhl2V2N7ZVLJ2Qso2narmOfw0iTBRCWBzc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "143fs64", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143fs64/understanding_cosine_similarity_in_python_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/cosine-similarity-python-scikit-learn", "subreddit_subscribers": 109367, "created_utc": 1686150526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Excuse my ignorance, this still is well before my time. Trying to learn as much as quick as possible.\n\nNeed to land mainframe data to raw S3 bucket. A couple of questions:\n\n1. Could Informatica be used to accomplish this? If not, what are other simple alternatives?\n2. I'm assuming it would be best practice to ingest this data in a more friendly format (e.g., JSON). Would there be a significant amount of effort required to do this?\n3. Any other general suggestions/resources/experiences would be greatly appreciated.", "author_fullname": "t2_1fco9rqs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting VSAM files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142sjv8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excuse my ignorance, this still is well before my time. Trying to learn as much as quick as possible.&lt;/p&gt;\n\n&lt;p&gt;Need to land mainframe data to raw S3 bucket. A couple of questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Could Informatica be used to accomplish this? If not, what are other simple alternatives?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m assuming it would be best practice to ingest this data in a more friendly format (e.g., JSON). Would there be a significant amount of effort required to do this?&lt;/li&gt;\n&lt;li&gt;Any other general suggestions/resources/experiences would be greatly appreciated.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142sjv8", "is_robot_indexable": true, "report_reasons": null, "author": "gencoupethrowaway69", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142sjv8/ingesting_vsam_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142sjv8/ingesting_vsam_files/", "subreddit_subscribers": 109367, "created_utc": 1686083949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DEs!\n\nI have my virtual onsite in 1-2 weeks for the Data Engineering role at Meta. I was wondering if anyone was also interviewing and would be interested in doing some Product Sense and Data Modeling mock interviews (we can both act as interviewers). If you are a Data Scientist or Product Manager who is also interviewing at Meta, feel free to also reach out. Let me know in a comment or DM.", "author_fullname": "t2_k349c07v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Partner for Meta Mock Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142sdcw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DEs!&lt;/p&gt;\n\n&lt;p&gt;I have my virtual onsite in 1-2 weeks for the Data Engineering role at Meta. I was wondering if anyone was also interviewing and would be interested in doing some Product Sense and Data Modeling mock interviews (we can both act as interviewers). If you are a Data Scientist or Product Manager who is also interviewing at Meta, feel free to also reach out. Let me know in a comment or DM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "142sdcw", "is_robot_indexable": true, "report_reasons": null, "author": "Maleficent-Flight775", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142sdcw/partner_for_meta_mock_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142sdcw/partner_for_meta_mock_interview/", "subreddit_subscribers": 109367, "created_utc": 1686083528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in search of a tool or a service that can be configured to keep two databases with slightly differing models in sync. \n\nI am looking to capture the source database changes in real time, transform the data as needed and propagate the changes to the destination database. \n\nWhat standalone tools OR AWS services can be used for this task?", "author_fullname": "t2_w1tdhbrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools or methods would work best for Postgres-&gt;Postgres CDC with transformations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143c63e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686141618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in search of a tool or a service that can be configured to keep two databases with slightly differing models in sync. &lt;/p&gt;\n\n&lt;p&gt;I am looking to capture the source database changes in real time, transform the data as needed and propagate the changes to the destination database. &lt;/p&gt;\n\n&lt;p&gt;What standalone tools OR AWS services can be used for this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143c63e", "is_robot_indexable": true, "report_reasons": null, "author": "data_pie3", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143c63e/what_tools_or_methods_would_work_best_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143c63e/what_tools_or_methods_would_work_best_for/", "subreddit_subscribers": 109367, "created_utc": 1686141618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# [Jailer Database Tools.](https://wisser.github.io/Jailer/)\n\nJailer is a tool for database subsetting and relational data browsing.\n\nIt creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Subsetter creates small slices from your database (consistent and referentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.\n\nThe Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.\n\n# Features\n\nExports consistent and referentially intact row-sets from your productive database and imports the data into your development and test environment.\n\nImproves database performance by removing and archiving obsolete data without violating integrity.\n\nGenerates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.\n\nData Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.", "author_fullname": "t2_5sa5b0ia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jailer Database Tools v15 released", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143aj03", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686137026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;&lt;a href=\"https://wisser.github.io/Jailer/\"&gt;Jailer Database Tools.&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;Jailer is a tool for database subsetting and relational data browsing.&lt;/p&gt;\n\n&lt;p&gt;It creates small slices from your database and lets you navigate through your database following the relationships.Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Subsetter creates small slices from your database (consistent and referentially intact) as SQL (topologically sorted), DbUnit records or XML. Ideal for creating small samples of test data or for local problem analysis with relevant production data.&lt;/p&gt;\n\n&lt;p&gt;The Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.&lt;/p&gt;\n\n&lt;h1&gt;Features&lt;/h1&gt;\n\n&lt;p&gt;Exports consistent and referentially intact row-sets from your productive database and imports the data into your development and test environment.&lt;/p&gt;\n\n&lt;p&gt;Improves database performance by removing and archiving obsolete data without violating integrity.&lt;/p&gt;\n\n&lt;p&gt;Generates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.&lt;/p&gt;\n\n&lt;p&gt;Data Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "143aj03", "is_robot_indexable": true, "report_reasons": null, "author": "Plane-Discussion", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143aj03/jailer_database_tools_v15_released/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143aj03/jailer_database_tools_v15_released/", "subreddit_subscribers": 109367, "created_utc": 1686137026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for a solid stack to start with data-science.\n\n\\- Pandas\n\n\\- NumPy\n\n\\- scikit-learn\n\n\\- TensorFlow\n\n\\- PyTorch\n\n\\- Keras\n\n\\- Jupyter Notebook\n\n\\- ...\n\n&amp;#x200B;\n\ncan anyone refer to such a stack, which i can deploy on my docker-server?\n\n&amp;#x200B;\n\ntyvm!!", "author_fullname": "t2_9yw5ycsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a MLOps Stack for Docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1435020", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686117754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a solid stack to start with data-science.&lt;/p&gt;\n\n&lt;p&gt;- Pandas&lt;/p&gt;\n\n&lt;p&gt;- NumPy&lt;/p&gt;\n\n&lt;p&gt;- scikit-learn&lt;/p&gt;\n\n&lt;p&gt;- TensorFlow&lt;/p&gt;\n\n&lt;p&gt;- PyTorch&lt;/p&gt;\n\n&lt;p&gt;- Keras&lt;/p&gt;\n\n&lt;p&gt;- Jupyter Notebook&lt;/p&gt;\n\n&lt;p&gt;- ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;can anyone refer to such a stack, which i can deploy on my docker-server?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;tyvm!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1435020", "is_robot_indexable": true, "report_reasons": null, "author": "alber7777", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1435020/looking_for_a_mlops_stack_for_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1435020/looking_for_a_mlops_stack_for_docker/", "subreddit_subscribers": 109367, "created_utc": 1686117754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is too dumb to ask this question, but I don't truly understand the importance of dimensional modelling. Can't is simply pull exact replicas of oltp data to a database/datawarehouse(as per my knowledge, I feel both are similar when we consider the storage part)  and build bi reports on top of it without building fact and dim tables. lets take the example of simple book sales.   \n\n\nIn OLTP we  have 3 tables books, authors and sales. even if we build fact and dim tables we will end up having same tables except that we will not have relation ships between dim tables(books and authors) that exists in oltp and aditionally we use date dim tables in dimensional modelling.  \n\n\nThese changes really make the process of building bi reports effective? because I can build the reports even on exact copy of oltp tables without spending time on building dim and fact tables.  \n\n\nAgain sorry if my question looks very basic and dumb. Thank you!", "author_fullname": "t2_v1vre9oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building Bi reports on OLTP model without dimensional modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1433zob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686114454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is too dumb to ask this question, but I don&amp;#39;t truly understand the importance of dimensional modelling. Can&amp;#39;t is simply pull exact replicas of oltp data to a database/datawarehouse(as per my knowledge, I feel both are similar when we consider the storage part)  and build bi reports on top of it without building fact and dim tables. lets take the example of simple book sales.   &lt;/p&gt;\n\n&lt;p&gt;In OLTP we  have 3 tables books, authors and sales. even if we build fact and dim tables we will end up having same tables except that we will not have relation ships between dim tables(books and authors) that exists in oltp and aditionally we use date dim tables in dimensional modelling.  &lt;/p&gt;\n\n&lt;p&gt;These changes really make the process of building bi reports effective? because I can build the reports even on exact copy of oltp tables without spending time on building dim and fact tables.  &lt;/p&gt;\n\n&lt;p&gt;Again sorry if my question looks very basic and dumb. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1433zob", "is_robot_indexable": true, "report_reasons": null, "author": "sach_mess10", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1433zob/building_bi_reports_on_oltp_model_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1433zob/building_bi_reports_on_oltp_model_without/", "subreddit_subscribers": 109367, "created_utc": 1686114454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need advice on picking a cloud data storage/server (AWS, Azure, Google Cloud) to store roughly 50GB of data in parquet, csv, and pickle. I run python scripts hourly and daily to update them. Which service is faster/cheaper/reliable. I run batch scripts hourly and daily to update that data and then push to Salesforce, so the data flow is non-stop. I am thinking of storing the python scripts into the cloud to have the whole ETL process in cloud. Please give me advice. I am new to Cloud Engineering/Architecture", "author_fullname": "t2_ue7511np", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142wfjj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686092994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need advice on picking a cloud data storage/server (AWS, Azure, Google Cloud) to store roughly 50GB of data in parquet, csv, and pickle. I run python scripts hourly and daily to update them. Which service is faster/cheaper/reliable. I run batch scripts hourly and daily to update that data and then push to Salesforce, so the data flow is non-stop. I am thinking of storing the python scripts into the cloud to have the whole ETL process in cloud. Please give me advice. I am new to Cloud Engineering/Architecture&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142wfjj", "is_robot_indexable": true, "report_reasons": null, "author": "datatulga", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142wfjj/i_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142wfjj/i_need_advice/", "subreddit_subscribers": 109367, "created_utc": 1686092994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using this custom data validation library called Cerberus. Cerberus has a pre-built function, validate(), that validates data based on a table schema and set of validation rules that you give it. The function can either take in a row(in the form of a dictionary), or an entire pandas dataframe. \n\nThe function itself can take a really long time if the amount of data is large. Is there any way that I can partition the data and run the function against the partitions on multiple clusters in parallel?\n\nI've tried looking at many of Sparks features and couldn't figure out a solution ):", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some help integrating Spark with an open source data validation library.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_143meqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686166010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using this custom data validation library called Cerberus. Cerberus has a pre-built function, validate(), that validates data based on a table schema and set of validation rules that you give it. The function can either take in a row(in the form of a dictionary), or an entire pandas dataframe. &lt;/p&gt;\n\n&lt;p&gt;The function itself can take a really long time if the amount of data is large. Is there any way that I can partition the data and run the function against the partitions on multiple clusters in parallel?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried looking at many of Sparks features and couldn&amp;#39;t figure out a solution ):&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143meqi", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/143meqi/need_some_help_integrating_spark_with_an_open/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143meqi/need_some_help_integrating_spark_with_an_open/", "subreddit_subscribers": 109367, "created_utc": 1686166010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,  \nJust wanted to ask whether the tech stack used in Oracle-based companies is the same as for AWS /  GCP / Azure?\n\nI apologise if this was already answered but nobody ever talks about Oracle Cloud so it's difficult to find any info.", "author_fullname": "t2_3zd9k2sf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tech stack is used with Oracle Cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143eupv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686148340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;br/&gt;\nJust wanted to ask whether the tech stack used in Oracle-based companies is the same as for AWS /  GCP / Azure?&lt;/p&gt;\n\n&lt;p&gt;I apologise if this was already answered but nobody ever talks about Oracle Cloud so it&amp;#39;s difficult to find any info.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "143eupv", "is_robot_indexable": true, "report_reasons": null, "author": "Fluffdaddy0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143eupv/what_tech_stack_is_used_with_oracle_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143eupv/what_tech_stack_is_used_with_oracle_cloud/", "subreddit_subscribers": 109367, "created_utc": 1686148340.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are building up our data warehouse/marts, but we\u2019ve got about 3k metabase queries which are complex and based on the raw tables.  Some queries hit snowflake and some hit Postgres directly.\n\nAre there any catalog/lineage/audit tools which can help identify \u201ccommon joins\u201d or \u201cCTE similarity\u201d?\n\nOr do we just need to:\n\n- elbow grease our way through the list, prioritized by usage/value, and extract common data model concepts \n\n- agile user development asking users what they want.", "author_fullname": "t2_1qwjj2kn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metabase/Dashboard Tech Debt: Tools to help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143e9ij", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686146873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are building up our data warehouse/marts, but we\u2019ve got about 3k metabase queries which are complex and based on the raw tables.  Some queries hit snowflake and some hit Postgres directly.&lt;/p&gt;\n\n&lt;p&gt;Are there any catalog/lineage/audit tools which can help identify \u201ccommon joins\u201d or \u201cCTE similarity\u201d?&lt;/p&gt;\n\n&lt;p&gt;Or do we just need to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;elbow grease our way through the list, prioritized by usage/value, and extract common data model concepts &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;agile user development asking users what they want.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143e9ij", "is_robot_indexable": true, "report_reasons": null, "author": "tomhallett", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143e9ij/metabasedashboard_tech_debt_tools_to_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143e9ij/metabasedashboard_tech_debt_tools_to_help/", "subreddit_subscribers": 109367, "created_utc": 1686146873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My objective is to create .parquet single file in Azure data lake. But when I do it with spark it always creates the additional nested folder and files and my name of the file starts with part. Is it even possible  to create a single .parquet file without using pandas or .to pandas() ? I am also restricted to use the file delete script. PS: I am using Azure Databricks.", "author_fullname": "t2_26t0xas9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": ".toParquet()", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143dxhm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686146048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My objective is to create .parquet single file in Azure data lake. But when I do it with spark it always creates the additional nested folder and files and my name of the file starts with part. Is it even possible  to create a single .parquet file without using pandas or .to pandas() ? I am also restricted to use the file delete script. PS: I am using Azure Databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "143dxhm", "is_robot_indexable": true, "report_reasons": null, "author": "engg_garbage98", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/143dxhm/toparquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/143dxhm/toparquet/", "subreddit_subscribers": 109367, "created_utc": 1686146048.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}