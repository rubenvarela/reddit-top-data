{"kind": "Listing", "data": {"after": "t3_142tari", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. [So far, we have archived 10.81 billion links, with 150 million to go](https://tracker.archiveteam.org/reddit/).\n\nRecent news of the [Reddit API cost changes](https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/) will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone *indefinitely* unless this issue is resolved. We are archiving Reddit posts so that in the event that the API cost change is never addressed, we can still access posts from those closed subreddits.\n\n#Here is how you can help:\n\n### [Choose the \"host\" that matches your current PC, probably Windows or macOS](https://www.virtualbox.org/wiki/Downloads)\n\n### [Download ArchiveTeam Warrior](https://tracker.archiveteam.org/)\n\n1. In VirtualBox, click File &gt; Import Appliance and open the file.\n2. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.\n\nOnce you\u2019ve started your warrior:\n\n1. Go to http://localhost:8001/ and check the Settings page.\n2. Choose a username \u2014 we\u2019ll show your progress on the leaderboard.\n3. Go to the \"All projects\" tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Reddit).\n\n### Alternative Method: Docker\n#### **[Download Docker on your \"host\" (Windows, macOS, Linux)](https://docs.docker.com/get-docker/)**\n### **[Follow the instructions on the ArchiveTeam website to set up Docker](https://wiki.archiveteam.org/index.php/Running_Archive_Team_Projects_with_Docker)**\n\nWhen setting up the project container, it will ask you to enter this command:\n\n```docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username]```\n\nMake sure to replace the [image address] with the Reddit project address (removing brackets): ```atdr.meo.ws/archiveteam/reddit-grab```\nAlso change the [username] to whatever you'd like, no need to register for anything.\n\n####More information about running this project:\n\n**[Information about setting up the project](https://github.com/ArchiveTeam/reddit-grab)**\n\n**[ArchiveTeam Wiki page on the Reddit project](https://wiki.archiveteam.org/index.php?title=Reddit)**\n\n##**IMPORTANT: Do NOT modify scripts or the Warrior client!**", "author_fullname": "t2_ucb1wzy", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142l1i0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1272, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1272, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1686104641.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686068100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. &lt;a href=\"https://tracker.archiveteam.org/reddit/\"&gt;So far, we have archived 10.81 billion links, with 150 million to go&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Recent news of the &lt;a href=\"https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/\"&gt;Reddit API cost changes&lt;/a&gt; will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone &lt;em&gt;indefinitely&lt;/em&gt; unless this issue is resolved. We are archiving Reddit posts so that in the event that the API cost change is never addressed, we can still access posts from those closed subreddits.&lt;/p&gt;\n\n&lt;h1&gt;Here is how you can help:&lt;/h1&gt;\n\n&lt;h3&gt;&lt;a href=\"https://www.virtualbox.org/wiki/Downloads\"&gt;Choose the &amp;quot;host&amp;quot; that matches your current PC, probably Windows or macOS&lt;/a&gt;&lt;/h3&gt;\n\n&lt;h3&gt;&lt;a href=\"https://tracker.archiveteam.org/\"&gt;Download ArchiveTeam Warrior&lt;/a&gt;&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In VirtualBox, click File &amp;gt; Import Appliance and open the file.&lt;/li&gt;\n&lt;li&gt;Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you\u2019ve started your warrior:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to http://localhost:8001/ and check the Settings page.&lt;/li&gt;\n&lt;li&gt;Choose a username \u2014 we\u2019ll show your progress on the leaderboard.&lt;/li&gt;\n&lt;li&gt;Go to the &amp;quot;All projects&amp;quot; tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Reddit).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Alternative Method: Docker&lt;/h3&gt;\n\n&lt;h4&gt;&lt;strong&gt;&lt;a href=\"https://docs.docker.com/get-docker/\"&gt;Download Docker on your &amp;quot;host&amp;quot; (Windows, macOS, Linux)&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt;\n\n&lt;h3&gt;&lt;strong&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php/Running_Archive_Team_Projects_with_Docker\"&gt;Follow the instructions on the ArchiveTeam website to set up Docker&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;p&gt;When setting up the project container, it will ask you to enter this command:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Make sure to replace the [image address] with the Reddit project address (removing brackets): &lt;code&gt;atdr.meo.ws/archiveteam/reddit-grab&lt;/code&gt;\nAlso change the [username] to whatever you&amp;#39;d like, no need to register for anything.&lt;/p&gt;\n\n&lt;h4&gt;More information about running this project:&lt;/h4&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://github.com/ArchiveTeam/reddit-grab\"&gt;Information about setting up the project&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php?title=Reddit\"&gt;ArchiveTeam Wiki page on the Reddit project&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h2&gt;&lt;strong&gt;IMPORTANT: Do NOT modify scripts or the Warrior client!&lt;/strong&gt;&lt;/h2&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142l1i0", "is_robot_indexable": true, "report_reasons": null, "author": "BananaBus43", "discussion_type": null, "num_comments": 141, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/", "subreddit_subscribers": 686361, "created_utc": 1686068100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nWith the recent news about RARBG going down, and us being saved by some archivist's scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don't we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.", "author_fullname": "t2_ctq800ysv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring torrent sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424g6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent news about RARBG going down, and us being saved by some archivist&amp;#39;s scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don&amp;#39;t we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424g6h", "is_robot_indexable": true, "report_reasons": null, "author": "Busy-Paramedic-4994", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "subreddit_subscribers": 686361, "created_utc": 1686028010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I figured I'd throw an option out there for Windows users looking to generate hashes and validate their files based on those hashes.\n\nGenerating hashes and validating files with those hashes should be a trivial matter.\n\nFor Linux you can easily use `md5sum` to generate hashes of files and run recursively through a folder with `find /folder -type f -exec md5sum {} + &gt;&gt; hash.log`\n\nFor Windows it's not quite as straightforward, usually you have to resort to third party apps like TeraCopy, Hashdeep, DirHash, CRCCheckCopy, among others. However, it is possible using native Windows Powershell.\n\nFIRST AND FOREMOST... I take no responsibility in anything happening to your data. I'm just trying to offer options. This has not been tested extensively, but I personally use it and it seems to work well. I am not a programmer but I did piecemeal this code together. It may not be the most efficient, but it works. This will not modify any data, only generate log files of MD5 hashes and validation results.\n\nIf you notice any bugs or anything odd about it, please let me know, but I don't plan on adding many additional features. I wanted it pretty bare bones. Feel free to modify as you see fit for your own use.\n\n**GENERATING MD5 HASHES RECURSIVELY**\n\nAt it's most basic level you can use this \"one-liner\" script to generate hashes recursively through folders, which outputs results to file \"hashlog.log\":\n\n    gci -Path \"&lt;folder to hash&gt;\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | ft -AutoSize &gt; \"hashlog.log\"\n\nBasic, but it works. It doesn't display anything to console and it uses absolute paths instead of relative paths, which might not be real useful for validation reasons. It also pads it with a bunch of trailing spaces equal to the longest path name. This is default behavior for this type of output.\n\nTo run it, just CD to the folder where you want to store the log file, copy/paste the above in a powershell window, modify \"&lt;folder to hash&gt;\" to the file path you want to recursively generate hashes for every file in, and hit [ENTER]. Obviously the more files you have the longer it will take. This code does not provide any output to console.\n\nIf you want something more robust, this \"one-liner\" is quite long but a bit more useful and mimics output from md5sum, basically `hashvalue \\relative\\path\\to\\file` Example Output:\n\n    19D4BB0121058149C33BA98DBC3ED149 \\SMALL\\10KB\\10KB_000000.txt\n    94E29C41059CB19819C1934B500797DD \\SMALL\\10KB\\10KB_000001.txt\n    EA5E5FC8B434FD4924FDEC5FDBFE226B \\SMALL\\10KB\\10KB_000002.txt\n    9EA101052926CB0BE10342EF9468FC3E \\SMALL\\10KB\\10KB_000003.txt\n    B952DA5732E657FB961310AC4AB55493 \\SMALL\\10KB\\10KB_000004.txt\n\nThis below script outputs to console, outputs paths relative to the input path, allows you to specify the name of the hashlog, trims any empty whitespace, and provides a header with date and file info at the top:\n\n    $hashpath=\"E:\\Test it\"; $hashlog=\"hashlog.log\"; $numfiles=(gci -Path \"$hashpath\" -Recurse -File | measure-object).Count; Write-Output \"Date: $(Get-Date) / Path: $hashpath / Files: $numfiles\" | Set-Content \"$hashlog\"; $count=1; gci -Path \"$hashpath\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | % {$_.Path=($_.Path -replace [regex]::Escape(\"$hashpath\"), ''); Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path; ++$count; $_} | ft -AutoSize -HideTableHeaders | Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath \"$hashlog\" -Append; (gc \"$hashlog\").Trim() -ne \"\" | sc \"$hashlog\"\n\nJust modify `$hashpath` to the folder you want to hash, `$hashlog` for the name of the log file, and let it rip.\n\nOr if you wish you can copy/paste the below into a notepad file, save it as something like `generatemd5hash.ps1`, then right click and run with powershell. It's the same as the above \"one-liner\" just a bit more readable:\n\n    $hashpath=\"E:\\Test it\"; $hashlog=\"hashlog.log\"\n    $numfiles=(gci -Path \"$hashpath\" -Recurse -File | measure-object).Count\n    Write-Output \"Date: $(Get-Date) / Path: $hashpath / Files: $numfiles\" | Set-Content \"$hashlog\"\n    $count=1\n    gci -Path \"$hashpath\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | \n        % {$_.Path=($_.Path -replace [regex]::Escape(\"$hashpath\"), ''); Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path; ++$count; $_} | \n        ft -AutoSize -HideTableHeaders | \n        Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath \"$hashlog\" -Append\n    (gc \"$hashlog\").Trim() -ne \"\" | sc \"$hashlog\"\n\n**VALIDATING HASHES**\n\nMost hashing utilities offer a way to validate hashes based on an existing pregenerated log of hashes. In this case you can't exactly validate directly using the log file. What you can do is generate a new set of hashes and compare the two.\n\nPowershell comes with a nifty little command called `Compare-Object` or also known as `Diff`. It will very quickly find differences between contents of text files.\n\nIt's basic command set is as follows:\n\n    Compare-Object -ReferenceObject (Get-Content \"&lt;hash log file 1&gt;\") -DifferenceObject (Get-Content \"&lt;hash log file 2&gt;\")\n\nThis will then spit out items in file 1 that aren't in file 2 and vice versa. File 1 objects are identified with a \"&lt;=\" flag and File 2 objects are identified with a \"=&gt;\" flag.\n\nSo if you generate hashes of your data, and then some time in the future, you can generate another set of hashes and then compare them using the above command to validate for any changes (due to bitrot or other).\n\nLike the hash generation code above, I also created a more robust hash comparison \"one-liner\" that you can use:\n\n    $hash1=\"hash1.md5\"; $hash2=\"hash2.md5\"; $hashdifflog=\"hashdifflog.log\"; Write-Output \"Date: $(Get-Date) / Compare '$hash1' (&lt;=) with '$hash2' (=&gt;)\" | sc \"$hashdifflog\"; diff -ReferenceObject (gc \"$hash1\" | Select -Skip 1) -DifferenceObject (gc \"$hash2\" | Select -Skip 1) | group { $_.InputObject -replace '^.+ ' } | % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | Out-File -Encoding ASCII -Filepath \"$hashdifflog\" -Append; gc $hashdifflog\n\nSet `$hash1` variable to the first hash log file and `$hash2` to the second hash log file you want to compare. Set `$hashdifflog` variable to whatever you want the log file to be named, by default it's \"hashdifflog.log\" and will store it to whatever folder you run this powershell script from. It will also provide date and file reference info at the top.\n\nAntoher thing to note is the `Select -Skip 1` command. This will effectively skip the first line of data because if you used the above Powershell script to generate hashes, it will genrate a header with Date and file info that you don't want to put in the comparison mix. So if your files don't have any header info you can just change the Skip value to 0.\n\nThis script will group matching file names from each log file that have non-matching hashes. Files that are unique to each file will be shown on individual lines. If there are no discrepencies it will be blank, which is ideally what you want.\n\nOr as before, you can also copy/paste the below into a notepad document, save it with a `.ps1` extension (i.e. `hashcompare.ps1`), right click and run with powershell.\n\n    $hash1=\"hash1.md5\"\n    $hash2=\"hash2.md5\"\n    $hashdifflog=\"hashdifflog.log\"\n    Write-Output \"Date: $(Get-Date) / Compare '$hash1' (&lt;=) with '$hash2' (=&gt;)\" | sc \"$hashdifflog\"\n    diff -ReferenceObject (gc \"$hash1\" | Select -Skip 1) -DifferenceObject (gc \"$hash2\" | Select -Skip 1) | \n        group { $_.InputObject -replace '^.+ ' } | \n        % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | \n        Out-File -Encoding ASCII -Filepath \"$hashdifflog\" -Append\n    gc $hashdifflog\n\n\n**INTERACTIVE SCRIPT**\n\nBut if you don't want to fuss with any of the above I wrote this basic Powershell script that will allow you generate hash log files, and compare hash log files just by entering the appropriate info at the prompts.\n\nA few notes:\n\n1. When you go to compare hash files, it will ask you how many lines to skip. It will present you with the first 9 lines of each log file you specify and you can tell it how many lines to omit from the comparison. So if you added any remarks or header info to the hash log files, you can just have it ignored. Selecting \"Skip 3\" will skip the first three lines shown, \"Skip 1\" just the first line, etc. Default will be to skip no lines.\n\n2. This does an EXACT COMPARE of file names as strings, although it is not case sensitive. It is designed so all paths will start with a slash. It has no logic to assume forward slash and backwards slash are interchangeable. So if you need to compare output from a Linux machine with md5sum to this script run from a Windows machine you will have to modify the slashes. This can be easily rectified with the following (backwards slash to forwards slash, just revere them to get the reverse effect):\n\n    `(gc \"hash1.md5\").replace('\\', '/') | sc \"hashes test 1.md5\"`\n\n3. Log files are appended with a date time stamp number value equal to `yyyymmdd_HHmmss`\n\n4. This has been tested with a log file with over 60 thousand entries and worked fine. File path length should not be an issue, but I haven't tested it past about 200 characters.\n\nYou can copy/paste the powershell script from pastebin here: https://pastebin.com/0UJ0gcdb\n\nOr here's the same code below. Just copy/paste into notepad and save as `generatemd5.ps1` or whatever as long as it has a `.ps1` extension. It will save log files where you save it, so just be wary of that.\n\n    # generatemd5.ps1 by HTWingNut 06 June 2023\n    Clear-Host\n    Write-Host \"\"\n    $datetime = Get-Date\n    $timestamp = $datetime.ToString(\"yyyyMMdd_HHmmss\")\n    $reqpath = 'Input path to generate md5 hashes'\n    $basepath = $hash1 = $hash2 = \"?\"\n    $choice=$null\n    $md5log = 'hashes_'+$timestamp+'.md5'\n    \n    function GenerateHash {\n    \n    Clear-Host\n    Write-Host \"\"\n    Write-Host \"Generate Hash Files ...\"\n    Write-Host \"\"\n    \n    while (!(Test-Path -Path $basepath)) { $basepath = Read-Host -Prompt $reqpath; if ( $basepath -eq \"\") { $basepath=\"?\" } }\n    $numfiles = ( Get-ChildItem -Path \"$basepath\" -Recurse -File | Measure-Object ).Count\n    \n    $lchar = $basepath.Substring($basepath.length-1,1)\n    If ($lchar -eq \"\\\") { $basepath = $basepath.Substring(0,$basepath.length-1) }\n    \n    Write-Host \"Total Files: $numfiles\"\n    Write-Host \"\"\n    $count=1\n    \n    Write-Output \"**** $($datetime) '$($basepath)'\" | Set-Content \"$md5log\"\n    \n    Get-ChildItem -Path \"$basepath\" -Recurse -File | \n        Get-FileHash -Algorithm MD5 | \n        Select-Object Hash,Path | \n        ForEach-Object { \n            $_.Path = ($_.Path -replace [regex]::Escape(\"$basepath\"), '')\n            Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path\n            ++$count\n            $_\n        } | \n        Format-Table -AutoSize -HideTableHeaders | \n        Out-String -Width 4096 | \n        Out-File \"$md5log\" -encoding ASCII -append\n    \n    (get-content $md5log).Trim() -ne '' | Set-Content $md5log\n    \n    Write-Host \"\"\n    Write-Host \"Hashes stored in file '$pwd\\$md5log'\"\n    Write-Host \"\"\n    \n    }\n    \n    function CompareHash {\n    \n    $continue = \"n\"\n    \n    While ([bool]$continue) {\n    \n    Clear-Host\n    Write-Host \"\"\n    Write-Host \"Compare Hash Files...\"\n    \n    $hash1 = $hash2 = \"?\"\n    $numksip1 = $numskip2 = \"0\"\n    \n    Write-Host \"\"\n    while (!(Test-Path $hash1)) { $hash1 = Read-Host -Prompt \"Enter Path/Name for Log 1\"; if ( $hash1 -eq \"\") { $hash1=\"?\" } }\n    Write-Host \"\"\n    Write-Host \"First 9 lines of $($hash1):\"\n    $i=1; Get-Content $hash1 | Select -First 9 | % {\"$($i) $_\"; $i++}\n    Write-Host \"\"\n    Do { $numskip1 = Read-Host \"Choose Number of Lines to Skip [0-9] (default 0)\" } until ($numskip1 -in 0,1,2,3,4,5,6,7,8,9)\n    Write-Host \"\"\n    \n    while (!(Test-Path $hash2)) { $hash2 = Read-Host -Prompt \"Enter Path/Name for Log 2\"; if ( $hash2 -eq \"\") { $hash2=\"?\" } }\n    Write-Host \"\"\n    Write-Host \"First 9 lines of $($hash2):\"\n    $i=1; Get-Content $hash2 | Select -First 9 | % {\"$($i) $_\"; $i++}\n    Write-Host \"\"\n    Do { $numskip2 = Read-Host \"Choose Number of Lines to Skip [0-9] (default 0)\" } until ($numskip2 -in 0,1,2,3,4,5,6,7,8,9)\n    \n    $hclog = \"HashCompare_$(((Get-Item $hash1).Basename).Replace(' ','_'))_vs_$(((Get-Item $hash2).Basename).Replace(' ','_'))_$timestamp.txt\"\n    \n        Write-Host \"\"\n        Write-Host \"---------------------\"\n        Write-Host \"\"\n    \n        Write-Host \"**** File: '$((Get-Item $hash1).Name)'\"\n        Write-Host \"**** Skipping Lines:\"\n        Get-Content $hash1 | Select -First $numskip1\n        Write-Host \"\"\n        Write-Host \"**** Starting with Line:\"\n        Get-Content $hash1 | Select -Index ($numskip1)\n        Write-Host \"\"\n        Write-Host \"---------------------\"\n        Write-Host \"\"\n        Write-Host \"**** File: '$((Get-Item $hash2).Name)'\"\n        Write-Host \"**** Skipping Lines:\"\n        Get-Content $hash2 | Select -First $numskip2\n        Write-Host \"\"\n        Write-Host \"**** Starting with Line:\"\n    \n        Get-Content $hash2 | Select -Index ($numskip2)\n        Write-Host\n        $continue = Read-Host \"Press [ENTER] to accept, any other key to choose again\"\n    \n    }\n    \n    # https://stackoverflow.com/questions/76415338/in-powershell-how-do-i-split-text-from-compare-object-input-and-sort-by-one-of/\n    \n    $diff = Compare-Object -ReferenceObject (Get-Content \"$hash1\" | Select -Skip $numskip1) -DifferenceObject (Get-Content \"$hash2\" | Select -Skip $numskip2 ) | \n        ForEach-Object {\n            if ($_.SideIndicator -eq \"&lt;=\") { $_.SideIndicator = \"($((Get-ChildItem $hash1).Name))\" } elseif ($_.SideIndicator -eq \"=&gt;\") { $_.SideIndicator = \"($((Get-ChildItem $hash2).Name))\" }\n            $_\n        } |\n        Group-Object { $_.InputObject -replace '^.+ ' } |\n        ForEach-Object {\n            $_.Group | Format-Table -HideTableHeaders | \n                Out-String | ForEach-Object TrimEnd\n        }\n    \n    if ($diff) {\n        Write-output \"**** $($datetime) '$($hash1)' vs '$($hash2)'`n**** Matching file path/names with mismatched hashes are grouped together`n**** Individual file names means unique file/hash in that log file\" $diff &gt; \"$hclog\"\n    } else {\n        Write-Output \"**** $($datetime) '$($hash1)' vs '$($hash2)' `n`n**** ALL CLEAR! NO DIFFERENCES FOUND!\" &gt; \"$hclog\"\n    }\n    \n    Write-Host \"\"\n    Write-Host \"**** Results stored in $($pwd)\\$($hclog)\"\n    Write-Host \"\"\n    \n    \n    }\n    \n    Do { $choice = Read-Host \"[G]enerate Hash or [C]ompare Hash Logs\" } until ($choice -in 'g','c')\n    If ( $choice -eq \"g\" ) { GenerateHash }\n    If ( $choice -eq \"c\" ) { CompareHash }\n    \n    cmd /c 'pause'", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here is a Windows Powershell Script to Generate and Validate MD5 Hashes of Your Data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ysqt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1686099759.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686099374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I figured I&amp;#39;d throw an option out there for Windows users looking to generate hashes and validate their files based on those hashes.&lt;/p&gt;\n\n&lt;p&gt;Generating hashes and validating files with those hashes should be a trivial matter.&lt;/p&gt;\n\n&lt;p&gt;For Linux you can easily use &lt;code&gt;md5sum&lt;/code&gt; to generate hashes of files and run recursively through a folder with &lt;code&gt;find /folder -type f -exec md5sum {} + &amp;gt;&amp;gt; hash.log&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;For Windows it&amp;#39;s not quite as straightforward, usually you have to resort to third party apps like TeraCopy, Hashdeep, DirHash, CRCCheckCopy, among others. However, it is possible using native Windows Powershell.&lt;/p&gt;\n\n&lt;p&gt;FIRST AND FOREMOST... I take no responsibility in anything happening to your data. I&amp;#39;m just trying to offer options. This has not been tested extensively, but I personally use it and it seems to work well. I am not a programmer but I did piecemeal this code together. It may not be the most efficient, but it works. This will not modify any data, only generate log files of MD5 hashes and validation results.&lt;/p&gt;\n\n&lt;p&gt;If you notice any bugs or anything odd about it, please let me know, but I don&amp;#39;t plan on adding many additional features. I wanted it pretty bare bones. Feel free to modify as you see fit for your own use.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GENERATING MD5 HASHES RECURSIVELY&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At it&amp;#39;s most basic level you can use this &amp;quot;one-liner&amp;quot; script to generate hashes recursively through folders, which outputs results to file &amp;quot;hashlog.log&amp;quot;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gci -Path &amp;quot;&amp;lt;folder to hash&amp;gt;&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | ft -AutoSize &amp;gt; &amp;quot;hashlog.log&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Basic, but it works. It doesn&amp;#39;t display anything to console and it uses absolute paths instead of relative paths, which might not be real useful for validation reasons. It also pads it with a bunch of trailing spaces equal to the longest path name. This is default behavior for this type of output.&lt;/p&gt;\n\n&lt;p&gt;To run it, just CD to the folder where you want to store the log file, copy/paste the above in a powershell window, modify &amp;quot;&amp;lt;folder to hash&amp;gt;&amp;quot; to the file path you want to recursively generate hashes for every file in, and hit [ENTER]. Obviously the more files you have the longer it will take. This code does not provide any output to console.&lt;/p&gt;\n\n&lt;p&gt;If you want something more robust, this &amp;quot;one-liner&amp;quot; is quite long but a bit more useful and mimics output from md5sum, basically &lt;code&gt;hashvalue \\relative\\path\\to\\file&lt;/code&gt; Example Output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;19D4BB0121058149C33BA98DBC3ED149 \\SMALL\\10KB\\10KB_000000.txt\n94E29C41059CB19819C1934B500797DD \\SMALL\\10KB\\10KB_000001.txt\nEA5E5FC8B434FD4924FDEC5FDBFE226B \\SMALL\\10KB\\10KB_000002.txt\n9EA101052926CB0BE10342EF9468FC3E \\SMALL\\10KB\\10KB_000003.txt\nB952DA5732E657FB961310AC4AB55493 \\SMALL\\10KB\\10KB_000004.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This below script outputs to console, outputs paths relative to the input path, allows you to specify the name of the hashlog, trims any empty whitespace, and provides a header with date and file info at the top:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hashpath=&amp;quot;E:\\Test it&amp;quot;; $hashlog=&amp;quot;hashlog.log&amp;quot;; $numfiles=(gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | measure-object).Count; Write-Output &amp;quot;Date: $(Get-Date) / Path: $hashpath / Files: $numfiles&amp;quot; | Set-Content &amp;quot;$hashlog&amp;quot;; $count=1; gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | % {$_.Path=($_.Path -replace [regex]::Escape(&amp;quot;$hashpath&amp;quot;), &amp;#39;&amp;#39;); Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path; ++$count; $_} | ft -AutoSize -HideTableHeaders | Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath &amp;quot;$hashlog&amp;quot; -Append; (gc &amp;quot;$hashlog&amp;quot;).Trim() -ne &amp;quot;&amp;quot; | sc &amp;quot;$hashlog&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Just modify &lt;code&gt;$hashpath&lt;/code&gt; to the folder you want to hash, &lt;code&gt;$hashlog&lt;/code&gt; for the name of the log file, and let it rip.&lt;/p&gt;\n\n&lt;p&gt;Or if you wish you can copy/paste the below into a notepad file, save it as something like &lt;code&gt;generatemd5hash.ps1&lt;/code&gt;, then right click and run with powershell. It&amp;#39;s the same as the above &amp;quot;one-liner&amp;quot; just a bit more readable:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hashpath=&amp;quot;E:\\Test it&amp;quot;; $hashlog=&amp;quot;hashlog.log&amp;quot;\n$numfiles=(gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | measure-object).Count\nWrite-Output &amp;quot;Date: $(Get-Date) / Path: $hashpath / Files: $numfiles&amp;quot; | Set-Content &amp;quot;$hashlog&amp;quot;\n$count=1\ngci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | \n    % {$_.Path=($_.Path -replace [regex]::Escape(&amp;quot;$hashpath&amp;quot;), &amp;#39;&amp;#39;); Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path; ++$count; $_} | \n    ft -AutoSize -HideTableHeaders | \n    Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath &amp;quot;$hashlog&amp;quot; -Append\n(gc &amp;quot;$hashlog&amp;quot;).Trim() -ne &amp;quot;&amp;quot; | sc &amp;quot;$hashlog&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;VALIDATING HASHES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Most hashing utilities offer a way to validate hashes based on an existing pregenerated log of hashes. In this case you can&amp;#39;t exactly validate directly using the log file. What you can do is generate a new set of hashes and compare the two.&lt;/p&gt;\n\n&lt;p&gt;Powershell comes with a nifty little command called &lt;code&gt;Compare-Object&lt;/code&gt; or also known as &lt;code&gt;Diff&lt;/code&gt;. It will very quickly find differences between contents of text files.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basic command set is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Compare-Object -ReferenceObject (Get-Content &amp;quot;&amp;lt;hash log file 1&amp;gt;&amp;quot;) -DifferenceObject (Get-Content &amp;quot;&amp;lt;hash log file 2&amp;gt;&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This will then spit out items in file 1 that aren&amp;#39;t in file 2 and vice versa. File 1 objects are identified with a &amp;quot;&amp;lt;=&amp;quot; flag and File 2 objects are identified with a &amp;quot;=&amp;gt;&amp;quot; flag.&lt;/p&gt;\n\n&lt;p&gt;So if you generate hashes of your data, and then some time in the future, you can generate another set of hashes and then compare them using the above command to validate for any changes (due to bitrot or other).&lt;/p&gt;\n\n&lt;p&gt;Like the hash generation code above, I also created a more robust hash comparison &amp;quot;one-liner&amp;quot; that you can use:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hash1=&amp;quot;hash1.md5&amp;quot;; $hash2=&amp;quot;hash2.md5&amp;quot;; $hashdifflog=&amp;quot;hashdifflog.log&amp;quot;; Write-Output &amp;quot;Date: $(Get-Date) / Compare &amp;#39;$hash1&amp;#39; (&amp;lt;=) with &amp;#39;$hash2&amp;#39; (=&amp;gt;)&amp;quot; | sc &amp;quot;$hashdifflog&amp;quot;; diff -ReferenceObject (gc &amp;quot;$hash1&amp;quot; | Select -Skip 1) -DifferenceObject (gc &amp;quot;$hash2&amp;quot; | Select -Skip 1) | group { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } | % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | Out-File -Encoding ASCII -Filepath &amp;quot;$hashdifflog&amp;quot; -Append; gc $hashdifflog\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Set &lt;code&gt;$hash1&lt;/code&gt; variable to the first hash log file and &lt;code&gt;$hash2&lt;/code&gt; to the second hash log file you want to compare. Set &lt;code&gt;$hashdifflog&lt;/code&gt; variable to whatever you want the log file to be named, by default it&amp;#39;s &amp;quot;hashdifflog.log&amp;quot; and will store it to whatever folder you run this powershell script from. It will also provide date and file reference info at the top.&lt;/p&gt;\n\n&lt;p&gt;Antoher thing to note is the &lt;code&gt;Select -Skip 1&lt;/code&gt; command. This will effectively skip the first line of data because if you used the above Powershell script to generate hashes, it will genrate a header with Date and file info that you don&amp;#39;t want to put in the comparison mix. So if your files don&amp;#39;t have any header info you can just change the Skip value to 0.&lt;/p&gt;\n\n&lt;p&gt;This script will group matching file names from each log file that have non-matching hashes. Files that are unique to each file will be shown on individual lines. If there are no discrepencies it will be blank, which is ideally what you want.&lt;/p&gt;\n\n&lt;p&gt;Or as before, you can also copy/paste the below into a notepad document, save it with a &lt;code&gt;.ps1&lt;/code&gt; extension (i.e. &lt;code&gt;hashcompare.ps1&lt;/code&gt;), right click and run with powershell.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hash1=&amp;quot;hash1.md5&amp;quot;\n$hash2=&amp;quot;hash2.md5&amp;quot;\n$hashdifflog=&amp;quot;hashdifflog.log&amp;quot;\nWrite-Output &amp;quot;Date: $(Get-Date) / Compare &amp;#39;$hash1&amp;#39; (&amp;lt;=) with &amp;#39;$hash2&amp;#39; (=&amp;gt;)&amp;quot; | sc &amp;quot;$hashdifflog&amp;quot;\ndiff -ReferenceObject (gc &amp;quot;$hash1&amp;quot; | Select -Skip 1) -DifferenceObject (gc &amp;quot;$hash2&amp;quot; | Select -Skip 1) | \n    group { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } | \n    % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | \n    Out-File -Encoding ASCII -Filepath &amp;quot;$hashdifflog&amp;quot; -Append\ngc $hashdifflog\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;INTERACTIVE SCRIPT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;But if you don&amp;#39;t want to fuss with any of the above I wrote this basic Powershell script that will allow you generate hash log files, and compare hash log files just by entering the appropriate info at the prompts.&lt;/p&gt;\n\n&lt;p&gt;A few notes:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;When you go to compare hash files, it will ask you how many lines to skip. It will present you with the first 9 lines of each log file you specify and you can tell it how many lines to omit from the comparison. So if you added any remarks or header info to the hash log files, you can just have it ignored. Selecting &amp;quot;Skip 3&amp;quot; will skip the first three lines shown, &amp;quot;Skip 1&amp;quot; just the first line, etc. Default will be to skip no lines.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This does an EXACT COMPARE of file names as strings, although it is not case sensitive. It is designed so all paths will start with a slash. It has no logic to assume forward slash and backwards slash are interchangeable. So if you need to compare output from a Linux machine with md5sum to this script run from a Windows machine you will have to modify the slashes. This can be easily rectified with the following (backwards slash to forwards slash, just revere them to get the reverse effect):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;(gc &amp;quot;hash1.md5&amp;quot;).replace(&amp;#39;\\&amp;#39;, &amp;#39;/&amp;#39;) | sc &amp;quot;hashes test 1.md5&amp;quot;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Log files are appended with a date time stamp number value equal to &lt;code&gt;yyyymmdd_HHmmss&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This has been tested with a log file with over 60 thousand entries and worked fine. File path length should not be an issue, but I haven&amp;#39;t tested it past about 200 characters.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;You can copy/paste the powershell script from pastebin here: &lt;a href=\"https://pastebin.com/0UJ0gcdb\"&gt;https://pastebin.com/0UJ0gcdb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Or here&amp;#39;s the same code below. Just copy/paste into notepad and save as &lt;code&gt;generatemd5.ps1&lt;/code&gt; or whatever as long as it has a &lt;code&gt;.ps1&lt;/code&gt; extension. It will save log files where you save it, so just be wary of that.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# generatemd5.ps1 by HTWingNut 06 June 2023\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\n$datetime = Get-Date\n$timestamp = $datetime.ToString(&amp;quot;yyyyMMdd_HHmmss&amp;quot;)\n$reqpath = &amp;#39;Input path to generate md5 hashes&amp;#39;\n$basepath = $hash1 = $hash2 = &amp;quot;?&amp;quot;\n$choice=$null\n$md5log = &amp;#39;hashes_&amp;#39;+$timestamp+&amp;#39;.md5&amp;#39;\n\nfunction GenerateHash {\n\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Generate Hash Files ...&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\nwhile (!(Test-Path -Path $basepath)) { $basepath = Read-Host -Prompt $reqpath; if ( $basepath -eq &amp;quot;&amp;quot;) { $basepath=&amp;quot;?&amp;quot; } }\n$numfiles = ( Get-ChildItem -Path &amp;quot;$basepath&amp;quot; -Recurse -File | Measure-Object ).Count\n\n$lchar = $basepath.Substring($basepath.length-1,1)\nIf ($lchar -eq &amp;quot;\\&amp;quot;) { $basepath = $basepath.Substring(0,$basepath.length-1) }\n\nWrite-Host &amp;quot;Total Files: $numfiles&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n$count=1\n\nWrite-Output &amp;quot;**** $($datetime) &amp;#39;$($basepath)&amp;#39;&amp;quot; | Set-Content &amp;quot;$md5log&amp;quot;\n\nGet-ChildItem -Path &amp;quot;$basepath&amp;quot; -Recurse -File | \n    Get-FileHash -Algorithm MD5 | \n    Select-Object Hash,Path | \n    ForEach-Object { \n        $_.Path = ($_.Path -replace [regex]::Escape(&amp;quot;$basepath&amp;quot;), &amp;#39;&amp;#39;)\n        Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path\n        ++$count\n        $_\n    } | \n    Format-Table -AutoSize -HideTableHeaders | \n    Out-String -Width 4096 | \n    Out-File &amp;quot;$md5log&amp;quot; -encoding ASCII -append\n\n(get-content $md5log).Trim() -ne &amp;#39;&amp;#39; | Set-Content $md5log\n\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Hashes stored in file &amp;#39;$pwd\\$md5log&amp;#39;&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\n}\n\nfunction CompareHash {\n\n$continue = &amp;quot;n&amp;quot;\n\nWhile ([bool]$continue) {\n\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Compare Hash Files...&amp;quot;\n\n$hash1 = $hash2 = &amp;quot;?&amp;quot;\n$numksip1 = $numskip2 = &amp;quot;0&amp;quot;\n\nWrite-Host &amp;quot;&amp;quot;\nwhile (!(Test-Path $hash1)) { $hash1 = Read-Host -Prompt &amp;quot;Enter Path/Name for Log 1&amp;quot;; if ( $hash1 -eq &amp;quot;&amp;quot;) { $hash1=&amp;quot;?&amp;quot; } }\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;First 9 lines of $($hash1):&amp;quot;\n$i=1; Get-Content $hash1 | Select -First 9 | % {&amp;quot;$($i) $_&amp;quot;; $i++}\nWrite-Host &amp;quot;&amp;quot;\nDo { $numskip1 = Read-Host &amp;quot;Choose Number of Lines to Skip [0-9] (default 0)&amp;quot; } until ($numskip1 -in 0,1,2,3,4,5,6,7,8,9)\nWrite-Host &amp;quot;&amp;quot;\n\nwhile (!(Test-Path $hash2)) { $hash2 = Read-Host -Prompt &amp;quot;Enter Path/Name for Log 2&amp;quot;; if ( $hash2 -eq &amp;quot;&amp;quot;) { $hash2=&amp;quot;?&amp;quot; } }\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;First 9 lines of $($hash2):&amp;quot;\n$i=1; Get-Content $hash2 | Select -First 9 | % {&amp;quot;$($i) $_&amp;quot;; $i++}\nWrite-Host &amp;quot;&amp;quot;\nDo { $numskip2 = Read-Host &amp;quot;Choose Number of Lines to Skip [0-9] (default 0)&amp;quot; } until ($numskip2 -in 0,1,2,3,4,5,6,7,8,9)\n\n$hclog = &amp;quot;HashCompare_$(((Get-Item $hash1).Basename).Replace(&amp;#39; &amp;#39;,&amp;#39;_&amp;#39;))_vs_$(((Get-Item $hash2).Basename).Replace(&amp;#39; &amp;#39;,&amp;#39;_&amp;#39;))_$timestamp.txt&amp;quot;\n\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;---------------------&amp;quot;\n    Write-Host &amp;quot;&amp;quot;\n\n    Write-Host &amp;quot;**** File: &amp;#39;$((Get-Item $hash1).Name)&amp;#39;&amp;quot;\n    Write-Host &amp;quot;**** Skipping Lines:&amp;quot;\n    Get-Content $hash1 | Select -First $numskip1\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** Starting with Line:&amp;quot;\n    Get-Content $hash1 | Select -Index ($numskip1)\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;---------------------&amp;quot;\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** File: &amp;#39;$((Get-Item $hash2).Name)&amp;#39;&amp;quot;\n    Write-Host &amp;quot;**** Skipping Lines:&amp;quot;\n    Get-Content $hash2 | Select -First $numskip2\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** Starting with Line:&amp;quot;\n\n    Get-Content $hash2 | Select -Index ($numskip2)\n    Write-Host\n    $continue = Read-Host &amp;quot;Press [ENTER] to accept, any other key to choose again&amp;quot;\n\n}\n\n# https://stackoverflow.com/questions/76415338/in-powershell-how-do-i-split-text-from-compare-object-input-and-sort-by-one-of/\n\n$diff = Compare-Object -ReferenceObject (Get-Content &amp;quot;$hash1&amp;quot; | Select -Skip $numskip1) -DifferenceObject (Get-Content &amp;quot;$hash2&amp;quot; | Select -Skip $numskip2 ) | \n    ForEach-Object {\n        if ($_.SideIndicator -eq &amp;quot;&amp;lt;=&amp;quot;) { $_.SideIndicator = &amp;quot;($((Get-ChildItem $hash1).Name))&amp;quot; } elseif ($_.SideIndicator -eq &amp;quot;=&amp;gt;&amp;quot;) { $_.SideIndicator = &amp;quot;($((Get-ChildItem $hash2).Name))&amp;quot; }\n        $_\n    } |\n    Group-Object { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } |\n    ForEach-Object {\n        $_.Group | Format-Table -HideTableHeaders | \n            Out-String | ForEach-Object TrimEnd\n    }\n\nif ($diff) {\n    Write-output &amp;quot;**** $($datetime) &amp;#39;$($hash1)&amp;#39; vs &amp;#39;$($hash2)&amp;#39;`n**** Matching file path/names with mismatched hashes are grouped together`n**** Individual file names means unique file/hash in that log file&amp;quot; $diff &amp;gt; &amp;quot;$hclog&amp;quot;\n} else {\n    Write-Output &amp;quot;**** $($datetime) &amp;#39;$($hash1)&amp;#39; vs &amp;#39;$($hash2)&amp;#39; `n`n**** ALL CLEAR! NO DIFFERENCES FOUND!&amp;quot; &amp;gt; &amp;quot;$hclog&amp;quot;\n}\n\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;**** Results stored in $($pwd)\\$($hclog)&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\n\n}\n\nDo { $choice = Read-Host &amp;quot;[G]enerate Hash or [C]ompare Hash Logs&amp;quot; } until ($choice -in &amp;#39;g&amp;#39;,&amp;#39;c&amp;#39;)\nIf ( $choice -eq &amp;quot;g&amp;quot; ) { GenerateHash }\nIf ( $choice -eq &amp;quot;c&amp;quot; ) { CompareHash }\n\ncmd /c &amp;#39;pause&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ysqt", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142ysqt/here_is_a_windows_powershell_script_to_generate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ysqt/here_is_a_windows_powershell_script_to_generate/", "subreddit_subscribers": 686361, "created_utc": 1686099374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It looks like The Eye does have it up until December 2022?  But I have important work from this year that I am trying to preserve (such as my definitions of [love](https://old.reddit.com/r/AbuseInterrupted/comments/13k3yq2/the_definition_of_love/) and [abuse](https://old.reddit.com/r/AbuseInterrupted/comments/13k597w/definition_of_abuse/), for example).\n\nI did also do the [Reddit Data Request](https://www.reddit.com/settings/data-request).\n\nI'm sorry to come in here like an idiot, but I have zero programming ability and am now having to figure out how to migrate this over to a website because people depend on these resources, and I also don't want to lose over a decade of my work.\n\nI looks like most people here are trying to save their personal Reddit history, but I am more focused on the subreddit.", "author_fullname": "t2_7djla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to archive/clone/migrate a subreddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142qq0l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686079715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like The Eye does have it up until December 2022?  But I have important work from this year that I am trying to preserve (such as my definitions of &lt;a href=\"https://old.reddit.com/r/AbuseInterrupted/comments/13k3yq2/the_definition_of_love/\"&gt;love&lt;/a&gt; and &lt;a href=\"https://old.reddit.com/r/AbuseInterrupted/comments/13k597w/definition_of_abuse/\"&gt;abuse&lt;/a&gt;, for example).&lt;/p&gt;\n\n&lt;p&gt;I did also do the &lt;a href=\"https://www.reddit.com/settings/data-request\"&gt;Reddit Data Request&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sorry to come in here like an idiot, but I have zero programming ability and am now having to figure out how to migrate this over to a website because people depend on these resources, and I also don&amp;#39;t want to lose over a decade of my work.&lt;/p&gt;\n\n&lt;p&gt;I looks like most people here are trying to save their personal Reddit history, but I am more focused on the subreddit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142qq0l", "is_robot_indexable": true, "report_reasons": null, "author": "invah", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142qq0l/how_to_archiveclonemigrate_a_subreddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142qq0l/how_to_archiveclonemigrate_a_subreddit/", "subreddit_subscribers": 686361, "created_utc": 1686079715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey so big question:\n\nLet's say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.", "author_fullname": "t2_2alh77oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presentation of Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142cub8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686050646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so big question:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "10TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142cub8", "is_robot_indexable": true, "report_reasons": null, "author": "Zaxoosh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142cub8/presentation_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142cub8/presentation_of_data/", "subreddit_subscribers": 686361, "created_utc": 1686050646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1qdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drobo is Officially Done as the Company Moves Into Liquidation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_1433ddy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PKTtj-THG-KuiQSFLPy51PQDf6fkTKM--MIVGPziJEY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686112523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "petapixel.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://petapixel.com/2023/06/06/drobo-is-officially-done-as-the-company-moves-into-liquidation/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?auto=webp&amp;v=enabled&amp;s=2d6845a04b8ae8051560e342fc34ed8054508455", "width": 1600, "height": 840}, "resolutions": [{"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=655acd7ef62b579c5bc7d0bd1608f3082f0bc50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5752f44af67c4c0bdee83005d37717c99d439271", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=182f548e9ad703f020ca4125fbae9121f8c5fb0e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75db6e3ec918e1d90db5276b1d63d58b0f632aec", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca500b5bbc31735cf6e11cf47433cebbdd28bd76", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cff3a17c7fcaecc2e0a03b4673dd71c0178ec15", "width": 1080, "height": 567}], "variants": {}, "id": "BH1g-ogwcIrRpjZws5J8L2362WMqje8vd5KcPJ649YU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1433ddy", "is_robot_indexable": true, "report_reasons": null, "author": "khaled", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1433ddy/drobo_is_officially_done_as_the_company_moves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://petapixel.com/2023/06/06/drobo-is-officially-done-as-the-company-moves-into-liquidation/", "subreddit_subscribers": 686361, "created_utc": 1686112523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "how should i visually format an archive of someone's tweets/twitter account? im just focusing on the tweets (both with and without media)\n\nmost of the programs/apps i find for this only pull the media &amp; data separately (and a lot don't go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it's possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?\n\ni did actually manage to get *something* with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there's nothing i'll just use that. but yeah just wondering if it's possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that's capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?\n\ni've been trying to figure this out for a few days now, any help is appreciated", "author_fullname": "t2_tkcopl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to display an archive of someone's twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428b1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686038535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how should i visually format an archive of someone&amp;#39;s tweets/twitter account? im just focusing on the tweets (both with and without media)&lt;/p&gt;\n\n&lt;p&gt;most of the programs/apps i find for this only pull the media &amp;amp; data separately (and a lot don&amp;#39;t go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp;amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it&amp;#39;s possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?&lt;/p&gt;\n\n&lt;p&gt;i did actually manage to get &lt;em&gt;something&lt;/em&gt; with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there&amp;#39;s nothing i&amp;#39;ll just use that. but yeah just wondering if it&amp;#39;s possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that&amp;#39;s capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve been trying to figure this out for a few days now, any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1428b1l", "is_robot_indexable": true, "report_reasons": null, "author": "Hootie_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "subreddit_subscribers": 686361, "created_utc": 1686038535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a self-hosted server to provide an e-hentai (NSFW) like experience for organizing my media. I want to be able to search for media using tags using the search (example below):  \n\n\n    series:Full Metal Alchemist$ language:english$\n\n  \nMain focus is tags. With list view and preview second. Any ideas?", "author_fullname": "t2_7g8d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-hentai like Self-hosted Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142sjs5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a self-hosted server to provide an e-hentai (NSFW) like experience for organizing my media. I want to be able to search for media using tags using the search (example below):  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;series:Full Metal Alchemist$ language:english$\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Main focus is tags. With list view and preview second. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142sjs5", "is_robot_indexable": true, "report_reasons": null, "author": "smokeyFlorence", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142sjs5/ehentai_like_selfhosted_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142sjs5/ehentai_like_selfhosted_server/", "subreddit_subscribers": 686361, "created_utc": 1686083944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  \n\n\nMy question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.", "author_fullname": "t2_lh08w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FTP or Rsync to offline backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142c4um", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686048908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  &lt;/p&gt;\n\n&lt;p&gt;My question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142c4um", "is_robot_indexable": true, "report_reasons": null, "author": "NeoID", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "subreddit_subscribers": 686361, "created_utc": 1686048908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought 4xWD HC560 20TB drives from a well-known european e-shop.\n\nI checked the warranty of the drives on the official WD website with the serial number and I found out they are OEM drives, not retail ones (PDQ Drive ASM OEM-STD) and therefore \"No limited warranty\" is applied (Product was originally sold to a system manufacturer. Please contact the system manufacturer or the place of purchase for warranty service).\n\nObviously the e-shop is not a system manufacturer and it may have done some sort of dropshipping with my money to a distributor that sells OEM drives to manufacturer brands (Dell, Asus and such) as the benefits are higher this way.\n\nI guess I am out of the 5 year limited warranty from WD as an end-user, but at least I should be getting the 2 year warranty from electronic goods bought in the EU.\n\nIs this normal? Getting OEM drives (2 year warranty) as an end-user buying from an e-shop which should be sending retail ones (5 year warranty)?\n\nThanks in advance.", "author_fullname": "t2_14jgu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought 4x20TB drives from e-shop, got sent OEM ones (2y warranty instead of 5y)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142w60a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686092352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought 4xWD HC560 20TB drives from a well-known european e-shop.&lt;/p&gt;\n\n&lt;p&gt;I checked the warranty of the drives on the official WD website with the serial number and I found out they are OEM drives, not retail ones (PDQ Drive ASM OEM-STD) and therefore &amp;quot;No limited warranty&amp;quot; is applied (Product was originally sold to a system manufacturer. Please contact the system manufacturer or the place of purchase for warranty service).&lt;/p&gt;\n\n&lt;p&gt;Obviously the e-shop is not a system manufacturer and it may have done some sort of dropshipping with my money to a distributor that sells OEM drives to manufacturer brands (Dell, Asus and such) as the benefits are higher this way.&lt;/p&gt;\n\n&lt;p&gt;I guess I am out of the 5 year limited warranty from WD as an end-user, but at least I should be getting the 2 year warranty from electronic goods bought in the EU.&lt;/p&gt;\n\n&lt;p&gt;Is this normal? Getting OEM drives (2 year warranty) as an end-user buying from an e-shop which should be sending retail ones (5 year warranty)?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142w60a", "is_robot_indexable": true, "report_reasons": null, "author": "jfromeo", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142w60a/bought_4x20tb_drives_from_eshop_got_sent_oem_ones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142w60a/bought_4x20tb_drives_from_eshop_got_sent_oem_ones/", "subreddit_subscribers": 686361, "created_utc": 1686092352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using a config for deviant art i found [here on github](https://github.com/mikf/gallery-dl/issues/2143#issuecomment-1002192200) but the post processors don't seem to take effect, no metadata folder or .html file is created containing the file descriptions for each image downloaded from a gallery, is there something wrong with this config? \n\nhere's my whole \\~/.config/gallery-dl/config.json:\n\n    {\n    \"deviantart\":\n            {\n                \"directory\": [\"[gallery-dl]\", \"[{category}] {author[username]}\"],\n                \"filename\": \"[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.{extension}\",\n                \"metadata\": true,\n                \"postprocessors\": [{\n                    \"name\": \"metadata\",\n                    \"mode\": \"custom\",\n                    \"filename\": \"[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.html\",\n                    \"directory\": \"metadata\",\n                    \"extension\": \"html\",\n                    \"content-format\": \"&lt;h1 style='display: inline'&gt;&lt;a href='{url}'&gt;{title}&lt;/a&gt;&lt;/h1&gt; by &lt;a href='https://www.deviantart.com/{username}'&gt;{a&gt;\n                }]\n            }\n    }", "author_fullname": "t2_a45skyv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gallery-dl deviantart postprocessor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142s7w5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a config for deviant art i found &lt;a href=\"https://github.com/mikf/gallery-dl/issues/2143#issuecomment-1002192200\"&gt;here on github&lt;/a&gt; but the post processors don&amp;#39;t seem to take effect, no metadata folder or .html file is created containing the file descriptions for each image downloaded from a gallery, is there something wrong with this config? &lt;/p&gt;\n\n&lt;p&gt;here&amp;#39;s my whole ~/.config/gallery-dl/config.json:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n&amp;quot;deviantart&amp;quot;:\n        {\n            &amp;quot;directory&amp;quot;: [&amp;quot;[gallery-dl]&amp;quot;, &amp;quot;[{category}] {author[username]}&amp;quot;],\n            &amp;quot;filename&amp;quot;: &amp;quot;[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.{extension}&amp;quot;,\n            &amp;quot;metadata&amp;quot;: true,\n            &amp;quot;postprocessors&amp;quot;: [{\n                &amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;,\n                &amp;quot;mode&amp;quot;: &amp;quot;custom&amp;quot;,\n                &amp;quot;filename&amp;quot;: &amp;quot;[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.html&amp;quot;,\n                &amp;quot;directory&amp;quot;: &amp;quot;metadata&amp;quot;,\n                &amp;quot;extension&amp;quot;: &amp;quot;html&amp;quot;,\n                &amp;quot;content-format&amp;quot;: &amp;quot;&amp;lt;h1 style=&amp;#39;display: inline&amp;#39;&amp;gt;&amp;lt;a href=&amp;#39;{url}&amp;#39;&amp;gt;{title}&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt; by &amp;lt;a href=&amp;#39;https://www.deviantart.com/{username}&amp;#39;&amp;gt;{a&amp;gt;\n            }]\n        }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?auto=webp&amp;v=enabled&amp;s=e1c208f37795d2e7e52c6134decd3760bebada06", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e4cefddbc7ce021dc36f5f18ff84270bab3eafc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da94179240cb63c0df86a4e66f3932d4e28991e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4dd201736c25732e17c7243f738268f53f2380b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4326eac2b616829b172c392c16dd9af37818bed", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb5dde7f635b56be3df5b7db6d42f202bda7dc89", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60f52964d482f5c12c29e1e3a1c94a47b5f2ff97", "width": 1080, "height": 540}], "variants": {}, "id": "fngzcYF-QRQCMlV24eNPc72Fym3jXH9DdYuS2yxRD5U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142s7w5", "is_robot_indexable": true, "report_reasons": null, "author": "H0rrorAssociation", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142s7w5/gallerydl_deviantart_postprocessor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142s7w5/gallerydl_deviantart_postprocessor/", "subreddit_subscribers": 686361, "created_utc": 1686083192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am reconsidering options for my current media server backup strategy. I am running Plex on Windows using DrivePool with 2x media folder duplication as my first backup step. Secondly, I have a set of off-site hard drives containing my media files and macrium drive images.\n\nThirdly, I have an external USB hard drive (14tb wd easystore) that I synchronize all of my data to at regular intervals. This drive is only powered up when a backup is run. My current issue is that my external hdd is running out of space. My plan has been to replace the drive with a larger capacity drive, shuck the currrent drive and add it to my DrivePool storage pool.\n\nOverall, I have been comfortable with my current arrangement. However, replacing the external drive with another of much greater capacity is an expensive option. Also, as I am adding media all the time, it would be a matter of time before the new external drive would be maxed out and the process would need to be repeated. Obviously, the benefit of this drive being external is that if disaster strikes my home or my media server, my data is preserved on the external drive which I could access very easily.\n\nIs there a better option to replace the external hdd in my backup strategy that is worth considering? Affordability is very important due to a restricted budget. I appreciate any comments you might offer.", "author_fullname": "t2_56nvy1n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Media Server Backup Strategy Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ov62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686076439.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686075846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reconsidering options for my current media server backup strategy. I am running Plex on Windows using DrivePool with 2x media folder duplication as my first backup step. Secondly, I have a set of off-site hard drives containing my media files and macrium drive images.&lt;/p&gt;\n\n&lt;p&gt;Thirdly, I have an external USB hard drive (14tb wd easystore) that I synchronize all of my data to at regular intervals. This drive is only powered up when a backup is run. My current issue is that my external hdd is running out of space. My plan has been to replace the drive with a larger capacity drive, shuck the currrent drive and add it to my DrivePool storage pool.&lt;/p&gt;\n\n&lt;p&gt;Overall, I have been comfortable with my current arrangement. However, replacing the external drive with another of much greater capacity is an expensive option. Also, as I am adding media all the time, it would be a matter of time before the new external drive would be maxed out and the process would need to be repeated. Obviously, the benefit of this drive being external is that if disaster strikes my home or my media server, my data is preserved on the external drive which I could access very easily.&lt;/p&gt;\n\n&lt;p&gt;Is there a better option to replace the external hdd in my backup strategy that is worth considering? Affordability is very important due to a restricted budget. I appreciate any comments you might offer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "142ov62", "is_robot_indexable": true, "report_reasons": null, "author": "bctf1", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ov62/media_server_backup_strategy_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ov62/media_server_backup_strategy_advice/", "subreddit_subscribers": 686361, "created_utc": 1686075846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lord Bung is deleting the Confinement series - For those interested, you might want to archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": true, "name": "t3_1432tyo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b38dubx", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8nKejTl_n0CbeitlAK4Cm3d44Gflyq8VQJbib8s7IWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "SCP", "selftext": "", "author_fullname": "t2_9z6t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lord Bung is deleting the Confinement series", "link_flair_richtext": [{"a": ":wMETA:", "e": "emoji", "u": "https://emoji.redditmedia.com/z5sbq9lqm2h41_t5_2r4ni/wMETA"}, {"e": "text", "t": " Meta Post"}], "subreddit_name_prefixed": "r/SCP", "hidden": false, "pwls": 7, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_142ye91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "#373c3f", "ups": 478, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "48f59682-d3a6-11eb-90ed-0e6924b8cf37", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": ":wMETA: Meta Post", "can_mod_post": false, "score": 478, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8nKejTl_n0CbeitlAK4Cm3d44Gflyq8VQJbib8s7IWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":wMTF_EPSILON-11:", "e": "emoji", "u": "https://emoji.redditmedia.com/m0d9ls4q8hf71_t5_2r4ni/wMTF_EPSILON-11"}, {"e": "text", "t": " MTF Epsilon-11 (\"Nine-Tailed Fox\")"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686098221.0, "link_flair_type": "richtext", "wls": 7, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1iigyr5grh4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1iigyr5grh4b1.png?auto=webp&amp;v=enabled&amp;s=92fbc47a6d7622a737379478e3a3b6dbd53d6fba", "width": 867, "height": 135}, "resolutions": [{"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02790e786e1e5bd654d0d9e77932b16175d7ee3", "width": 108, "height": 16}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98093a44126560d23facf77035f007ac97bc22b9", "width": 216, "height": 33}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0e5211f4b1d1989191561e769d0dff36c8832ce", "width": 320, "height": 49}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b14ac65b955b622012f8a4621bee9b47edda1ba", "width": 640, "height": 99}], "variants": {}, "id": "o0Vf4I3a5Sbmh7Xw5l7GG0bfzgThMpqC7u2VHsWPIX4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bba31b8e-45bd-11e3-b66a-12313d188143", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":wMTF_EPSILON-11: MTF Epsilon-11 (\"Nine-Tailed Fox\")", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2r4ni", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "142ye91", "is_robot_indexable": true, "report_reasons": null, "author": "mikeBE11", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "some_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/SCP/comments/142ye91/lord_bung_is_deleting_the_confinement_series/", "parent_whitelist_status": "some_ads", "stickied": false, "url": "https://i.redd.it/1iigyr5grh4b1.png", "subreddit_subscribers": 680545, "created_utc": 1686098221.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1686110829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1iigyr5grh4b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1iigyr5grh4b1.png?auto=webp&amp;v=enabled&amp;s=92fbc47a6d7622a737379478e3a3b6dbd53d6fba", "width": 867, "height": 135}, "resolutions": [{"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02790e786e1e5bd654d0d9e77932b16175d7ee3", "width": 108, "height": 16}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98093a44126560d23facf77035f007ac97bc22b9", "width": 216, "height": 33}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0e5211f4b1d1989191561e769d0dff36c8832ce", "width": 320, "height": 49}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b14ac65b955b622012f8a4621bee9b47edda1ba", "width": 640, "height": 99}], "variants": {}, "id": "o0Vf4I3a5Sbmh7Xw5l7GG0bfzgThMpqC7u2VHsWPIX4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1432tyo", "is_robot_indexable": true, "report_reasons": null, "author": "Tzar_Jberk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_142ye91", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1432tyo/lord_bung_is_deleting_the_confinement_series_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1iigyr5grh4b1.png", "subreddit_subscribers": 686361, "created_utc": 1686110829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many Redditors reported their failure cases on SanDisk Extreme Portable SSDs, which caused a lot of problems, including data loss. The related discussion can be found below: \n\n[https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer\\_beware\\_some\\_sandisk\\_extreme\\_ssds\\_are\\_wiping/](https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/)\n\n[https://www.reddit.com/r/editors/comments/10syawa/a\\_warning\\_about\\_sandisk\\_extreme\\_pro\\_ssds/](https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/)\u00a0\n\nWestern Digital has released the claimed firmware for Windows, but not yet for Mac\u00a0  \n**Before the firmware update, it's strongly suggested to back up the SSD first.**\u00a0\n\nHere are the official update on this issue-\n\n[https://support-en.wd.com/app/firmwareupdate](https://support-en.wd.com/app/firmwareupdate) \n\n[https://www.youtube.com/watch?v=tvxHuTUS9is](https://www.youtube.com/watch?v=tvxHuTUS9is)\n\n  \n**Based on this event, EaseUS is working to offer data recovery help (offering data recovery software or online services for free)**, aiming to assist\u00a0SanDisk Extreme Portable SSD users to find their important data back while taking the chance to know better about the data loss caused by SSD crush.\u00a0  \nDetail info can be found on this page -\u00a0[https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html](https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html). It's not complicated. Just send an email to [redepmtion@easeus.com](mailto:redepmtion@easeus.com) with a basic description of the case you experienced. \n\n  \nWith the release of the firmware, there are many more Mac users who came to us for data recovery help, which seems that the firmware works well for Windows. Just back up the data before the update. And with this offer, EaseUS is also offering EaseUS Todo Backup Home for free. Check the page if you need it.\u00a0", "author_fullname": "t2_a0iii5xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Info Sync - Data Recovery Help for SanDisk Extreme SSD Failure is Offered by EaseUS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1431wj8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686108067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many Redditors reported their failure cases on SanDisk Extreme Portable SSDs, which caused a lot of problems, including data loss. The related discussion can be found below: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/\"&gt;https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/&lt;/a&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;Western Digital has released the claimed firmware for Windows, but not yet for Mac\u00a0&lt;br/&gt;\n&lt;strong&gt;Before the firmware update, it&amp;#39;s strongly suggested to back up the SSD first.&lt;/strong&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;Here are the official update on this issue-&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://support-en.wd.com/app/firmwareupdate\"&gt;https://support-en.wd.com/app/firmwareupdate&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=tvxHuTUS9is\"&gt;https://www.youtube.com/watch?v=tvxHuTUS9is&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Based on this event, EaseUS is working to offer data recovery help (offering data recovery software or online services for free)&lt;/strong&gt;, aiming to assist\u00a0SanDisk Extreme Portable SSD users to find their important data back while taking the chance to know better about the data loss caused by SSD crush.\u00a0&lt;br/&gt;\nDetail info can be found on this page -\u00a0&lt;a href=\"https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html\"&gt;https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html&lt;/a&gt;. It&amp;#39;s not complicated. Just send an email to [&lt;a href=\"mailto:redepmtion@easeus.com\"&gt;redepmtion@easeus.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:redepmtion@easeus.com\"&gt;redepmtion@easeus.com&lt;/a&gt;) with a basic description of the case you experienced. &lt;/p&gt;\n\n&lt;p&gt;With the release of the firmware, there are many more Mac users who came to us for data recovery help, which seems that the firmware works well for Windows. Just back up the data before the update. And with this offer, EaseUS is also offering EaseUS Todo Backup Home for free. Check the page if you need it.\u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1431wj8", "is_robot_indexable": true, "report_reasons": null, "author": "Adaaaaaaaaaaaaaaaaa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1431wj8/info_sync_data_recovery_help_for_sandisk_extreme/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1431wj8/info_sync_data_recovery_help_for_sandisk_extreme/", "subreddit_subscribers": 686361, "created_utc": 1686108067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been \"half assing\" for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.\n\nZero issues so far, and I don't want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn't have to be on all the time, backups, etc.\n\nBest way to accomplish the following?\n\n1. Expandable Storage.\n2. Backups.\n3. No need for PC to run.\n4. Shield able to connect via Plex.\n5. Can connect to the device on my PC to manage media.\n6. Able to stream 100mbs movies without any issues.\n\nCurrent plan:\n\n* [Synology DS1522+](https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;psc=1)\n* x2 - [Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache](https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;psc=1)\n\nIs that a good way to accomplish it?", "author_fullname": "t2_ia3wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning on my first NAS set up. Thoughts on the parts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142v8fm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686090096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been &amp;quot;half assing&amp;quot; for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.&lt;/p&gt;\n\n&lt;p&gt;Zero issues so far, and I don&amp;#39;t want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn&amp;#39;t have to be on all the time, backups, etc.&lt;/p&gt;\n\n&lt;p&gt;Best way to accomplish the following?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Expandable Storage.&lt;/li&gt;\n&lt;li&gt;Backups.&lt;/li&gt;\n&lt;li&gt;No need for PC to run.&lt;/li&gt;\n&lt;li&gt;Shield able to connect via Plex.&lt;/li&gt;\n&lt;li&gt;Can connect to the device on my PC to manage media.&lt;/li&gt;\n&lt;li&gt;Able to stream 100mbs movies without any issues.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Current plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Synology DS1522+&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;x2 - &lt;a href=\"https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is that a good way to accomplish it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142v8fm", "is_robot_indexable": true, "report_reasons": null, "author": "nsoifer", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "subreddit_subscribers": 686361, "created_utc": 1686090096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We are looking to replace our Xerox scanner with a smart scanner for book scanning in our Interlibrary Loan department. We have been considering the CZUR Aura scanner as a potential alternative. However, we have seen some problems: \"We have tried various settings, including using overhead and side lights, but are unable to scan two pages clearly without blinding.\"\n\nWe are now seeking advice from anyone who has experience using both the CZUR Aura and CZUR ET scanners. Would the more expensive ET models be more effective in addressing our scanning needs? We would like to invest in a smart scanner that can scan books with high picture content, while reducing our paper waste.\n\nAny feedback or recommendations on the best scanner for our needs would be greatly appreciated. Thank you for your time.", "author_fullname": "t2_c0xq3j9y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Smart Book Scanners for Interlibrary Loan Documents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142u8y7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686087821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking to replace our Xerox scanner with a smart scanner for book scanning in our Interlibrary Loan department. We have been considering the CZUR Aura scanner as a potential alternative. However, we have seen some problems: &amp;quot;We have tried various settings, including using overhead and side lights, but are unable to scan two pages clearly without blinding.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;We are now seeking advice from anyone who has experience using both the CZUR Aura and CZUR ET scanners. Would the more expensive ET models be more effective in addressing our scanning needs? We would like to invest in a smart scanner that can scan books with high picture content, while reducing our paper waste.&lt;/p&gt;\n\n&lt;p&gt;Any feedback or recommendations on the best scanner for our needs would be greatly appreciated. Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142u8y7", "is_robot_indexable": true, "report_reasons": null, "author": "durkefigne", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142u8y7/seeking_advice_on_smart_book_scanners_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142u8y7/seeking_advice_on_smart_book_scanners_for/", "subreddit_subscribers": 686361, "created_utc": 1686087821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently my little pile of data is hosted on my network in an ancient Synology 413J along with 4x 6TB drives. Capacity is fine, but it\u2019s unable to serve data at speeds over 30mbps despite gigabit ports, gigabit switch, router, etc. \n\nI\u2019ve enjoyed synology. But end of the day what I want to lowest cost highest performance along with the most flexibility. \n\nTo that end, I\u2019m considering an 8th Gen HP Microserver to install either SnapRAID (which appears to meet my needs) or TrueNAS. \n\nMy question is, have any of you played with the microserver? Will the celeron model be sufficient for serving data at higher speeds than what I\u2019m seeing currently?\n\nI have two proxmox servers sharing ISO directory and another directory for VM backups, as well as Plex who\u2019s library is on the NAS, usually serving 1-2 streams. Plus various other backups, etc, but those are more copy to storage and forget about it. \n\nWhat are your thought? Or other ideas in the $300-400 range with at least 4 3.5 inch bays and as quiet as possible", "author_fullname": "t2_5abuxk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology 413j vs HP Microservwr 8thGen", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14311e0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686105607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently my little pile of data is hosted on my network in an ancient Synology 413J along with 4x 6TB drives. Capacity is fine, but it\u2019s unable to serve data at speeds over 30mbps despite gigabit ports, gigabit switch, router, etc. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve enjoyed synology. But end of the day what I want to lowest cost highest performance along with the most flexibility. &lt;/p&gt;\n\n&lt;p&gt;To that end, I\u2019m considering an 8th Gen HP Microserver to install either SnapRAID (which appears to meet my needs) or TrueNAS. &lt;/p&gt;\n\n&lt;p&gt;My question is, have any of you played with the microserver? Will the celeron model be sufficient for serving data at higher speeds than what I\u2019m seeing currently?&lt;/p&gt;\n\n&lt;p&gt;I have two proxmox servers sharing ISO directory and another directory for VM backups, as well as Plex who\u2019s library is on the NAS, usually serving 1-2 streams. Plus various other backups, etc, but those are more copy to storage and forget about it. &lt;/p&gt;\n\n&lt;p&gt;What are your thought? Or other ideas in the $300-400 range with at least 4 3.5 inch bays and as quiet as possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14311e0", "is_robot_indexable": true, "report_reasons": null, "author": "AdhessiveBaker", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14311e0/synology_413j_vs_hp_microservwr_8thgen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14311e0/synology_413j_vs_hp_microservwr_8thgen/", "subreddit_subscribers": 686361, "created_utc": 1686105607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Salutations, Data Hoarders.\n\nI\u2019ve been lurking here for a while and my current storage solution is on the lacking side. As I was looking into preserving media I like I ran into the reality that in my quest to secure all of the stuff I remembered And anything new I was interested in that three external 4 terabyte HDD drives shrunk down to about 800 gigs sooner than I thought.\n\nI thought it would last me about four years before I ran into any serious limitations of remaining space, so because of underestimating the size of video files, here I am, asking for help on finding an ideal DAS and drives to fill it with.\n\n**With all of that said, this would be the planned use case:**\n\n&amp;#x200B;\n\n* I\u2019m not going to run the DAS 24/7/365. I would only power it up when I need to either add to or retrieve something from it.  \n \n* It needs to be fairly portable because physical space is a concern.  \n \n* Not looking into RAID.  \n \n* I would like the option to only turn on one of the drives in the at a time when I \tneed to retrieve something (no sense in spinning up all the drives for pulling something off one, especially since they\u2019d all have the exact same things on it since all the drives are a backup).  \n \n* I\u2019m not looking to do wholesale system backups, just to preservation of individual files that matter to me.  \n \n* The DAS would house no less than three but no more than four.  \n \n* As this would be pretty expensive, I would like to get HDDs robust enough to last me ideally 10 years. Would going for enterprise drives help with this noticeably or am I just making a fools wish? You know, buy it nice or buy it twice.  \n \n* As I want to keep this for as long as possible, the drives would have storage of no less than 12 TB.  \n \n\nAs this would be my first DAS, it would be grateful if any of you can impart any advice or wisdom you wish you had when setting one of these up yourselves as well as any pitfalls that I should be aware of.\n\nThank you in advance.", "author_fullname": "t2_16a6im", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking to Setup My First DAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1430ceo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Seeking Setup Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686103676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Salutations, Data Hoarders.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been lurking here for a while and my current storage solution is on the lacking side. As I was looking into preserving media I like I ran into the reality that in my quest to secure all of the stuff I remembered And anything new I was interested in that three external 4 terabyte HDD drives shrunk down to about 800 gigs sooner than I thought.&lt;/p&gt;\n\n&lt;p&gt;I thought it would last me about four years before I ran into any serious limitations of remaining space, so because of underestimating the size of video files, here I am, asking for help on finding an ideal DAS and drives to fill it with.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;With all of that said, this would be the planned use case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I\u2019m not going to run the DAS 24/7/365. I would only power it up when I need to either add to or retrieve something from it.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It needs to be fairly portable because physical space is a concern.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Not looking into RAID.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would like the option to only turn on one of the drives in the at a time when I     need to retrieve something (no sense in spinning up all the drives for pulling something off one, especially since they\u2019d all have the exact same things on it since all the drives are a backup).  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I\u2019m not looking to do wholesale system backups, just to preservation of individual files that matter to me.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The DAS would house no less than three but no more than four.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;As this would be pretty expensive, I would like to get HDDs robust enough to last me ideally 10 years. Would going for enterprise drives help with this noticeably or am I just making a fools wish? You know, buy it nice or buy it twice.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;As I want to keep this for as long as possible, the drives would have storage of no less than 12 TB.  &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As this would be my first DAS, it would be grateful if any of you can impart any advice or wisdom you wish you had when setting one of these up yourselves as well as any pitfalls that I should be aware of.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1430ceo", "is_robot_indexable": true, "report_reasons": null, "author": "Jesse_Graves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1430ceo/seeking_to_setup_my_first_das/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1430ceo/seeking_to_setup_my_first_das/", "subreddit_subscribers": 686361, "created_utc": 1686103676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm planning to purchase two 1 TB SSDs to use it in my TrueNAS setup (probably in mirror mode). I don't write too much data but I don't want the drives to wear out quickly either.\n\nWD Blue SA510 and Crucial MX500 costs %40 more than Team CX2, but their TBW values are ~400 TB while it's 800 TB for Team CX2.\n\nOn paper, Team CX2 seems a lot better. However, I don't know much about their products' build quality etc. Is it a solid choice? Should I pay %40 more to get the better-known MX500?", "author_fullname": "t2_3r6bmdnz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Team CX2, WD Blue SA510 or Crucial MX500?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142yein", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686098241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to purchase two 1 TB SSDs to use it in my TrueNAS setup (probably in mirror mode). I don&amp;#39;t write too much data but I don&amp;#39;t want the drives to wear out quickly either.&lt;/p&gt;\n\n&lt;p&gt;WD Blue SA510 and Crucial MX500 costs %40 more than Team CX2, but their TBW values are ~400 TB while it&amp;#39;s 800 TB for Team CX2.&lt;/p&gt;\n\n&lt;p&gt;On paper, Team CX2 seems a lot better. However, I don&amp;#39;t know much about their products&amp;#39; build quality etc. Is it a solid choice? Should I pay %40 more to get the better-known MX500?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142yein", "is_robot_indexable": true, "report_reasons": null, "author": "johndoe4000", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142yein/team_cx2_wd_blue_sa510_or_crucial_mx500/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142yein/team_cx2_wd_blue_sa510_or_crucial_mx500/", "subreddit_subscribers": 686361, "created_utc": 1686098241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Out of these choices, which would you choose? Is Syncovery deliver value over and above the free alternatives I listed? \n\nI am looking for a fully local backup solution which I can set up and then forget until something happens. Would like it to encrypt the backups and increments inside of a .rar or .7z archive with full checksumming. Since it's not a backup strategy if you can't easily restore, I want to be able to access the files without needing to use the local backup utility. \n\nPlease do not recommend any others if it's not compatible with Windows, MacOS and Linux. \n\nI am thinking of using SyncThing to transfer the backups files around so I don't have another thing running in the background on my main PC. The most recent backups would be stored locally on each PC in a dedicated partition of equal size to the main one and my main PC would backup all the archive files online to Backblaze.", "author_fullname": "t2_8hjaccg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Before I deploy to several computers: UrBackup, Bacula, Duplicati or Syncovery (paid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142whfe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686093121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Out of these choices, which would you choose? Is Syncovery deliver value over and above the free alternatives I listed? &lt;/p&gt;\n\n&lt;p&gt;I am looking for a fully local backup solution which I can set up and then forget until something happens. Would like it to encrypt the backups and increments inside of a .rar or .7z archive with full checksumming. Since it&amp;#39;s not a backup strategy if you can&amp;#39;t easily restore, I want to be able to access the files without needing to use the local backup utility. &lt;/p&gt;\n\n&lt;p&gt;Please do not recommend any others if it&amp;#39;s not compatible with Windows, MacOS and Linux. &lt;/p&gt;\n\n&lt;p&gt;I am thinking of using SyncThing to transfer the backups files around so I don&amp;#39;t have another thing running in the background on my main PC. The most recent backups would be stored locally on each PC in a dedicated partition of equal size to the main one and my main PC would backup all the archive files online to Backblaze.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142whfe", "is_robot_indexable": true, "report_reasons": null, "author": "LieVirus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142whfe/before_i_deploy_to_several_computers_urbackup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142whfe/before_i_deploy_to_several_computers_urbackup/", "subreddit_subscribers": 686361, "created_utc": 1686093121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I'm looking for a msata SSD to replace the one used in my pfSense router. The current one is a Dogfish 128GB SSD. It worked well for years to serve as the boot disk of pfSense. However, since I decided to virtualize the router and run more service on the device, I started to noticed dropped speed when copying large amount of data to the disk. The initial speed could reach 300MB/s, and then dropped to 18MB/s which indicates cache running out.\n\nI would appreciate if someone could recommend a msata SSD that has bigger cache size and doesn't suffer from the speed drop problem. Thanks!", "author_fullname": "t2_hq1o67a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a good quality msata SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142u24g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686087392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I&amp;#39;m looking for a msata SSD to replace the one used in my pfSense router. The current one is a Dogfish 128GB SSD. It worked well for years to serve as the boot disk of pfSense. However, since I decided to virtualize the router and run more service on the device, I started to noticed dropped speed when copying large amount of data to the disk. The initial speed could reach 300MB/s, and then dropped to 18MB/s which indicates cache running out.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate if someone could recommend a msata SSD that has bigger cache size and doesn&amp;#39;t suffer from the speed drop problem. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142u24g", "is_robot_indexable": true, "report_reasons": null, "author": "left4taco", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142u24g/looking_for_a_good_quality_msata_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142u24g/looking_for_a_good_quality_msata_ssd/", "subreddit_subscribers": 686361, "created_utc": 1686087392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI tried to use gallery-dl (using Media Downloader as gui) to download twitter user media. But it doesn't finish and gets error 403. So I tried using `--download-archive FILE`  but from what I understand, it just skips the download part, it still looks up the url right ? So I still get 403. I tried then to add `--sleep SECONDS` with 10 seconds but seems it's not enough. Also, is there a way to reduce the amount of requests made ? For exemple, start again from where it left off when it failed to finish, or not look up at all already downloaded media.\n\nI saw this post [\\[gallery-dl / Twitter\\] Is there a way to skip all remaining tweets for a user once everything new has been collected?](https://www.reddit.com/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/) but I don't understand the the answer.\n\nAlso, can I move the already downloaded files (and the archive file) and not \"confuse\" gallery-dl (if i change the output path too ofc) ?\n\nThx for any help,", "author_fullname": "t2_r3ui1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with gallery-dl for twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142rsla", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686082195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I tried to use gallery-dl (using Media Downloader as gui) to download twitter user media. But it doesn&amp;#39;t finish and gets error 403. So I tried using &lt;code&gt;--download-archive FILE&lt;/code&gt;  but from what I understand, it just skips the download part, it still looks up the url right ? So I still get 403. I tried then to add &lt;code&gt;--sleep SECONDS&lt;/code&gt; with 10 seconds but seems it&amp;#39;s not enough. Also, is there a way to reduce the amount of requests made ? For exemple, start again from where it left off when it failed to finish, or not look up at all already downloaded media.&lt;/p&gt;\n\n&lt;p&gt;I saw this post &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/\"&gt;[gallery-dl / Twitter] Is there a way to skip all remaining tweets for a user once everything new has been collected?&lt;/a&gt; but I don&amp;#39;t understand the the answer.&lt;/p&gt;\n\n&lt;p&gt;Also, can I move the already downloaded files (and the archive file) and not &amp;quot;confuse&amp;quot; gallery-dl (if i change the output path too ofc) ?&lt;/p&gt;\n\n&lt;p&gt;Thx for any help,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142rsla", "is_robot_indexable": true, "report_reasons": null, "author": "TheArtofWarPIGEON", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142rsla/help_with_gallerydl_for_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142rsla/help_with_gallerydl_for_twitter/", "subreddit_subscribers": 686361, "created_utc": 1686082195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this subreddit might not be the best place to ask, but, I had to give up with configs.\n\nI have two question about my config (won't download files into subreddit folders), and about archive-format (fighting against duplicates and Reddit galleries).\n\n&amp;#x200B;\n\n1) I'm trying to achieve **reddit\\_user\\_username** and **reddit\\_sub\\_subreddit** folders under base-directory. At the moment, user -object works; it correctly stores user media into specific folders.\n\nHowever, configuration for subreddits does not work. I've tried \"subreddit(s)\", \"submission(s)\".\n\nWhat is wrong with this part of code?\n\n    \"reddit\": {\n    \"videos\": \"ytdl\",\n    \"archive-format\": \"{title}\",\n    \"parent-directory\": true,\n    \"user\": {\n        \"directory\": [\n            \"reddit_user_{author}\"\n     \u00a0  ],\n        \"filename\": \"{date}-{subreddit}-{id}-{title}.{extension}\",\n        \"whitelist\": [\n            \"imgur\",\n            \"redgifs\",\n            \"gfycat\",\n            \"imgur\"\n    \u00a0 \u00a0 ],\n        \"parent-directory\": true\n    },\n    \"subreddit\": {\n        \"directory\": [\n            \"reddit_sub_{subreddit}\"\n\u00a0 \u00a0 \u00a0 \u00a0 ],\n        \"filename\": \"{date}-{author}-{id}-{title}.{extension}\"\n\u00a0 \u00a0 },\n    \"parent-metadata\": \"_reddit_\"\n},\n\n*Extra question:* How I can find names for these objects, like \"user\"? Couldn't find with \\`./gallery-dl -K &lt;url-address&gt;\\`.\n\n&amp;#x200B;\n\n2) At the moment I'm fetching user's images, which are mostly duplicates because posted in multiple subreddits. As \\`archive-format\\`, I used \\`{title}\\`, but this causes issues with posts with multiple images. If the post has multiple images (gallery), it only downloads the first image and skips the rest.\n\nMaybe there is a way to set specific configs only for reddit gallery images, but I'm not sure how it works. Gallery-dl's [documentation](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst) did not help me, and I also tried to find information from the [source code](https://github.com/mikf/gallery-dl/blob/master/gallery_dl/extractor/reddit.py).\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_3sy9yn65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl (custom folder configs, reddit \"galleries\" and archive-format)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142oxf2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686075963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this subreddit might not be the best place to ask, but, I had to give up with configs.&lt;/p&gt;\n\n&lt;p&gt;I have two question about my config (won&amp;#39;t download files into subreddit folders), and about archive-format (fighting against duplicates and Reddit galleries).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1) I&amp;#39;m trying to achieve &lt;strong&gt;reddit_user_username&lt;/strong&gt; and &lt;strong&gt;reddit_sub_subreddit&lt;/strong&gt; folders under base-directory. At the moment, user -object works; it correctly stores user media into specific folders.&lt;/p&gt;\n\n&lt;p&gt;However, configuration for subreddits does not work. I&amp;#39;ve tried &amp;quot;subreddit(s)&amp;quot;, &amp;quot;submission(s)&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What is wrong with this part of code?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;quot;reddit&amp;quot;: {\n&amp;quot;videos&amp;quot;: &amp;quot;ytdl&amp;quot;,\n&amp;quot;archive-format&amp;quot;: &amp;quot;{title}&amp;quot;,\n&amp;quot;parent-directory&amp;quot;: true,\n&amp;quot;user&amp;quot;: {\n    &amp;quot;directory&amp;quot;: [\n        &amp;quot;reddit_user_{author}&amp;quot;\n \u00a0  ],\n    &amp;quot;filename&amp;quot;: &amp;quot;{date}-{subreddit}-{id}-{title}.{extension}&amp;quot;,\n    &amp;quot;whitelist&amp;quot;: [\n        &amp;quot;imgur&amp;quot;,\n        &amp;quot;redgifs&amp;quot;,\n        &amp;quot;gfycat&amp;quot;,\n        &amp;quot;imgur&amp;quot;\n\u00a0 \u00a0 ],\n    &amp;quot;parent-directory&amp;quot;: true\n},\n&amp;quot;subreddit&amp;quot;: {\n    &amp;quot;directory&amp;quot;: [\n        &amp;quot;reddit_sub_{subreddit}&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;\u00a0 \u00a0 \u00a0 \u00a0 ],\n        &amp;quot;filename&amp;quot;: &amp;quot;{date}-{author}-{id}-{title}.{extension}&amp;quot;\n\u00a0 \u00a0 },\n    &amp;quot;parent-metadata&amp;quot;: &amp;quot;&lt;em&gt;reddit&lt;/em&gt;&amp;quot;\n},&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Extra question:&lt;/em&gt; How I can find names for these objects, like &amp;quot;user&amp;quot;? Couldn&amp;#39;t find with `./gallery-dl -K &amp;lt;url-address&amp;gt;`.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2) At the moment I&amp;#39;m fetching user&amp;#39;s images, which are mostly duplicates because posted in multiple subreddits. As `archive-format`, I used `{title}`, but this causes issues with posts with multiple images. If the post has multiple images (gallery), it only downloads the first image and skips the rest.&lt;/p&gt;\n\n&lt;p&gt;Maybe there is a way to set specific configs only for reddit gallery images, but I&amp;#39;m not sure how it works. Gallery-dl&amp;#39;s &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst\"&gt;documentation&lt;/a&gt; did not help me, and I also tried to find information from the &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/gallery_dl/extractor/reddit.py\"&gt;source code&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?auto=webp&amp;v=enabled&amp;s=a0a36aa0277859b6647ca9ead294685ed8eb9ba6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b11803dc23b1a0dac19980e6631cae06e637474", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b6565624f5360f043c4b0a4317ff63c99533061", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fead4655807f833f87d3f932ff47724acc6be920", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99b67ae321d92fd707e7238acc502c80b2c9a143", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b42ab615c2af6f4e56e14167f2c31f7ae7548c2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3423b17f3ce5637d6d89e62a3d5302f32d991024", "width": 1080, "height": 540}], "variants": {}, "id": "2ujLMGJDXtZQwUdJOcM6MdxWqZJO1wuswfwQ1l5pTaU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142oxf2", "is_robot_indexable": true, "report_reasons": null, "author": "qusmar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142oxf2/gallerydl_custom_folder_configs_reddit_galleries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142oxf2/gallerydl_custom_folder_configs_reddit_galleries/", "subreddit_subscribers": 686361, "created_utc": 1686075963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Community, \n\nI wanted to get your views on the different ways of working with storage.   \nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?  \nMaybe a mix model?\n\nI wanted to see how you guys are usually working and what do you feel comfortable / already tested.  \nThank you for sharing your thoughts \ud83d\ude09", "author_fullname": "t2_meba9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflows and different ways of working with storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428wiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686040255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Community, &lt;/p&gt;\n\n&lt;p&gt;I wanted to get your views on the different ways of working with storage.&lt;br/&gt;\nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?&lt;br/&gt;\nMaybe a mix model?&lt;/p&gt;\n\n&lt;p&gt;I wanted to see how you guys are usually working and what do you feel comfortable / already tested.&lt;br/&gt;\nThank you for sharing your thoughts \ud83d\ude09&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1428wiz", "is_robot_indexable": true, "report_reasons": null, "author": "fscheps", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "subreddit_subscribers": 686361, "created_utc": 1686040255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did an rsync mirror of my data ~3TB and would like to verify the backup every once in a while. I tried using rsync's --checksum option for this but I was wondering if there's a better way to do this.", "author_fullname": "t2_9vb9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to verify backup on linux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142tari", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686085671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did an rsync mirror of my data ~3TB and would like to verify the backup every once in a while. I tried using rsync&amp;#39;s --checksum option for this but I was wondering if there&amp;#39;s a better way to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142tari", "is_robot_indexable": true, "report_reasons": null, "author": "Scholes_SC2", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142tari/best_way_to_verify_backup_on_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142tari/best_way_to_verify_backup_on_linux/", "subreddit_subscribers": 686361, "created_utc": 1686085671.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}