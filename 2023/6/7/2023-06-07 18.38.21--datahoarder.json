{"kind": "Listing", "data": {"after": "t3_1430ceo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1qdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drobo is Officially Done as the Company Moves Into Liquidation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1433ddy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 137, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 137, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PKTtj-THG-KuiQSFLPy51PQDf6fkTKM--MIVGPziJEY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686112523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "petapixel.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://petapixel.com/2023/06/06/drobo-is-officially-done-as-the-company-moves-into-liquidation/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?auto=webp&amp;v=enabled&amp;s=2d6845a04b8ae8051560e342fc34ed8054508455", "width": 1600, "height": 840}, "resolutions": [{"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=655acd7ef62b579c5bc7d0bd1608f3082f0bc50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5752f44af67c4c0bdee83005d37717c99d439271", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=182f548e9ad703f020ca4125fbae9121f8c5fb0e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75db6e3ec918e1d90db5276b1d63d58b0f632aec", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca500b5bbc31735cf6e11cf47433cebbdd28bd76", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cff3a17c7fcaecc2e0a03b4673dd71c0178ec15", "width": 1080, "height": 567}], "variants": {}, "id": "BH1g-ogwcIrRpjZws5J8L2362WMqje8vd5KcPJ649YU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1433ddy", "is_robot_indexable": true, "report_reasons": null, "author": "khaled", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1433ddy/drobo_is_officially_done_as_the_company_moves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://petapixel.com/2023/06/06/drobo-is-officially-done-as-the-company-moves-into-liquidation/", "subreddit_subscribers": 686435, "created_utc": 1686112523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I figured I'd throw an option out there for Windows users looking to generate hashes and validate their files based on those hashes.\n\nGenerating hashes and validating files with those hashes should be a trivial matter.\n\nFor Linux you can easily use `md5sum` to generate hashes of files and run recursively through a folder with `find /folder -type f -exec md5sum {} + &gt;&gt; hash.log`\n\nFor Windows it's not quite as straightforward, usually you have to resort to third party apps like TeraCopy, Hashdeep, DirHash, CRCCheckCopy, among others. However, it is possible using native Windows Powershell.\n\nFIRST AND FOREMOST... I take no responsibility in anything happening to your data. I'm just trying to offer options. This has not been tested extensively, but I personally use it and it seems to work well. I am not a programmer but I did piecemeal this code together. It may not be the most efficient, but it works. This will not modify any data, only generate log files of MD5 hashes and validation results.\n\nIf you notice any bugs or anything odd about it, please let me know, but I don't plan on adding many additional features. I wanted it pretty bare bones. Feel free to modify as you see fit for your own use.\n\n**GENERATING MD5 HASHES RECURSIVELY**\n\nAt it's most basic level you can use this \"one-liner\" script to generate hashes recursively through folders, which outputs results to file \"hashlog.log\":\n\n    gci -Path \"&lt;folder to hash&gt;\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | ft -AutoSize &gt; \"hashlog.log\"\n\nBasic, but it works. It doesn't display anything to console and it uses absolute paths instead of relative paths, which might not be real useful for validation reasons. It also pads it with a bunch of trailing spaces equal to the longest path name. This is default behavior for this type of output.\n\nTo run it, just CD to the folder where you want to store the log file, copy/paste the above in a powershell window, modify \"&lt;folder to hash&gt;\" to the file path you want to recursively generate hashes for every file in, and hit [ENTER]. Obviously the more files you have the longer it will take. This code does not provide any output to console.\n\nIf you want something more robust, this \"one-liner\" is quite long but a bit more useful and mimics output from md5sum, basically `hashvalue \\relative\\path\\to\\file` Example Output:\n\n    19D4BB0121058149C33BA98DBC3ED149 \\SMALL\\10KB\\10KB_000000.txt\n    94E29C41059CB19819C1934B500797DD \\SMALL\\10KB\\10KB_000001.txt\n    EA5E5FC8B434FD4924FDEC5FDBFE226B \\SMALL\\10KB\\10KB_000002.txt\n    9EA101052926CB0BE10342EF9468FC3E \\SMALL\\10KB\\10KB_000003.txt\n    B952DA5732E657FB961310AC4AB55493 \\SMALL\\10KB\\10KB_000004.txt\n\nThis below script outputs to console, outputs paths relative to the input path, allows you to specify the name of the hashlog, trims any empty whitespace, and provides a header with date and file info at the top:\n\n    $hashpath=\"E:\\Test it\"; $hashlog=\"hashlog.log\"; $numfiles=(gci -Path \"$hashpath\" -Recurse -File | measure-object).Count; Write-Output \"Date: $(Get-Date) / Path: $hashpath / Files: $numfiles\" | Set-Content \"$hashlog\"; $count=1; gci -Path \"$hashpath\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | % {$_.Path=($_.Path -replace [regex]::Escape(\"$hashpath\"), ''); Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path; ++$count; $_} | ft -AutoSize -HideTableHeaders | Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath \"$hashlog\" -Append; (gc \"$hashlog\").Trim() -ne \"\" | sc \"$hashlog\"\n\nJust modify `$hashpath` to the folder you want to hash, `$hashlog` for the name of the log file, and let it rip.\n\nOr if you wish you can copy/paste the below into a notepad file, save it as something like `generatemd5hash.ps1`, then right click and run with powershell. It's the same as the above \"one-liner\" just a bit more readable:\n\n    $hashpath=\"E:\\Test it\"; $hashlog=\"hashlog.log\"\n    $numfiles=(gci -Path \"$hashpath\" -Recurse -File | measure-object).Count\n    Write-Output \"Date: $(Get-Date) / Path: $hashpath / Files: $numfiles\" | Set-Content \"$hashlog\"\n    $count=1\n    gci -Path \"$hashpath\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | \n        % {$_.Path=($_.Path -replace [regex]::Escape(\"$hashpath\"), ''); Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path; ++$count; $_} | \n        ft -AutoSize -HideTableHeaders | \n        Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath \"$hashlog\" -Append\n    (gc \"$hashlog\").Trim() -ne \"\" | sc \"$hashlog\"\n\n**VALIDATING HASHES**\n\nMost hashing utilities offer a way to validate hashes based on an existing pregenerated log of hashes. In this case you can't exactly validate directly using the log file. What you can do is generate a new set of hashes and compare the two.\n\nPowershell comes with a nifty little command called `Compare-Object` or also known as `Diff`. It will very quickly find differences between contents of text files.\n\nIt's basic command set is as follows:\n\n    Compare-Object -ReferenceObject (Get-Content \"&lt;hash log file 1&gt;\") -DifferenceObject (Get-Content \"&lt;hash log file 2&gt;\")\n\nThis will then spit out items in file 1 that aren't in file 2 and vice versa. File 1 objects are identified with a \"&lt;=\" flag and File 2 objects are identified with a \"=&gt;\" flag.\n\nSo if you generate hashes of your data, and then some time in the future, you can generate another set of hashes and then compare them using the above command to validate for any changes (due to bitrot or other).\n\nLike the hash generation code above, I also created a more robust hash comparison \"one-liner\" that you can use:\n\n    $hash1=\"hash1.md5\"; $hash2=\"hash2.md5\"; $hashdifflog=\"hashdifflog.log\"; Write-Output \"Date: $(Get-Date) / Compare '$hash1' (&lt;=) with '$hash2' (=&gt;)\" | sc \"$hashdifflog\"; diff -ReferenceObject (gc \"$hash1\" | Select -Skip 1) -DifferenceObject (gc \"$hash2\" | Select -Skip 1) | group { $_.InputObject -replace '^.+ ' } | % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | Out-File -Encoding ASCII -Filepath \"$hashdifflog\" -Append; gc $hashdifflog\n\nSet `$hash1` variable to the first hash log file and `$hash2` to the second hash log file you want to compare. Set `$hashdifflog` variable to whatever you want the log file to be named, by default it's \"hashdifflog.log\" and will store it to whatever folder you run this powershell script from. It will also provide date and file reference info at the top.\n\nAntoher thing to note is the `Select -Skip 1` command. This will effectively skip the first line of data because if you used the above Powershell script to generate hashes, it will genrate a header with Date and file info that you don't want to put in the comparison mix. So if your files don't have any header info you can just change the Skip value to 0.\n\nThis script will group matching file names from each log file that have non-matching hashes. Files that are unique to each file will be shown on individual lines. If there are no discrepencies it will be blank, which is ideally what you want.\n\nOr as before, you can also copy/paste the below into a notepad document, save it with a `.ps1` extension (i.e. `hashcompare.ps1`), right click and run with powershell.\n\n    $hash1=\"hash1.md5\"\n    $hash2=\"hash2.md5\"\n    $hashdifflog=\"hashdifflog.log\"\n    Write-Output \"Date: $(Get-Date) / Compare '$hash1' (&lt;=) with '$hash2' (=&gt;)\" | sc \"$hashdifflog\"\n    diff -ReferenceObject (gc \"$hash1\" | Select -Skip 1) -DifferenceObject (gc \"$hash2\" | Select -Skip 1) | \n        group { $_.InputObject -replace '^.+ ' } | \n        % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | \n        Out-File -Encoding ASCII -Filepath \"$hashdifflog\" -Append\n    gc $hashdifflog\n\n\n**INTERACTIVE SCRIPT**\n\nBut if you don't want to fuss with any of the above I wrote this basic Powershell script that will allow you generate hash log files, and compare hash log files just by entering the appropriate info at the prompts.\n\nA few notes:\n\n1. When you go to compare hash files, it will ask you how many lines to skip. It will present you with the first 9 lines of each log file you specify and you can tell it how many lines to omit from the comparison. So if you added any remarks or header info to the hash log files, you can just have it ignored. Selecting \"Skip 3\" will skip the first three lines shown, \"Skip 1\" just the first line, etc. Default will be to skip no lines.\n\n2. This does an EXACT COMPARE of file names as strings, although it is not case sensitive. It is designed so all paths will start with a slash. It has no logic to assume forward slash and backwards slash are interchangeable. So if you need to compare output from a Linux machine with md5sum to this script run from a Windows machine you will have to modify the slashes. This can be easily rectified with the following (backwards slash to forwards slash, just revere them to get the reverse effect):\n\n    `(gc \"hash1.md5\").replace('\\', '/') | sc \"hash1.md5\"`\n\n3. Log files are appended with a date time stamp number value equal to `yyyymmdd_HHmmss`\n\n4. This has been tested with a log file with over 60 thousand entries and worked fine. File path length should not be an issue, but I haven't tested it past about 200 characters.\n\nYou can copy/paste the powershell script from pastebin here: https://pastebin.com/0UJ0gcdb\n\nOr here's the same code below. Just copy/paste into notepad and save as `generatemd5.ps1` or whatever as long as it has a `.ps1` extension. It will save log files where you save it, so just be wary of that.\n\n    # generatemd5.ps1 by HTWingNut 06 June 2023\n    Clear-Host\n    Write-Host \"\"\n    $datetime = Get-Date\n    $timestamp = $datetime.ToString(\"yyyyMMdd_HHmmss\")\n    $reqpath = 'Input path to generate md5 hashes'\n    $basepath = $hash1 = $hash2 = \"?\"\n    $choice=$null\n    $md5log = 'hashes_'+$timestamp+'.md5'\n    \n    function GenerateHash {\n    \n    Clear-Host\n    Write-Host \"\"\n    Write-Host \"Generate Hash Files ...\"\n    Write-Host \"\"\n    \n    while (!(Test-Path -Path $basepath)) { $basepath = Read-Host -Prompt $reqpath; if ( $basepath -eq \"\") { $basepath=\"?\" } }\n    $numfiles = ( Get-ChildItem -Path \"$basepath\" -Recurse -File | Measure-Object ).Count\n    \n    $lchar = $basepath.Substring($basepath.length-1,1)\n    If ($lchar -eq \"\\\") { $basepath = $basepath.Substring(0,$basepath.length-1) }\n    \n    Write-Host \"Total Files: $numfiles\"\n    Write-Host \"\"\n    $count=1\n    \n    Write-Output \"**** $($datetime) '$($basepath)'\" | Set-Content \"$md5log\"\n    \n    Get-ChildItem -Path \"$basepath\" -Recurse -File | \n        Get-FileHash -Algorithm MD5 | \n        Select-Object Hash,Path | \n        ForEach-Object { \n            $_.Path = ($_.Path -replace [regex]::Escape(\"$basepath\"), '')\n            Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path\n            ++$count\n            $_\n        } | \n        Format-Table -AutoSize -HideTableHeaders | \n        Out-String -Width 4096 | \n        Out-File \"$md5log\" -encoding ASCII -append\n    \n    (get-content $md5log).Trim() -ne '' | Set-Content $md5log\n    \n    Write-Host \"\"\n    Write-Host \"Hashes stored in file '$pwd\\$md5log'\"\n    Write-Host \"\"\n    \n    }\n    \n    function CompareHash {\n    \n    $continue = \"n\"\n    \n    While ([bool]$continue) {\n    \n    Clear-Host\n    Write-Host \"\"\n    Write-Host \"Compare Hash Files...\"\n    \n    $hash1 = $hash2 = \"?\"\n    $numksip1 = $numskip2 = \"0\"\n    \n    Write-Host \"\"\n    while (!(Test-Path $hash1)) { $hash1 = Read-Host -Prompt \"Enter Path/Name for Log 1\"; if ( $hash1 -eq \"\") { $hash1=\"?\" } }\n    Write-Host \"\"\n    Write-Host \"First 9 lines of $($hash1):\"\n    $i=1; Get-Content $hash1 | Select -First 9 | % {\"$($i) $_\"; $i++}\n    Write-Host \"\"\n    Do { $numskip1 = Read-Host \"Choose Number of Lines to Skip [0-9] (default 0)\" } until ($numskip1 -in 0,1,2,3,4,5,6,7,8,9)\n    Write-Host \"\"\n    \n    while (!(Test-Path $hash2)) { $hash2 = Read-Host -Prompt \"Enter Path/Name for Log 2\"; if ( $hash2 -eq \"\") { $hash2=\"?\" } }\n    Write-Host \"\"\n    Write-Host \"First 9 lines of $($hash2):\"\n    $i=1; Get-Content $hash2 | Select -First 9 | % {\"$($i) $_\"; $i++}\n    Write-Host \"\"\n    Do { $numskip2 = Read-Host \"Choose Number of Lines to Skip [0-9] (default 0)\" } until ($numskip2 -in 0,1,2,3,4,5,6,7,8,9)\n    \n    $hclog = \"HashCompare_$(((Get-Item $hash1).Basename).Replace(' ','_'))_vs_$(((Get-Item $hash2).Basename).Replace(' ','_'))_$timestamp.txt\"\n    \n        Write-Host \"\"\n        Write-Host \"---------------------\"\n        Write-Host \"\"\n    \n        Write-Host \"**** File: '$((Get-Item $hash1).Name)'\"\n        Write-Host \"**** Skipping Lines:\"\n        Get-Content $hash1 | Select -First $numskip1\n        Write-Host \"\"\n        Write-Host \"**** Starting with Line:\"\n        Get-Content $hash1 | Select -Index ($numskip1)\n        Write-Host \"\"\n        Write-Host \"---------------------\"\n        Write-Host \"\"\n        Write-Host \"**** File: '$((Get-Item $hash2).Name)'\"\n        Write-Host \"**** Skipping Lines:\"\n        Get-Content $hash2 | Select -First $numskip2\n        Write-Host \"\"\n        Write-Host \"**** Starting with Line:\"\n    \n        Get-Content $hash2 | Select -Index ($numskip2)\n        Write-Host\n        $continue = Read-Host \"Press [ENTER] to accept, any other key to choose again\"\n    \n    }\n    \n    # https://stackoverflow.com/questions/76415338/in-powershell-how-do-i-split-text-from-compare-object-input-and-sort-by-one-of/\n    \n    $diff = Compare-Object -ReferenceObject (Get-Content \"$hash1\" | Select -Skip $numskip1) -DifferenceObject (Get-Content \"$hash2\" | Select -Skip $numskip2 ) | \n        ForEach-Object {\n            if ($_.SideIndicator -eq \"&lt;=\") { $_.SideIndicator = \"($((Get-ChildItem $hash1).Name))\" } elseif ($_.SideIndicator -eq \"=&gt;\") { $_.SideIndicator = \"($((Get-ChildItem $hash2).Name))\" }\n            $_\n        } |\n        Group-Object { $_.InputObject -replace '^.+ ' } |\n        ForEach-Object {\n            $_.Group | Format-Table -HideTableHeaders | \n                Out-String | ForEach-Object TrimEnd\n        }\n    \n    if ($diff) {\n        Write-output \"**** $($datetime) '$($hash1)' vs '$($hash2)'`n**** Matching file path/names with mismatched hashes are grouped together`n**** Individual file names means unique file/hash in that log file\" $diff &gt; \"$hclog\"\n    } else {\n        Write-Output \"**** $($datetime) '$($hash1)' vs '$($hash2)' `n`n**** ALL CLEAR! NO DIFFERENCES FOUND!\" &gt; \"$hclog\"\n    }\n    \n    Write-Host \"\"\n    Write-Host \"**** Results stored in $($pwd)\\$($hclog)\"\n    Write-Host \"\"\n    \n    \n    }\n    \n    Do { $choice = Read-Host \"[G]enerate Hash or [C]ompare Hash Logs\" } until ($choice -in 'g','c')\n    If ( $choice -eq \"g\" ) { GenerateHash }\n    If ( $choice -eq \"c\" ) { CompareHash }\n    \n    cmd /c 'pause'", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here is a Windows Powershell Script to Generate and Validate MD5 Hashes of Your Data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ysqt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1686143230.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686099374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I figured I&amp;#39;d throw an option out there for Windows users looking to generate hashes and validate their files based on those hashes.&lt;/p&gt;\n\n&lt;p&gt;Generating hashes and validating files with those hashes should be a trivial matter.&lt;/p&gt;\n\n&lt;p&gt;For Linux you can easily use &lt;code&gt;md5sum&lt;/code&gt; to generate hashes of files and run recursively through a folder with &lt;code&gt;find /folder -type f -exec md5sum {} + &amp;gt;&amp;gt; hash.log&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;For Windows it&amp;#39;s not quite as straightforward, usually you have to resort to third party apps like TeraCopy, Hashdeep, DirHash, CRCCheckCopy, among others. However, it is possible using native Windows Powershell.&lt;/p&gt;\n\n&lt;p&gt;FIRST AND FOREMOST... I take no responsibility in anything happening to your data. I&amp;#39;m just trying to offer options. This has not been tested extensively, but I personally use it and it seems to work well. I am not a programmer but I did piecemeal this code together. It may not be the most efficient, but it works. This will not modify any data, only generate log files of MD5 hashes and validation results.&lt;/p&gt;\n\n&lt;p&gt;If you notice any bugs or anything odd about it, please let me know, but I don&amp;#39;t plan on adding many additional features. I wanted it pretty bare bones. Feel free to modify as you see fit for your own use.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GENERATING MD5 HASHES RECURSIVELY&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At it&amp;#39;s most basic level you can use this &amp;quot;one-liner&amp;quot; script to generate hashes recursively through folders, which outputs results to file &amp;quot;hashlog.log&amp;quot;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gci -Path &amp;quot;&amp;lt;folder to hash&amp;gt;&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | ft -AutoSize &amp;gt; &amp;quot;hashlog.log&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Basic, but it works. It doesn&amp;#39;t display anything to console and it uses absolute paths instead of relative paths, which might not be real useful for validation reasons. It also pads it with a bunch of trailing spaces equal to the longest path name. This is default behavior for this type of output.&lt;/p&gt;\n\n&lt;p&gt;To run it, just CD to the folder where you want to store the log file, copy/paste the above in a powershell window, modify &amp;quot;&amp;lt;folder to hash&amp;gt;&amp;quot; to the file path you want to recursively generate hashes for every file in, and hit [ENTER]. Obviously the more files you have the longer it will take. This code does not provide any output to console.&lt;/p&gt;\n\n&lt;p&gt;If you want something more robust, this &amp;quot;one-liner&amp;quot; is quite long but a bit more useful and mimics output from md5sum, basically &lt;code&gt;hashvalue \\relative\\path\\to\\file&lt;/code&gt; Example Output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;19D4BB0121058149C33BA98DBC3ED149 \\SMALL\\10KB\\10KB_000000.txt\n94E29C41059CB19819C1934B500797DD \\SMALL\\10KB\\10KB_000001.txt\nEA5E5FC8B434FD4924FDEC5FDBFE226B \\SMALL\\10KB\\10KB_000002.txt\n9EA101052926CB0BE10342EF9468FC3E \\SMALL\\10KB\\10KB_000003.txt\nB952DA5732E657FB961310AC4AB55493 \\SMALL\\10KB\\10KB_000004.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This below script outputs to console, outputs paths relative to the input path, allows you to specify the name of the hashlog, trims any empty whitespace, and provides a header with date and file info at the top:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hashpath=&amp;quot;E:\\Test it&amp;quot;; $hashlog=&amp;quot;hashlog.log&amp;quot;; $numfiles=(gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | measure-object).Count; Write-Output &amp;quot;Date: $(Get-Date) / Path: $hashpath / Files: $numfiles&amp;quot; | Set-Content &amp;quot;$hashlog&amp;quot;; $count=1; gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | % {$_.Path=($_.Path -replace [regex]::Escape(&amp;quot;$hashpath&amp;quot;), &amp;#39;&amp;#39;); Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path; ++$count; $_} | ft -AutoSize -HideTableHeaders | Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath &amp;quot;$hashlog&amp;quot; -Append; (gc &amp;quot;$hashlog&amp;quot;).Trim() -ne &amp;quot;&amp;quot; | sc &amp;quot;$hashlog&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Just modify &lt;code&gt;$hashpath&lt;/code&gt; to the folder you want to hash, &lt;code&gt;$hashlog&lt;/code&gt; for the name of the log file, and let it rip.&lt;/p&gt;\n\n&lt;p&gt;Or if you wish you can copy/paste the below into a notepad file, save it as something like &lt;code&gt;generatemd5hash.ps1&lt;/code&gt;, then right click and run with powershell. It&amp;#39;s the same as the above &amp;quot;one-liner&amp;quot; just a bit more readable:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hashpath=&amp;quot;E:\\Test it&amp;quot;; $hashlog=&amp;quot;hashlog.log&amp;quot;\n$numfiles=(gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | measure-object).Count\nWrite-Output &amp;quot;Date: $(Get-Date) / Path: $hashpath / Files: $numfiles&amp;quot; | Set-Content &amp;quot;$hashlog&amp;quot;\n$count=1\ngci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | \n    % {$_.Path=($_.Path -replace [regex]::Escape(&amp;quot;$hashpath&amp;quot;), &amp;#39;&amp;#39;); Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path; ++$count; $_} | \n    ft -AutoSize -HideTableHeaders | \n    Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath &amp;quot;$hashlog&amp;quot; -Append\n(gc &amp;quot;$hashlog&amp;quot;).Trim() -ne &amp;quot;&amp;quot; | sc &amp;quot;$hashlog&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;VALIDATING HASHES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Most hashing utilities offer a way to validate hashes based on an existing pregenerated log of hashes. In this case you can&amp;#39;t exactly validate directly using the log file. What you can do is generate a new set of hashes and compare the two.&lt;/p&gt;\n\n&lt;p&gt;Powershell comes with a nifty little command called &lt;code&gt;Compare-Object&lt;/code&gt; or also known as &lt;code&gt;Diff&lt;/code&gt;. It will very quickly find differences between contents of text files.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basic command set is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Compare-Object -ReferenceObject (Get-Content &amp;quot;&amp;lt;hash log file 1&amp;gt;&amp;quot;) -DifferenceObject (Get-Content &amp;quot;&amp;lt;hash log file 2&amp;gt;&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This will then spit out items in file 1 that aren&amp;#39;t in file 2 and vice versa. File 1 objects are identified with a &amp;quot;&amp;lt;=&amp;quot; flag and File 2 objects are identified with a &amp;quot;=&amp;gt;&amp;quot; flag.&lt;/p&gt;\n\n&lt;p&gt;So if you generate hashes of your data, and then some time in the future, you can generate another set of hashes and then compare them using the above command to validate for any changes (due to bitrot or other).&lt;/p&gt;\n\n&lt;p&gt;Like the hash generation code above, I also created a more robust hash comparison &amp;quot;one-liner&amp;quot; that you can use:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hash1=&amp;quot;hash1.md5&amp;quot;; $hash2=&amp;quot;hash2.md5&amp;quot;; $hashdifflog=&amp;quot;hashdifflog.log&amp;quot;; Write-Output &amp;quot;Date: $(Get-Date) / Compare &amp;#39;$hash1&amp;#39; (&amp;lt;=) with &amp;#39;$hash2&amp;#39; (=&amp;gt;)&amp;quot; | sc &amp;quot;$hashdifflog&amp;quot;; diff -ReferenceObject (gc &amp;quot;$hash1&amp;quot; | Select -Skip 1) -DifferenceObject (gc &amp;quot;$hash2&amp;quot; | Select -Skip 1) | group { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } | % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | Out-File -Encoding ASCII -Filepath &amp;quot;$hashdifflog&amp;quot; -Append; gc $hashdifflog\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Set &lt;code&gt;$hash1&lt;/code&gt; variable to the first hash log file and &lt;code&gt;$hash2&lt;/code&gt; to the second hash log file you want to compare. Set &lt;code&gt;$hashdifflog&lt;/code&gt; variable to whatever you want the log file to be named, by default it&amp;#39;s &amp;quot;hashdifflog.log&amp;quot; and will store it to whatever folder you run this powershell script from. It will also provide date and file reference info at the top.&lt;/p&gt;\n\n&lt;p&gt;Antoher thing to note is the &lt;code&gt;Select -Skip 1&lt;/code&gt; command. This will effectively skip the first line of data because if you used the above Powershell script to generate hashes, it will genrate a header with Date and file info that you don&amp;#39;t want to put in the comparison mix. So if your files don&amp;#39;t have any header info you can just change the Skip value to 0.&lt;/p&gt;\n\n&lt;p&gt;This script will group matching file names from each log file that have non-matching hashes. Files that are unique to each file will be shown on individual lines. If there are no discrepencies it will be blank, which is ideally what you want.&lt;/p&gt;\n\n&lt;p&gt;Or as before, you can also copy/paste the below into a notepad document, save it with a &lt;code&gt;.ps1&lt;/code&gt; extension (i.e. &lt;code&gt;hashcompare.ps1&lt;/code&gt;), right click and run with powershell.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hash1=&amp;quot;hash1.md5&amp;quot;\n$hash2=&amp;quot;hash2.md5&amp;quot;\n$hashdifflog=&amp;quot;hashdifflog.log&amp;quot;\nWrite-Output &amp;quot;Date: $(Get-Date) / Compare &amp;#39;$hash1&amp;#39; (&amp;lt;=) with &amp;#39;$hash2&amp;#39; (=&amp;gt;)&amp;quot; | sc &amp;quot;$hashdifflog&amp;quot;\ndiff -ReferenceObject (gc &amp;quot;$hash1&amp;quot; | Select -Skip 1) -DifferenceObject (gc &amp;quot;$hash2&amp;quot; | Select -Skip 1) | \n    group { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } | \n    % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | \n    Out-File -Encoding ASCII -Filepath &amp;quot;$hashdifflog&amp;quot; -Append\ngc $hashdifflog\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;INTERACTIVE SCRIPT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;But if you don&amp;#39;t want to fuss with any of the above I wrote this basic Powershell script that will allow you generate hash log files, and compare hash log files just by entering the appropriate info at the prompts.&lt;/p&gt;\n\n&lt;p&gt;A few notes:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;When you go to compare hash files, it will ask you how many lines to skip. It will present you with the first 9 lines of each log file you specify and you can tell it how many lines to omit from the comparison. So if you added any remarks or header info to the hash log files, you can just have it ignored. Selecting &amp;quot;Skip 3&amp;quot; will skip the first three lines shown, &amp;quot;Skip 1&amp;quot; just the first line, etc. Default will be to skip no lines.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This does an EXACT COMPARE of file names as strings, although it is not case sensitive. It is designed so all paths will start with a slash. It has no logic to assume forward slash and backwards slash are interchangeable. So if you need to compare output from a Linux machine with md5sum to this script run from a Windows machine you will have to modify the slashes. This can be easily rectified with the following (backwards slash to forwards slash, just revere them to get the reverse effect):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;(gc &amp;quot;hash1.md5&amp;quot;).replace(&amp;#39;\\&amp;#39;, &amp;#39;/&amp;#39;) | sc &amp;quot;hash1.md5&amp;quot;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Log files are appended with a date time stamp number value equal to &lt;code&gt;yyyymmdd_HHmmss&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This has been tested with a log file with over 60 thousand entries and worked fine. File path length should not be an issue, but I haven&amp;#39;t tested it past about 200 characters.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;You can copy/paste the powershell script from pastebin here: &lt;a href=\"https://pastebin.com/0UJ0gcdb\"&gt;https://pastebin.com/0UJ0gcdb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Or here&amp;#39;s the same code below. Just copy/paste into notepad and save as &lt;code&gt;generatemd5.ps1&lt;/code&gt; or whatever as long as it has a &lt;code&gt;.ps1&lt;/code&gt; extension. It will save log files where you save it, so just be wary of that.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# generatemd5.ps1 by HTWingNut 06 June 2023\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\n$datetime = Get-Date\n$timestamp = $datetime.ToString(&amp;quot;yyyyMMdd_HHmmss&amp;quot;)\n$reqpath = &amp;#39;Input path to generate md5 hashes&amp;#39;\n$basepath = $hash1 = $hash2 = &amp;quot;?&amp;quot;\n$choice=$null\n$md5log = &amp;#39;hashes_&amp;#39;+$timestamp+&amp;#39;.md5&amp;#39;\n\nfunction GenerateHash {\n\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Generate Hash Files ...&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\nwhile (!(Test-Path -Path $basepath)) { $basepath = Read-Host -Prompt $reqpath; if ( $basepath -eq &amp;quot;&amp;quot;) { $basepath=&amp;quot;?&amp;quot; } }\n$numfiles = ( Get-ChildItem -Path &amp;quot;$basepath&amp;quot; -Recurse -File | Measure-Object ).Count\n\n$lchar = $basepath.Substring($basepath.length-1,1)\nIf ($lchar -eq &amp;quot;\\&amp;quot;) { $basepath = $basepath.Substring(0,$basepath.length-1) }\n\nWrite-Host &amp;quot;Total Files: $numfiles&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n$count=1\n\nWrite-Output &amp;quot;**** $($datetime) &amp;#39;$($basepath)&amp;#39;&amp;quot; | Set-Content &amp;quot;$md5log&amp;quot;\n\nGet-ChildItem -Path &amp;quot;$basepath&amp;quot; -Recurse -File | \n    Get-FileHash -Algorithm MD5 | \n    Select-Object Hash,Path | \n    ForEach-Object { \n        $_.Path = ($_.Path -replace [regex]::Escape(&amp;quot;$basepath&amp;quot;), &amp;#39;&amp;#39;)\n        Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path\n        ++$count\n        $_\n    } | \n    Format-Table -AutoSize -HideTableHeaders | \n    Out-String -Width 4096 | \n    Out-File &amp;quot;$md5log&amp;quot; -encoding ASCII -append\n\n(get-content $md5log).Trim() -ne &amp;#39;&amp;#39; | Set-Content $md5log\n\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Hashes stored in file &amp;#39;$pwd\\$md5log&amp;#39;&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\n}\n\nfunction CompareHash {\n\n$continue = &amp;quot;n&amp;quot;\n\nWhile ([bool]$continue) {\n\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Compare Hash Files...&amp;quot;\n\n$hash1 = $hash2 = &amp;quot;?&amp;quot;\n$numksip1 = $numskip2 = &amp;quot;0&amp;quot;\n\nWrite-Host &amp;quot;&amp;quot;\nwhile (!(Test-Path $hash1)) { $hash1 = Read-Host -Prompt &amp;quot;Enter Path/Name for Log 1&amp;quot;; if ( $hash1 -eq &amp;quot;&amp;quot;) { $hash1=&amp;quot;?&amp;quot; } }\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;First 9 lines of $($hash1):&amp;quot;\n$i=1; Get-Content $hash1 | Select -First 9 | % {&amp;quot;$($i) $_&amp;quot;; $i++}\nWrite-Host &amp;quot;&amp;quot;\nDo { $numskip1 = Read-Host &amp;quot;Choose Number of Lines to Skip [0-9] (default 0)&amp;quot; } until ($numskip1 -in 0,1,2,3,4,5,6,7,8,9)\nWrite-Host &amp;quot;&amp;quot;\n\nwhile (!(Test-Path $hash2)) { $hash2 = Read-Host -Prompt &amp;quot;Enter Path/Name for Log 2&amp;quot;; if ( $hash2 -eq &amp;quot;&amp;quot;) { $hash2=&amp;quot;?&amp;quot; } }\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;First 9 lines of $($hash2):&amp;quot;\n$i=1; Get-Content $hash2 | Select -First 9 | % {&amp;quot;$($i) $_&amp;quot;; $i++}\nWrite-Host &amp;quot;&amp;quot;\nDo { $numskip2 = Read-Host &amp;quot;Choose Number of Lines to Skip [0-9] (default 0)&amp;quot; } until ($numskip2 -in 0,1,2,3,4,5,6,7,8,9)\n\n$hclog = &amp;quot;HashCompare_$(((Get-Item $hash1).Basename).Replace(&amp;#39; &amp;#39;,&amp;#39;_&amp;#39;))_vs_$(((Get-Item $hash2).Basename).Replace(&amp;#39; &amp;#39;,&amp;#39;_&amp;#39;))_$timestamp.txt&amp;quot;\n\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;---------------------&amp;quot;\n    Write-Host &amp;quot;&amp;quot;\n\n    Write-Host &amp;quot;**** File: &amp;#39;$((Get-Item $hash1).Name)&amp;#39;&amp;quot;\n    Write-Host &amp;quot;**** Skipping Lines:&amp;quot;\n    Get-Content $hash1 | Select -First $numskip1\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** Starting with Line:&amp;quot;\n    Get-Content $hash1 | Select -Index ($numskip1)\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;---------------------&amp;quot;\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** File: &amp;#39;$((Get-Item $hash2).Name)&amp;#39;&amp;quot;\n    Write-Host &amp;quot;**** Skipping Lines:&amp;quot;\n    Get-Content $hash2 | Select -First $numskip2\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** Starting with Line:&amp;quot;\n\n    Get-Content $hash2 | Select -Index ($numskip2)\n    Write-Host\n    $continue = Read-Host &amp;quot;Press [ENTER] to accept, any other key to choose again&amp;quot;\n\n}\n\n# https://stackoverflow.com/questions/76415338/in-powershell-how-do-i-split-text-from-compare-object-input-and-sort-by-one-of/\n\n$diff = Compare-Object -ReferenceObject (Get-Content &amp;quot;$hash1&amp;quot; | Select -Skip $numskip1) -DifferenceObject (Get-Content &amp;quot;$hash2&amp;quot; | Select -Skip $numskip2 ) | \n    ForEach-Object {\n        if ($_.SideIndicator -eq &amp;quot;&amp;lt;=&amp;quot;) { $_.SideIndicator = &amp;quot;($((Get-ChildItem $hash1).Name))&amp;quot; } elseif ($_.SideIndicator -eq &amp;quot;=&amp;gt;&amp;quot;) { $_.SideIndicator = &amp;quot;($((Get-ChildItem $hash2).Name))&amp;quot; }\n        $_\n    } |\n    Group-Object { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } |\n    ForEach-Object {\n        $_.Group | Format-Table -HideTableHeaders | \n            Out-String | ForEach-Object TrimEnd\n    }\n\nif ($diff) {\n    Write-output &amp;quot;**** $($datetime) &amp;#39;$($hash1)&amp;#39; vs &amp;#39;$($hash2)&amp;#39;`n**** Matching file path/names with mismatched hashes are grouped together`n**** Individual file names means unique file/hash in that log file&amp;quot; $diff &amp;gt; &amp;quot;$hclog&amp;quot;\n} else {\n    Write-Output &amp;quot;**** $($datetime) &amp;#39;$($hash1)&amp;#39; vs &amp;#39;$($hash2)&amp;#39; `n`n**** ALL CLEAR! NO DIFFERENCES FOUND!&amp;quot; &amp;gt; &amp;quot;$hclog&amp;quot;\n}\n\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;**** Results stored in $($pwd)\\$($hclog)&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\n\n}\n\nDo { $choice = Read-Host &amp;quot;[G]enerate Hash or [C]ompare Hash Logs&amp;quot; } until ($choice -in &amp;#39;g&amp;#39;,&amp;#39;c&amp;#39;)\nIf ( $choice -eq &amp;quot;g&amp;quot; ) { GenerateHash }\nIf ( $choice -eq &amp;quot;c&amp;quot; ) { CompareHash }\n\ncmd /c &amp;#39;pause&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ysqt", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142ysqt/here_is_a_windows_powershell_script_to_generate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ysqt/here_is_a_windows_powershell_script_to_generate/", "subreddit_subscribers": 686435, "created_utc": 1686099374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It looks like The Eye does have it up until December 2022?  But I have important work from this year that I am trying to preserve (such as my definitions of [love](https://old.reddit.com/r/AbuseInterrupted/comments/13k3yq2/the_definition_of_love/) and [abuse](https://old.reddit.com/r/AbuseInterrupted/comments/13k597w/definition_of_abuse/), for example).\n\nI did also do the [Reddit Data Request](https://www.reddit.com/settings/data-request).\n\nI'm sorry to come in here like an idiot, but I have zero programming ability and am now having to figure out how to migrate this over to a website because people depend on these resources, and I also don't want to lose over a decade of my work.\n\nI looks like most people here are trying to save their personal Reddit history, but I am more focused on the subreddit.", "author_fullname": "t2_7djla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to archive/clone/migrate a subreddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142qq0l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686079715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like The Eye does have it up until December 2022?  But I have important work from this year that I am trying to preserve (such as my definitions of &lt;a href=\"https://old.reddit.com/r/AbuseInterrupted/comments/13k3yq2/the_definition_of_love/\"&gt;love&lt;/a&gt; and &lt;a href=\"https://old.reddit.com/r/AbuseInterrupted/comments/13k597w/definition_of_abuse/\"&gt;abuse&lt;/a&gt;, for example).&lt;/p&gt;\n\n&lt;p&gt;I did also do the &lt;a href=\"https://www.reddit.com/settings/data-request\"&gt;Reddit Data Request&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sorry to come in here like an idiot, but I have zero programming ability and am now having to figure out how to migrate this over to a website because people depend on these resources, and I also don&amp;#39;t want to lose over a decade of my work.&lt;/p&gt;\n\n&lt;p&gt;I looks like most people here are trying to save their personal Reddit history, but I am more focused on the subreddit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142qq0l", "is_robot_indexable": true, "report_reasons": null, "author": "invah", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142qq0l/how_to_archiveclonemigrate_a_subreddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142qq0l/how_to_archiveclonemigrate_a_subreddit/", "subreddit_subscribers": 686435, "created_utc": 1686079715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lord Bung is deleting the Confinement series - For those interested, you might want to archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_1432tyo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b38dubx", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8nKejTl_n0CbeitlAK4Cm3d44Gflyq8VQJbib8s7IWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "SCP", "selftext": "", "author_fullname": "t2_9z6t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lord Bung is deleting the Confinement series", "link_flair_richtext": [{"a": ":wMETA:", "e": "emoji", "u": "https://emoji.redditmedia.com/z5sbq9lqm2h41_t5_2r4ni/wMETA"}, {"e": "text", "t": " Meta Post"}], "subreddit_name_prefixed": "r/SCP", "hidden": false, "pwls": 7, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_142ye91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": "#373c3f", "ups": 1780, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "48f59682-d3a6-11eb-90ed-0e6924b8cf37", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": ":wMETA: Meta Post", "can_mod_post": false, "score": 1780, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8nKejTl_n0CbeitlAK4Cm3d44Gflyq8VQJbib8s7IWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":wMTF_EPSILON-11:", "e": "emoji", "u": "https://emoji.redditmedia.com/m0d9ls4q8hf71_t5_2r4ni/wMTF_EPSILON-11"}, {"e": "text", "t": " MTF Epsilon-11 (\"Nine-Tailed Fox\")"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686098221.0, "link_flair_type": "richtext", "wls": 7, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1iigyr5grh4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1iigyr5grh4b1.png?auto=webp&amp;v=enabled&amp;s=92fbc47a6d7622a737379478e3a3b6dbd53d6fba", "width": 867, "height": 135}, "resolutions": [{"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02790e786e1e5bd654d0d9e77932b16175d7ee3", "width": 108, "height": 16}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98093a44126560d23facf77035f007ac97bc22b9", "width": 216, "height": 33}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0e5211f4b1d1989191561e769d0dff36c8832ce", "width": 320, "height": 49}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b14ac65b955b622012f8a4621bee9b47edda1ba", "width": 640, "height": 99}], "variants": {}, "id": "o0Vf4I3a5Sbmh7Xw5l7GG0bfzgThMpqC7u2VHsWPIX4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bba31b8e-45bd-11e3-b66a-12313d188143", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":wMTF_EPSILON-11: MTF Epsilon-11 (\"Nine-Tailed Fox\")", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2r4ni", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "142ye91", "is_robot_indexable": true, "report_reasons": null, "author": "mikeBE11", "discussion_type": null, "num_comments": 270, "send_replies": true, "whitelist_status": "some_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/SCP/comments/142ye91/lord_bung_is_deleting_the_confinement_series/", "parent_whitelist_status": "some_ads", "stickied": false, "url": "https://i.redd.it/1iigyr5grh4b1.png", "subreddit_subscribers": 680587, "created_utc": 1686098221.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1686110829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1iigyr5grh4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1iigyr5grh4b1.png?auto=webp&amp;v=enabled&amp;s=92fbc47a6d7622a737379478e3a3b6dbd53d6fba", "width": 867, "height": 135}, "resolutions": [{"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02790e786e1e5bd654d0d9e77932b16175d7ee3", "width": 108, "height": 16}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98093a44126560d23facf77035f007ac97bc22b9", "width": 216, "height": 33}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0e5211f4b1d1989191561e769d0dff36c8832ce", "width": 320, "height": 49}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b14ac65b955b622012f8a4621bee9b47edda1ba", "width": 640, "height": 99}], "variants": {}, "id": "o0Vf4I3a5Sbmh7Xw5l7GG0bfzgThMpqC7u2VHsWPIX4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1432tyo", "is_robot_indexable": true, "report_reasons": null, "author": "Tzar_Jberk", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_142ye91", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1432tyo/lord_bung_is_deleting_the_confinement_series_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1iigyr5grh4b1.png", "subreddit_subscribers": 686435, "created_utc": 1686110829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a self-hosted server to provide an e-hentai (NSFW) like experience for organizing my media. I want to be able to search for media using tags using the search (example below):  \n\n\n    series:Full Metal Alchemist$ language:english$\n\n  \nMain focus is tags. With list view and preview second. Any ideas?", "author_fullname": "t2_7g8d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-hentai like Self-hosted Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142sjs5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a self-hosted server to provide an e-hentai (NSFW) like experience for organizing my media. I want to be able to search for media using tags using the search (example below):  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;series:Full Metal Alchemist$ language:english$\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Main focus is tags. With list view and preview second. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142sjs5", "is_robot_indexable": true, "report_reasons": null, "author": "smokeyFlorence", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142sjs5/ehentai_like_selfhosted_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142sjs5/ehentai_like_selfhosted_server/", "subreddit_subscribers": 686435, "created_utc": 1686083944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks, \n\nI'm the sad owner of a broken Raritan PX-5514 (a 0U vertical PDU) that's gone EOL and EOS a long time ago. Unfortunately, the seller appears to have botched a firmware upgrade and then passed it on to me, because the unit does not progress past the self-test phase.\n\nAccording to Raritan's manuals, there was a PDU recovery tool that could be used to rescue bad firmware flashes. However, Raritan themselves claim to no longer have any copies of the tool on hand and is therefore unable to help me.\n\nWould any of you happen to have a copy on hand, or know of someone who might have one squirreled away somewhere? I'd be extermely grateful for any leads.", "author_fullname": "t2_90lnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a copy of Raritan's PDU Recovery Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143h63i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686153743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m the sad owner of a broken Raritan PX-5514 (a 0U vertical PDU) that&amp;#39;s gone EOL and EOS a long time ago. Unfortunately, the seller appears to have botched a firmware upgrade and then passed it on to me, because the unit does not progress past the self-test phase.&lt;/p&gt;\n\n&lt;p&gt;According to Raritan&amp;#39;s manuals, there was a PDU recovery tool that could be used to rescue bad firmware flashes. However, Raritan themselves claim to no longer have any copies of the tool on hand and is therefore unable to help me.&lt;/p&gt;\n\n&lt;p&gt;Would any of you happen to have a copy on hand, or know of someone who might have one squirreled away somewhere? I&amp;#39;d be extermely grateful for any leads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143h63i", "is_robot_indexable": true, "report_reasons": null, "author": "JargonTheRed", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/143h63i/looking_for_a_copy_of_raritans_pdu_recovery_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143h63i/looking_for_a_copy_of_raritans_pdu_recovery_tool/", "subreddit_subscribers": 686435, "created_utc": 1686153743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I'm looking for is something like gitea migrate of a repo but have it auto update with any new changes. The problem with just normal gitea mirror is it's missing the  release notes and binary attachments. The same is true for  git clone --mirror xxx. When I search around for solutions I cant seem to find anything useful so I thought I'd post here.\n\nCurrently, I let gitea mirror the repo and every time I think about it I manually go though my list using firefox to take a screenshot of the page, download each binary, and stick it all in a folder of named the same as the release. finally I git push that to my selfhosted gitea under xxx_releases. I could automate this myself but I'm hopeful there's a tool to do it for me. \n\nHow do you guys back this up?", "author_fullname": "t2_vwv9flfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to mirror github/gitlab that will include release notes and binaries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143c0cf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686141199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I&amp;#39;m looking for is something like gitea migrate of a repo but have it auto update with any new changes. The problem with just normal gitea mirror is it&amp;#39;s missing the  release notes and binary attachments. The same is true for  git clone --mirror xxx. When I search around for solutions I cant seem to find anything useful so I thought I&amp;#39;d post here.&lt;/p&gt;\n\n&lt;p&gt;Currently, I let gitea mirror the repo and every time I think about it I manually go though my list using firefox to take a screenshot of the page, download each binary, and stick it all in a folder of named the same as the release. finally I git push that to my selfhosted gitea under xxx_releases. I could automate this myself but I&amp;#39;m hopeful there&amp;#39;s a tool to do it for me. &lt;/p&gt;\n\n&lt;p&gt;How do you guys back this up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143c0cf", "is_robot_indexable": true, "report_reasons": null, "author": "ShitParticleInYaNose", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143c0cf/any_tools_to_mirror_githubgitlab_that_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143c0cf/any_tools_to_mirror_githubgitlab_that_will/", "subreddit_subscribers": 686435, "created_utc": 1686141199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm having issues in creating the rules or filters so that all images on a same page will go into a subfolder with the name of the webpage. As of now, when i use the &lt;jd:packagename&gt; addition, it saves every image in it's separate folder. and without it, it saves all images in the main folder without making a subfolder. Can anyone help me with the settings to get this to save correctly, i.e. all images from a single page in a single subfolder. ?", "author_fullname": "t2_1hcoykky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jdownloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14398vo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686132980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having issues in creating the rules or filters so that all images on a same page will go into a subfolder with the name of the webpage. As of now, when i use the &amp;lt;jd:packagename&amp;gt; addition, it saves every image in it&amp;#39;s separate folder. and without it, it saves all images in the main folder without making a subfolder. Can anyone help me with the settings to get this to save correctly, i.e. all images from a single page in a single subfolder. ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14398vo", "is_robot_indexable": true, "report_reasons": null, "author": "Fosterkid87", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14398vo/jdownloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14398vo/jdownloader/", "subreddit_subscribers": 686435, "created_utc": 1686132980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this subreddit might not be the best place to ask, but, I had to give up with configs.\n\nI have two question about my config (won't download files into subreddit folders), and about archive-format (fighting against duplicates and Reddit galleries).\n\n&amp;#x200B;\n\n1) I'm trying to achieve **reddit\\_user\\_username** and **reddit\\_sub\\_subreddit** folders under base-directory. At the moment, user -object works; it correctly stores user media into specific folders.\n\nHowever, configuration for subreddits does not work. I've tried \"subreddit(s)\", \"submission(s)\".\n\nWhat is wrong with this part of code?\n\n    \"reddit\": {\n    \"videos\": \"ytdl\",\n    \"archive-format\": \"{title}\",\n    \"parent-directory\": true,\n    \"user\": {\n        \"directory\": [\n            \"reddit_user_{author}\"\n     \u00a0  ],\n        \"filename\": \"{date}-{subreddit}-{id}-{title}.{extension}\",\n        \"whitelist\": [\n            \"imgur\",\n            \"redgifs\",\n            \"gfycat\",\n            \"imgur\"\n    \u00a0 \u00a0 ],\n        \"parent-directory\": true\n    },\n    \"subreddit\": {\n        \"directory\": [\n            \"reddit_sub_{subreddit}\"\n\u00a0 \u00a0 \u00a0 \u00a0 ],\n        \"filename\": \"{date}-{author}-{id}-{title}.{extension}\"\n\u00a0 \u00a0 },\n    \"parent-metadata\": \"_reddit_\"\n},\n\n*Extra question:* How I can find names for these objects, like \"user\"? Couldn't find with \\`./gallery-dl -K &lt;url-address&gt;\\`.\n\n&amp;#x200B;\n\n2) At the moment I'm fetching user's images, which are mostly duplicates because posted in multiple subreddits. As \\`archive-format\\`, I used \\`{title}\\`, but this causes issues with posts with multiple images. If the post has multiple images (gallery), it only downloads the first image and skips the rest.\n\nMaybe there is a way to set specific configs only for reddit gallery images, but I'm not sure how it works. Gallery-dl's [documentation](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst) did not help me, and I also tried to find information from the [source code](https://github.com/mikf/gallery-dl/blob/master/gallery_dl/extractor/reddit.py).\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_3sy9yn65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl (custom folder configs, reddit \"galleries\" and archive-format)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142oxf2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686075963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this subreddit might not be the best place to ask, but, I had to give up with configs.&lt;/p&gt;\n\n&lt;p&gt;I have two question about my config (won&amp;#39;t download files into subreddit folders), and about archive-format (fighting against duplicates and Reddit galleries).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1) I&amp;#39;m trying to achieve &lt;strong&gt;reddit_user_username&lt;/strong&gt; and &lt;strong&gt;reddit_sub_subreddit&lt;/strong&gt; folders under base-directory. At the moment, user -object works; it correctly stores user media into specific folders.&lt;/p&gt;\n\n&lt;p&gt;However, configuration for subreddits does not work. I&amp;#39;ve tried &amp;quot;subreddit(s)&amp;quot;, &amp;quot;submission(s)&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What is wrong with this part of code?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;quot;reddit&amp;quot;: {\n&amp;quot;videos&amp;quot;: &amp;quot;ytdl&amp;quot;,\n&amp;quot;archive-format&amp;quot;: &amp;quot;{title}&amp;quot;,\n&amp;quot;parent-directory&amp;quot;: true,\n&amp;quot;user&amp;quot;: {\n    &amp;quot;directory&amp;quot;: [\n        &amp;quot;reddit_user_{author}&amp;quot;\n \u00a0  ],\n    &amp;quot;filename&amp;quot;: &amp;quot;{date}-{subreddit}-{id}-{title}.{extension}&amp;quot;,\n    &amp;quot;whitelist&amp;quot;: [\n        &amp;quot;imgur&amp;quot;,\n        &amp;quot;redgifs&amp;quot;,\n        &amp;quot;gfycat&amp;quot;,\n        &amp;quot;imgur&amp;quot;\n\u00a0 \u00a0 ],\n    &amp;quot;parent-directory&amp;quot;: true\n},\n&amp;quot;subreddit&amp;quot;: {\n    &amp;quot;directory&amp;quot;: [\n        &amp;quot;reddit_sub_{subreddit}&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;\u00a0 \u00a0 \u00a0 \u00a0 ],\n        &amp;quot;filename&amp;quot;: &amp;quot;{date}-{author}-{id}-{title}.{extension}&amp;quot;\n\u00a0 \u00a0 },\n    &amp;quot;parent-metadata&amp;quot;: &amp;quot;&lt;em&gt;reddit&lt;/em&gt;&amp;quot;\n},&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Extra question:&lt;/em&gt; How I can find names for these objects, like &amp;quot;user&amp;quot;? Couldn&amp;#39;t find with `./gallery-dl -K &amp;lt;url-address&amp;gt;`.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2) At the moment I&amp;#39;m fetching user&amp;#39;s images, which are mostly duplicates because posted in multiple subreddits. As `archive-format`, I used `{title}`, but this causes issues with posts with multiple images. If the post has multiple images (gallery), it only downloads the first image and skips the rest.&lt;/p&gt;\n\n&lt;p&gt;Maybe there is a way to set specific configs only for reddit gallery images, but I&amp;#39;m not sure how it works. Gallery-dl&amp;#39;s &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst\"&gt;documentation&lt;/a&gt; did not help me, and I also tried to find information from the &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/gallery_dl/extractor/reddit.py\"&gt;source code&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?auto=webp&amp;v=enabled&amp;s=a0a36aa0277859b6647ca9ead294685ed8eb9ba6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b11803dc23b1a0dac19980e6631cae06e637474", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b6565624f5360f043c4b0a4317ff63c99533061", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fead4655807f833f87d3f932ff47724acc6be920", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99b67ae321d92fd707e7238acc502c80b2c9a143", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b42ab615c2af6f4e56e14167f2c31f7ae7548c2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3423b17f3ce5637d6d89e62a3d5302f32d991024", "width": 1080, "height": 540}], "variants": {}, "id": "2ujLMGJDXtZQwUdJOcM6MdxWqZJO1wuswfwQ1l5pTaU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142oxf2", "is_robot_indexable": true, "report_reasons": null, "author": "qusmar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142oxf2/gallerydl_custom_folder_configs_reddit_galleries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142oxf2/gallerydl_custom_folder_configs_reddit_galleries/", "subreddit_subscribers": 686435, "created_utc": 1686075963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am reconsidering options for my current media server backup strategy. I am running Plex on Windows using DrivePool with 2x media folder duplication as my first backup step. Secondly, I have a set of off-site hard drives containing my media files and macrium drive images.\n\nThirdly, I have an external USB hard drive (14tb wd easystore) that I synchronize all of my data to at regular intervals. This drive is only powered up when a backup is run. My current issue is that my external hdd is running out of space. My plan has been to replace the drive with a larger capacity drive, shuck the currrent drive and add it to my DrivePool storage pool.\n\nOverall, I have been comfortable with my current arrangement. However, replacing the external drive with another of much greater capacity is an expensive option. Also, as I am adding media all the time, it would be a matter of time before the new external drive would be maxed out and the process would need to be repeated. Obviously, the benefit of this drive being external is that if disaster strikes my home or my media server, my data is preserved on the external drive which I could access very easily.\n\nIs there a better option to replace the external hdd in my backup strategy that is worth considering? Affordability is very important due to a restricted budget. I appreciate any comments you might offer.", "author_fullname": "t2_56nvy1n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Media Server Backup Strategy Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ov62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686076439.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686075846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reconsidering options for my current media server backup strategy. I am running Plex on Windows using DrivePool with 2x media folder duplication as my first backup step. Secondly, I have a set of off-site hard drives containing my media files and macrium drive images.&lt;/p&gt;\n\n&lt;p&gt;Thirdly, I have an external USB hard drive (14tb wd easystore) that I synchronize all of my data to at regular intervals. This drive is only powered up when a backup is run. My current issue is that my external hdd is running out of space. My plan has been to replace the drive with a larger capacity drive, shuck the currrent drive and add it to my DrivePool storage pool.&lt;/p&gt;\n\n&lt;p&gt;Overall, I have been comfortable with my current arrangement. However, replacing the external drive with another of much greater capacity is an expensive option. Also, as I am adding media all the time, it would be a matter of time before the new external drive would be maxed out and the process would need to be repeated. Obviously, the benefit of this drive being external is that if disaster strikes my home or my media server, my data is preserved on the external drive which I could access very easily.&lt;/p&gt;\n\n&lt;p&gt;Is there a better option to replace the external hdd in my backup strategy that is worth considering? Affordability is very important due to a restricted budget. I appreciate any comments you might offer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "142ov62", "is_robot_indexable": true, "report_reasons": null, "author": "bctf1", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ov62/media_server_backup_strategy_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ov62/media_server_backup_strategy_advice/", "subreddit_subscribers": 686435, "created_utc": 1686075846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_mvvjw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good solution to replace the clunky power bricks with possibly a USB C for my D2 hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_143fv9u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TtMM_bLhWvip2byhyPIuPtlg32sX0_GM2-rnbqASUUE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686150724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/a/saxwuSr/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?auto=webp&amp;v=enabled&amp;s=b3f038c435f6743c47ab7fea8e9284b074b5ecaa", "width": 2000, "height": 1500}, "resolutions": [{"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ffff85e53598b6ac89ea3dbfde22774340bc095", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488b1ff8856521c9f6e3a66cde9ce1af41ecd78a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9804ba8c79339299734afdec1744bd89447a7c68", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29f69a8e08795da4104832afd5c929d6ae5a8001", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d1b538689c41ac9f0899a4b07d0297ddd68b312", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc2971c0555d72104b82bed4421ecfe38746b8a5", "width": 1080, "height": 810}], "variants": {}, "id": "8r-o0Ir09e8_BtLns3BacUy30j8Ubz-LqRogi1evhms"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143fv9u", "is_robot_indexable": true, "report_reasons": null, "author": "drawcody", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143fv9u/does_anyone_have_a_good_solution_to_replace_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/a/saxwuSr/", "subreddit_subscribers": 686435, "created_utc": 1686150724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of data I'm going to back up on a hard drive and store off-site in a deposit box. It's a variety of data: Binary, video, pictures, text, it's all over the place.\n\nI know NTFS supports compression natively, but is it the best way to shove a bunch of data onto a disk? Would it be better to do something like 7-Zip at some extreme compression level? Would it be better to do both NTFS compression AND 7-Zip? Is there a better filesystem and/or compression software I should be using?", "author_fullname": "t2_92oly", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to best configure an HDD for cold storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143b901", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686139143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of data I&amp;#39;m going to back up on a hard drive and store off-site in a deposit box. It&amp;#39;s a variety of data: Binary, video, pictures, text, it&amp;#39;s all over the place.&lt;/p&gt;\n\n&lt;p&gt;I know NTFS supports compression natively, but is it the best way to shove a bunch of data onto a disk? Would it be better to do something like 7-Zip at some extreme compression level? Would it be better to do both NTFS compression AND 7-Zip? Is there a better filesystem and/or compression software I should be using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143b901", "is_robot_indexable": true, "report_reasons": null, "author": "dwkindig", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143b901/how_to_best_configure_an_hdd_for_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143b901/how_to_best_configure_an_hdd_for_cold_storage/", "subreddit_subscribers": 686435, "created_utc": 1686139143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi data hoarders, since I find this community really useful and inspiring, I want to give you something back. Hope that this will help someone\n\nI know I'm coming late, but I am writing a tool that does not use APIs. **I  am not encouraging using it, as I am not aware of the consequences that  twitter could apply to you and/or your account in case you run the code**, or even if it is legally allowed to run such automation.  \n**Read carefully the readme** as there are some settings (such as the theme of the browser) **required** to work.  \nI  am just writing some code to make people aware about how this world  works, and it may not be perfect. If you guys want to bring some  contribution, or leave a star if you find it useful, feel free to have a  look on github (**apologize in advance if it is not perfect, e.g., for now it works only with dark theme of chromium ... I wrote it in some spare hours I had**). For sure, out there you may find something better, but anyway, here it is the link:\n\n[https://github.com/alessandriniluca/postget](https://github.com/alessandriniluca/postget)\n\nAgain,  sorry for the code. The code, and also the tool, are not perfect, let me  stress that you are free to both contribute with pull requests and/or open issues in order to improve it!\n\nHoping this will help someone!", "author_fullname": "t2_3hwuhm84", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collecting data from Twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14369ll", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scraping Code", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686122194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi data hoarders, since I find this community really useful and inspiring, I want to give you something back. Hope that this will help someone&lt;/p&gt;\n\n&lt;p&gt;I know I&amp;#39;m coming late, but I am writing a tool that does not use APIs. &lt;strong&gt;I  am not encouraging using it, as I am not aware of the consequences that  twitter could apply to you and/or your account in case you run the code&lt;/strong&gt;, or even if it is legally allowed to run such automation.&lt;br/&gt;\n&lt;strong&gt;Read carefully the readme&lt;/strong&gt; as there are some settings (such as the theme of the browser) &lt;strong&gt;required&lt;/strong&gt; to work.&lt;br/&gt;\nI  am just writing some code to make people aware about how this world  works, and it may not be perfect. If you guys want to bring some  contribution, or leave a star if you find it useful, feel free to have a  look on github (&lt;strong&gt;apologize in advance if it is not perfect, e.g., for now it works only with dark theme of chromium ... I wrote it in some spare hours I had&lt;/strong&gt;). For sure, out there you may find something better, but anyway, here it is the link:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/alessandriniluca/postget\"&gt;https://github.com/alessandriniluca/postget&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Again,  sorry for the code. The code, and also the tool, are not perfect, let me  stress that you are free to both contribute with pull requests and/or open issues in order to improve it!&lt;/p&gt;\n\n&lt;p&gt;Hoping this will help someone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?auto=webp&amp;v=enabled&amp;s=13f88f88fa74c033d43d7876cf86dedf085cd268", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f541b6d79a46636f137776486441f3b6e1dde18", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c79b8b1a1e528f0f7e8fa3f1a383e9c1f206d416", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e526356931ca87d132ca5e5cfb63899114b89dcc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a4122b3900beb6b583079efef692d24afdc2a88", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46263c4e43578d3beeb523407a2c01c2d24cddc1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa829bad51da5223f5e285097b9147dbb646c14c", "width": 1080, "height": 540}], "variants": {}, "id": "WURKifmQCGyNKgafaeMq_8z0ipxqnYA3Rs304W5ax84"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14369ll", "is_robot_indexable": true, "report_reasons": null, "author": "Landomix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14369ll/collecting_data_from_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14369ll/collecting_data_from_twitter/", "subreddit_subscribers": 686435, "created_utc": 1686122194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to extract an ipa file from an app on a friend's jailbroken iPhone to archive the app (it's no longer on the app store). I plan to share the ipa with an internet community I know will appreciate it. Is there any risk to my friend in doing this? Will his Apple ID be linked to the ipa file in any way, and if so could his account somehow be compromised? TIA", "author_fullname": "t2_6dsylxrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any risk to the original app owner in extracting a decrypted ipa?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1434hzc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686116120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to extract an ipa file from an app on a friend&amp;#39;s jailbroken iPhone to archive the app (it&amp;#39;s no longer on the app store). I plan to share the ipa with an internet community I know will appreciate it. Is there any risk to my friend in doing this? Will his Apple ID be linked to the ipa file in any way, and if so could his account somehow be compromised? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1434hzc", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished-Arm4538", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1434hzc/is_there_any_risk_to_the_original_app_owner_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1434hzc/is_there_any_risk_to_the_original_app_owner_in/", "subreddit_subscribers": 686435, "created_utc": 1686116120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Out of these choices, which would you choose? Is Syncovery deliver value over and above the free alternatives I listed? \n\nI am looking for a fully local backup solution which I can set up and then forget until something happens. Would like it to encrypt the backups and increments inside of a .rar or .7z archive with full checksumming. Since it's not a backup strategy if you can't easily restore, I want to be able to access the files without needing to use the local backup utility. \n\nPlease do not recommend any others if it's not compatible with Windows, MacOS and Linux. \n\nI am thinking of using SyncThing to transfer the backups files around so I don't have another thing running in the background on my main PC. The most recent backups would be stored locally on each PC in a dedicated partition of equal size to the main one and my main PC would backup all the archive files online to Backblaze.", "author_fullname": "t2_8hjaccg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Before I deploy to several computers: UrBackup, Bacula, Duplicati or Syncovery (paid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142whfe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686093121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Out of these choices, which would you choose? Is Syncovery deliver value over and above the free alternatives I listed? &lt;/p&gt;\n\n&lt;p&gt;I am looking for a fully local backup solution which I can set up and then forget until something happens. Would like it to encrypt the backups and increments inside of a .rar or .7z archive with full checksumming. Since it&amp;#39;s not a backup strategy if you can&amp;#39;t easily restore, I want to be able to access the files without needing to use the local backup utility. &lt;/p&gt;\n\n&lt;p&gt;Please do not recommend any others if it&amp;#39;s not compatible with Windows, MacOS and Linux. &lt;/p&gt;\n\n&lt;p&gt;I am thinking of using SyncThing to transfer the backups files around so I don&amp;#39;t have another thing running in the background on my main PC. The most recent backups would be stored locally on each PC in a dedicated partition of equal size to the main one and my main PC would backup all the archive files online to Backblaze.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142whfe", "is_robot_indexable": true, "report_reasons": null, "author": "LieVirus", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142whfe/before_i_deploy_to_several_computers_urbackup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142whfe/before_i_deploy_to_several_computers_urbackup/", "subreddit_subscribers": 686435, "created_utc": 1686093121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought 4xWD HC560 20TB drives from a well-known european e-shop.\n\nI checked the warranty of the drives on the official WD website with the serial number and I found out they are OEM drives, not retail ones (PDQ Drive ASM OEM-STD) and therefore \"No limited warranty\" is applied (Product was originally sold to a system manufacturer. Please contact the system manufacturer or the place of purchase for warranty service).\n\nObviously the e-shop is not a system manufacturer and it may have done some sort of dropshipping with my money to a distributor that sells OEM drives to manufacturer brands (Dell, Asus and such) as the benefits are higher this way.\n\nI guess I am out of the 5 year limited warranty from WD as an end-user, but at least I should be getting the 2 year warranty from electronic goods bought in the EU.\n\nIs this normal? Getting OEM drives (2 year warranty) as an end-user buying from an e-shop which should be sending retail ones (5 year warranty)?\n\nThanks in advance.", "author_fullname": "t2_14jgu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought 4x20TB drives from e-shop, got sent OEM ones (2y warranty instead of 5y)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142w60a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686092352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought 4xWD HC560 20TB drives from a well-known european e-shop.&lt;/p&gt;\n\n&lt;p&gt;I checked the warranty of the drives on the official WD website with the serial number and I found out they are OEM drives, not retail ones (PDQ Drive ASM OEM-STD) and therefore &amp;quot;No limited warranty&amp;quot; is applied (Product was originally sold to a system manufacturer. Please contact the system manufacturer or the place of purchase for warranty service).&lt;/p&gt;\n\n&lt;p&gt;Obviously the e-shop is not a system manufacturer and it may have done some sort of dropshipping with my money to a distributor that sells OEM drives to manufacturer brands (Dell, Asus and such) as the benefits are higher this way.&lt;/p&gt;\n\n&lt;p&gt;I guess I am out of the 5 year limited warranty from WD as an end-user, but at least I should be getting the 2 year warranty from electronic goods bought in the EU.&lt;/p&gt;\n\n&lt;p&gt;Is this normal? Getting OEM drives (2 year warranty) as an end-user buying from an e-shop which should be sending retail ones (5 year warranty)?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142w60a", "is_robot_indexable": true, "report_reasons": null, "author": "jfromeo", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142w60a/bought_4x20tb_drives_from_eshop_got_sent_oem_ones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142w60a/bought_4x20tb_drives_from_eshop_got_sent_oem_ones/", "subreddit_subscribers": 686435, "created_utc": 1686092352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been \"half assing\" for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.\n\nZero issues so far, and I don't want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn't have to be on all the time, backups, etc.\n\nBest way to accomplish the following?\n\n1. Expandable Storage.\n2. Backups.\n3. No need for PC to run.\n4. Shield able to connect via Plex.\n5. Can connect to the device on my PC to manage media.\n6. Able to stream 100mbs movies without any issues.\n\nCurrent plan:\n\n* [Synology DS1522+](https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;psc=1)\n* x2 - [Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache](https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;psc=1)\n\nIs that a good way to accomplish it?", "author_fullname": "t2_ia3wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning on my first NAS set up. Thoughts on the parts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142v8fm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686090096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been &amp;quot;half assing&amp;quot; for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.&lt;/p&gt;\n\n&lt;p&gt;Zero issues so far, and I don&amp;#39;t want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn&amp;#39;t have to be on all the time, backups, etc.&lt;/p&gt;\n\n&lt;p&gt;Best way to accomplish the following?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Expandable Storage.&lt;/li&gt;\n&lt;li&gt;Backups.&lt;/li&gt;\n&lt;li&gt;No need for PC to run.&lt;/li&gt;\n&lt;li&gt;Shield able to connect via Plex.&lt;/li&gt;\n&lt;li&gt;Can connect to the device on my PC to manage media.&lt;/li&gt;\n&lt;li&gt;Able to stream 100mbs movies without any issues.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Current plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Synology DS1522+&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;x2 - &lt;a href=\"https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is that a good way to accomplish it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142v8fm", "is_robot_indexable": true, "report_reasons": null, "author": "nsoifer", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "subreddit_subscribers": 686435, "created_utc": 1686090096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We are looking to replace our Xerox scanner with a smart scanner for book scanning in our Interlibrary Loan department. We have been considering the CZUR Aura scanner as a potential alternative. However, we have seen some problems: \"We have tried various settings, including using overhead and side lights, but are unable to scan two pages clearly without blinding.\"\n\nWe are now seeking advice from anyone who has experience using both the CZUR Aura and CZUR ET scanners. Would the more expensive ET models be more effective in addressing our scanning needs? We would like to invest in a smart scanner that can scan books with high picture content, while reducing our paper waste.\n\nAny feedback or recommendations on the best scanner for our needs would be greatly appreciated. Thank you for your time.", "author_fullname": "t2_c0xq3j9y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Smart Book Scanners for Interlibrary Loan Documents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142u8y7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686087821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking to replace our Xerox scanner with a smart scanner for book scanning in our Interlibrary Loan department. We have been considering the CZUR Aura scanner as a potential alternative. However, we have seen some problems: &amp;quot;We have tried various settings, including using overhead and side lights, but are unable to scan two pages clearly without blinding.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;We are now seeking advice from anyone who has experience using both the CZUR Aura and CZUR ET scanners. Would the more expensive ET models be more effective in addressing our scanning needs? We would like to invest in a smart scanner that can scan books with high picture content, while reducing our paper waste.&lt;/p&gt;\n\n&lt;p&gt;Any feedback or recommendations on the best scanner for our needs would be greatly appreciated. Thank you for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142u8y7", "is_robot_indexable": true, "report_reasons": null, "author": "durkefigne", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142u8y7/seeking_advice_on_smart_book_scanners_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142u8y7/seeking_advice_on_smart_book_scanners_for/", "subreddit_subscribers": 686435, "created_utc": 1686087821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a large library of stuff gathered throughout the years. I have been paid to gather some elements for a company and Im going to be sending a lot of old stuff, as if I had just gathered it now. They are paying me to gather these, and I don\u00b4t think it would sit well if they see 80% of it is recycled.\n\n&amp;#x200B;\n\nI don\u00b4t want it to keep the \"date created\" and for it to show it\u00b4s actually from a year ago.\n\n&amp;#x200B;\n\nIf I paste these files in a new folder, will it appear that they were created on the day of the copy? If not, how about if I WeTransfer the files to myself and re-download them?\n\n&amp;#x200B;\n\nPlease help me out ! I know they won\u00b4t go using fancy softwares to detect the original date, but I don\u00b4t want it to appear on the files", "author_fullname": "t2_90me57qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does sending a WeTransfer remove date of creation of files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_143k206", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686160495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large library of stuff gathered throughout the years. I have been paid to gather some elements for a company and Im going to be sending a lot of old stuff, as if I had just gathered it now. They are paying me to gather these, and I don\u00b4t think it would sit well if they see 80% of it is recycled.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don\u00b4t want it to keep the &amp;quot;date created&amp;quot; and for it to show it\u00b4s actually from a year ago.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I paste these files in a new folder, will it appear that they were created on the day of the copy? If not, how about if I WeTransfer the files to myself and re-download them?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Please help me out ! I know they won\u00b4t go using fancy softwares to detect the original date, but I don\u00b4t want it to appear on the files&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143k206", "is_robot_indexable": true, "report_reasons": null, "author": "Powerful-Employer-20", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143k206/does_sending_a_wetransfer_remove_date_of_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143k206/does_sending_a_wetransfer_remove_date_of_creation/", "subreddit_subscribers": 686435, "created_utc": 1686160495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://github.com/hamsterbase/hamsterbase-highlighter](https://github.com/hamsterbase/hamsterbase-highlighter)  \n\n\nPresenting an open-source Chrome extension that allows you to annotate and take notes directly on web pages. The best part? Your data remains intact even after refreshing the page.\n\nThe backend is highly flexible and can be easily switched, with current support for Hamsterbase and Notion. Future updates will include support for GitHub as well.\n\nWhen you choose to save your annotations to Hamsterbase, a convenient feature automatically captures a snapshot of the web page for your reference.\n\n**Notes:**\n\n1. The extension is entirely open-source and does not rely on Hamsterbase. It will remain free and open-source for the foreseeable future.\n2. Hamsterbase is a closed-source software developed by me. It is currently available as a Docker image and a desktop app.", "author_fullname": "t2_m9f6hryu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a opensource web highlighter for notion and hamsterbase.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143hlez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686154707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/hamsterbase/hamsterbase-highlighter\"&gt;https://github.com/hamsterbase/hamsterbase-highlighter&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Presenting an open-source Chrome extension that allows you to annotate and take notes directly on web pages. The best part? Your data remains intact even after refreshing the page.&lt;/p&gt;\n\n&lt;p&gt;The backend is highly flexible and can be easily switched, with current support for Hamsterbase and Notion. Future updates will include support for GitHub as well.&lt;/p&gt;\n\n&lt;p&gt;When you choose to save your annotations to Hamsterbase, a convenient feature automatically captures a snapshot of the web page for your reference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The extension is entirely open-source and does not rely on Hamsterbase. It will remain free and open-source for the foreseeable future.&lt;/li&gt;\n&lt;li&gt;Hamsterbase is a closed-source software developed by me. It is currently available as a Docker image and a desktop app.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?auto=webp&amp;v=enabled&amp;s=2640dc46f1ae50315815ae358f9ce50e7a89341f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b526d74290187df8224354b864444bef7ddf58e5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=264bb12f61e570fb292bb8981eff1b1edf51804a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=075bb02613db0998468224d3f12b3e6a2a6915ec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d8b7abc75279a9ec1ca9e3bdd4b668a39cb0983", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a3b0dc6ca70ee29f7ecd5d9e827ffd34c1fced9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c5cf182f1a46c5f5f249d293a418af06791fffa", "width": 1080, "height": 540}], "variants": {}, "id": "DoA6h_gRJCQfhJ1glv3gsPpLDuFzXlaqck0HZ2RAt6c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143hlez", "is_robot_indexable": true, "report_reasons": null, "author": "HamsterBaseMaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143hlez/i_made_a_opensource_web_highlighter_for_notion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143hlez/i_made_a_opensource_web_highlighter_for_notion/", "subreddit_subscribers": 686435, "created_utc": 1686154707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made a Mobage account yesterday so I could try to preserve Mobage games. Posting because I need some help working out how to go about preserving these games.\n\nThe games are all hosted in different places, but one common thing would be all use server-side code and all of the images don't need cookies or a mobile user agent to mirror\n\nMost of the games I've tried need mobage login cookies and all need an Android user agent\n\nI'm just kind of overwhelmed on where to start here", "author_fullname": "t2_d9gcu53d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "archiving Mobage games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143dxvc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686146077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a Mobage account yesterday so I could try to preserve Mobage games. Posting because I need some help working out how to go about preserving these games.&lt;/p&gt;\n\n&lt;p&gt;The games are all hosted in different places, but one common thing would be all use server-side code and all of the images don&amp;#39;t need cookies or a mobile user agent to mirror&lt;/p&gt;\n\n&lt;p&gt;Most of the games I&amp;#39;ve tried need mobage login cookies and all need an Android user agent&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just kind of overwhelmed on where to start here&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143dxvc", "is_robot_indexable": true, "report_reasons": null, "author": "gosc_reddit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143dxvc/archiving_mobage_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143dxvc/archiving_mobage_games/", "subreddit_subscribers": 686435, "created_utc": 1686146077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There\u2019s a really good amazon reviewer of history books that I want to save a copy of all of his reviews, especially because Amazon tends to delete reviews now and then by different reviewers (for some reason I don\u2019t know). And this guy has a lot of reviews that I have to scroll down, click, and copy and paste. Would take too much time and heat up my computer cpu.", "author_fullname": "t2_y7ufo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all reviews by a specific Amazon reviewer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1435gs7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686119399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There\u2019s a really good amazon reviewer of history books that I want to save a copy of all of his reviews, especially because Amazon tends to delete reviews now and then by different reviewers (for some reason I don\u2019t know). And this guy has a lot of reviews that I have to scroll down, click, and copy and paste. Would take too much time and heat up my computer cpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1435gs7", "is_robot_indexable": true, "report_reasons": null, "author": "Saphsin", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1435gs7/how_to_download_all_reviews_by_a_specific_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1435gs7/how_to_download_all_reviews_by_a_specific_amazon/", "subreddit_subscribers": 686435, "created_utc": 1686119399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many Redditors reported their failure cases on SanDisk Extreme Portable SSDs, which caused a lot of problems, including data loss. The related discussion can be found below: \n\n[https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer\\_beware\\_some\\_sandisk\\_extreme\\_ssds\\_are\\_wiping/](https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/)\n\n[https://www.reddit.com/r/editors/comments/10syawa/a\\_warning\\_about\\_sandisk\\_extreme\\_pro\\_ssds/](https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/)\u00a0\n\nWestern Digital has released the claimed firmware for Windows, but not yet for Mac\u00a0  \n**Before the firmware update, it's strongly suggested to back up the SSD first.**\u00a0\n\nHere are the official update on this issue-\n\n[https://support-en.wd.com/app/firmwareupdate](https://support-en.wd.com/app/firmwareupdate) \n\n[https://www.youtube.com/watch?v=tvxHuTUS9is](https://www.youtube.com/watch?v=tvxHuTUS9is)\n\n  \n**Based on this event, EaseUS is working to offer data recovery help (offering data recovery software or online services for free)**, aiming to assist\u00a0SanDisk Extreme Portable SSD users to find their important data back while taking the chance to know better about the data loss caused by SSD crush.\u00a0  \nDetail info can be found on this page -\u00a0[https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html](https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html). It's not complicated. Just send an email to [redepmtion@easeus.com](mailto:redepmtion@easeus.com) with a basic description of the case you experienced. \n\n  \nWith the release of the firmware, there are many more Mac users who came to us for data recovery help, which seems that the firmware works well for Windows. Just back up the data before the update. And with this offer, EaseUS is also offering EaseUS Todo Backup Home for free. Check the page if you need it.\u00a0", "author_fullname": "t2_a0iii5xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Info Sync - Data Recovery Help for SanDisk Extreme SSD Failure is Offered by EaseUS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1431wj8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686108067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many Redditors reported their failure cases on SanDisk Extreme Portable SSDs, which caused a lot of problems, including data loss. The related discussion can be found below: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/\"&gt;https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/&lt;/a&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;Western Digital has released the claimed firmware for Windows, but not yet for Mac\u00a0&lt;br/&gt;\n&lt;strong&gt;Before the firmware update, it&amp;#39;s strongly suggested to back up the SSD first.&lt;/strong&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;Here are the official update on this issue-&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://support-en.wd.com/app/firmwareupdate\"&gt;https://support-en.wd.com/app/firmwareupdate&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=tvxHuTUS9is\"&gt;https://www.youtube.com/watch?v=tvxHuTUS9is&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Based on this event, EaseUS is working to offer data recovery help (offering data recovery software or online services for free)&lt;/strong&gt;, aiming to assist\u00a0SanDisk Extreme Portable SSD users to find their important data back while taking the chance to know better about the data loss caused by SSD crush.\u00a0&lt;br/&gt;\nDetail info can be found on this page -\u00a0&lt;a href=\"https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html\"&gt;https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html&lt;/a&gt;. It&amp;#39;s not complicated. Just send an email to [&lt;a href=\"mailto:redepmtion@easeus.com\"&gt;redepmtion@easeus.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:redepmtion@easeus.com\"&gt;redepmtion@easeus.com&lt;/a&gt;) with a basic description of the case you experienced. &lt;/p&gt;\n\n&lt;p&gt;With the release of the firmware, there are many more Mac users who came to us for data recovery help, which seems that the firmware works well for Windows. Just back up the data before the update. And with this offer, EaseUS is also offering EaseUS Todo Backup Home for free. Check the page if you need it.\u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1431wj8", "is_robot_indexable": true, "report_reasons": null, "author": "Adaaaaaaaaaaaaaaaaa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1431wj8/info_sync_data_recovery_help_for_sandisk_extreme/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1431wj8/info_sync_data_recovery_help_for_sandisk_extreme/", "subreddit_subscribers": 686435, "created_utc": 1686108067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently my little pile of data is hosted on my network in an ancient Synology 413J along with 4x 6TB drives. Capacity is fine, but it\u2019s unable to serve data at speeds over 30mbps despite gigabit ports, gigabit switch, router, etc. \n\nI\u2019ve enjoyed synology. But end of the day what I want to lowest cost highest performance along with the most flexibility. \n\nTo that end, I\u2019m considering an 8th Gen HP Microserver to install either SnapRAID (which appears to meet my needs) or TrueNAS. \n\nMy question is, have any of you played with the microserver? Will the celeron model be sufficient for serving data at higher speeds than what I\u2019m seeing currently?\n\nI have two proxmox servers sharing ISO directory and another directory for VM backups, as well as Plex who\u2019s library is on the NAS, usually serving 1-2 streams. Plus various other backups, etc, but those are more copy to storage and forget about it. \n\nWhat are your thought? Or other ideas in the $300-400 range with at least 4 3.5 inch bays and as quiet as possible", "author_fullname": "t2_5abuxk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology 413j vs HP Microservwr 8thGen", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14311e0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686105607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently my little pile of data is hosted on my network in an ancient Synology 413J along with 4x 6TB drives. Capacity is fine, but it\u2019s unable to serve data at speeds over 30mbps despite gigabit ports, gigabit switch, router, etc. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve enjoyed synology. But end of the day what I want to lowest cost highest performance along with the most flexibility. &lt;/p&gt;\n\n&lt;p&gt;To that end, I\u2019m considering an 8th Gen HP Microserver to install either SnapRAID (which appears to meet my needs) or TrueNAS. &lt;/p&gt;\n\n&lt;p&gt;My question is, have any of you played with the microserver? Will the celeron model be sufficient for serving data at higher speeds than what I\u2019m seeing currently?&lt;/p&gt;\n\n&lt;p&gt;I have two proxmox servers sharing ISO directory and another directory for VM backups, as well as Plex who\u2019s library is on the NAS, usually serving 1-2 streams. Plus various other backups, etc, but those are more copy to storage and forget about it. &lt;/p&gt;\n\n&lt;p&gt;What are your thought? Or other ideas in the $300-400 range with at least 4 3.5 inch bays and as quiet as possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14311e0", "is_robot_indexable": true, "report_reasons": null, "author": "AdhessiveBaker", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14311e0/synology_413j_vs_hp_microservwr_8thgen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14311e0/synology_413j_vs_hp_microservwr_8thgen/", "subreddit_subscribers": 686435, "created_utc": 1686105607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Salutations, Data Hoarders.\n\nI\u2019ve been lurking here for a while and my current storage solution is on the lacking side. As I was looking into preserving media I like I ran into the reality that in my quest to secure all of the stuff I remembered And anything new I was interested in that three external 4 terabyte HDD drives shrunk down to about 800 gigs sooner than I thought.\n\nI thought it would last me about four years before I ran into any serious limitations of remaining space, so because of underestimating the size of video files, here I am, asking for help on finding an ideal DAS and drives to fill it with.\n\n**With all of that said, this would be the planned use case:**\n\n&amp;#x200B;\n\n* I\u2019m not going to run the DAS 24/7/365. I would only power it up when I need to either add to or retrieve something from it.  \n \n* It needs to be fairly portable because physical space is a concern.  \n \n* Not looking into RAID.  \n \n* I would like the option to only turn on one of the drives in the at a time when I \tneed to retrieve something (no sense in spinning up all the drives for pulling something off one, especially since they\u2019d all have the exact same things on it since all the drives are a backup).  \n \n* I\u2019m not looking to do wholesale system backups, just to preservation of individual files that matter to me.  \n \n* The DAS would house no less than three but no more than four.  \n \n* As this would be pretty expensive, I would like to get HDDs robust enough to last me ideally 10 years. Would going for enterprise drives help with this noticeably or am I just making a fools wish? You know, buy it nice or buy it twice.  \n \n* As I want to keep this for as long as possible, the drives would have storage of no less than 12 TB.  \n \n\nAs this would be my first DAS, it would be grateful if any of you can impart any advice or wisdom you wish you had when setting one of these up yourselves as well as any pitfalls that I should be aware of.\n\nThank you in advance.", "author_fullname": "t2_16a6im", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking to Setup My First DAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1430ceo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Seeking Setup Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686103676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Salutations, Data Hoarders.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been lurking here for a while and my current storage solution is on the lacking side. As I was looking into preserving media I like I ran into the reality that in my quest to secure all of the stuff I remembered And anything new I was interested in that three external 4 terabyte HDD drives shrunk down to about 800 gigs sooner than I thought.&lt;/p&gt;\n\n&lt;p&gt;I thought it would last me about four years before I ran into any serious limitations of remaining space, so because of underestimating the size of video files, here I am, asking for help on finding an ideal DAS and drives to fill it with.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;With all of that said, this would be the planned use case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I\u2019m not going to run the DAS 24/7/365. I would only power it up when I need to either add to or retrieve something from it.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It needs to be fairly portable because physical space is a concern.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Not looking into RAID.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would like the option to only turn on one of the drives in the at a time when I     need to retrieve something (no sense in spinning up all the drives for pulling something off one, especially since they\u2019d all have the exact same things on it since all the drives are a backup).  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I\u2019m not looking to do wholesale system backups, just to preservation of individual files that matter to me.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The DAS would house no less than three but no more than four.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;As this would be pretty expensive, I would like to get HDDs robust enough to last me ideally 10 years. Would going for enterprise drives help with this noticeably or am I just making a fools wish? You know, buy it nice or buy it twice.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;As I want to keep this for as long as possible, the drives would have storage of no less than 12 TB.  &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As this would be my first DAS, it would be grateful if any of you can impart any advice or wisdom you wish you had when setting one of these up yourselves as well as any pitfalls that I should be aware of.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1430ceo", "is_robot_indexable": true, "report_reasons": null, "author": "Jesse_Graves", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1430ceo/seeking_to_setup_my_first_das/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1430ceo/seeking_to_setup_my_first_das/", "subreddit_subscribers": 686435, "created_utc": 1686103676.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}