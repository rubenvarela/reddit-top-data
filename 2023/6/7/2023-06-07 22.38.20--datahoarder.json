{"kind": "Listing", "data": {"after": "t3_142w60a", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1qdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drobo is Officially Done as the Company Moves Into Liquidation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1433ddy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 209, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 209, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PKTtj-THG-KuiQSFLPy51PQDf6fkTKM--MIVGPziJEY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686112523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "petapixel.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://petapixel.com/2023/06/06/drobo-is-officially-done-as-the-company-moves-into-liquidation/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?auto=webp&amp;v=enabled&amp;s=2d6845a04b8ae8051560e342fc34ed8054508455", "width": 1600, "height": 840}, "resolutions": [{"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=655acd7ef62b579c5bc7d0bd1608f3082f0bc50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5752f44af67c4c0bdee83005d37717c99d439271", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=182f548e9ad703f020ca4125fbae9121f8c5fb0e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75db6e3ec918e1d90db5276b1d63d58b0f632aec", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca500b5bbc31735cf6e11cf47433cebbdd28bd76", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qP9qvs1hUus2-TPtrVE8TagJQAYDhINukmHUFWWYHaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cff3a17c7fcaecc2e0a03b4673dd71c0178ec15", "width": 1080, "height": 567}], "variants": {}, "id": "BH1g-ogwcIrRpjZws5J8L2362WMqje8vd5KcPJ649YU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1433ddy", "is_robot_indexable": true, "report_reasons": null, "author": "khaled", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1433ddy/drobo_is_officially_done_as_the_company_moves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://petapixel.com/2023/06/06/drobo-is-officially-done-as-the-company-moves-into-liquidation/", "subreddit_subscribers": 686458, "created_utc": 1686112523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3syqg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warrior moves", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_143luvh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/x15fa6uz8n4b1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/x15fa6uz8n4b1/DASH_96.mp4", "dash_url": "https://v.redd.it/x15fa6uz8n4b1/DASHPlaylist.mpd?a=1688769500%2CNDllOGVhMDhkN2E0OWFhOTE2Yjk5MDQxMTMyMzJjZDhmNjVhZDY3MzU4NmU5YTY0OWMzMjg2MGJkZjY3Y2VmNA%3D%3D&amp;v=1&amp;f=sd", "duration": 30, "hls_url": "https://v.redd.it/x15fa6uz8n4b1/HLSPlaylist.m3u8?a=1688769500%2CNjgyMWFiMTNiZTk4MTVlZjVkOTdjZjVjZTBiZWJkMDA0Y2RlYWYwYjBkZDhhZDJlMjI5Nzk3N2I1MmExYTRkNw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-Lh4O7DaM0Y-dfLUJOWDZSRfIX297i4FF7wjolkvrjI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686164733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/x15fa6uz8n4b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=95f2994c96bcb337277e4322b7973e6e2ccd7b7e", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e16c81cba42794876c49c5649ae6c5a47337889f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=f3f6ef574f7e7fa2c4feb7b0066f1af8429971d1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=743cd96b5c33f1fae4e8a15b442c63d541afd2c3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=35d0d6e961970bdaa97fb3decadf2685043c9439", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=92a547207e2dabbd6e0e63fb69e4e723c1214fb8", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/HOvFwVhNvbXgKO5ou-4AtReEdofBR4Ztvq1pC6fULLs.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b4e70122e7034691f3476568cd23ada2cfd320e4", "width": 1080, "height": 607}], "variants": {}, "id": "rw2avaV-mfcds0E_ZsQyD7zYHGszofqntxSW1MrKScU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143luvh", "is_robot_indexable": true, "report_reasons": null, "author": "nawts", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143luvh/warrior_moves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/x15fa6uz8n4b1", "subreddit_subscribers": 686458, "created_utc": 1686164733.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/x15fa6uz8n4b1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/x15fa6uz8n4b1/DASH_96.mp4", "dash_url": "https://v.redd.it/x15fa6uz8n4b1/DASHPlaylist.mpd?a=1688769500%2CNDllOGVhMDhkN2E0OWFhOTE2Yjk5MDQxMTMyMzJjZDhmNjVhZDY3MzU4NmU5YTY0OWMzMjg2MGJkZjY3Y2VmNA%3D%3D&amp;v=1&amp;f=sd", "duration": 30, "hls_url": "https://v.redd.it/x15fa6uz8n4b1/HLSPlaylist.m3u8?a=1688769500%2CNjgyMWFiMTNiZTk4MTVlZjVkOTdjZjVjZTBiZWJkMDA0Y2RlYWYwYjBkZDhhZDJlMjI5Nzk3N2I1MmExYTRkNw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I figured I'd throw an option out there for Windows users looking to generate hashes and validate their files based on those hashes.\n\nGenerating hashes and validating files with those hashes should be a trivial matter.\n\nFor Linux you can easily use `md5sum` to generate hashes of files and run recursively through a folder with `find /folder -type f -exec md5sum {} + &gt;&gt; hash.log`\n\nFor Windows it's not quite as straightforward, usually you have to resort to third party apps like TeraCopy, Hashdeep, DirHash, CRCCheckCopy, among others. However, it is possible using native Windows Powershell.\n\nFIRST AND FOREMOST... I take no responsibility in anything happening to your data. I'm just trying to offer options. This has not been tested extensively, but I personally use it and it seems to work well. I am not a programmer but I did piecemeal this code together. It may not be the most efficient, but it works. This will not modify any data, only generate log files of MD5 hashes and validation results.\n\nIf you notice any bugs or anything odd about it, please let me know, but I don't plan on adding many additional features. I wanted it pretty bare bones. Feel free to modify as you see fit for your own use.\n\n**GENERATING MD5 HASHES RECURSIVELY**\n\nAt it's most basic level you can use this \"one-liner\" script to generate hashes recursively through folders, which outputs results to file \"hashlog.log\":\n\n    gci -Path \"&lt;folder to hash&gt;\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | ft -AutoSize &gt; \"hashlog.log\"\n\nBasic, but it works. It doesn't display anything to console and it uses absolute paths instead of relative paths, which might not be real useful for validation reasons. It also pads it with a bunch of trailing spaces equal to the longest path name. This is default behavior for this type of output.\n\nTo run it, just CD to the folder where you want to store the log file, copy/paste the above in a powershell window, modify \"&lt;folder to hash&gt;\" to the file path you want to recursively generate hashes for every file in, and hit [ENTER]. Obviously the more files you have the longer it will take. This code does not provide any output to console.\n\nIf you want something more robust, this \"one-liner\" is quite long but a bit more useful and mimics output from md5sum, basically `hashvalue \\relative\\path\\to\\file` Example Output:\n\n    19D4BB0121058149C33BA98DBC3ED149 \\SMALL\\10KB\\10KB_000000.txt\n    94E29C41059CB19819C1934B500797DD \\SMALL\\10KB\\10KB_000001.txt\n    EA5E5FC8B434FD4924FDEC5FDBFE226B \\SMALL\\10KB\\10KB_000002.txt\n    9EA101052926CB0BE10342EF9468FC3E \\SMALL\\10KB\\10KB_000003.txt\n    B952DA5732E657FB961310AC4AB55493 \\SMALL\\10KB\\10KB_000004.txt\n\nThis below script outputs to console, outputs paths relative to the input path, allows you to specify the name of the hashlog, trims any empty whitespace, and provides a header with date and file info at the top:\n\n    $hashpath=\"E:\\Test it\"; $hashlog=\"hashlog.log\"; $numfiles=(gci -Path \"$hashpath\" -Recurse -File | measure-object).Count; Write-Output \"Date: $(Get-Date) / Path: $hashpath / Files: $numfiles\" | Set-Content \"$hashlog\"; $count=1; gci -Path \"$hashpath\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | % {$_.Path=($_.Path -replace [regex]::Escape(\"$hashpath\"), ''); Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path; ++$count; $_} | ft -AutoSize -HideTableHeaders | Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath \"$hashlog\" -Append; (gc \"$hashlog\").Trim() -ne \"\" | sc \"$hashlog\"\n\nJust modify `$hashpath` to the folder you want to hash, `$hashlog` for the name of the log file, and let it rip.\n\nOr if you wish you can copy/paste the below into a notepad file, save it as something like `generatemd5hash.ps1`, then right click and run with powershell. It's the same as the above \"one-liner\" just a bit more readable:\n\n    $hashpath=\"E:\\Test it\"; $hashlog=\"hashlog.log\"\n    $numfiles=(gci -Path \"$hashpath\" -Recurse -File | measure-object).Count\n    Write-Output \"Date: $(Get-Date) / Path: $hashpath / Files: $numfiles\" | Set-Content \"$hashlog\"\n    $count=1\n    gci -Path \"$hashpath\" -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | \n        % {$_.Path=($_.Path -replace [regex]::Escape(\"$hashpath\"), ''); Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path; ++$count; $_} | \n        ft -AutoSize -HideTableHeaders | \n        Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath \"$hashlog\" -Append\n    (gc \"$hashlog\").Trim() -ne \"\" | sc \"$hashlog\"\n\n**VALIDATING HASHES**\n\nMost hashing utilities offer a way to validate hashes based on an existing pregenerated log of hashes. In this case you can't exactly validate directly using the log file. What you can do is generate a new set of hashes and compare the two.\n\nPowershell comes with a nifty little command called `Compare-Object` or also known as `Diff`. It will very quickly find differences between contents of text files.\n\nIt's basic command set is as follows:\n\n    Compare-Object -ReferenceObject (Get-Content \"&lt;hash log file 1&gt;\") -DifferenceObject (Get-Content \"&lt;hash log file 2&gt;\")\n\nThis will then spit out items in file 1 that aren't in file 2 and vice versa. File 1 objects are identified with a \"&lt;=\" flag and File 2 objects are identified with a \"=&gt;\" flag.\n\nSo if you generate hashes of your data, and then some time in the future, you can generate another set of hashes and then compare them using the above command to validate for any changes (due to bitrot or other).\n\nLike the hash generation code above, I also created a more robust hash comparison \"one-liner\" that you can use:\n\n    $hash1=\"hash1.md5\"; $hash2=\"hash2.md5\"; $hashdifflog=\"hashdifflog.log\"; Write-Output \"Date: $(Get-Date) / Compare '$hash1' (&lt;=) with '$hash2' (=&gt;)\" | sc \"$hashdifflog\"; diff -ReferenceObject (gc \"$hash1\" | Select -Skip 1) -DifferenceObject (gc \"$hash2\" | Select -Skip 1) | group { $_.InputObject -replace '^.+ ' } | % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | Out-File -Encoding ASCII -Filepath \"$hashdifflog\" -Append; gc $hashdifflog\n\nSet `$hash1` variable to the first hash log file and `$hash2` to the second hash log file you want to compare. Set `$hashdifflog` variable to whatever you want the log file to be named, by default it's \"hashdifflog.log\" and will store it to whatever folder you run this powershell script from. It will also provide date and file reference info at the top.\n\nAntoher thing to note is the `Select -Skip 1` command. This will effectively skip the first line of data because if you used the above Powershell script to generate hashes, it will genrate a header with Date and file info that you don't want to put in the comparison mix. So if your files don't have any header info you can just change the Skip value to 0.\n\nThis script will group matching file names from each log file that have non-matching hashes. Files that are unique to each file will be shown on individual lines. If there are no discrepencies it will be blank, which is ideally what you want.\n\nOr as before, you can also copy/paste the below into a notepad document, save it with a `.ps1` extension (i.e. `hashcompare.ps1`), right click and run with powershell.\n\n    $hash1=\"hash1.md5\"\n    $hash2=\"hash2.md5\"\n    $hashdifflog=\"hashdifflog.log\"\n    Write-Output \"Date: $(Get-Date) / Compare '$hash1' (&lt;=) with '$hash2' (=&gt;)\" | sc \"$hashdifflog\"\n    diff -ReferenceObject (gc \"$hash1\" | Select -Skip 1) -DifferenceObject (gc \"$hash2\" | Select -Skip 1) | \n        group { $_.InputObject -replace '^.+ ' } | \n        % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | \n        Out-File -Encoding ASCII -Filepath \"$hashdifflog\" -Append\n    gc $hashdifflog\n\n\n**INTERACTIVE SCRIPT**\n\nBut if you don't want to fuss with any of the above I wrote this basic Powershell script that will allow you generate hash log files, and compare hash log files just by entering the appropriate info at the prompts.\n\nA few notes:\n\n1. When you go to compare hash files, it will ask you how many lines to skip. It will present you with the first 9 lines of each log file you specify and you can tell it how many lines to omit from the comparison. So if you added any remarks or header info to the hash log files, you can just have it ignored. Selecting \"Skip 3\" will skip the first three lines shown, \"Skip 1\" just the first line, etc. Default will be to skip no lines.\n\n2. This does an EXACT COMPARE of file names as strings, although it is not case sensitive. It is designed so all paths will start with a slash. It has no logic to assume forward slash and backwards slash are interchangeable. So if you need to compare output from a Linux machine with md5sum to this script run from a Windows machine you will have to modify the slashes. This can be easily rectified with the following (backwards slash to forwards slash, just revere them to get the reverse effect):\n\n    `(gc \"hash1.md5\").replace('\\', '/') | sc \"hash1.md5\"`\n\n3. Log files are appended with a date time stamp number value equal to `yyyymmdd_HHmmss`\n\n4. This has been tested with a log file with over 60 thousand entries and worked fine. File path length should not be an issue, but I haven't tested it past about 200 characters.\n\nYou can copy/paste the powershell script from pastebin here: https://pastebin.com/0UJ0gcdb\n\nOr here's the same code below. Just copy/paste into notepad and save as `generatemd5.ps1` or whatever as long as it has a `.ps1` extension. It will save log files where you save it, so just be wary of that.\n\n    # generatemd5.ps1 by HTWingNut 06 June 2023\n    Clear-Host\n    Write-Host \"\"\n    $datetime = Get-Date\n    $timestamp = $datetime.ToString(\"yyyyMMdd_HHmmss\")\n    $reqpath = 'Input path to generate md5 hashes'\n    $basepath = $hash1 = $hash2 = \"?\"\n    $choice=$null\n    $md5log = 'hashes_'+$timestamp+'.md5'\n    \n    function GenerateHash {\n    \n    Clear-Host\n    Write-Host \"\"\n    Write-Host \"Generate Hash Files ...\"\n    Write-Host \"\"\n    \n    while (!(Test-Path -Path $basepath)) { $basepath = Read-Host -Prompt $reqpath; if ( $basepath -eq \"\") { $basepath=\"?\" } }\n    $numfiles = ( Get-ChildItem -Path \"$basepath\" -Recurse -File | Measure-Object ).Count\n    \n    $lchar = $basepath.Substring($basepath.length-1,1)\n    If ($lchar -eq \"\\\") { $basepath = $basepath.Substring(0,$basepath.length-1) }\n    \n    Write-Host \"Total Files: $numfiles\"\n    Write-Host \"\"\n    $count=1\n    \n    Write-Output \"**** $($datetime) '$($basepath)'\" | Set-Content \"$md5log\"\n    \n    Get-ChildItem -Path \"$basepath\" -Recurse -File | \n        Get-FileHash -Algorithm MD5 | \n        Select-Object Hash,Path | \n        ForEach-Object { \n            $_.Path = ($_.Path -replace [regex]::Escape(\"$basepath\"), '')\n            Write-Host \"$($count) of $($numfiles)\" $_.Hash $_.Path\n            ++$count\n            $_\n        } | \n        Format-Table -AutoSize -HideTableHeaders | \n        Out-String -Width 4096 | \n        Out-File \"$md5log\" -encoding ASCII -append\n    \n    (get-content $md5log).Trim() -ne '' | Set-Content $md5log\n    \n    Write-Host \"\"\n    Write-Host \"Hashes stored in file '$pwd\\$md5log'\"\n    Write-Host \"\"\n    \n    }\n    \n    function CompareHash {\n    \n    $continue = \"n\"\n    \n    While ([bool]$continue) {\n    \n    Clear-Host\n    Write-Host \"\"\n    Write-Host \"Compare Hash Files...\"\n    \n    $hash1 = $hash2 = \"?\"\n    $numksip1 = $numskip2 = \"0\"\n    \n    Write-Host \"\"\n    while (!(Test-Path $hash1)) { $hash1 = Read-Host -Prompt \"Enter Path/Name for Log 1\"; if ( $hash1 -eq \"\") { $hash1=\"?\" } }\n    Write-Host \"\"\n    Write-Host \"First 9 lines of $($hash1):\"\n    $i=1; Get-Content $hash1 | Select -First 9 | % {\"$($i) $_\"; $i++}\n    Write-Host \"\"\n    Do { $numskip1 = Read-Host \"Choose Number of Lines to Skip [0-9] (default 0)\" } until ($numskip1 -in 0,1,2,3,4,5,6,7,8,9)\n    Write-Host \"\"\n    \n    while (!(Test-Path $hash2)) { $hash2 = Read-Host -Prompt \"Enter Path/Name for Log 2\"; if ( $hash2 -eq \"\") { $hash2=\"?\" } }\n    Write-Host \"\"\n    Write-Host \"First 9 lines of $($hash2):\"\n    $i=1; Get-Content $hash2 | Select -First 9 | % {\"$($i) $_\"; $i++}\n    Write-Host \"\"\n    Do { $numskip2 = Read-Host \"Choose Number of Lines to Skip [0-9] (default 0)\" } until ($numskip2 -in 0,1,2,3,4,5,6,7,8,9)\n    \n    $hclog = \"HashCompare_$(((Get-Item $hash1).Basename).Replace(' ','_'))_vs_$(((Get-Item $hash2).Basename).Replace(' ','_'))_$timestamp.txt\"\n    \n        Write-Host \"\"\n        Write-Host \"---------------------\"\n        Write-Host \"\"\n    \n        Write-Host \"**** File: '$((Get-Item $hash1).Name)'\"\n        Write-Host \"**** Skipping Lines:\"\n        Get-Content $hash1 | Select -First $numskip1\n        Write-Host \"\"\n        Write-Host \"**** Starting with Line:\"\n        Get-Content $hash1 | Select -Index ($numskip1)\n        Write-Host \"\"\n        Write-Host \"---------------------\"\n        Write-Host \"\"\n        Write-Host \"**** File: '$((Get-Item $hash2).Name)'\"\n        Write-Host \"**** Skipping Lines:\"\n        Get-Content $hash2 | Select -First $numskip2\n        Write-Host \"\"\n        Write-Host \"**** Starting with Line:\"\n    \n        Get-Content $hash2 | Select -Index ($numskip2)\n        Write-Host\n        $continue = Read-Host \"Press [ENTER] to accept, any other key to choose again\"\n    \n    }\n    \n    # https://stackoverflow.com/questions/76415338/in-powershell-how-do-i-split-text-from-compare-object-input-and-sort-by-one-of/\n    \n    $diff = Compare-Object -ReferenceObject (Get-Content \"$hash1\" | Select -Skip $numskip1) -DifferenceObject (Get-Content \"$hash2\" | Select -Skip $numskip2 ) | \n        ForEach-Object {\n            if ($_.SideIndicator -eq \"&lt;=\") { $_.SideIndicator = \"($((Get-ChildItem $hash1).Name))\" } elseif ($_.SideIndicator -eq \"=&gt;\") { $_.SideIndicator = \"($((Get-ChildItem $hash2).Name))\" }\n            $_\n        } |\n        Group-Object { $_.InputObject -replace '^.+ ' } |\n        ForEach-Object {\n            $_.Group | Format-Table -HideTableHeaders | \n                Out-String | ForEach-Object TrimEnd\n        }\n    \n    if ($diff) {\n        Write-output \"**** $($datetime) '$($hash1)' vs '$($hash2)'`n**** Matching file path/names with mismatched hashes are grouped together`n**** Individual file names means unique file/hash in that log file\" $diff &gt; \"$hclog\"\n    } else {\n        Write-Output \"**** $($datetime) '$($hash1)' vs '$($hash2)' `n`n**** ALL CLEAR! NO DIFFERENCES FOUND!\" &gt; \"$hclog\"\n    }\n    \n    Write-Host \"\"\n    Write-Host \"**** Results stored in $($pwd)\\$($hclog)\"\n    Write-Host \"\"\n    \n    \n    }\n    \n    Do { $choice = Read-Host \"[G]enerate Hash or [C]ompare Hash Logs\" } until ($choice -in 'g','c')\n    If ( $choice -eq \"g\" ) { GenerateHash }\n    If ( $choice -eq \"c\" ) { CompareHash }\n    \n    cmd /c 'pause'", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here is a Windows Powershell Script to Generate and Validate MD5 Hashes of Your Data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ysqt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1686143230.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686099374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I figured I&amp;#39;d throw an option out there for Windows users looking to generate hashes and validate their files based on those hashes.&lt;/p&gt;\n\n&lt;p&gt;Generating hashes and validating files with those hashes should be a trivial matter.&lt;/p&gt;\n\n&lt;p&gt;For Linux you can easily use &lt;code&gt;md5sum&lt;/code&gt; to generate hashes of files and run recursively through a folder with &lt;code&gt;find /folder -type f -exec md5sum {} + &amp;gt;&amp;gt; hash.log&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;For Windows it&amp;#39;s not quite as straightforward, usually you have to resort to third party apps like TeraCopy, Hashdeep, DirHash, CRCCheckCopy, among others. However, it is possible using native Windows Powershell.&lt;/p&gt;\n\n&lt;p&gt;FIRST AND FOREMOST... I take no responsibility in anything happening to your data. I&amp;#39;m just trying to offer options. This has not been tested extensively, but I personally use it and it seems to work well. I am not a programmer but I did piecemeal this code together. It may not be the most efficient, but it works. This will not modify any data, only generate log files of MD5 hashes and validation results.&lt;/p&gt;\n\n&lt;p&gt;If you notice any bugs or anything odd about it, please let me know, but I don&amp;#39;t plan on adding many additional features. I wanted it pretty bare bones. Feel free to modify as you see fit for your own use.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GENERATING MD5 HASHES RECURSIVELY&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At it&amp;#39;s most basic level you can use this &amp;quot;one-liner&amp;quot; script to generate hashes recursively through folders, which outputs results to file &amp;quot;hashlog.log&amp;quot;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gci -Path &amp;quot;&amp;lt;folder to hash&amp;gt;&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | ft -AutoSize &amp;gt; &amp;quot;hashlog.log&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Basic, but it works. It doesn&amp;#39;t display anything to console and it uses absolute paths instead of relative paths, which might not be real useful for validation reasons. It also pads it with a bunch of trailing spaces equal to the longest path name. This is default behavior for this type of output.&lt;/p&gt;\n\n&lt;p&gt;To run it, just CD to the folder where you want to store the log file, copy/paste the above in a powershell window, modify &amp;quot;&amp;lt;folder to hash&amp;gt;&amp;quot; to the file path you want to recursively generate hashes for every file in, and hit [ENTER]. Obviously the more files you have the longer it will take. This code does not provide any output to console.&lt;/p&gt;\n\n&lt;p&gt;If you want something more robust, this &amp;quot;one-liner&amp;quot; is quite long but a bit more useful and mimics output from md5sum, basically &lt;code&gt;hashvalue \\relative\\path\\to\\file&lt;/code&gt; Example Output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;19D4BB0121058149C33BA98DBC3ED149 \\SMALL\\10KB\\10KB_000000.txt\n94E29C41059CB19819C1934B500797DD \\SMALL\\10KB\\10KB_000001.txt\nEA5E5FC8B434FD4924FDEC5FDBFE226B \\SMALL\\10KB\\10KB_000002.txt\n9EA101052926CB0BE10342EF9468FC3E \\SMALL\\10KB\\10KB_000003.txt\nB952DA5732E657FB961310AC4AB55493 \\SMALL\\10KB\\10KB_000004.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This below script outputs to console, outputs paths relative to the input path, allows you to specify the name of the hashlog, trims any empty whitespace, and provides a header with date and file info at the top:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hashpath=&amp;quot;E:\\Test it&amp;quot;; $hashlog=&amp;quot;hashlog.log&amp;quot;; $numfiles=(gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | measure-object).Count; Write-Output &amp;quot;Date: $(Get-Date) / Path: $hashpath / Files: $numfiles&amp;quot; | Set-Content &amp;quot;$hashlog&amp;quot;; $count=1; gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | % {$_.Path=($_.Path -replace [regex]::Escape(&amp;quot;$hashpath&amp;quot;), &amp;#39;&amp;#39;); Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path; ++$count; $_} | ft -AutoSize -HideTableHeaders | Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath &amp;quot;$hashlog&amp;quot; -Append; (gc &amp;quot;$hashlog&amp;quot;).Trim() -ne &amp;quot;&amp;quot; | sc &amp;quot;$hashlog&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Just modify &lt;code&gt;$hashpath&lt;/code&gt; to the folder you want to hash, &lt;code&gt;$hashlog&lt;/code&gt; for the name of the log file, and let it rip.&lt;/p&gt;\n\n&lt;p&gt;Or if you wish you can copy/paste the below into a notepad file, save it as something like &lt;code&gt;generatemd5hash.ps1&lt;/code&gt;, then right click and run with powershell. It&amp;#39;s the same as the above &amp;quot;one-liner&amp;quot; just a bit more readable:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hashpath=&amp;quot;E:\\Test it&amp;quot;; $hashlog=&amp;quot;hashlog.log&amp;quot;\n$numfiles=(gci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | measure-object).Count\nWrite-Output &amp;quot;Date: $(Get-Date) / Path: $hashpath / Files: $numfiles&amp;quot; | Set-Content &amp;quot;$hashlog&amp;quot;\n$count=1\ngci -Path &amp;quot;$hashpath&amp;quot; -Recurse -File | Get-FileHash -Algorithm MD5 | Select Hash,Path | \n    % {$_.Path=($_.Path -replace [regex]::Escape(&amp;quot;$hashpath&amp;quot;), &amp;#39;&amp;#39;); Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path; ++$count; $_} | \n    ft -AutoSize -HideTableHeaders | \n    Out-String -Width 4096 |  Out-File -Encoding ASCII -FilePath &amp;quot;$hashlog&amp;quot; -Append\n(gc &amp;quot;$hashlog&amp;quot;).Trim() -ne &amp;quot;&amp;quot; | sc &amp;quot;$hashlog&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;VALIDATING HASHES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Most hashing utilities offer a way to validate hashes based on an existing pregenerated log of hashes. In this case you can&amp;#39;t exactly validate directly using the log file. What you can do is generate a new set of hashes and compare the two.&lt;/p&gt;\n\n&lt;p&gt;Powershell comes with a nifty little command called &lt;code&gt;Compare-Object&lt;/code&gt; or also known as &lt;code&gt;Diff&lt;/code&gt;. It will very quickly find differences between contents of text files.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basic command set is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Compare-Object -ReferenceObject (Get-Content &amp;quot;&amp;lt;hash log file 1&amp;gt;&amp;quot;) -DifferenceObject (Get-Content &amp;quot;&amp;lt;hash log file 2&amp;gt;&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This will then spit out items in file 1 that aren&amp;#39;t in file 2 and vice versa. File 1 objects are identified with a &amp;quot;&amp;lt;=&amp;quot; flag and File 2 objects are identified with a &amp;quot;=&amp;gt;&amp;quot; flag.&lt;/p&gt;\n\n&lt;p&gt;So if you generate hashes of your data, and then some time in the future, you can generate another set of hashes and then compare them using the above command to validate for any changes (due to bitrot or other).&lt;/p&gt;\n\n&lt;p&gt;Like the hash generation code above, I also created a more robust hash comparison &amp;quot;one-liner&amp;quot; that you can use:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hash1=&amp;quot;hash1.md5&amp;quot;; $hash2=&amp;quot;hash2.md5&amp;quot;; $hashdifflog=&amp;quot;hashdifflog.log&amp;quot;; Write-Output &amp;quot;Date: $(Get-Date) / Compare &amp;#39;$hash1&amp;#39; (&amp;lt;=) with &amp;#39;$hash2&amp;#39; (=&amp;gt;)&amp;quot; | sc &amp;quot;$hashdifflog&amp;quot;; diff -ReferenceObject (gc &amp;quot;$hash1&amp;quot; | Select -Skip 1) -DifferenceObject (gc &amp;quot;$hash2&amp;quot; | Select -Skip 1) | group { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } | % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | Out-File -Encoding ASCII -Filepath &amp;quot;$hashdifflog&amp;quot; -Append; gc $hashdifflog\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Set &lt;code&gt;$hash1&lt;/code&gt; variable to the first hash log file and &lt;code&gt;$hash2&lt;/code&gt; to the second hash log file you want to compare. Set &lt;code&gt;$hashdifflog&lt;/code&gt; variable to whatever you want the log file to be named, by default it&amp;#39;s &amp;quot;hashdifflog.log&amp;quot; and will store it to whatever folder you run this powershell script from. It will also provide date and file reference info at the top.&lt;/p&gt;\n\n&lt;p&gt;Antoher thing to note is the &lt;code&gt;Select -Skip 1&lt;/code&gt; command. This will effectively skip the first line of data because if you used the above Powershell script to generate hashes, it will genrate a header with Date and file info that you don&amp;#39;t want to put in the comparison mix. So if your files don&amp;#39;t have any header info you can just change the Skip value to 0.&lt;/p&gt;\n\n&lt;p&gt;This script will group matching file names from each log file that have non-matching hashes. Files that are unique to each file will be shown on individual lines. If there are no discrepencies it will be blank, which is ideally what you want.&lt;/p&gt;\n\n&lt;p&gt;Or as before, you can also copy/paste the below into a notepad document, save it with a &lt;code&gt;.ps1&lt;/code&gt; extension (i.e. &lt;code&gt;hashcompare.ps1&lt;/code&gt;), right click and run with powershell.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$hash1=&amp;quot;hash1.md5&amp;quot;\n$hash2=&amp;quot;hash2.md5&amp;quot;\n$hashdifflog=&amp;quot;hashdifflog.log&amp;quot;\nWrite-Output &amp;quot;Date: $(Get-Date) / Compare &amp;#39;$hash1&amp;#39; (&amp;lt;=) with &amp;#39;$hash2&amp;#39; (=&amp;gt;)&amp;quot; | sc &amp;quot;$hashdifflog&amp;quot;\ndiff -ReferenceObject (gc &amp;quot;$hash1&amp;quot; | Select -Skip 1) -DifferenceObject (gc &amp;quot;$hash2&amp;quot; | Select -Skip 1) | \n    group { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } | \n    % { $_.Group | ft -HideTableHeaders | Out-String | % TrimEnd } | \n    Out-File -Encoding ASCII -Filepath &amp;quot;$hashdifflog&amp;quot; -Append\ngc $hashdifflog\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;INTERACTIVE SCRIPT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;But if you don&amp;#39;t want to fuss with any of the above I wrote this basic Powershell script that will allow you generate hash log files, and compare hash log files just by entering the appropriate info at the prompts.&lt;/p&gt;\n\n&lt;p&gt;A few notes:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;When you go to compare hash files, it will ask you how many lines to skip. It will present you with the first 9 lines of each log file you specify and you can tell it how many lines to omit from the comparison. So if you added any remarks or header info to the hash log files, you can just have it ignored. Selecting &amp;quot;Skip 3&amp;quot; will skip the first three lines shown, &amp;quot;Skip 1&amp;quot; just the first line, etc. Default will be to skip no lines.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This does an EXACT COMPARE of file names as strings, although it is not case sensitive. It is designed so all paths will start with a slash. It has no logic to assume forward slash and backwards slash are interchangeable. So if you need to compare output from a Linux machine with md5sum to this script run from a Windows machine you will have to modify the slashes. This can be easily rectified with the following (backwards slash to forwards slash, just revere them to get the reverse effect):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;(gc &amp;quot;hash1.md5&amp;quot;).replace(&amp;#39;\\&amp;#39;, &amp;#39;/&amp;#39;) | sc &amp;quot;hash1.md5&amp;quot;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Log files are appended with a date time stamp number value equal to &lt;code&gt;yyyymmdd_HHmmss&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This has been tested with a log file with over 60 thousand entries and worked fine. File path length should not be an issue, but I haven&amp;#39;t tested it past about 200 characters.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;You can copy/paste the powershell script from pastebin here: &lt;a href=\"https://pastebin.com/0UJ0gcdb\"&gt;https://pastebin.com/0UJ0gcdb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Or here&amp;#39;s the same code below. Just copy/paste into notepad and save as &lt;code&gt;generatemd5.ps1&lt;/code&gt; or whatever as long as it has a &lt;code&gt;.ps1&lt;/code&gt; extension. It will save log files where you save it, so just be wary of that.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# generatemd5.ps1 by HTWingNut 06 June 2023\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\n$datetime = Get-Date\n$timestamp = $datetime.ToString(&amp;quot;yyyyMMdd_HHmmss&amp;quot;)\n$reqpath = &amp;#39;Input path to generate md5 hashes&amp;#39;\n$basepath = $hash1 = $hash2 = &amp;quot;?&amp;quot;\n$choice=$null\n$md5log = &amp;#39;hashes_&amp;#39;+$timestamp+&amp;#39;.md5&amp;#39;\n\nfunction GenerateHash {\n\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Generate Hash Files ...&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\nwhile (!(Test-Path -Path $basepath)) { $basepath = Read-Host -Prompt $reqpath; if ( $basepath -eq &amp;quot;&amp;quot;) { $basepath=&amp;quot;?&amp;quot; } }\n$numfiles = ( Get-ChildItem -Path &amp;quot;$basepath&amp;quot; -Recurse -File | Measure-Object ).Count\n\n$lchar = $basepath.Substring($basepath.length-1,1)\nIf ($lchar -eq &amp;quot;\\&amp;quot;) { $basepath = $basepath.Substring(0,$basepath.length-1) }\n\nWrite-Host &amp;quot;Total Files: $numfiles&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n$count=1\n\nWrite-Output &amp;quot;**** $($datetime) &amp;#39;$($basepath)&amp;#39;&amp;quot; | Set-Content &amp;quot;$md5log&amp;quot;\n\nGet-ChildItem -Path &amp;quot;$basepath&amp;quot; -Recurse -File | \n    Get-FileHash -Algorithm MD5 | \n    Select-Object Hash,Path | \n    ForEach-Object { \n        $_.Path = ($_.Path -replace [regex]::Escape(&amp;quot;$basepath&amp;quot;), &amp;#39;&amp;#39;)\n        Write-Host &amp;quot;$($count) of $($numfiles)&amp;quot; $_.Hash $_.Path\n        ++$count\n        $_\n    } | \n    Format-Table -AutoSize -HideTableHeaders | \n    Out-String -Width 4096 | \n    Out-File &amp;quot;$md5log&amp;quot; -encoding ASCII -append\n\n(get-content $md5log).Trim() -ne &amp;#39;&amp;#39; | Set-Content $md5log\n\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Hashes stored in file &amp;#39;$pwd\\$md5log&amp;#39;&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\n}\n\nfunction CompareHash {\n\n$continue = &amp;quot;n&amp;quot;\n\nWhile ([bool]$continue) {\n\nClear-Host\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;Compare Hash Files...&amp;quot;\n\n$hash1 = $hash2 = &amp;quot;?&amp;quot;\n$numksip1 = $numskip2 = &amp;quot;0&amp;quot;\n\nWrite-Host &amp;quot;&amp;quot;\nwhile (!(Test-Path $hash1)) { $hash1 = Read-Host -Prompt &amp;quot;Enter Path/Name for Log 1&amp;quot;; if ( $hash1 -eq &amp;quot;&amp;quot;) { $hash1=&amp;quot;?&amp;quot; } }\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;First 9 lines of $($hash1):&amp;quot;\n$i=1; Get-Content $hash1 | Select -First 9 | % {&amp;quot;$($i) $_&amp;quot;; $i++}\nWrite-Host &amp;quot;&amp;quot;\nDo { $numskip1 = Read-Host &amp;quot;Choose Number of Lines to Skip [0-9] (default 0)&amp;quot; } until ($numskip1 -in 0,1,2,3,4,5,6,7,8,9)\nWrite-Host &amp;quot;&amp;quot;\n\nwhile (!(Test-Path $hash2)) { $hash2 = Read-Host -Prompt &amp;quot;Enter Path/Name for Log 2&amp;quot;; if ( $hash2 -eq &amp;quot;&amp;quot;) { $hash2=&amp;quot;?&amp;quot; } }\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;First 9 lines of $($hash2):&amp;quot;\n$i=1; Get-Content $hash2 | Select -First 9 | % {&amp;quot;$($i) $_&amp;quot;; $i++}\nWrite-Host &amp;quot;&amp;quot;\nDo { $numskip2 = Read-Host &amp;quot;Choose Number of Lines to Skip [0-9] (default 0)&amp;quot; } until ($numskip2 -in 0,1,2,3,4,5,6,7,8,9)\n\n$hclog = &amp;quot;HashCompare_$(((Get-Item $hash1).Basename).Replace(&amp;#39; &amp;#39;,&amp;#39;_&amp;#39;))_vs_$(((Get-Item $hash2).Basename).Replace(&amp;#39; &amp;#39;,&amp;#39;_&amp;#39;))_$timestamp.txt&amp;quot;\n\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;---------------------&amp;quot;\n    Write-Host &amp;quot;&amp;quot;\n\n    Write-Host &amp;quot;**** File: &amp;#39;$((Get-Item $hash1).Name)&amp;#39;&amp;quot;\n    Write-Host &amp;quot;**** Skipping Lines:&amp;quot;\n    Get-Content $hash1 | Select -First $numskip1\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** Starting with Line:&amp;quot;\n    Get-Content $hash1 | Select -Index ($numskip1)\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;---------------------&amp;quot;\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** File: &amp;#39;$((Get-Item $hash2).Name)&amp;#39;&amp;quot;\n    Write-Host &amp;quot;**** Skipping Lines:&amp;quot;\n    Get-Content $hash2 | Select -First $numskip2\n    Write-Host &amp;quot;&amp;quot;\n    Write-Host &amp;quot;**** Starting with Line:&amp;quot;\n\n    Get-Content $hash2 | Select -Index ($numskip2)\n    Write-Host\n    $continue = Read-Host &amp;quot;Press [ENTER] to accept, any other key to choose again&amp;quot;\n\n}\n\n# https://stackoverflow.com/questions/76415338/in-powershell-how-do-i-split-text-from-compare-object-input-and-sort-by-one-of/\n\n$diff = Compare-Object -ReferenceObject (Get-Content &amp;quot;$hash1&amp;quot; | Select -Skip $numskip1) -DifferenceObject (Get-Content &amp;quot;$hash2&amp;quot; | Select -Skip $numskip2 ) | \n    ForEach-Object {\n        if ($_.SideIndicator -eq &amp;quot;&amp;lt;=&amp;quot;) { $_.SideIndicator = &amp;quot;($((Get-ChildItem $hash1).Name))&amp;quot; } elseif ($_.SideIndicator -eq &amp;quot;=&amp;gt;&amp;quot;) { $_.SideIndicator = &amp;quot;($((Get-ChildItem $hash2).Name))&amp;quot; }\n        $_\n    } |\n    Group-Object { $_.InputObject -replace &amp;#39;^.+ &amp;#39; } |\n    ForEach-Object {\n        $_.Group | Format-Table -HideTableHeaders | \n            Out-String | ForEach-Object TrimEnd\n    }\n\nif ($diff) {\n    Write-output &amp;quot;**** $($datetime) &amp;#39;$($hash1)&amp;#39; vs &amp;#39;$($hash2)&amp;#39;`n**** Matching file path/names with mismatched hashes are grouped together`n**** Individual file names means unique file/hash in that log file&amp;quot; $diff &amp;gt; &amp;quot;$hclog&amp;quot;\n} else {\n    Write-Output &amp;quot;**** $($datetime) &amp;#39;$($hash1)&amp;#39; vs &amp;#39;$($hash2)&amp;#39; `n`n**** ALL CLEAR! NO DIFFERENCES FOUND!&amp;quot; &amp;gt; &amp;quot;$hclog&amp;quot;\n}\n\nWrite-Host &amp;quot;&amp;quot;\nWrite-Host &amp;quot;**** Results stored in $($pwd)\\$($hclog)&amp;quot;\nWrite-Host &amp;quot;&amp;quot;\n\n\n}\n\nDo { $choice = Read-Host &amp;quot;[G]enerate Hash or [C]ompare Hash Logs&amp;quot; } until ($choice -in &amp;#39;g&amp;#39;,&amp;#39;c&amp;#39;)\nIf ( $choice -eq &amp;quot;g&amp;quot; ) { GenerateHash }\nIf ( $choice -eq &amp;quot;c&amp;quot; ) { CompareHash }\n\ncmd /c &amp;#39;pause&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ysqt", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142ysqt/here_is_a_windows_powershell_script_to_generate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ysqt/here_is_a_windows_powershell_script_to_generate/", "subreddit_subscribers": 686458, "created_utc": 1686099374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks, \n\nI'm the sad owner of a broken Raritan PX-5514 (a 0U vertical PDU) that's gone EOL and EOS a long time ago. Unfortunately, the seller appears to have botched a firmware upgrade and then passed it on to me, because the unit does not progress past the self-test phase.\n\nAccording to Raritan's manuals, there was a PDU recovery tool that could be used to rescue bad firmware flashes. However, Raritan themselves claim to no longer have any copies of the tool on hand and is therefore unable to help me.\n\nWould any of you happen to have a copy on hand, or know of someone who might have one squirreled away somewhere? I'd be extermely grateful for any leads.", "author_fullname": "t2_90lnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a copy of Raritan's PDU Recovery Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143h63i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686153743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m the sad owner of a broken Raritan PX-5514 (a 0U vertical PDU) that&amp;#39;s gone EOL and EOS a long time ago. Unfortunately, the seller appears to have botched a firmware upgrade and then passed it on to me, because the unit does not progress past the self-test phase.&lt;/p&gt;\n\n&lt;p&gt;According to Raritan&amp;#39;s manuals, there was a PDU recovery tool that could be used to rescue bad firmware flashes. However, Raritan themselves claim to no longer have any copies of the tool on hand and is therefore unable to help me.&lt;/p&gt;\n\n&lt;p&gt;Would any of you happen to have a copy on hand, or know of someone who might have one squirreled away somewhere? I&amp;#39;d be extermely grateful for any leads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143h63i", "is_robot_indexable": true, "report_reasons": null, "author": "JargonTheRed", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/143h63i/looking_for_a_copy_of_raritans_pdu_recovery_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143h63i/looking_for_a_copy_of_raritans_pdu_recovery_tool/", "subreddit_subscribers": 686458, "created_utc": 1686153743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lord Bung is deleting the Confinement series - For those interested, you might want to archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_1432tyo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b38dubx", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8nKejTl_n0CbeitlAK4Cm3d44Gflyq8VQJbib8s7IWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "SCP", "selftext": "", "author_fullname": "t2_9z6t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lord Bung is deleting the Confinement series", "link_flair_richtext": [{"a": ":wMETA:", "e": "emoji", "u": "https://emoji.redditmedia.com/z5sbq9lqm2h41_t5_2r4ni/wMETA"}, {"e": "text", "t": " Meta Post"}], "subreddit_name_prefixed": "r/SCP", "hidden": false, "pwls": 7, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "name": "t3_142ye91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": "#373c3f", "ups": 2040, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "48f59682-d3a6-11eb-90ed-0e6924b8cf37", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": ":wMETA: Meta Post", "can_mod_post": false, "score": 2040, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8nKejTl_n0CbeitlAK4Cm3d44Gflyq8VQJbib8s7IWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":wMTF_EPSILON-11:", "e": "emoji", "u": "https://emoji.redditmedia.com/m0d9ls4q8hf71_t5_2r4ni/wMTF_EPSILON-11"}, {"e": "text", "t": " MTF Epsilon-11 (\"Nine-Tailed Fox\")"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686098221.0, "link_flair_type": "richtext", "wls": 7, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1iigyr5grh4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1iigyr5grh4b1.png?auto=webp&amp;v=enabled&amp;s=92fbc47a6d7622a737379478e3a3b6dbd53d6fba", "width": 867, "height": 135}, "resolutions": [{"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02790e786e1e5bd654d0d9e77932b16175d7ee3", "width": 108, "height": 16}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98093a44126560d23facf77035f007ac97bc22b9", "width": 216, "height": 33}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0e5211f4b1d1989191561e769d0dff36c8832ce", "width": 320, "height": 49}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b14ac65b955b622012f8a4621bee9b47edda1ba", "width": 640, "height": 99}], "variants": {}, "id": "o0Vf4I3a5Sbmh7Xw5l7GG0bfzgThMpqC7u2VHsWPIX4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bba31b8e-45bd-11e3-b66a-12313d188143", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":wMTF_EPSILON-11: MTF Epsilon-11 (\"Nine-Tailed Fox\")", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2r4ni", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "142ye91", "is_robot_indexable": true, "report_reasons": null, "author": "mikeBE11", "discussion_type": null, "num_comments": 293, "send_replies": true, "whitelist_status": "some_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/SCP/comments/142ye91/lord_bung_is_deleting_the_confinement_series/", "parent_whitelist_status": "some_ads", "stickied": false, "url": "https://i.redd.it/1iigyr5grh4b1.png", "subreddit_subscribers": 680615, "created_utc": 1686098221.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1686110829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1iigyr5grh4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1iigyr5grh4b1.png?auto=webp&amp;v=enabled&amp;s=92fbc47a6d7622a737379478e3a3b6dbd53d6fba", "width": 867, "height": 135}, "resolutions": [{"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02790e786e1e5bd654d0d9e77932b16175d7ee3", "width": 108, "height": 16}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98093a44126560d23facf77035f007ac97bc22b9", "width": 216, "height": 33}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0e5211f4b1d1989191561e769d0dff36c8832ce", "width": 320, "height": 49}, {"url": "https://preview.redd.it/1iigyr5grh4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b14ac65b955b622012f8a4621bee9b47edda1ba", "width": 640, "height": 99}], "variants": {}, "id": "o0Vf4I3a5Sbmh7Xw5l7GG0bfzgThMpqC7u2VHsWPIX4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1432tyo", "is_robot_indexable": true, "report_reasons": null, "author": "Tzar_Jberk", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_142ye91", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1432tyo/lord_bung_is_deleting_the_confinement_series_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1iigyr5grh4b1.png", "subreddit_subscribers": 686458, "created_utc": 1686110829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found a program named Reddsaver but not sure it can download posts with all the comments. Since most of my saved posts are some kind of tutorial (some comments are also useful there), they're mostly texts but some has videos or galleries too. It would be nice if I can pull them preferably as markdown.", "author_fullname": "t2_vemre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download all my saved posts on Reddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143lal5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686163420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found a program named Reddsaver but not sure it can download posts with all the comments. Since most of my saved posts are some kind of tutorial (some comments are also useful there), they&amp;#39;re mostly texts but some has videos or galleries too. It would be nice if I can pull them preferably as markdown.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143lal5", "is_robot_indexable": true, "report_reasons": null, "author": "muhyb", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143lal5/how_can_i_download_all_my_saved_posts_on_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143lal5/how_can_i_download_all_my_saved_posts_on_reddit/", "subreddit_subscribers": 686458, "created_utc": 1686163420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_mvvjw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good solution to replace the clunky power bricks with possibly a USB C for my D2 hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_143fv9u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TtMM_bLhWvip2byhyPIuPtlg32sX0_GM2-rnbqASUUE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686150724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/a/saxwuSr/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?auto=webp&amp;v=enabled&amp;s=b3f038c435f6743c47ab7fea8e9284b074b5ecaa", "width": 2000, "height": 1500}, "resolutions": [{"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ffff85e53598b6ac89ea3dbfde22774340bc095", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=488b1ff8856521c9f6e3a66cde9ce1af41ecd78a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9804ba8c79339299734afdec1744bd89447a7c68", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29f69a8e08795da4104832afd5c929d6ae5a8001", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d1b538689c41ac9f0899a4b07d0297ddd68b312", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/BNlKILfv3sD9KBuEybmWpPFxmIcD6JrTyurD8w9aIaM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc2971c0555d72104b82bed4421ecfe38746b8a5", "width": 1080, "height": 810}], "variants": {}, "id": "8r-o0Ir09e8_BtLns3BacUy30j8Ubz-LqRogi1evhms"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143fv9u", "is_robot_indexable": true, "report_reasons": null, "author": "drawcody", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143fv9u/does_anyone_have_a_good_solution_to_replace_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/a/saxwuSr/", "subreddit_subscribers": 686458, "created_utc": 1686150724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I'm looking for is something like gitea migrate of a repo but have it auto update with any new changes. The problem with just normal gitea mirror is it's missing the  release notes and binary attachments. The same is true for  git clone --mirror xxx. When I search around for solutions I cant seem to find anything useful so I thought I'd post here.\n\nCurrently, I let gitea mirror the repo and every time I think about it I manually go though my list using firefox to take a screenshot of the page, download each binary, and stick it all in a folder of named the same as the release. finally I git push that to my selfhosted gitea under xxx_releases. I could automate this myself but I'm hopeful there's a tool to do it for me. \n\nHow do you guys back this up?", "author_fullname": "t2_vwv9flfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to mirror github/gitlab that will include release notes and binaries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143c0cf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686141199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I&amp;#39;m looking for is something like gitea migrate of a repo but have it auto update with any new changes. The problem with just normal gitea mirror is it&amp;#39;s missing the  release notes and binary attachments. The same is true for  git clone --mirror xxx. When I search around for solutions I cant seem to find anything useful so I thought I&amp;#39;d post here.&lt;/p&gt;\n\n&lt;p&gt;Currently, I let gitea mirror the repo and every time I think about it I manually go though my list using firefox to take a screenshot of the page, download each binary, and stick it all in a folder of named the same as the release. finally I git push that to my selfhosted gitea under xxx_releases. I could automate this myself but I&amp;#39;m hopeful there&amp;#39;s a tool to do it for me. &lt;/p&gt;\n\n&lt;p&gt;How do you guys back this up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143c0cf", "is_robot_indexable": true, "report_reasons": null, "author": "ShitParticleInYaNose", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143c0cf/any_tools_to_mirror_githubgitlab_that_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143c0cf/any_tools_to_mirror_githubgitlab_that_will/", "subreddit_subscribers": 686458, "created_utc": 1686141199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm having issues in creating the rules or filters so that all images on a same page will go into a subfolder with the name of the webpage. As of now, when i use the &lt;jd:packagename&gt; addition, it saves every image in it's separate folder. and without it, it saves all images in the main folder without making a subfolder. Can anyone help me with the settings to get this to save correctly, i.e. all images from a single page in a single subfolder. ?", "author_fullname": "t2_1hcoykky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jdownloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14398vo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686132980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having issues in creating the rules or filters so that all images on a same page will go into a subfolder with the name of the webpage. As of now, when i use the &amp;lt;jd:packagename&amp;gt; addition, it saves every image in it&amp;#39;s separate folder. and without it, it saves all images in the main folder without making a subfolder. Can anyone help me with the settings to get this to save correctly, i.e. all images from a single page in a single subfolder. ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14398vo", "is_robot_indexable": true, "report_reasons": null, "author": "Fosterkid87", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14398vo/jdownloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14398vo/jdownloader/", "subreddit_subscribers": 686458, "created_utc": 1686132980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://github.com/hamsterbase/hamsterbase-highlighter](https://github.com/hamsterbase/hamsterbase-highlighter)  \n\n\nPresenting an open-source Chrome extension that allows you to annotate and take notes directly on web pages. The best part? Your data remains intact even after refreshing the page.\n\nThe backend is highly flexible and can be easily switched, with current support for Hamsterbase and Notion. Future updates will include support for GitHub as well.\n\nWhen you choose to save your annotations to Hamsterbase, a convenient feature automatically captures a snapshot of the web page for your reference.\n\n**Notes:**\n\n1. The extension is entirely open-source and does not rely on Hamsterbase. It will remain free and open-source for the foreseeable future.\n2. Hamsterbase is a closed-source software developed by me. It is currently available as a Docker image and a desktop app.", "author_fullname": "t2_m9f6hryu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a opensource web highlighter for notion and hamsterbase.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143hlez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686154707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/hamsterbase/hamsterbase-highlighter\"&gt;https://github.com/hamsterbase/hamsterbase-highlighter&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Presenting an open-source Chrome extension that allows you to annotate and take notes directly on web pages. The best part? Your data remains intact even after refreshing the page.&lt;/p&gt;\n\n&lt;p&gt;The backend is highly flexible and can be easily switched, with current support for Hamsterbase and Notion. Future updates will include support for GitHub as well.&lt;/p&gt;\n\n&lt;p&gt;When you choose to save your annotations to Hamsterbase, a convenient feature automatically captures a snapshot of the web page for your reference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The extension is entirely open-source and does not rely on Hamsterbase. It will remain free and open-source for the foreseeable future.&lt;/li&gt;\n&lt;li&gt;Hamsterbase is a closed-source software developed by me. It is currently available as a Docker image and a desktop app.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?auto=webp&amp;v=enabled&amp;s=2640dc46f1ae50315815ae358f9ce50e7a89341f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b526d74290187df8224354b864444bef7ddf58e5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=264bb12f61e570fb292bb8981eff1b1edf51804a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=075bb02613db0998468224d3f12b3e6a2a6915ec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d8b7abc75279a9ec1ca9e3bdd4b668a39cb0983", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a3b0dc6ca70ee29f7ecd5d9e827ffd34c1fced9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TSUC0Y5JB5RX0Z1plDtGBT-Tgcn9rGkQDCbgB03Yaus.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c5cf182f1a46c5f5f249d293a418af06791fffa", "width": 1080, "height": 540}], "variants": {}, "id": "DoA6h_gRJCQfhJ1glv3gsPpLDuFzXlaqck0HZ2RAt6c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143hlez", "is_robot_indexable": true, "report_reasons": null, "author": "HamsterBaseMaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143hlez/i_made_a_opensource_web_highlighter_for_notion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143hlez/i_made_a_opensource_web_highlighter_for_notion/", "subreddit_subscribers": 686458, "created_utc": 1686154707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Synology system with about 160TB, and back up 100tb of that to google, until recently.  Deep archive sounded like a solution until I looked at restore and download costs, so I want to take the plunge into tape.\n\nLooks like a tape library would be the best option.  Since transfer speed is not a big issue to me (anything onsite will be faster than going to the cloud), I thought a refurbished LTO-5 library would be good, but looks like I might need 3 of those to back everything up.   I could go bigger with HPE LTO-7 with library that holds 40 tapes and expandable, but before the tape cost, that is $8k.  \n\nAre there other options I have not found yet?\n\nWhat tape system are you using?", "author_fullname": "t2_tix5tw2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "moving from google to tape, there are SO many options tho.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_143oesz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686170633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synology system with about 160TB, and back up 100tb of that to google, until recently.  Deep archive sounded like a solution until I looked at restore and download costs, so I want to take the plunge into tape.&lt;/p&gt;\n\n&lt;p&gt;Looks like a tape library would be the best option.  Since transfer speed is not a big issue to me (anything onsite will be faster than going to the cloud), I thought a refurbished LTO-5 library would be good, but looks like I might need 3 of those to back everything up.   I could go bigger with HPE LTO-7 with library that holds 40 tapes and expandable, but before the tape cost, that is $8k.  &lt;/p&gt;\n\n&lt;p&gt;Are there other options I have not found yet?&lt;/p&gt;\n\n&lt;p&gt;What tape system are you using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143oesz", "is_robot_indexable": true, "report_reasons": null, "author": "webshammo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143oesz/moving_from_google_to_tape_there_are_so_many/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143oesz/moving_from_google_to_tape_there_are_so_many/", "subreddit_subscribers": 686458, "created_utc": 1686170633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'll be using it for Plex, Hoarding, Backup, and some light server tasks. Probably Ubuntu and ZFS set up so I can rebuild if I loose a disk. Mostly concerned that I may not have picked good drives for the tasks or that the MB won't handle so many disks well but open to any suggestions. \n\n[PCPartPicker Part List](https://pcpartpicker.com/list/6CZcd9)\n\nType|Item|Price\n:----|:----|:----\n**CPU** | [AMD Ryzen 5 5600G 3.9 GHz 6-Core Processor](https://pcpartpicker.com/product/sYmmP6/amd-ryzen-5-5600g-39-ghz-6-core-processor-100-100000252box) | $121.66 @ Amazon \n**Motherboard** | [\\*ASRock B550 Phantom Gaming 4 ATX AM4 Motherboard](https://pcpartpicker.com/product/4mjNnQ/asrock-b550-phantom-gaming-4-atx-am4-motherboard-b550-phantom-gaming-4) | $99.99 @ Newegg \n**Memory** | [\\*Patriot Viper Steel 16 GB (2 x 8 GB) DDR4-3733 CL17 Memory](https://pcpartpicker.com/product/hkTzK8/patriot-viper-steel-16-gb-2-x-8-gb-ddr4-3733-cl17-memory-pvs416g373c7k) | $46.99 @ Amazon \n**Storage** | [\\*Western Digital Green SN350 1 TB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/zRnypg/western-digital-green-sn350-1-tb-m2-2280-nvme-solid-state-drive-wds100t3g0c) | $40.50 @ Newegg \n**Storage** | [\\*Seagate ST4000NM0024 4 TB 3.5\" 7200 RPM Internal Hard Drive](https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024) | $53.50 @ Amazon \n**Storage** | [\\*Seagate ST4000NM0024 4 TB 3.5\" 7200 RPM Internal Hard Drive](https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024) | $53.50 @ Amazon \n**Storage** | [\\*Seagate ST4000NM0024 4 TB 3.5\" 7200 RPM Internal Hard Drive](https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024) | $53.50 @ Amazon \n**Storage** | [\\*Seagate ST4000NM0024 4 TB 3.5\" 7200 RPM Internal Hard Drive](https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024) | $53.50 @ Amazon \n**Storage** | [\\*Seagate ST4000NM0024 4 TB 3.5\" 7200 RPM Internal Hard Drive](https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024) | $53.50 @ Amazon \n**Case** | [Cooler Master N400 ATX Mid Tower Case](https://pcpartpicker.com/product/wNrG3C/cooler-master-case-nse400kkn2) | $93.98 @ Newegg \n**Power Supply** | [\\*Thermaltake Smart BM2 550 W 80+ Bronze Certified Semi-modular ATX Power Supply](https://pcpartpicker.com/product/xQpmP6/thermaltake-smart-bm2-550-w-80-bronze-certified-semi-modular-atx-power-supply-ps-spd-0550mnfabu-1) | $60.99 @ Amazon \n | *Prices include shipping, taxes, rebates, and discounts* |\n | **Total** | **$731.61**\n | \\*Lowest price parts chosen from parametric criteria |\n | Generated by [PCPartPicker](https://pcpartpicker.com) 2023-06-07 14:17 EDT-0400 |", "author_fullname": "t2_9s66x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Build advice/thoughts for future NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143kpva", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686171088.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686162041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll be using it for Plex, Hoarding, Backup, and some light server tasks. Probably Ubuntu and ZFS set up so I can rebuild if I loose a disk. Mostly concerned that I may not have picked good drives for the tasks or that the MB won&amp;#39;t handle so many disks well but open to any suggestions. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pcpartpicker.com/list/6CZcd9\"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Type&lt;/th&gt;\n&lt;th align=\"left\"&gt;Item&lt;/th&gt;\n&lt;th align=\"left\"&gt;Price&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/sYmmP6/amd-ryzen-5-5600g-39-ghz-6-core-processor-100-100000252box\"&gt;AMD Ryzen 5 5600G 3.9 GHz 6-Core Processor&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$121.66 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/4mjNnQ/asrock-b550-phantom-gaming-4-atx-am4-motherboard-b550-phantom-gaming-4\"&gt;*ASRock B550 Phantom Gaming 4 ATX AM4 Motherboard&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$99.99 @ Newegg&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/hkTzK8/patriot-viper-steel-16-gb-2-x-8-gb-ddr4-3733-cl17-memory-pvs416g373c7k\"&gt;*Patriot Viper Steel 16 GB (2 x 8 GB) DDR4-3733 CL17 Memory&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$46.99 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/zRnypg/western-digital-green-sn350-1-tb-m2-2280-nvme-solid-state-drive-wds100t3g0c\"&gt;*Western Digital Green SN350 1 TB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$40.50 @ Newegg&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024\"&gt;*Seagate ST4000NM0024 4 TB 3.5&amp;quot; 7200 RPM Internal Hard Drive&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$53.50 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024\"&gt;*Seagate ST4000NM0024 4 TB 3.5&amp;quot; 7200 RPM Internal Hard Drive&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$53.50 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024\"&gt;*Seagate ST4000NM0024 4 TB 3.5&amp;quot; 7200 RPM Internal Hard Drive&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$53.50 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024\"&gt;*Seagate ST4000NM0024 4 TB 3.5&amp;quot; 7200 RPM Internal Hard Drive&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$53.50 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/cFw323/seagate-internal-hard-drive-st4000nm0024\"&gt;*Seagate ST4000NM0024 4 TB 3.5&amp;quot; 7200 RPM Internal Hard Drive&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$53.50 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/wNrG3C/cooler-master-case-nse400kkn2\"&gt;Cooler Master N400 ATX Mid Tower Case&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$93.98 @ Newegg&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Power Supply&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://pcpartpicker.com/product/xQpmP6/thermaltake-smart-bm2-550-w-80-bronze-certified-semi-modular-atx-power-supply-ps-spd-0550mnfabu-1\"&gt;*Thermaltake Smart BM2 550 W 80+ Bronze Certified Semi-modular ATX Power Supply&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$60.99 @ Amazon&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;$731.61&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;*Lowest price parts chosen from parametric criteria&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Generated by &lt;a href=\"https://pcpartpicker.com\"&gt;PCPartPicker&lt;/a&gt; 2023-06-07 14:17 EDT-0400&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143kpva", "is_robot_indexable": true, "report_reasons": null, "author": "xaocon", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143kpva/build_advicethoughts_for_future_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143kpva/build_advicethoughts_for_future_nas/", "subreddit_subscribers": 686458, "created_utc": 1686162041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi data hoarders, since I find this community really useful and inspiring, I want to give you something back. Hope that this will help someone\n\nI know I'm coming late, but I am writing a tool that does not use APIs. **I  am not encouraging using it, as I am not aware of the consequences that  twitter could apply to you and/or your account in case you run the code**, or even if it is legally allowed to run such automation.  \n**Read carefully the readme** as there are some settings (such as the theme of the browser) **required** to work.  \nI  am just writing some code to make people aware about how this world  works, and it may not be perfect. If you guys want to bring some  contribution, or leave a star if you find it useful, feel free to have a  look on github (**apologize in advance if it is not perfect, e.g., for now it works only with dark theme of chromium ... I wrote it in some spare hours I had**). For sure, out there you may find something better, but anyway, here it is the link:\n\n[https://github.com/alessandriniluca/postget](https://github.com/alessandriniluca/postget)\n\nAgain,  sorry for the code. The code, and also the tool, are not perfect, let me  stress that you are free to both contribute with pull requests and/or open issues in order to improve it!\n\nHoping this will help someone!", "author_fullname": "t2_3hwuhm84", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collecting data from Twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14369ll", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scraping Code", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686122194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi data hoarders, since I find this community really useful and inspiring, I want to give you something back. Hope that this will help someone&lt;/p&gt;\n\n&lt;p&gt;I know I&amp;#39;m coming late, but I am writing a tool that does not use APIs. &lt;strong&gt;I  am not encouraging using it, as I am not aware of the consequences that  twitter could apply to you and/or your account in case you run the code&lt;/strong&gt;, or even if it is legally allowed to run such automation.&lt;br/&gt;\n&lt;strong&gt;Read carefully the readme&lt;/strong&gt; as there are some settings (such as the theme of the browser) &lt;strong&gt;required&lt;/strong&gt; to work.&lt;br/&gt;\nI  am just writing some code to make people aware about how this world  works, and it may not be perfect. If you guys want to bring some  contribution, or leave a star if you find it useful, feel free to have a  look on github (&lt;strong&gt;apologize in advance if it is not perfect, e.g., for now it works only with dark theme of chromium ... I wrote it in some spare hours I had&lt;/strong&gt;). For sure, out there you may find something better, but anyway, here it is the link:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/alessandriniluca/postget\"&gt;https://github.com/alessandriniluca/postget&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Again,  sorry for the code. The code, and also the tool, are not perfect, let me  stress that you are free to both contribute with pull requests and/or open issues in order to improve it!&lt;/p&gt;\n\n&lt;p&gt;Hoping this will help someone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?auto=webp&amp;v=enabled&amp;s=13f88f88fa74c033d43d7876cf86dedf085cd268", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f541b6d79a46636f137776486441f3b6e1dde18", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c79b8b1a1e528f0f7e8fa3f1a383e9c1f206d416", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e526356931ca87d132ca5e5cfb63899114b89dcc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a4122b3900beb6b583079efef692d24afdc2a88", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46263c4e43578d3beeb523407a2c01c2d24cddc1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/zr3BlrQvf4qTWw8Kky32sP7H0zoSFyqM7ttkYSyjEQw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa829bad51da5223f5e285097b9147dbb646c14c", "width": 1080, "height": 540}], "variants": {}, "id": "WURKifmQCGyNKgafaeMq_8z0ipxqnYA3Rs304W5ax84"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14369ll", "is_robot_indexable": true, "report_reasons": null, "author": "Landomix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14369ll/collecting_data_from_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14369ll/collecting_data_from_twitter/", "subreddit_subscribers": 686458, "created_utc": 1686122194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Out of these choices, which would you choose? Is Syncovery deliver value over and above the free alternatives I listed? \n\nI am looking for a fully local backup solution which I can set up and then forget until something happens. Would like it to encrypt the backups and increments inside of a .rar or .7z archive with full checksumming. Since it's not a backup strategy if you can't easily restore, I want to be able to access the files without needing to use the local backup utility. \n\nPlease do not recommend any others if it's not compatible with Windows, MacOS and Linux. \n\nI am thinking of using SyncThing to transfer the backups files around so I don't have another thing running in the background on my main PC. The most recent backups would be stored locally on each PC in a dedicated partition of equal size to the main one and my main PC would backup all the archive files online to Backblaze.", "author_fullname": "t2_8hjaccg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Before I deploy to several computers: UrBackup, Bacula, Duplicati or Syncovery (paid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142whfe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686093121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Out of these choices, which would you choose? Is Syncovery deliver value over and above the free alternatives I listed? &lt;/p&gt;\n\n&lt;p&gt;I am looking for a fully local backup solution which I can set up and then forget until something happens. Would like it to encrypt the backups and increments inside of a .rar or .7z archive with full checksumming. Since it&amp;#39;s not a backup strategy if you can&amp;#39;t easily restore, I want to be able to access the files without needing to use the local backup utility. &lt;/p&gt;\n\n&lt;p&gt;Please do not recommend any others if it&amp;#39;s not compatible with Windows, MacOS and Linux. &lt;/p&gt;\n\n&lt;p&gt;I am thinking of using SyncThing to transfer the backups files around so I don&amp;#39;t have another thing running in the background on my main PC. The most recent backups would be stored locally on each PC in a dedicated partition of equal size to the main one and my main PC would backup all the archive files online to Backblaze.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142whfe", "is_robot_indexable": true, "report_reasons": null, "author": "LieVirus", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142whfe/before_i_deploy_to_several_computers_urbackup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142whfe/before_i_deploy_to_several_computers_urbackup/", "subreddit_subscribers": 686458, "created_utc": 1686093121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of data I'm going to back up on a hard drive and store off-site in a deposit box. It's a variety of data: Binary, video, pictures, text, it's all over the place.\n\nI know NTFS supports compression natively, but is it the best way to shove a bunch of data onto a disk? Would it be better to do something like 7-Zip at some extreme compression level? Would it be better to do both NTFS compression AND 7-Zip? Is there a better filesystem and/or compression software I should be using?", "author_fullname": "t2_92oly", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to best configure an HDD for cold storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143b901", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686139143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of data I&amp;#39;m going to back up on a hard drive and store off-site in a deposit box. It&amp;#39;s a variety of data: Binary, video, pictures, text, it&amp;#39;s all over the place.&lt;/p&gt;\n\n&lt;p&gt;I know NTFS supports compression natively, but is it the best way to shove a bunch of data onto a disk? Would it be better to do something like 7-Zip at some extreme compression level? Would it be better to do both NTFS compression AND 7-Zip? Is there a better filesystem and/or compression software I should be using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143b901", "is_robot_indexable": true, "report_reasons": null, "author": "dwkindig", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143b901/how_to_best_configure_an_hdd_for_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143b901/how_to_best_configure_an_hdd_for_cold_storage/", "subreddit_subscribers": 686458, "created_utc": 1686139143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to extract an ipa file from an app on a friend's jailbroken iPhone to archive the app (it's no longer on the app store). I plan to share the ipa with an internet community I know will appreciate it. Is there any risk to my friend in doing this? Will his Apple ID be linked to the ipa file in any way, and if so could his account somehow be compromised? TIA", "author_fullname": "t2_6dsylxrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any risk to the original app owner in extracting a decrypted ipa?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1434hzc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686116120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to extract an ipa file from an app on a friend&amp;#39;s jailbroken iPhone to archive the app (it&amp;#39;s no longer on the app store). I plan to share the ipa with an internet community I know will appreciate it. Is there any risk to my friend in doing this? Will his Apple ID be linked to the ipa file in any way, and if so could his account somehow be compromised? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1434hzc", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished-Arm4538", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1434hzc/is_there_any_risk_to_the_original_app_owner_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1434hzc/is_there_any_risk_to_the_original_app_owner_in/", "subreddit_subscribers": 686458, "created_utc": 1686116120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been \"half assing\" for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.\n\nZero issues so far, and I don't want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn't have to be on all the time, backups, etc.\n\nBest way to accomplish the following?\n\n1. Expandable Storage.\n2. Backups.\n3. No need for PC to run.\n4. Shield able to connect via Plex.\n5. Can connect to the device on my PC to manage media.\n6. Able to stream 100mbs movies without any issues.\n\nCurrent plan:\n\n* [Synology DS1522+](https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;psc=1)\n* x2 - [Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache](https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;psc=1)\n\nIs that a good way to accomplish it?", "author_fullname": "t2_ia3wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning on my first NAS set up. Thoughts on the parts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142v8fm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686090096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been &amp;quot;half assing&amp;quot; for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.&lt;/p&gt;\n\n&lt;p&gt;Zero issues so far, and I don&amp;#39;t want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn&amp;#39;t have to be on all the time, backups, etc.&lt;/p&gt;\n\n&lt;p&gt;Best way to accomplish the following?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Expandable Storage.&lt;/li&gt;\n&lt;li&gt;Backups.&lt;/li&gt;\n&lt;li&gt;No need for PC to run.&lt;/li&gt;\n&lt;li&gt;Shield able to connect via Plex.&lt;/li&gt;\n&lt;li&gt;Can connect to the device on my PC to manage media.&lt;/li&gt;\n&lt;li&gt;Able to stream 100mbs movies without any issues.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Current plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Synology DS1522+&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;x2 - &lt;a href=\"https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is that a good way to accomplish it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142v8fm", "is_robot_indexable": true, "report_reasons": null, "author": "nsoifer", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "subreddit_subscribers": 686458, "created_utc": 1686090096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ordered a new HC550 and it came in a bag like this.ive never seen a Uline bag used from WD. I did call support but they suggested I return it because they were unsure.", "author_fullname": "t2_a33jxmmgw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anybody ever gotten a WD ultrastar in this type of bag?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_143ppfk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cNyaexzkrOdcNu9c9lAvszg6cDX_rwQxvZ7JfGqEnYg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686173544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ordered a new HC550 and it came in a bag like this.ive never seen a Uline bag used from WD. I did call support but they suggested I return it because they were unsure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/GGyq3wV.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?auto=webp&amp;v=enabled&amp;s=f5c3ead7646be4336ee41024d5a78f9e547261da", "width": 3072, "height": 4080}, "resolutions": [{"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a96e342914845b19a08d16d897139d2989345d5f", "width": 108, "height": 143}, {"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbb8263fe5d0698a52f69a744f9feeabd0bbc974", "width": 216, "height": 286}, {"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56612b7ae05c070b30490c93b23059f1426585fa", "width": 320, "height": 425}, {"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77d743fdfa64152dc7667ce9bd1acc265ce1346e", "width": 640, "height": 850}, {"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92893c0073301ec073f663e2e0c3338bc38c4a80", "width": 960, "height": 1275}, {"url": "https://external-preview.redd.it/bzi_6CmNEHopLuzz97FhM5PU259B_vfMBLKWoHiBjK8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1be19d4c1c6c19751ebae3379ff49ea4c624b05", "width": 1080, "height": 1434}], "variants": {}, "id": "moOO_ZC05g2J1aQCOzY0v0uZzYBC6IjyWrIwP0CLxyY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143ppfk", "is_robot_indexable": true, "report_reasons": null, "author": "Overnightboat", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143ppfk/has_anybody_ever_gotten_a_wd_ultrastar_in_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/GGyq3wV.jpg", "subreddit_subscribers": 686458, "created_utc": 1686173544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't have a NAS. My Plex setup is simply my 5TB external seagate plugged to my laptop. My 5TB drive is full, so I'm looking to expand my storage. I know that all drives will fail eventually, but I still want the best I can get. I was originally looking for another external HDD (around 10-15 TB), but then I started reading more about them and how the drive that's inside them isn't usually the best. So I'm starting to think that it'd be better to get something like a 12TB WD gold or red pro, and put it in an enclosure, for close to the same price (compared to the external hdd).\n\nWhat do you guys think? Also if getting an internal HDD with an enclosure is better, what would be the \"best\" HDD? I'm mainly looking at the WD red pro (they're supposedly made for NAS in mind, whatever that means, and are quiet), the WD gold (supposedly the best WD has, but loud), or something from seagate. But I'm a bit confused by the seagate naming convention of their drives. They have their X and E series which seem to be their best (?), or I could get an ironwolf, but it's apparently a lot louder than the wd red pro. I also don't know how much noise is too noisy. My 5TB seagate external makes some noise but I need to focus on it to hear it. But I'm not sure how much louder a WD gold is.\n\nMoney isn't too much an issue, and I don't care about read or write speed, or any other metrics (except longevity). 95% of my data are .mkv files that will be written once, and only read after with plex, and the remaining 5% are photos, documents, and game setup files (not the actual game, just the files for the setup, the game installation will be on another drive) that will also be written once, and rarely read after.", "author_fullname": "t2_4bxhznee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD vs internal HDD with enclosure mainly for plex.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143mb6t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686165782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t have a NAS. My Plex setup is simply my 5TB external seagate plugged to my laptop. My 5TB drive is full, so I&amp;#39;m looking to expand my storage. I know that all drives will fail eventually, but I still want the best I can get. I was originally looking for another external HDD (around 10-15 TB), but then I started reading more about them and how the drive that&amp;#39;s inside them isn&amp;#39;t usually the best. So I&amp;#39;m starting to think that it&amp;#39;d be better to get something like a 12TB WD gold or red pro, and put it in an enclosure, for close to the same price (compared to the external hdd).&lt;/p&gt;\n\n&lt;p&gt;What do you guys think? Also if getting an internal HDD with an enclosure is better, what would be the &amp;quot;best&amp;quot; HDD? I&amp;#39;m mainly looking at the WD red pro (they&amp;#39;re supposedly made for NAS in mind, whatever that means, and are quiet), the WD gold (supposedly the best WD has, but loud), or something from seagate. But I&amp;#39;m a bit confused by the seagate naming convention of their drives. They have their X and E series which seem to be their best (?), or I could get an ironwolf, but it&amp;#39;s apparently a lot louder than the wd red pro. I also don&amp;#39;t know how much noise is too noisy. My 5TB seagate external makes some noise but I need to focus on it to hear it. But I&amp;#39;m not sure how much louder a WD gold is.&lt;/p&gt;\n\n&lt;p&gt;Money isn&amp;#39;t too much an issue, and I don&amp;#39;t care about read or write speed, or any other metrics (except longevity). 95% of my data are .mkv files that will be written once, and only read after with plex, and the remaining 5% are photos, documents, and game setup files (not the actual game, just the files for the setup, the game installation will be on another drive) that will also be written once, and rarely read after.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143mb6t", "is_robot_indexable": true, "report_reasons": null, "author": "MoonlessNightss", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143mb6t/external_hdd_vs_internal_hdd_with_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143mb6t/external_hdd_vs_internal_hdd_with_enclosure/", "subreddit_subscribers": 686458, "created_utc": 1686165782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I made a Mobage account yesterday so I could try to preserve Mobage games. Posting because I need some help working out how to go about preserving these games.\n\nThe games are all hosted in different places, but one common thing would be all use server-side code and all of the images don't need cookies or a mobile user agent to mirror\n\nMost of the games I've tried need mobage login cookies and all need an Android user agent\n\nI'm just kind of overwhelmed on where to start here", "author_fullname": "t2_d9gcu53d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "archiving Mobage games", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_143dxvc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686146077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a Mobage account yesterday so I could try to preserve Mobage games. Posting because I need some help working out how to go about preserving these games.&lt;/p&gt;\n\n&lt;p&gt;The games are all hosted in different places, but one common thing would be all use server-side code and all of the images don&amp;#39;t need cookies or a mobile user agent to mirror&lt;/p&gt;\n\n&lt;p&gt;Most of the games I&amp;#39;ve tried need mobage login cookies and all need an Android user agent&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just kind of overwhelmed on where to start here&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "143dxvc", "is_robot_indexable": true, "report_reasons": null, "author": "gosc_reddit", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/143dxvc/archiving_mobage_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/143dxvc/archiving_mobage_games/", "subreddit_subscribers": 686458, "created_utc": 1686146077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There\u2019s a really good amazon reviewer of history books that I want to save a copy of all of his reviews, especially because Amazon tends to delete reviews now and then by different reviewers (for some reason I don\u2019t know). And this guy has a lot of reviews that I have to scroll down, click, and copy and paste. Would take too much time and heat up my computer cpu.", "author_fullname": "t2_y7ufo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all reviews by a specific Amazon reviewer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1435gs7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686119399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There\u2019s a really good amazon reviewer of history books that I want to save a copy of all of his reviews, especially because Amazon tends to delete reviews now and then by different reviewers (for some reason I don\u2019t know). And this guy has a lot of reviews that I have to scroll down, click, and copy and paste. Would take too much time and heat up my computer cpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1435gs7", "is_robot_indexable": true, "report_reasons": null, "author": "Saphsin", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1435gs7/how_to_download_all_reviews_by_a_specific_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1435gs7/how_to_download_all_reviews_by_a_specific_amazon/", "subreddit_subscribers": 686458, "created_utc": 1686119399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many Redditors reported their failure cases on SanDisk Extreme Portable SSDs, which caused a lot of problems, including data loss. The related discussion can be found below: \n\n[https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer\\_beware\\_some\\_sandisk\\_extreme\\_ssds\\_are\\_wiping/](https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/)\n\n[https://www.reddit.com/r/editors/comments/10syawa/a\\_warning\\_about\\_sandisk\\_extreme\\_pro\\_ssds/](https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/)\u00a0\n\nWestern Digital has released the claimed firmware for Windows, but not yet for Mac\u00a0  \n**Before the firmware update, it's strongly suggested to back up the SSD first.**\u00a0\n\nHere are the official update on this issue-\n\n[https://support-en.wd.com/app/firmwareupdate](https://support-en.wd.com/app/firmwareupdate) \n\n[https://www.youtube.com/watch?v=tvxHuTUS9is](https://www.youtube.com/watch?v=tvxHuTUS9is)\n\n  \n**Based on this event, EaseUS is working to offer data recovery help (offering data recovery software or online services for free)**, aiming to assist\u00a0SanDisk Extreme Portable SSD users to find their important data back while taking the chance to know better about the data loss caused by SSD crush.\u00a0  \nDetail info can be found on this page -\u00a0[https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html](https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html). It's not complicated. Just send an email to [redepmtion@easeus.com](mailto:redepmtion@easeus.com) with a basic description of the case you experienced. \n\n  \nWith the release of the firmware, there are many more Mac users who came to us for data recovery help, which seems that the firmware works well for Windows. Just back up the data before the update. And with this offer, EaseUS is also offering EaseUS Todo Backup Home for free. Check the page if you need it.\u00a0", "author_fullname": "t2_a0iii5xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Info Sync - Data Recovery Help for SanDisk Extreme SSD Failure is Offered by EaseUS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1431wj8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686108067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many Redditors reported their failure cases on SanDisk Extreme Portable SSDs, which caused a lot of problems, including data loss. The related discussion can be found below: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/\"&gt;https://www.reddit.com/r/editors/comments/10syawa/a_warning_about_sandisk_extreme_pro_ssds/&lt;/a&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;Western Digital has released the claimed firmware for Windows, but not yet for Mac\u00a0&lt;br/&gt;\n&lt;strong&gt;Before the firmware update, it&amp;#39;s strongly suggested to back up the SSD first.&lt;/strong&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;Here are the official update on this issue-&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://support-en.wd.com/app/firmwareupdate\"&gt;https://support-en.wd.com/app/firmwareupdate&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=tvxHuTUS9is\"&gt;https://www.youtube.com/watch?v=tvxHuTUS9is&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Based on this event, EaseUS is working to offer data recovery help (offering data recovery software or online services for free)&lt;/strong&gt;, aiming to assist\u00a0SanDisk Extreme Portable SSD users to find their important data back while taking the chance to know better about the data loss caused by SSD crush.\u00a0&lt;br/&gt;\nDetail info can be found on this page -\u00a0&lt;a href=\"https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html\"&gt;https://www.easeus.com/computer-instruction/tips-for-recovering-your-data-on-broken-sandisk-extreme-ssds.html&lt;/a&gt;. It&amp;#39;s not complicated. Just send an email to [&lt;a href=\"mailto:redepmtion@easeus.com\"&gt;redepmtion@easeus.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:redepmtion@easeus.com\"&gt;redepmtion@easeus.com&lt;/a&gt;) with a basic description of the case you experienced. &lt;/p&gt;\n\n&lt;p&gt;With the release of the firmware, there are many more Mac users who came to us for data recovery help, which seems that the firmware works well for Windows. Just back up the data before the update. And with this offer, EaseUS is also offering EaseUS Todo Backup Home for free. Check the page if you need it.\u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1431wj8", "is_robot_indexable": true, "report_reasons": null, "author": "Adaaaaaaaaaaaaaaaaa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1431wj8/info_sync_data_recovery_help_for_sandisk_extreme/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1431wj8/info_sync_data_recovery_help_for_sandisk_extreme/", "subreddit_subscribers": 686458, "created_utc": 1686108067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently my little pile of data is hosted on my network in an ancient Synology 413J along with 4x 6TB drives. Capacity is fine, but it\u2019s unable to serve data at speeds over 30mbps despite gigabit ports, gigabit switch, router, etc. \n\nI\u2019ve enjoyed synology. But end of the day what I want to lowest cost highest performance along with the most flexibility. \n\nTo that end, I\u2019m considering an 8th Gen HP Microserver to install either SnapRAID (which appears to meet my needs) or TrueNAS. \n\nMy question is, have any of you played with the microserver? Will the celeron model be sufficient for serving data at higher speeds than what I\u2019m seeing currently?\n\nI have two proxmox servers sharing ISO directory and another directory for VM backups, as well as Plex who\u2019s library is on the NAS, usually serving 1-2 streams. Plus various other backups, etc, but those are more copy to storage and forget about it. \n\nWhat are your thought? Or other ideas in the $300-400 range with at least 4 3.5 inch bays and as quiet as possible", "author_fullname": "t2_5abuxk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology 413j vs HP Microservwr 8thGen", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14311e0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686105607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently my little pile of data is hosted on my network in an ancient Synology 413J along with 4x 6TB drives. Capacity is fine, but it\u2019s unable to serve data at speeds over 30mbps despite gigabit ports, gigabit switch, router, etc. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve enjoyed synology. But end of the day what I want to lowest cost highest performance along with the most flexibility. &lt;/p&gt;\n\n&lt;p&gt;To that end, I\u2019m considering an 8th Gen HP Microserver to install either SnapRAID (which appears to meet my needs) or TrueNAS. &lt;/p&gt;\n\n&lt;p&gt;My question is, have any of you played with the microserver? Will the celeron model be sufficient for serving data at higher speeds than what I\u2019m seeing currently?&lt;/p&gt;\n\n&lt;p&gt;I have two proxmox servers sharing ISO directory and another directory for VM backups, as well as Plex who\u2019s library is on the NAS, usually serving 1-2 streams. Plus various other backups, etc, but those are more copy to storage and forget about it. &lt;/p&gt;\n\n&lt;p&gt;What are your thought? Or other ideas in the $300-400 range with at least 4 3.5 inch bays and as quiet as possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14311e0", "is_robot_indexable": true, "report_reasons": null, "author": "AdhessiveBaker", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14311e0/synology_413j_vs_hp_microservwr_8thgen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14311e0/synology_413j_vs_hp_microservwr_8thgen/", "subreddit_subscribers": 686458, "created_utc": 1686105607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Salutations, Data Hoarders.\n\nI\u2019ve been lurking here for a while and my current storage solution is on the lacking side. As I was looking into preserving media I like I ran into the reality that in my quest to secure all of the stuff I remembered And anything new I was interested in that three external 4 terabyte HDD drives shrunk down to about 800 gigs sooner than I thought.\n\nI thought it would last me about four years before I ran into any serious limitations of remaining space, so because of underestimating the size of video files, here I am, asking for help on finding an ideal DAS and drives to fill it with.\n\n**With all of that said, this would be the planned use case:**\n\n&amp;#x200B;\n\n* I\u2019m not going to run the DAS 24/7/365. I would only power it up when I need to either add to or retrieve something from it.  \n \n* It needs to be fairly portable because physical space is a concern.  \n \n* Not looking into RAID.  \n \n* I would like the option to only turn on one of the drives in the at a time when I \tneed to retrieve something (no sense in spinning up all the drives for pulling something off one, especially since they\u2019d all have the exact same things on it since all the drives are a backup).  \n \n* I\u2019m not looking to do wholesale system backups, just to preservation of individual files that matter to me.  \n \n* The DAS would house no less than three but no more than four.  \n \n* As this would be pretty expensive, I would like to get HDDs robust enough to last me ideally 10 years. Would going for enterprise drives help with this noticeably or am I just making a fools wish? You know, buy it nice or buy it twice.  \n \n* As I want to keep this for as long as possible, the drives would have storage of no less than 12 TB.  \n \n\nAs this would be my first DAS, it would be grateful if any of you can impart any advice or wisdom you wish you had when setting one of these up yourselves as well as any pitfalls that I should be aware of.\n\nThank you in advance.", "author_fullname": "t2_16a6im", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking to Setup My First DAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1430ceo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Seeking Setup Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686103676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Salutations, Data Hoarders.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been lurking here for a while and my current storage solution is on the lacking side. As I was looking into preserving media I like I ran into the reality that in my quest to secure all of the stuff I remembered And anything new I was interested in that three external 4 terabyte HDD drives shrunk down to about 800 gigs sooner than I thought.&lt;/p&gt;\n\n&lt;p&gt;I thought it would last me about four years before I ran into any serious limitations of remaining space, so because of underestimating the size of video files, here I am, asking for help on finding an ideal DAS and drives to fill it with.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;With all of that said, this would be the planned use case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I\u2019m not going to run the DAS 24/7/365. I would only power it up when I need to either add to or retrieve something from it.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It needs to be fairly portable because physical space is a concern.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Not looking into RAID.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would like the option to only turn on one of the drives in the at a time when I     need to retrieve something (no sense in spinning up all the drives for pulling something off one, especially since they\u2019d all have the exact same things on it since all the drives are a backup).  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I\u2019m not looking to do wholesale system backups, just to preservation of individual files that matter to me.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The DAS would house no less than three but no more than four.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;As this would be pretty expensive, I would like to get HDDs robust enough to last me ideally 10 years. Would going for enterprise drives help with this noticeably or am I just making a fools wish? You know, buy it nice or buy it twice.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;As I want to keep this for as long as possible, the drives would have storage of no less than 12 TB.  &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As this would be my first DAS, it would be grateful if any of you can impart any advice or wisdom you wish you had when setting one of these up yourselves as well as any pitfalls that I should be aware of.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1430ceo", "is_robot_indexable": true, "report_reasons": null, "author": "Jesse_Graves", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1430ceo/seeking_to_setup_my_first_das/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1430ceo/seeking_to_setup_my_first_das/", "subreddit_subscribers": 686458, "created_utc": 1686103676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought 4xWD HC560 20TB drives from a well-known european e-shop.\n\nI checked the warranty of the drives on the official WD website with the serial number and I found out they are OEM drives, not retail ones (PDQ Drive ASM OEM-STD) and therefore \"No limited warranty\" is applied (Product was originally sold to a system manufacturer. Please contact the system manufacturer or the place of purchase for warranty service).\n\nObviously the e-shop is not a system manufacturer and it may have done some sort of dropshipping with my money to a distributor that sells OEM drives to manufacturer brands (Dell, Asus and such) as the benefits are higher this way.\n\nI guess I am out of the 5 year limited warranty from WD as an end-user, but at least I should be getting the 2 year warranty from electronic goods bought in the EU.\n\nIs this normal? Getting OEM drives (2 year warranty) as an end-user buying from an e-shop which should be sending retail ones (5 year warranty)?\n\nThanks in advance.", "author_fullname": "t2_14jgu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought 4x20TB drives from e-shop, got sent OEM ones (2y warranty instead of 5y)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142w60a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686092352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought 4xWD HC560 20TB drives from a well-known european e-shop.&lt;/p&gt;\n\n&lt;p&gt;I checked the warranty of the drives on the official WD website with the serial number and I found out they are OEM drives, not retail ones (PDQ Drive ASM OEM-STD) and therefore &amp;quot;No limited warranty&amp;quot; is applied (Product was originally sold to a system manufacturer. Please contact the system manufacturer or the place of purchase for warranty service).&lt;/p&gt;\n\n&lt;p&gt;Obviously the e-shop is not a system manufacturer and it may have done some sort of dropshipping with my money to a distributor that sells OEM drives to manufacturer brands (Dell, Asus and such) as the benefits are higher this way.&lt;/p&gt;\n\n&lt;p&gt;I guess I am out of the 5 year limited warranty from WD as an end-user, but at least I should be getting the 2 year warranty from electronic goods bought in the EU.&lt;/p&gt;\n\n&lt;p&gt;Is this normal? Getting OEM drives (2 year warranty) as an end-user buying from an e-shop which should be sending retail ones (5 year warranty)?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142w60a", "is_robot_indexable": true, "report_reasons": null, "author": "jfromeo", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142w60a/bought_4x20tb_drives_from_eshop_got_sent_oem_ones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142w60a/bought_4x20tb_drives_from_eshop_got_sent_oem_ones/", "subreddit_subscribers": 686458, "created_utc": 1686092352.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}