{"kind": "Listing", "data": {"after": "t3_141zgm0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. [So far, we have archived 10.81 billion links, with 150 million to go](https://tracker.archiveteam.org/reddit/).\n\nRecent news of the [Reddit API cost changes](https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/) will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone *indefinitely* unless this issue is resolved.\n\n#Here is how you can help:\n\n### [Choose the \"host\" that matches your current PC, probably Windows or macOS](https://www.virtualbox.org/wiki/Downloads)\n\n### [Download ArchiveTeam Warrior](https://tracker.archiveteam.org/)\n\n1. In VirtualBox, click File &gt; Import Appliance and open the file.\n2. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.\n\nOnce you\u2019ve started your warrior:\n\n1. Go to http://localhost:8001/ and check the Settings page.\n2. Choose a username \u2014 we\u2019ll show your progress on the leaderboard.\n3. Go to the \"All projects\" tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Reddit).\n\n### Alternative Method: Docker\n#### **[Download Docker on your \"host\" (Windows, macOS, Linux)](https://docs.docker.com/get-docker/)**\n### **[Follow the instructions on the ArchiveTeam website to set up Docker](https://wiki.archiveteam.org/index.php/Running_Archive_Team_Projects_with_Docker)**\n\nWhen setting up the project container, it will ask you to enter this command:\n\n```docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username]```\n\nMake sure to replace the [image address] with the Reddit project address (removing brackets): ```atdr.meo.ws/archiveteam/reddit-grab```\nAlso change the [username] to whatever you'd like, no need to register for anything.\n\n####More information about running this project:\n\n**[Information about setting up the project](https://github.com/ArchiveTeam/reddit-grab)**\n\n**[ArchiveTeam Wiki page on the Reddit project](https://wiki.archiveteam.org/index.php?title=Reddit)**\n\n##**IMPORTANT: Do NOT modify scripts or the Warrior client!**", "author_fullname": "t2_ucb1wzy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142l1i0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 749, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 749, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686068100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. &lt;a href=\"https://tracker.archiveteam.org/reddit/\"&gt;So far, we have archived 10.81 billion links, with 150 million to go&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Recent news of the &lt;a href=\"https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/\"&gt;Reddit API cost changes&lt;/a&gt; will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone &lt;em&gt;indefinitely&lt;/em&gt; unless this issue is resolved.&lt;/p&gt;\n\n&lt;h1&gt;Here is how you can help:&lt;/h1&gt;\n\n&lt;h3&gt;&lt;a href=\"https://www.virtualbox.org/wiki/Downloads\"&gt;Choose the &amp;quot;host&amp;quot; that matches your current PC, probably Windows or macOS&lt;/a&gt;&lt;/h3&gt;\n\n&lt;h3&gt;&lt;a href=\"https://tracker.archiveteam.org/\"&gt;Download ArchiveTeam Warrior&lt;/a&gt;&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In VirtualBox, click File &amp;gt; Import Appliance and open the file.&lt;/li&gt;\n&lt;li&gt;Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you\u2019ve started your warrior:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to http://localhost:8001/ and check the Settings page.&lt;/li&gt;\n&lt;li&gt;Choose a username \u2014 we\u2019ll show your progress on the leaderboard.&lt;/li&gt;\n&lt;li&gt;Go to the &amp;quot;All projects&amp;quot; tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Reddit).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Alternative Method: Docker&lt;/h3&gt;\n\n&lt;h4&gt;&lt;strong&gt;&lt;a href=\"https://docs.docker.com/get-docker/\"&gt;Download Docker on your &amp;quot;host&amp;quot; (Windows, macOS, Linux)&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt;\n\n&lt;h3&gt;&lt;strong&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php/Running_Archive_Team_Projects_with_Docker\"&gt;Follow the instructions on the ArchiveTeam website to set up Docker&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;p&gt;When setting up the project container, it will ask you to enter this command:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Make sure to replace the [image address] with the Reddit project address (removing brackets): &lt;code&gt;atdr.meo.ws/archiveteam/reddit-grab&lt;/code&gt;\nAlso change the [username] to whatever you&amp;#39;d like, no need to register for anything.&lt;/p&gt;\n\n&lt;h4&gt;More information about running this project:&lt;/h4&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://github.com/ArchiveTeam/reddit-grab\"&gt;Information about setting up the project&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php?title=Reddit\"&gt;ArchiveTeam Wiki page on the Reddit project&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h2&gt;&lt;strong&gt;IMPORTANT: Do NOT modify scripts or the Warrior client!&lt;/strong&gt;&lt;/h2&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142l1i0", "is_robot_indexable": true, "report_reasons": null, "author": "BananaBus43", "discussion_type": null, "num_comments": 105, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/", "subreddit_subscribers": 686329, "created_utc": 1686068100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are several things I would like to download from Reddit before they kill off API access: \n\n- Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. \n\n- Every single post I have upvoted or saved, if possible. \n\n- Specific subreddits, particularly /r/HFY. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. \n\nWhat are the best tools to do this with, saving as much metadata as possible in a machine-readable format? \n\nAny other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.", "author_fullname": "t2_qp8d2", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Best tools for downloading Reddit before API access is cut off?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1423a1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 319, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 319, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are several things I would like to download from Reddit before they kill off API access: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Every single post I have upvoted or saved, if possible. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Specific subreddits, particularly &lt;a href=\"/r/HFY\"&gt;/r/HFY&lt;/a&gt;. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are the best tools to do this with, saving as much metadata as possible in a machine-readable format? &lt;/p&gt;\n\n&lt;p&gt;Any other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "11TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1423a1q", "is_robot_indexable": true, "report_reasons": null, "author": "happysmash27", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "subreddit_subscribers": 686329, "created_utc": 1686024982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nWith the recent news about RARBG going down, and us being saved by some archivist's scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don't we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.", "author_fullname": "t2_ctq800ysv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring torrent sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424g6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent news about RARBG going down, and us being saved by some archivist&amp;#39;s scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don&amp;#39;t we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424g6h", "is_robot_indexable": true, "report_reasons": null, "author": "Busy-Paramedic-4994", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "subreddit_subscribers": 686329, "created_utc": 1686028010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey so big question:\n\nLet's say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.", "author_fullname": "t2_2alh77oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presentation of Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142cub8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686050646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so big question:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "10TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142cub8", "is_robot_indexable": true, "report_reasons": null, "author": "Zaxoosh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142cub8/presentation_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142cub8/presentation_of_data/", "subreddit_subscribers": 686329, "created_utc": 1686050646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It looks like The Eye does have it up until December 2022?  But I have important work from this year that I am trying to preserve (such as my definitions of [love](https://old.reddit.com/r/AbuseInterrupted/comments/13k3yq2/the_definition_of_love/) and [abuse](https://old.reddit.com/r/AbuseInterrupted/comments/13k597w/definition_of_abuse/), for example).\n\nI did also do the [Reddit Data Request](https://www.reddit.com/settings/data-request).\n\nI'm sorry to come in here like an idiot, but I have zero programming ability and am now having to figure out how to migrate this over to a website because people depend on these resources, and I also don't want to lose over a decade of my work.\n\nI looks like most people here are trying to save their personal Reddit history, but I am more focused on the subreddit.", "author_fullname": "t2_7djla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to archive/clone/migrate a subreddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142qq0l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686079715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like The Eye does have it up until December 2022?  But I have important work from this year that I am trying to preserve (such as my definitions of &lt;a href=\"https://old.reddit.com/r/AbuseInterrupted/comments/13k3yq2/the_definition_of_love/\"&gt;love&lt;/a&gt; and &lt;a href=\"https://old.reddit.com/r/AbuseInterrupted/comments/13k597w/definition_of_abuse/\"&gt;abuse&lt;/a&gt;, for example).&lt;/p&gt;\n\n&lt;p&gt;I did also do the &lt;a href=\"https://www.reddit.com/settings/data-request\"&gt;Reddit Data Request&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sorry to come in here like an idiot, but I have zero programming ability and am now having to figure out how to migrate this over to a website because people depend on these resources, and I also don&amp;#39;t want to lose over a decade of my work.&lt;/p&gt;\n\n&lt;p&gt;I looks like most people here are trying to save their personal Reddit history, but I am more focused on the subreddit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142qq0l", "is_robot_indexable": true, "report_reasons": null, "author": "invah", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142qq0l/how_to_archiveclonemigrate_a_subreddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142qq0l/how_to_archiveclonemigrate_a_subreddit/", "subreddit_subscribers": 686329, "created_utc": 1686079715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "how should i visually format an archive of someone's tweets/twitter account? im just focusing on the tweets (both with and without media)\n\nmost of the programs/apps i find for this only pull the media &amp; data separately (and a lot don't go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it's possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?\n\ni did actually manage to get *something* with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there's nothing i'll just use that. but yeah just wondering if it's possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that's capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?\n\ni've been trying to figure this out for a few days now, any help is appreciated", "author_fullname": "t2_tkcopl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to display an archive of someone's twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428b1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686038535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how should i visually format an archive of someone&amp;#39;s tweets/twitter account? im just focusing on the tweets (both with and without media)&lt;/p&gt;\n\n&lt;p&gt;most of the programs/apps i find for this only pull the media &amp;amp; data separately (and a lot don&amp;#39;t go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp;amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it&amp;#39;s possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?&lt;/p&gt;\n\n&lt;p&gt;i did actually manage to get &lt;em&gt;something&lt;/em&gt; with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there&amp;#39;s nothing i&amp;#39;ll just use that. but yeah just wondering if it&amp;#39;s possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that&amp;#39;s capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve been trying to figure this out for a few days now, any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1428b1l", "is_robot_indexable": true, "report_reasons": null, "author": "Hootie_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "subreddit_subscribers": 686329, "created_utc": 1686038535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  \n\n\nMy question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.", "author_fullname": "t2_lh08w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FTP or Rsync to offline backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142c4um", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686048908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  &lt;/p&gt;\n\n&lt;p&gt;My question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142c4um", "is_robot_indexable": true, "report_reasons": null, "author": "NeoID", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "subreddit_subscribers": 686329, "created_utc": 1686048908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am reconsidering options for my current media server backup strategy. I am running Plex on Windows using DrivePool with 2x media folder duplication as my first backup step. Secondly, I have a set of off-site hard drives containing my media files and macrium drive images.\n\nThirdly, I have an external USB hard drive (14tb wd easystore) that I synchronize all of my data to at regular intervals. This drive is only powered up when a backup is run. My current issue is that my external hdd is running out of space. My plan has been to replace the drive with a larger capacity drive, shuck the currrent drive and add it to my DrivePool storage pool.\n\nOverall, I have been comfortable with my current arrangement. However, replacing the external drive with another of much greater capacity is an expensive option. Also, as I am adding media all the time, it would be a matter of time before the new external drive would be maxed out and the process would need to be repeated. Obviously, the benefit of this drive being external is that if disaster strikes my home or my media server, my data is preserved on the external drive which I could access very easily.\n\nIs there a better option to replace the external hdd in my backup strategy that is worth considering? Affordability is very important due to a restricted budget. I appreciate any comments you might offer.", "author_fullname": "t2_56nvy1n8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Media Server Backup Strategy Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ov62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686076439.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686075846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reconsidering options for my current media server backup strategy. I am running Plex on Windows using DrivePool with 2x media folder duplication as my first backup step. Secondly, I have a set of off-site hard drives containing my media files and macrium drive images.&lt;/p&gt;\n\n&lt;p&gt;Thirdly, I have an external USB hard drive (14tb wd easystore) that I synchronize all of my data to at regular intervals. This drive is only powered up when a backup is run. My current issue is that my external hdd is running out of space. My plan has been to replace the drive with a larger capacity drive, shuck the currrent drive and add it to my DrivePool storage pool.&lt;/p&gt;\n\n&lt;p&gt;Overall, I have been comfortable with my current arrangement. However, replacing the external drive with another of much greater capacity is an expensive option. Also, as I am adding media all the time, it would be a matter of time before the new external drive would be maxed out and the process would need to be repeated. Obviously, the benefit of this drive being external is that if disaster strikes my home or my media server, my data is preserved on the external drive which I could access very easily.&lt;/p&gt;\n\n&lt;p&gt;Is there a better option to replace the external hdd in my backup strategy that is worth considering? Affordability is very important due to a restricted budget. I appreciate any comments you might offer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "142ov62", "is_robot_indexable": true, "report_reasons": null, "author": "bctf1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ov62/media_server_backup_strategy_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ov62/media_server_backup_strategy_advice/", "subreddit_subscribers": 686329, "created_utc": 1686075846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been \"half assing\" for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.\n\nZero issues so far, and I don't want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn't have to be on all the time, backups, etc.\n\nBest way to accomplish the following?\n\n1. Expandable Storage.\n2. Backups.\n3. No need for PC to run.\n4. Shield able to connect via Plex.\n5. Can connect to the device on my PC to manage media.\n6. Able to stream 100mbs movies without any issues.\n\nCurrent plan:\n\n* [Synology DS1522+](https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;psc=1)\n* x2 - [Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache](https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;psc=1)\n\nIs that a good way to accomplish it?", "author_fullname": "t2_ia3wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning on my first NAS set up. Thoughts on the parts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_142v8fm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686090096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been &amp;quot;half assing&amp;quot; for a while now using an external 4TB HDD storage connected to my PC and just running Plex on it and watching through Nvidia Shield on my CX.&lt;/p&gt;\n\n&lt;p&gt;Zero issues so far, and I don&amp;#39;t want to upgrade from Plex, but I know there are better ways of doing it which include having NAS server, running stuff through a different PC where your main one doesn&amp;#39;t have to be on all the time, backups, etc.&lt;/p&gt;\n\n&lt;p&gt;Best way to accomplish the following?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Expandable Storage.&lt;/li&gt;\n&lt;li&gt;Backups.&lt;/li&gt;\n&lt;li&gt;No need for PC to run.&lt;/li&gt;\n&lt;li&gt;Shield able to connect via Plex.&lt;/li&gt;\n&lt;li&gt;Can connect to the device on my PC to manage media.&lt;/li&gt;\n&lt;li&gt;Able to stream 100mbs movies without any issues.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Current plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/gp/product/B0B4DFBRZV/ref=ox_sc_act_title_2?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Synology DS1522+&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;x2 - &lt;a href=\"https://www.amazon.com/gp/product/B0B94PNF7P/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;amp;psc=1\"&gt;Seagate IronWolf Pro 16TB HDD CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is that a good way to accomplish it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142v8fm", "is_robot_indexable": true, "report_reasons": null, "author": "nsoifer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142v8fm/planning_on_my_first_nas_set_up_thoughts_on_the/", "subreddit_subscribers": 686329, "created_utc": 1686090096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I'm looking for a msata SSD to replace the one used in my pfSense router. The current one is a Dogfish 128GB SSD. It worked well for years to serve as the boot disk of pfSense. However, since I decided to virtualize the router and run more service on the device, I started to noticed dropped speed when copying large amount of data to the disk. The initial speed could reach 300MB/s, and then dropped to 18MB/s which indicates cache running out.\n\nI would appreciate if someone could recommend a msata SSD that has bigger cache size and doesn't suffer from the speed drop problem. Thanks!", "author_fullname": "t2_hq1o67a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a good quality msata SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142u24g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686087392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I&amp;#39;m looking for a msata SSD to replace the one used in my pfSense router. The current one is a Dogfish 128GB SSD. It worked well for years to serve as the boot disk of pfSense. However, since I decided to virtualize the router and run more service on the device, I started to noticed dropped speed when copying large amount of data to the disk. The initial speed could reach 300MB/s, and then dropped to 18MB/s which indicates cache running out.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate if someone could recommend a msata SSD that has bigger cache size and doesn&amp;#39;t suffer from the speed drop problem. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142u24g", "is_robot_indexable": true, "report_reasons": null, "author": "left4taco", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142u24g/looking_for_a_good_quality_msata_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142u24g/looking_for_a_good_quality_msata_ssd/", "subreddit_subscribers": 686329, "created_utc": 1686087392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did an rsync mirror of my data ~3TB and would like to verify the backup every once in a while. I tried using rsync's --checksum option for this but I was wondering if there's a better way to do this.", "author_fullname": "t2_9vb9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to verify backup on linux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142tari", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686085671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did an rsync mirror of my data ~3TB and would like to verify the backup every once in a while. I tried using rsync&amp;#39;s --checksum option for this but I was wondering if there&amp;#39;s a better way to do this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142tari", "is_robot_indexable": true, "report_reasons": null, "author": "Scholes_SC2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142tari/best_way_to_verify_backup_on_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142tari/best_way_to_verify_backup_on_linux/", "subreddit_subscribers": 686329, "created_utc": 1686085671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a self-hosted server to provide an e-hentai (NSFW) like experience for organizing my media. I want to be able to search for media using tags using the search (example below):  \n\n\n    series:Full Metal Alchemist$ language:english$\n\n  \nMain focus is tags. With list view and preview second. Any ideas?", "author_fullname": "t2_7g8d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E-hentai like Self-hosted Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142sjs5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a self-hosted server to provide an e-hentai (NSFW) like experience for organizing my media. I want to be able to search for media using tags using the search (example below):  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;series:Full Metal Alchemist$ language:english$\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Main focus is tags. With list view and preview second. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142sjs5", "is_robot_indexable": true, "report_reasons": null, "author": "smokeyFlorence", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142sjs5/ehentai_like_selfhosted_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142sjs5/ehentai_like_selfhosted_server/", "subreddit_subscribers": 686329, "created_utc": 1686083944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using a config for deviant art i found [here on github](https://github.com/mikf/gallery-dl/issues/2143#issuecomment-1002192200) but the post processors don't seem to take effect, no metadata folder or .html file is created containing the file descriptions for each image downloaded from a gallery, is there something wrong with this config? \n\nhere's my whole \\~/.config/gallery-dl/config.json:\n\n    {\n    \"deviantart\":\n            {\n                \"directory\": [\"[gallery-dl]\", \"[{category}] {author[username]}\"],\n                \"filename\": \"[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.{extension}\",\n                \"metadata\": true,\n                \"postprocessors\": [{\n                    \"name\": \"metadata\",\n                    \"mode\": \"custom\",\n                    \"filename\": \"[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.html\",\n                    \"directory\": \"metadata\",\n                    \"extension\": \"html\",\n                    \"content-format\": \"&lt;h1 style='display: inline'&gt;&lt;a href='{url}'&gt;{title}&lt;/a&gt;&lt;/h1&gt; by &lt;a href='https://www.deviantart.com/{username}'&gt;{a&gt;\n                }]\n            }\n    }", "author_fullname": "t2_a45skyv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gallery-dl deviantart postprocessor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142s7w5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686083192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a config for deviant art i found &lt;a href=\"https://github.com/mikf/gallery-dl/issues/2143#issuecomment-1002192200\"&gt;here on github&lt;/a&gt; but the post processors don&amp;#39;t seem to take effect, no metadata folder or .html file is created containing the file descriptions for each image downloaded from a gallery, is there something wrong with this config? &lt;/p&gt;\n\n&lt;p&gt;here&amp;#39;s my whole ~/.config/gallery-dl/config.json:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n&amp;quot;deviantart&amp;quot;:\n        {\n            &amp;quot;directory&amp;quot;: [&amp;quot;[gallery-dl]&amp;quot;, &amp;quot;[{category}] {author[username]}&amp;quot;],\n            &amp;quot;filename&amp;quot;: &amp;quot;[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.{extension}&amp;quot;,\n            &amp;quot;metadata&amp;quot;: true,\n            &amp;quot;postprocessors&amp;quot;: [{\n                &amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;,\n                &amp;quot;mode&amp;quot;: &amp;quot;custom&amp;quot;,\n                &amp;quot;filename&amp;quot;: &amp;quot;[{category}] {author[username]}\u2014{index}\u2014{date:%Y.%m.%d}\u2014{title}.html&amp;quot;,\n                &amp;quot;directory&amp;quot;: &amp;quot;metadata&amp;quot;,\n                &amp;quot;extension&amp;quot;: &amp;quot;html&amp;quot;,\n                &amp;quot;content-format&amp;quot;: &amp;quot;&amp;lt;h1 style=&amp;#39;display: inline&amp;#39;&amp;gt;&amp;lt;a href=&amp;#39;{url}&amp;#39;&amp;gt;{title}&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt; by &amp;lt;a href=&amp;#39;https://www.deviantart.com/{username}&amp;#39;&amp;gt;{a&amp;gt;\n            }]\n        }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?auto=webp&amp;v=enabled&amp;s=e1c208f37795d2e7e52c6134decd3760bebada06", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e4cefddbc7ce021dc36f5f18ff84270bab3eafc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da94179240cb63c0df86a4e66f3932d4e28991e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4dd201736c25732e17c7243f738268f53f2380b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4326eac2b616829b172c392c16dd9af37818bed", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb5dde7f635b56be3df5b7db6d42f202bda7dc89", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/mmxbL1DAlD5YdsS4gO5td_TRk1WGcgjJj2w22eHwAOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60f52964d482f5c12c29e1e3a1c94a47b5f2ff97", "width": 1080, "height": 540}], "variants": {}, "id": "fngzcYF-QRQCMlV24eNPc72Fym3jXH9DdYuS2yxRD5U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142s7w5", "is_robot_indexable": true, "report_reasons": null, "author": "H0rrorAssociation", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142s7w5/gallerydl_deviantart_postprocessor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142s7w5/gallerydl_deviantart_postprocessor/", "subreddit_subscribers": 686329, "created_utc": 1686083192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI tried to use gallery-dl (using Media Downloader as gui) to download twitter user media. But it doesn't finish and gets error 403. So I tried using `--download-archive FILE`  but from what I understand, it just skips the download part, it still looks up the url right ? So I still get 403. I tried then to add `--sleep SECONDS` with 10 seconds but seems it's not enough. Also, is there a way to reduce the amount of requests made ? For exemple, start again from where it left off when it failed to finish, or not look up at all already downloaded media.\n\nI saw this post [\\[gallery-dl / Twitter\\] Is there a way to skip all remaining tweets for a user once everything new has been collected?](https://www.reddit.com/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/) but I don't understand the the answer.\n\nAlso, can I move the already downloaded files (and the archive file) and not \"confuse\" gallery-dl (if i change the output path too ofc) ?\n\nThx for any help,", "author_fullname": "t2_r3ui1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with gallery-dl for twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142rsla", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686082195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I tried to use gallery-dl (using Media Downloader as gui) to download twitter user media. But it doesn&amp;#39;t finish and gets error 403. So I tried using &lt;code&gt;--download-archive FILE&lt;/code&gt;  but from what I understand, it just skips the download part, it still looks up the url right ? So I still get 403. I tried then to add &lt;code&gt;--sleep SECONDS&lt;/code&gt; with 10 seconds but seems it&amp;#39;s not enough. Also, is there a way to reduce the amount of requests made ? For exemple, start again from where it left off when it failed to finish, or not look up at all already downloaded media.&lt;/p&gt;\n\n&lt;p&gt;I saw this post &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/\"&gt;[gallery-dl / Twitter] Is there a way to skip all remaining tweets for a user once everything new has been collected?&lt;/a&gt; but I don&amp;#39;t understand the the answer.&lt;/p&gt;\n\n&lt;p&gt;Also, can I move the already downloaded files (and the archive file) and not &amp;quot;confuse&amp;quot; gallery-dl (if i change the output path too ofc) ?&lt;/p&gt;\n\n&lt;p&gt;Thx for any help,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142rsla", "is_robot_indexable": true, "report_reasons": null, "author": "TheArtofWarPIGEON", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142rsla/help_with_gallerydl_for_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142rsla/help_with_gallerydl_for_twitter/", "subreddit_subscribers": 686329, "created_utc": 1686082195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a number of tiny drives that all together should fit on a few-TB drive, but my issue is that I'm  \u2728 stupid \u2728  and forget to manually backup my files regularly. I also use all my drives frequently, so having a system to bulk-drop any files that are added/modified on one of the drives onto the \"master drive\" would be great. (and potentially setting up a backup for my backup as well)\n\nI've considered drive-mirroring/auto-backup through win10, but I can only seem to get it to back up one drive at a time.\n\n I've also traveled various forums for a bit, but I only get one of two answers; 1.) click-and-drag each one individually to a bigger drive and remember to do it regularly (this has, needless to say, failed me in the past), or   \n2.) get some sus third-party software to do it for me after plugging it into my registry and hope they don't lock my PC and demand gift cards to get it back.\n\nHow do you guys think I should go about this? Is there some way to do this automatically? If I consider drive-mirroring and something glitches up on one drive, will it mess up the other?\n\nI know this is a very all-over the place question and my filing system probably is a birds nest in retrospect, but I still thank you guys for reading and for any techniques or preferences you may suggest :)", "author_fullname": "t2_d9gzywue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[BACKUP QUESTION] How do I backup multiple drives of various types to another single drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ridk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686081545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a number of tiny drives that all together should fit on a few-TB drive, but my issue is that I&amp;#39;m  \u2728 stupid \u2728  and forget to manually backup my files regularly. I also use all my drives frequently, so having a system to bulk-drop any files that are added/modified on one of the drives onto the &amp;quot;master drive&amp;quot; would be great. (and potentially setting up a backup for my backup as well)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve considered drive-mirroring/auto-backup through win10, but I can only seem to get it to back up one drive at a time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also traveled various forums for a bit, but I only get one of two answers; 1.) click-and-drag each one individually to a bigger drive and remember to do it regularly (this has, needless to say, failed me in the past), or&lt;br/&gt;\n2.) get some sus third-party software to do it for me after plugging it into my registry and hope they don&amp;#39;t lock my PC and demand gift cards to get it back.&lt;/p&gt;\n\n&lt;p&gt;How do you guys think I should go about this? Is there some way to do this automatically? If I consider drive-mirroring and something glitches up on one drive, will it mess up the other?&lt;/p&gt;\n\n&lt;p&gt;I know this is a very all-over the place question and my filing system probably is a birds nest in retrospect, but I still thank you guys for reading and for any techniques or preferences you may suggest :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ridk", "is_robot_indexable": true, "report_reasons": null, "author": "HereFerGrinz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ridk/backup_question_how_do_i_backup_multiple_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ridk/backup_question_how_do_i_backup_multiple_drives/", "subreddit_subscribers": 686329, "created_utc": 1686081545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this subreddit might not be the best place to ask, but, I had to give up with configs.\n\nI have two question about my config (won't download files into subreddit folders), and about archive-format (fighting against duplicates and Reddit galleries).\n\n&amp;#x200B;\n\n1) I'm trying to achieve **reddit\\_user\\_username** and **reddit\\_sub\\_subreddit** folders under base-directory. At the moment, user -object works; it correctly stores user media into specific folders.\n\nHowever, configuration for subreddits does not work. I've tried \"subreddit(s)\", \"submission(s)\".\n\nWhat is wrong with this part of code?\n\n    \"reddit\": {\n    \"videos\": \"ytdl\",\n    \"archive-format\": \"{title}\",\n    \"parent-directory\": true,\n    \"user\": {\n        \"directory\": [\n            \"reddit_user_{author}\"\n     \u00a0  ],\n        \"filename\": \"{date}-{subreddit}-{id}-{title}.{extension}\",\n        \"whitelist\": [\n            \"imgur\",\n            \"redgifs\",\n            \"gfycat\",\n            \"imgur\"\n    \u00a0 \u00a0 ],\n        \"parent-directory\": true\n    },\n    \"subreddit\": {\n        \"directory\": [\n            \"reddit_sub_{subreddit}\"\n\u00a0 \u00a0 \u00a0 \u00a0 ],\n        \"filename\": \"{date}-{author}-{id}-{title}.{extension}\"\n\u00a0 \u00a0 },\n    \"parent-metadata\": \"_reddit_\"\n},\n\n*Extra question:* How I can find names for these objects, like \"user\"? Couldn't find with \\`./gallery-dl -K &lt;url-address&gt;\\`.\n\n&amp;#x200B;\n\n2) At the moment I'm fetching user's images, which are mostly duplicates because posted in multiple subreddits. As \\`archive-format\\`, I used \\`{title}\\`, but this causes issues with posts with multiple images. If the post has multiple images (gallery), it only downloads the first image and skips the rest.\n\nMaybe there is a way to set specific configs only for reddit gallery images, but I'm not sure how it works. Gallery-dl's [documentation](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst) did not help me, and I also tried to find information from the [source code](https://github.com/mikf/gallery-dl/blob/master/gallery_dl/extractor/reddit.py).\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_3sy9yn65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl (custom folder configs, reddit \"galleries\" and archive-format)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142oxf2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686075963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this subreddit might not be the best place to ask, but, I had to give up with configs.&lt;/p&gt;\n\n&lt;p&gt;I have two question about my config (won&amp;#39;t download files into subreddit folders), and about archive-format (fighting against duplicates and Reddit galleries).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1) I&amp;#39;m trying to achieve &lt;strong&gt;reddit_user_username&lt;/strong&gt; and &lt;strong&gt;reddit_sub_subreddit&lt;/strong&gt; folders under base-directory. At the moment, user -object works; it correctly stores user media into specific folders.&lt;/p&gt;\n\n&lt;p&gt;However, configuration for subreddits does not work. I&amp;#39;ve tried &amp;quot;subreddit(s)&amp;quot;, &amp;quot;submission(s)&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What is wrong with this part of code?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;quot;reddit&amp;quot;: {\n&amp;quot;videos&amp;quot;: &amp;quot;ytdl&amp;quot;,\n&amp;quot;archive-format&amp;quot;: &amp;quot;{title}&amp;quot;,\n&amp;quot;parent-directory&amp;quot;: true,\n&amp;quot;user&amp;quot;: {\n    &amp;quot;directory&amp;quot;: [\n        &amp;quot;reddit_user_{author}&amp;quot;\n \u00a0  ],\n    &amp;quot;filename&amp;quot;: &amp;quot;{date}-{subreddit}-{id}-{title}.{extension}&amp;quot;,\n    &amp;quot;whitelist&amp;quot;: [\n        &amp;quot;imgur&amp;quot;,\n        &amp;quot;redgifs&amp;quot;,\n        &amp;quot;gfycat&amp;quot;,\n        &amp;quot;imgur&amp;quot;\n\u00a0 \u00a0 ],\n    &amp;quot;parent-directory&amp;quot;: true\n},\n&amp;quot;subreddit&amp;quot;: {\n    &amp;quot;directory&amp;quot;: [\n        &amp;quot;reddit_sub_{subreddit}&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;\u00a0 \u00a0 \u00a0 \u00a0 ],\n        &amp;quot;filename&amp;quot;: &amp;quot;{date}-{author}-{id}-{title}.{extension}&amp;quot;\n\u00a0 \u00a0 },\n    &amp;quot;parent-metadata&amp;quot;: &amp;quot;&lt;em&gt;reddit&lt;/em&gt;&amp;quot;\n},&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Extra question:&lt;/em&gt; How I can find names for these objects, like &amp;quot;user&amp;quot;? Couldn&amp;#39;t find with `./gallery-dl -K &amp;lt;url-address&amp;gt;`.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2) At the moment I&amp;#39;m fetching user&amp;#39;s images, which are mostly duplicates because posted in multiple subreddits. As `archive-format`, I used `{title}`, but this causes issues with posts with multiple images. If the post has multiple images (gallery), it only downloads the first image and skips the rest.&lt;/p&gt;\n\n&lt;p&gt;Maybe there is a way to set specific configs only for reddit gallery images, but I&amp;#39;m not sure how it works. Gallery-dl&amp;#39;s &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst\"&gt;documentation&lt;/a&gt; did not help me, and I also tried to find information from the &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/gallery_dl/extractor/reddit.py\"&gt;source code&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?auto=webp&amp;v=enabled&amp;s=a0a36aa0277859b6647ca9ead294685ed8eb9ba6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b11803dc23b1a0dac19980e6631cae06e637474", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b6565624f5360f043c4b0a4317ff63c99533061", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fead4655807f833f87d3f932ff47724acc6be920", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99b67ae321d92fd707e7238acc502c80b2c9a143", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b42ab615c2af6f4e56e14167f2c31f7ae7548c2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ys0GqNBFK4iJR3XwJkdDwE3yguKbMHlcQXQ8WL776GA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3423b17f3ce5637d6d89e62a3d5302f32d991024", "width": 1080, "height": 540}], "variants": {}, "id": "2ujLMGJDXtZQwUdJOcM6MdxWqZJO1wuswfwQ1l5pTaU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142oxf2", "is_robot_indexable": true, "report_reasons": null, "author": "qusmar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142oxf2/gallerydl_custom_folder_configs_reddit_galleries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142oxf2/gallerydl_custom_folder_configs_reddit_galleries/", "subreddit_subscribers": 686329, "created_utc": 1686075963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys! So my unlimited cloud storage with google is coming to an end since they're no longer offering unlimited with my old univ's alumni accounts. Have been considering NAS for a while but they're just so darn expensive.\n\nI'm mainly a mac guy, have 2 MBPs, and not a shit ton of data. ATM, something like an external 1TB or 2TB solution would probably fit the bill but I do like the idea of being able to just throw any files to a space that's dedicated for storage without the wires each time.\n\nI do some light photography, and video editing here and there, but nothing that warrants &gt;2TB of storage space for the time being.\n\nI saw a YT video of someone converting a mac mini to use as NAS and was wondering if that would be a good option for me. I'd ideally try to spend &lt;$200 bucks on it. I was thinking maybe picking up a 2012 Mac mini, swap out the internal storage to like 1-2 TB SSD, whatever it can allow for and then if needed down the road, I can have a HD enclosure for additional storage. The purpose of the NAS will be mainly for local storage of files, photos, videos. I don't do any streaming as I have streaming services and a jailbroken firestick lol.\n\nYour thoughts and input will be greatly appreciated! Thanks!", "author_fullname": "t2_o07xp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mac Mini as a NAS solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142odpm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686074919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! So my unlimited cloud storage with google is coming to an end since they&amp;#39;re no longer offering unlimited with my old univ&amp;#39;s alumni accounts. Have been considering NAS for a while but they&amp;#39;re just so darn expensive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m mainly a mac guy, have 2 MBPs, and not a shit ton of data. ATM, something like an external 1TB or 2TB solution would probably fit the bill but I do like the idea of being able to just throw any files to a space that&amp;#39;s dedicated for storage without the wires each time.&lt;/p&gt;\n\n&lt;p&gt;I do some light photography, and video editing here and there, but nothing that warrants &amp;gt;2TB of storage space for the time being.&lt;/p&gt;\n\n&lt;p&gt;I saw a YT video of someone converting a mac mini to use as NAS and was wondering if that would be a good option for me. I&amp;#39;d ideally try to spend &amp;lt;$200 bucks on it. I was thinking maybe picking up a 2012 Mac mini, swap out the internal storage to like 1-2 TB SSD, whatever it can allow for and then if needed down the road, I can have a HD enclosure for additional storage. The purpose of the NAS will be mainly for local storage of files, photos, videos. I don&amp;#39;t do any streaming as I have streaming services and a jailbroken firestick lol.&lt;/p&gt;\n\n&lt;p&gt;Your thoughts and input will be greatly appreciated! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142odpm", "is_robot_indexable": true, "report_reasons": null, "author": "alexandled", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142odpm/mac_mini_as_a_nas_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142odpm/mac_mini_as_a_nas_solution/", "subreddit_subscribers": 686329, "created_utc": 1686074919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought 4x4tb hard drives a few months ago and i connect them to my old pc that i use as homeserver using a \"hdd toaster\" ([https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr\\_1\\_6](https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_6)) I noticed that they are getting real hot. Is it an option to point a little fan to the hard drives ? ([https://www.amazon.com/ELUTENG-Computer-Receiver-Compatible-Playstation/dp/B06Y5WWBHH/ref=sr\\_1\\_14](https://www.amazon.com/ELUTENG-Computer-Receiver-Compatible-Playstation/dp/B06Y5WWBHH/ref=sr_1_14)) Im just a poor student so im trying not to spend to much money. I hope you can help thank you and have a nice day!", "author_fullname": "t2_c90ffdi5n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard drives are getting real hot in hdd toaster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142mrc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686071906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought 4x4tb hard drives a few months ago and i connect them to my old pc that i use as homeserver using a &amp;quot;hdd toaster&amp;quot; (&lt;a href=\"https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_6\"&gt;https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_6&lt;/a&gt;) I noticed that they are getting real hot. Is it an option to point a little fan to the hard drives ? (&lt;a href=\"https://www.amazon.com/ELUTENG-Computer-Receiver-Compatible-Playstation/dp/B06Y5WWBHH/ref=sr_1_14\"&gt;https://www.amazon.com/ELUTENG-Computer-Receiver-Compatible-Playstation/dp/B06Y5WWBHH/ref=sr_1_14&lt;/a&gt;) Im just a poor student so im trying not to spend to much money. I hope you can help thank you and have a nice day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142mrc3", "is_robot_indexable": true, "report_reasons": null, "author": "LawyerImaginary9249", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142mrc3/hard_drives_are_getting_real_hot_in_hdd_toaster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142mrc3/hard_drives_are_getting_real_hot_in_hdd_toaster/", "subreddit_subscribers": 686329, "created_utc": 1686071906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI'm a filmmaker and need to both collect + share lots of video files with clients. I'm comparing options and would hugely appreciate any advice. What I'm looking for in my cloud storage (most important at top)\n\n1. I can share a view only link with clients to **watch** and **download** videos (including sharing an entire folder with subfolders if I need to) - they don't need to log in. Can use mobile or desktop\n2. As above but: ***Upload*** link with clients to give me files\n   1. I'd really like the same folder to be able to have active links for viewing + uploading to (at the same time. [Mega.nz](https://Mega.nz) for example will deactivate the upload link if I create a viewing link, which seems really dumb to me)\n3. General user interface is nice\n   1. good for browsing photos and videos quickly, so videos play (with sound) and ideally (like Google Drive does; offers different playback quality options \\[Youtube-like\\] for snappy streaming) \n   2. Ideally some media file (and folder) info. I love how [Mega.nz](https://Mega.nz) shows folder sizes, and video durations, codec, etc)\n4. Ideally a separate app for batch uploading (eg. Google drive, Dropbox, Mega, etc). Preferably with a vaguely accurate ETA ([Mega.nz](https://Mega.nz) only gives ETA on individual files, which seems lazy to me)\n\nPrice matters, but willing to pay for the right tool. I'm looking for about 2tb.\n\n\\---\n\nRight now I pay for Google Drive + Mega. Love Drive for being part of Google docs but doesn't support upload links (without creating/ signing into a Google account). And user interface isn't my fav.\n\nLove Mega for general sexy user interface (just looks and feels more pro, dark theme, better layout and more info..) but like Drive, fails on share links (Dropbox allows a folder to have active viewing + uploading links at same time) and also really annoyingly fails to play audio on video files.\n\nThanks again for any ideas", "author_fullname": "t2_6olz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help! Still searching for the most suitable cloud storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142kzt9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686068003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a filmmaker and need to both collect + share lots of video files with clients. I&amp;#39;m comparing options and would hugely appreciate any advice. What I&amp;#39;m looking for in my cloud storage (most important at top)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I can share a view only link with clients to &lt;strong&gt;watch&lt;/strong&gt; and &lt;strong&gt;download&lt;/strong&gt; videos (including sharing an entire folder with subfolders if I need to) - they don&amp;#39;t need to log in. Can use mobile or desktop&lt;/li&gt;\n&lt;li&gt;As above but: &lt;strong&gt;&lt;em&gt;Upload&lt;/em&gt;&lt;/strong&gt; link with clients to give me files\n\n&lt;ol&gt;\n&lt;li&gt;I&amp;#39;d really like the same folder to be able to have active links for viewing + uploading to (at the same time. &lt;a href=\"https://Mega.nz\"&gt;Mega.nz&lt;/a&gt; for example will deactivate the upload link if I create a viewing link, which seems really dumb to me)&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;General user interface is nice\n\n&lt;ol&gt;\n&lt;li&gt;good for browsing photos and videos quickly, so videos play (with sound) and ideally (like Google Drive does; offers different playback quality options [Youtube-like] for snappy streaming) &lt;/li&gt;\n&lt;li&gt;Ideally some media file (and folder) info. I love how &lt;a href=\"https://Mega.nz\"&gt;Mega.nz&lt;/a&gt; shows folder sizes, and video durations, codec, etc)&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Ideally a separate app for batch uploading (eg. Google drive, Dropbox, Mega, etc). Preferably with a vaguely accurate ETA (&lt;a href=\"https://Mega.nz\"&gt;Mega.nz&lt;/a&gt; only gives ETA on individual files, which seems lazy to me)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Price matters, but willing to pay for the right tool. I&amp;#39;m looking for about 2tb.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Right now I pay for Google Drive + Mega. Love Drive for being part of Google docs but doesn&amp;#39;t support upload links (without creating/ signing into a Google account). And user interface isn&amp;#39;t my fav.&lt;/p&gt;\n\n&lt;p&gt;Love Mega for general sexy user interface (just looks and feels more pro, dark theme, better layout and more info..) but like Drive, fails on share links (Dropbox allows a folder to have active viewing + uploading links at same time) and also really annoyingly fails to play audio on video files.&lt;/p&gt;\n\n&lt;p&gt;Thanks again for any ideas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?auto=webp&amp;v=enabled&amp;s=208361445100beccae9dc054d72cbf90871eb32e", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5e47694cd1d0bb1669cde982776060c7c23c709", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2884aea2e60504ad5b678f6dc7aaa29a9dc13578", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc3833f2778f678f802e69e41d0f4e4d4434a158", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b4bbe9a3a0de4c6206ec1afea216ab72827f3cf", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ae83189ed6c60a3fc4b64bfc1f8c0c906d5f5c2", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/mnITTlqmFVFoGUndG7Xt2HlkBF6NCuihdavogBYdArk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c3840d86fca2d88b94410985f861ad387fb4cbc", "width": 1080, "height": 1080}], "variants": {}, "id": "znNoxecLMIva0RV5qTE3-eGpx58Bw_7Od3bJVlmK9bU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142kzt9", "is_robot_indexable": true, "report_reasons": null, "author": "lukeflegg", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142kzt9/help_still_searching_for_the_most_suitable_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142kzt9/help_still_searching_for_the_most_suitable_cloud/", "subreddit_subscribers": 686329, "created_utc": 1686068003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello All,\n\nWe are a creative studio made by two people. We are looking for some suggestion concerning storage hardware and - eventually - methods.\n\nWe work as designers and artists and we produce a large and heterogeneous amount of files (3D, video, images, etc). We have no experience in NAS storage and we have done backups manually until now, but we are open to change our workflow if it worth the effort.\n\nWhat you experts suggest?  Go with the NAS (we heard enthusiastic opinion about the WD MyCloud) or buy for example a WD MyBook instead?\n\nThank you.", "author_fullname": "t2_dt415yx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage suggestion (NAS vs 'just storage')", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ku0k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686067691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;We are a creative studio made by two people. We are looking for some suggestion concerning storage hardware and - eventually - methods.&lt;/p&gt;\n\n&lt;p&gt;We work as designers and artists and we produce a large and heterogeneous amount of files (3D, video, images, etc). We have no experience in NAS storage and we have done backups manually until now, but we are open to change our workflow if it worth the effort.&lt;/p&gt;\n\n&lt;p&gt;What you experts suggest?  Go with the NAS (we heard enthusiastic opinion about the WD MyCloud) or buy for example a WD MyBook instead?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ku0k", "is_robot_indexable": true, "report_reasons": null, "author": "DwayneDardago", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ku0k/storage_suggestion_nas_vs_just_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ku0k/storage_suggestion_nas_vs_just_storage/", "subreddit_subscribers": 686329, "created_utc": 1686067691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have tons of iPhone text/iMessage chats with at least 30gb of attachments across them.\nI know how to find and extract the attachments from the iTunes backup files.\n\nThe issue I am having is that the iTunes backup doesn\u2019t have all the attachments (photos, videos, PDF, vCard) since I have iCloud messages turned on, so the attachments in the cloud won\u2019t appear in the local iTunes backup.\n\nHowever, when I turn off messages in iCloud (and press the option to download and delete messages from the cloud to my phone) many attachments don\u2019t download, they just have a \u201c?\u201d icon. I can only access those attachments when I turn iCloud messages back on.\n\nHow can get get all message attachments from the cloud onto my phone and into a local iTunes backup of the \u201cdisable and delete\u201d from iCloud doesn\u2019t wort properly?", "author_fullname": "t2_e6drpuch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting photos from iPhone backup - cloud issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142iue7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686063763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have tons of iPhone text/iMessage chats with at least 30gb of attachments across them.\nI know how to find and extract the attachments from the iTunes backup files.&lt;/p&gt;\n\n&lt;p&gt;The issue I am having is that the iTunes backup doesn\u2019t have all the attachments (photos, videos, PDF, vCard) since I have iCloud messages turned on, so the attachments in the cloud won\u2019t appear in the local iTunes backup.&lt;/p&gt;\n\n&lt;p&gt;However, when I turn off messages in iCloud (and press the option to download and delete messages from the cloud to my phone) many attachments don\u2019t download, they just have a \u201c?\u201d icon. I can only access those attachments when I turn iCloud messages back on.&lt;/p&gt;\n\n&lt;p&gt;How can get get all message attachments from the cloud onto my phone and into a local iTunes backup of the \u201cdisable and delete\u201d from iCloud doesn\u2019t wort properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142iue7", "is_robot_indexable": true, "report_reasons": null, "author": "aviator_L1011", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142iue7/extracting_photos_from_iphone_backup_cloud_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142iue7/extracting_photos_from_iphone_backup_cloud_issues/", "subreddit_subscribers": 686329, "created_utc": 1686063763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nI've been trying to organise all my drives and started wondering... Is it possible to combine drives/folders into 1 virtual drive in Windows 11? \n\nI've got so many games from so many different platforms (Steam, Epic, EA etc...) all spread across 6 different drives on my PC at the moment. I know the solution is to buy a big drive and transfer everything into 1 drive but cmon... the cost of living these days is no joke XD\n\nAnyways is this possible? Can I create some sort of virtual drive that can link all my games across 6 drives into 1? Now that would be amazing!", "author_fullname": "t2_5251j5ai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Combining multiple drives/folders into one", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1429p61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686042446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to organise all my drives and started wondering... Is it possible to combine drives/folders into 1 virtual drive in Windows 11? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got so many games from so many different platforms (Steam, Epic, EA etc...) all spread across 6 different drives on my PC at the moment. I know the solution is to buy a big drive and transfer everything into 1 drive but cmon... the cost of living these days is no joke XD&lt;/p&gt;\n\n&lt;p&gt;Anyways is this possible? Can I create some sort of virtual drive that can link all my games across 6 drives into 1? Now that would be amazing!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1429p61", "is_robot_indexable": true, "report_reasons": null, "author": "SnooChipmunks4789", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1429p61/combining_multiple_drivesfolders_into_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1429p61/combining_multiple_drivesfolders_into_one/", "subreddit_subscribers": 686329, "created_utc": 1686042446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to save all the torrent files from a specific user in rutracker (the person has INCREDIBLY rare stuff - I mean stuff you can't even BUY because there were only very limited editions of it and they are sold out everywhere and I worry about a RARBG situation happening).\n\nConsulting the person's page states they have 3000+ torrents, but when I go to the page I am limited to 10 pages with 50 files each. If I search for something that I'm sure the person has, it shows up in the search results, it's just the person's list that is capped to 500 results.\n\nIs there any way to \"unlock\" their full catalogue?", "author_fullname": "t2_csnfseuy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hoarding specific users from rutracker: list is limited to 10 pages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1429727", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686041098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to save all the torrent files from a specific user in rutracker (the person has INCREDIBLY rare stuff - I mean stuff you can&amp;#39;t even BUY because there were only very limited editions of it and they are sold out everywhere and I worry about a RARBG situation happening).&lt;/p&gt;\n\n&lt;p&gt;Consulting the person&amp;#39;s page states they have 3000+ torrents, but when I go to the page I am limited to 10 pages with 50 files each. If I search for something that I&amp;#39;m sure the person has, it shows up in the search results, it&amp;#39;s just the person&amp;#39;s list that is capped to 500 results.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to &amp;quot;unlock&amp;quot; their full catalogue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1429727", "is_robot_indexable": true, "report_reasons": null, "author": "Thick_Blackberry3377", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1429727/hoarding_specific_users_from_rutracker_list_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1429727/hoarding_specific_users_from_rutracker_list_is/", "subreddit_subscribers": 686329, "created_utc": 1686041098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Community, \n\nI wanted to get your views on the different ways of working with storage.   \nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?  \nMaybe a mix model?\n\nI wanted to see how you guys are usually working and what do you feel comfortable / already tested.  \nThank you for sharing your thoughts \ud83d\ude09", "author_fullname": "t2_meba9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflows and different ways of working with storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428wiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686040255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Community, &lt;/p&gt;\n\n&lt;p&gt;I wanted to get your views on the different ways of working with storage.&lt;br/&gt;\nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?&lt;br/&gt;\nMaybe a mix model?&lt;/p&gt;\n\n&lt;p&gt;I wanted to see how you guys are usually working and what do you feel comfortable / already tested.&lt;br/&gt;\nThank you for sharing your thoughts \ud83d\ude09&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1428wiz", "is_robot_indexable": true, "report_reasons": null, "author": "fscheps", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "subreddit_subscribers": 686329, "created_utc": 1686040255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "want to buy a lot of drives and I have found no posts talking about their support once something goes wrong", "author_fullname": "t2_gw5a2p3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have RMA experience with Solidigm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141zgm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686015673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;want to buy a lot of drives and I have found no posts talking about their support once something goes wrong&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141zgm0", "is_robot_indexable": true, "report_reasons": null, "author": "masturbaiter696969", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "subreddit_subscribers": 686329, "created_utc": 1686015673.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}