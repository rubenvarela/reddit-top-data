{"kind": "Listing", "data": {"after": "t3_141e1x7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are several things I would like to download from Reddit before they kill off API access: \n\n- Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. \n\n- Every single post I have upvoted or saved, if possible. \n\n- Specific subreddits, particularly /r/HFY. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. \n\nWhat are the best tools to do this with, saving as much metadata as possible in a machine-readable format? \n\nAny other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.", "author_fullname": "t2_qp8d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools for downloading Reddit before API access is cut off?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1423a1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 116, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 116, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are several things I would like to download from Reddit before they kill off API access: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Every single post I have upvoted or saved, if possible. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Specific subreddits, particularly &lt;a href=\"/r/HFY\"&gt;/r/HFY&lt;/a&gt;. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are the best tools to do this with, saving as much metadata as possible in a machine-readable format? &lt;/p&gt;\n\n&lt;p&gt;Any other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "11TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1423a1q", "is_robot_indexable": true, "report_reasons": null, "author": "happysmash27", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "subreddit_subscribers": 686258, "created_utc": 1686024982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Datahoarders! We were thinking that it's time to run another giveaway thread. This time, we are giving away an **IronWolf 125 1TB SSD** to one lucky winner in this thread.\n\nThe prize is: one **IronWolf 125 1TB SSD**\n\n*How to enter:*\nJust reply to this post once with a comment about what you are thankful for. We ask entrants to please include the terms **RunWithIronWolf** and **Seagate** in your comment to be considered for the prize drawing.\n\nFeel free to let us know what project(s) this would help with!\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until June 23, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)\n\nUS\n\nCanada (will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Official Giveaway: June 2023 Seagate IronWolf Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141lmhg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": "", "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685985755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Datahoarders! We were thinking that it&amp;#39;s time to run another giveaway thread. This time, we are giving away an &lt;strong&gt;IronWolf 125 1TB SSD&lt;/strong&gt; to one lucky winner in this thread.&lt;/p&gt;\n\n&lt;p&gt;The prize is: one &lt;strong&gt;IronWolf 125 1TB SSD&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;How to enter:&lt;/em&gt;\nJust reply to this post once with a comment about what you are thankful for. We ask entrants to please include the terms &lt;strong&gt;RunWithIronWolf&lt;/strong&gt; and &lt;strong&gt;Seagate&lt;/strong&gt; in your comment to be considered for the prize drawing.&lt;/p&gt;\n\n&lt;p&gt;Feel free to let us know what project(s) this would help with!&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until June 23, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "141lmhg", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 224, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/141lmhg/official_giveaway_june_2023_seagate_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/141lmhg/official_giveaway_june_2023_seagate_ironwolf/", "subreddit_subscribers": 686258, "created_utc": 1685985755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nWith the recent news about RARBG going down, and us being saved by some archivist's scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don't we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.", "author_fullname": "t2_ctq800ysv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring torrent sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424g6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent news about RARBG going down, and us being saved by some archivist&amp;#39;s scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don&amp;#39;t we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424g6h", "is_robot_indexable": true, "report_reasons": null, "author": "Busy-Paramedic-4994", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "subreddit_subscribers": 686258, "created_utc": 1686028010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I seem to have a bit of it in everything now, from ZFS special vdevs to the boot drive in my battle station (dat *latency* \ud83d\udc40 \ud83d\udc40 \ud83d\udc40 ). \n\nNewegg just ran out of the 118GB NVMe m.2 and I\u2019m trying to see if anyone has any more ideas to justify stocking up before its all gone for good.", "author_fullname": "t2_u4u6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone else taking advantage of the Optane fire sale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141e7uv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685970880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seem to have a bit of it in everything now, from ZFS special vdevs to the boot drive in my battle station (dat &lt;em&gt;latency&lt;/em&gt; \ud83d\udc40 \ud83d\udc40 \ud83d\udc40 ). &lt;/p&gt;\n\n&lt;p&gt;Newegg just ran out of the 118GB NVMe m.2 and I\u2019m trying to see if anyone has any more ideas to justify stocking up before its all gone for good.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141e7uv", "is_robot_indexable": true, "report_reasons": null, "author": "certifiedintelligent", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141e7uv/anyone_else_taking_advantage_of_the_optane_fire/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141e7uv/anyone_else_taking_advantage_of_the_optane_fire/", "subreddit_subscribers": 686258, "created_utc": 1685970880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Datahorder community!\n\nI just have a question referance backups of all your precious and (in my case sometimes) usless data! I have for a long time admired the setups people have on here and I have for a very long time wanted a data hording setup of my own (since I was about 15 as im a little bit sad!). I currently have 1TB on Google Drive and I am currently looking at purchasing a Synology DS923+ with 8TB of storage (which I will keep in RAID5 giving me 6TB usable). \n\n&amp;#x200B;\n\nI was just wanting to use you experts as a sounding bored, If i was to get say 3 6TB external HDD's and keep 1 in my bedroom (which will be backed up to daily, or everyother day), 1 in my office (weekly backed up) and 1 in my parents house (monthly backed up). \n\nWould this be sufficent? I feel like paying for a backup to google drive etc defeats purchasing the NAS. I am trying to become self hosted in as many ways as possible.\n\n&amp;#x200B;\n\nThank you for any advice\n\n&amp;#x200B;\n\nAdded:\n\nOn this NAS will be a few docker containers for testing some stuff and just general files and photos\n\nI am also looking at purchasing a older server at some point and that will run my testing and dockers and will backup to the NAS.", "author_fullname": "t2_1wgqbq6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup suggestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141shz5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686000533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Datahorder community!&lt;/p&gt;\n\n&lt;p&gt;I just have a question referance backups of all your precious and (in my case sometimes) usless data! I have for a long time admired the setups people have on here and I have for a very long time wanted a data hording setup of my own (since I was about 15 as im a little bit sad!). I currently have 1TB on Google Drive and I am currently looking at purchasing a Synology DS923+ with 8TB of storage (which I will keep in RAID5 giving me 6TB usable). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was just wanting to use you experts as a sounding bored, If i was to get say 3 6TB external HDD&amp;#39;s and keep 1 in my bedroom (which will be backed up to daily, or everyother day), 1 in my office (weekly backed up) and 1 in my parents house (monthly backed up). &lt;/p&gt;\n\n&lt;p&gt;Would this be sufficent? I feel like paying for a backup to google drive etc defeats purchasing the NAS. I am trying to become self hosted in as many ways as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for any advice&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Added:&lt;/p&gt;\n\n&lt;p&gt;On this NAS will be a few docker containers for testing some stuff and just general files and photos&lt;/p&gt;\n\n&lt;p&gt;I am also looking at purchasing a older server at some point and that will run my testing and dockers and will backup to the NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141shz5", "is_robot_indexable": true, "report_reasons": null, "author": "Zetta666", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141shz5/backup_suggestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141shz5/backup_suggestion/", "subreddit_subscribers": 686258, "created_utc": 1686000533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there!\n\nI have downloaded alot of data from a site called justcritics or smth along the lines.... \n\nAnyway, i've got millions of files (videos and images) which i now need to filter out and categorize.\n\nNow i notice that ntfs/windows isnt well suited for that job, the explorer is constantly crashing and filling up the cache when having a folder with more than 100 images/videos.\n\nCan anyone recommend a good solution for that?", "author_fullname": "t2_onogq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141jcc6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685981219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there!&lt;/p&gt;\n\n&lt;p&gt;I have downloaded alot of data from a site called justcritics or smth along the lines.... &lt;/p&gt;\n\n&lt;p&gt;Anyway, i&amp;#39;ve got millions of files (videos and images) which i now need to filter out and categorize.&lt;/p&gt;\n\n&lt;p&gt;Now i notice that ntfs/windows isnt well suited for that job, the explorer is constantly crashing and filling up the cache when having a folder with more than 100 images/videos.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a good solution for that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141jcc6", "is_robot_indexable": true, "report_reasons": null, "author": "AllCowsAreBurgers", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141jcc6/managing_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141jcc6/managing_content/", "subreddit_subscribers": 686258, "created_utc": 1685981219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "how should i visually format an archive of someone's tweets/twitter account? im just focusing on the tweets (both with and without media)\n\nmost of the programs/apps i find for this only pull the media &amp; data separately (and a lot don't go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it's possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?\n\ni did actually manage to get *something* with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there's nothing i'll just use that. but yeah just wondering if it's possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that's capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?\n\ni've been trying to figure this out for a few days now, any help is appreciated", "author_fullname": "t2_tkcopl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to display an archive of someone's twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428b1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686038535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how should i visually format an archive of someone&amp;#39;s tweets/twitter account? im just focusing on the tweets (both with and without media)&lt;/p&gt;\n\n&lt;p&gt;most of the programs/apps i find for this only pull the media &amp;amp; data separately (and a lot don&amp;#39;t go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp;amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it&amp;#39;s possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?&lt;/p&gt;\n\n&lt;p&gt;i did actually manage to get &lt;em&gt;something&lt;/em&gt; with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there&amp;#39;s nothing i&amp;#39;ll just use that. but yeah just wondering if it&amp;#39;s possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that&amp;#39;s capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve been trying to figure this out for a few days now, any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1428b1l", "is_robot_indexable": true, "report_reasons": null, "author": "Hootie_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "subreddit_subscribers": 686258, "created_utc": 1686038535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It will mostly be a media/jellyfin server, but will do some general use as well. A VM and/or some containers. \n\nFor storage I am looking at either 4x16TB or 6x16TB for storage. Which would be best, and what RAID/UNRAID config should I run for 4 and 6 drives? Should the drives be the same or should I split between 2 or 3 brands? There will be some data that would suck to lose. Most of it is media though. Not critical, but a big pain to find again and would preferably want to avoid doing so. I am willing to pay more for an extra drive or two if necessary.\n\nAs for other hardware. How powerful cpu should I get for running maybe a VM, a container and a jellyfin server? And what about memory?\n\nBig thanks in advance for any input!", "author_fullname": "t2_y5g3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to build a home server. 4 or 6 drives? How powerful hardware", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1420occ", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686018563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It will mostly be a media/jellyfin server, but will do some general use as well. A VM and/or some containers. &lt;/p&gt;\n\n&lt;p&gt;For storage I am looking at either 4x16TB or 6x16TB for storage. Which would be best, and what RAID/UNRAID config should I run for 4 and 6 drives? Should the drives be the same or should I split between 2 or 3 brands? There will be some data that would suck to lose. Most of it is media though. Not critical, but a big pain to find again and would preferably want to avoid doing so. I am willing to pay more for an extra drive or two if necessary.&lt;/p&gt;\n\n&lt;p&gt;As for other hardware. How powerful cpu should I get for running maybe a VM, a container and a jellyfin server? And what about memory?&lt;/p&gt;\n\n&lt;p&gt;Big thanks in advance for any input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1420occ", "is_robot_indexable": true, "report_reasons": null, "author": "TheOptiGamer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1420occ/looking_to_build_a_home_server_4_or_6_drives_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1420occ/looking_to_build_a_home_server_4_or_6_drives_how/", "subreddit_subscribers": 686258, "created_utc": 1686018563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "want to buy a lot of drives and I have found no posts talking about their support once something goes wrong", "author_fullname": "t2_gw5a2p3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have RMA experience with Solidigm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141zgm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686015673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;want to buy a lot of drives and I have found no posts talking about their support once something goes wrong&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141zgm0", "is_robot_indexable": true, "report_reasons": null, "author": "masturbaiter696969", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "subreddit_subscribers": 686258, "created_utc": 1686015673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any cases out there that can hold the same amount of drives as the 7 XL (18 I believe) or more, for a regular nas build? I\u2019ve seen people say the Lian Li D8000 but it\u2019s discontinued.", "author_fullname": "t2_ba3y5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rival to Fractal Define 7 XL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141o74a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685991024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any cases out there that can hold the same amount of drives as the 7 XL (18 I believe) or more, for a regular nas build? I\u2019ve seen people say the Lian Li D8000 but it\u2019s discontinued.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141o74a", "is_robot_indexable": true, "report_reasons": null, "author": "Conorsavage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141o74a/rival_to_fractal_define_7_xl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141o74a/rival_to_fractal_define_7_xl/", "subreddit_subscribers": 686258, "created_utc": 1685991024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  \n\n\nMy question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.", "author_fullname": "t2_lh08w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FTP or Rsync to offline backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142c4um", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686048908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  &lt;/p&gt;\n\n&lt;p&gt;My question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142c4um", "is_robot_indexable": true, "report_reasons": null, "author": "NeoID", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "subreddit_subscribers": 686258, "created_utc": 1686048908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.bbc.com/news/business-65669537", "author_fullname": "t2_x843n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why millions of usable hard drives are being destroyed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142c0ru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686048605.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bbc.com/news/business-65669537\"&gt;https://www.bbc.com/news/business-65669537&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LmTQXitPm8cIHBGqyHxbPo6cvYLikefSFwxinnowOJo.jpg?auto=webp&amp;v=enabled&amp;s=5293583f25dcef051140136af70cc8bb08204f8f", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/LmTQXitPm8cIHBGqyHxbPo6cvYLikefSFwxinnowOJo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a8a40e8a1560e7751ff555c7f4caabb93bb4256", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LmTQXitPm8cIHBGqyHxbPo6cvYLikefSFwxinnowOJo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c49d1e35f5f1b6e01b791fdadaeda6e34d575302", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LmTQXitPm8cIHBGqyHxbPo6cvYLikefSFwxinnowOJo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a8eec382f8476ffe09dc70a491116465553bc75", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LmTQXitPm8cIHBGqyHxbPo6cvYLikefSFwxinnowOJo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67c60d2066a5987dc0d64ccedcf262041a4ba404", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LmTQXitPm8cIHBGqyHxbPo6cvYLikefSFwxinnowOJo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e76f498f9469b49b0e94109d720980b57840416", "width": 960, "height": 540}], "variants": {}, "id": "ymYeDGaFMz2SitA8ZkK5oUCX3LGzFXqj2Nokq-JKWRU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142c0ru", "is_robot_indexable": true, "report_reasons": null, "author": "838Joel", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142c0ru/why_millions_of_usable_hard_drives_are_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142c0ru/why_millions_of_usable_hard_drives_are_being/", "subreddit_subscribers": 686258, "created_utc": 1686048605.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use a USB drive to store my work data. The USB drive is about 250 GB, and my data typically amount to about 30 GB. I backup the data by copy/pasting them to my computer on a daily basis. Does this type of frequent copy/paste reduce the USB drive's life?", "author_fullname": "t2_23yqc963", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does copy/paste reduce USB drive's life?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1429czj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686041555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use a USB drive to store my work data. The USB drive is about 250 GB, and my data typically amount to about 30 GB. I backup the data by copy/pasting them to my computer on a daily basis. Does this type of frequent copy/paste reduce the USB drive&amp;#39;s life?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1429czj", "is_robot_indexable": true, "report_reasons": null, "author": "BOBOLIU", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1429czj/does_copypaste_reduce_usb_drives_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1429czj/does_copypaste_reduce_usb_drives_life/", "subreddit_subscribers": 686258, "created_utc": 1686041555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Community, \n\nI wanted to get your views on the different ways of working with storage.   \nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?  \nMaybe a mix model?\n\nI wanted to see how you guys are usually working and what do you feel comfortable / already tested.  \nThank you for sharing your thoughts \ud83d\ude09", "author_fullname": "t2_meba9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflows and different ways of working with storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428wiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686040255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Community, &lt;/p&gt;\n\n&lt;p&gt;I wanted to get your views on the different ways of working with storage.&lt;br/&gt;\nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?&lt;br/&gt;\nMaybe a mix model?&lt;/p&gt;\n\n&lt;p&gt;I wanted to see how you guys are usually working and what do you feel comfortable / already tested.&lt;br/&gt;\nThank you for sharing your thoughts \ud83d\ude09&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1428wiz", "is_robot_indexable": true, "report_reasons": null, "author": "fscheps", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "subreddit_subscribers": 686258, "created_utc": 1686040255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone been able to download youtube videos on the ```http_dash_segments``` protocol at speed &gt; 500KB/sec ? \n\nWhen I download a video using the ```https``` protocol, I can get speed like 10MB/sec, but other formats of the same video that are listed on the ```http_dash_segments``` protocol, speed is always capped, that is with ```Youtube Download [yt-dlp]``` or ```jDownloader```. \n\nThanks", "author_fullname": "t2_ch4bnghs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "youtube ideos: http_dash_segments protocol speed capped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14234ic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to download youtube videos on the &lt;code&gt;http_dash_segments&lt;/code&gt; protocol at speed &amp;gt; 500KB/sec ? &lt;/p&gt;\n\n&lt;p&gt;When I download a video using the &lt;code&gt;https&lt;/code&gt; protocol, I can get speed like 10MB/sec, but other formats of the same video that are listed on the &lt;code&gt;http_dash_segments&lt;/code&gt; protocol, speed is always capped, that is with &lt;code&gt;Youtube Download [yt-dlp]&lt;/code&gt; or &lt;code&gt;jDownloader&lt;/code&gt;. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14234ic", "is_robot_indexable": true, "report_reasons": null, "author": "cybercastor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14234ic/youtube_ideos_http_dash_segments_protocol_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14234ic/youtube_ideos_http_dash_segments_protocol_speed/", "subreddit_subscribers": 686258, "created_utc": 1686024601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\nThanks for reading!\n\nPlease excuse me if my noob question makes you cringe.\n\nI have been looking for a solution to set all my hdds in one single enclosure so I can access to all of them. Lurking here I read about the Terramaster d5-330 and I got one.\n\nHere is where my problems started. I followed the official guide, watching the videos and spent the last 24hrs reading all I can find on how to set it up. I failed. \n\nI want to use it in SINGLE mode that for what I understand should be pretty much plug and play, but the thunderbolt ports on my ROG ZEPHIRUS g15 only detect it as PD (power device) and I don\u2019t get the prompt to install the drivers nor the raid shows up in my device manager window.\n\nI tried force the update on the thuderbolt to install the official drivers but it got me nowhere. I installed the rain manager and the raid pro manager software. Obviously the drive doesn\u2019t show up so also that is a nonstarter. \n\nI tried several recommended hdd in different all the bays of the D5-330 with always the same results. Btw the light of the drives stay off all the time. I tested the drives before and after, and it works no problem. \n\nIs my first time fiddling with a das so I am sure I can mess up something but I can\u2019t see what \u2026 I am stupid or is this unit defective?\n\nThanks!", "author_fullname": "t2_3cl0d5g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help/troubleshooting: Terramaster D5-330 failing to be recognized, is it a dudd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1420fb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686017968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!\nThanks for reading!&lt;/p&gt;\n\n&lt;p&gt;Please excuse me if my noob question makes you cringe.&lt;/p&gt;\n\n&lt;p&gt;I have been looking for a solution to set all my hdds in one single enclosure so I can access to all of them. Lurking here I read about the Terramaster d5-330 and I got one.&lt;/p&gt;\n\n&lt;p&gt;Here is where my problems started. I followed the official guide, watching the videos and spent the last 24hrs reading all I can find on how to set it up. I failed. &lt;/p&gt;\n\n&lt;p&gt;I want to use it in SINGLE mode that for what I understand should be pretty much plug and play, but the thunderbolt ports on my ROG ZEPHIRUS g15 only detect it as PD (power device) and I don\u2019t get the prompt to install the drivers nor the raid shows up in my device manager window.&lt;/p&gt;\n\n&lt;p&gt;I tried force the update on the thuderbolt to install the official drivers but it got me nowhere. I installed the rain manager and the raid pro manager software. Obviously the drive doesn\u2019t show up so also that is a nonstarter. &lt;/p&gt;\n\n&lt;p&gt;I tried several recommended hdd in different all the bays of the D5-330 with always the same results. Btw the light of the drives stay off all the time. I tested the drives before and after, and it works no problem. &lt;/p&gt;\n\n&lt;p&gt;Is my first time fiddling with a das so I am sure I can mess up something but I can\u2019t see what \u2026 I am stupid or is this unit defective?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1420fb0", "is_robot_indexable": true, "report_reasons": null, "author": "FresconeFrizzantino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1420fb0/helptroubleshooting_terramaster_d5330_failing_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1420fb0/helptroubleshooting_terramaster_d5330_failing_to/", "subreddit_subscribers": 686258, "created_utc": 1686017968.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve heard that Reddit users will get free access to the API, so shouldn\u2019t tools like Reddit Media Downloader still work after the changes are implemented? Or will it stop working?\n\nThanks", "author_fullname": "t2_gvdnvhdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit media downloader and Reddit API changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141m2m3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685986652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve heard that Reddit users will get free access to the API, so shouldn\u2019t tools like Reddit Media Downloader still work after the changes are implemented? Or will it stop working?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141m2m3", "is_robot_indexable": true, "report_reasons": null, "author": "Fresh_Air13", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141m2m3/reddit_media_downloader_and_reddit_api_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141m2m3/reddit_media_downloader_and_reddit_api_changes/", "subreddit_subscribers": 686258, "created_utc": 1685986652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title, but I'll elaborate just in case. I did some research on the topic and found threads from 5+ years ago about Macbooks so I think it's ok to ask -\n\nI am using this storage space to hold video files and misc. assets, and have always been told to place an abundance of caution on how the hard drive is removed from the computer. Thoughtless unplugging of a hard drive could cause hours upon hours of lost work and I would like to avoid that. \n\nI've placed this external drive underneath my desk along with my docking station to save space &amp; keep them out of the way. A perfect scenario would be that I can plug my laptop in and get straight to work, and then unplug safely when I'm done, keeping the drive where it is without touching it. \n\nIs this possible, should I just eject before I unplug the USB connection to my laptop? Thank you!", "author_fullname": "t2_6md0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2 TB external drive connected to USB docking station - do I need to worry about ejecting the drive after using the dock?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141l4zw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685984789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title, but I&amp;#39;ll elaborate just in case. I did some research on the topic and found threads from 5+ years ago about Macbooks so I think it&amp;#39;s ok to ask -&lt;/p&gt;\n\n&lt;p&gt;I am using this storage space to hold video files and misc. assets, and have always been told to place an abundance of caution on how the hard drive is removed from the computer. Thoughtless unplugging of a hard drive could cause hours upon hours of lost work and I would like to avoid that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve placed this external drive underneath my desk along with my docking station to save space &amp;amp; keep them out of the way. A perfect scenario would be that I can plug my laptop in and get straight to work, and then unplug safely when I&amp;#39;m done, keeping the drive where it is without touching it. &lt;/p&gt;\n\n&lt;p&gt;Is this possible, should I just eject before I unplug the USB connection to my laptop? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141l4zw", "is_robot_indexable": true, "report_reasons": null, "author": "delmarman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141l4zw/2_tb_external_drive_connected_to_usb_docking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141l4zw/2_tb_external_drive_connected_to_usb_docking/", "subreddit_subscribers": 686258, "created_utc": 1685984789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have 4 WD Elements externals that I use for Kodi. One of which (that was not yet backed up) was tipped to the side and now won't recognize in Windows. When trying to initialize the drive I get the Hardware malfunction error.   \n\n\nIs this drive completely dead? Or is it worth shucking? Would it make a difference if it weren't in the WD enclosure?", "author_fullname": "t2_tv8y5ruk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Elements not showing up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141j7d1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685980942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have 4 WD Elements externals that I use for Kodi. One of which (that was not yet backed up) was tipped to the side and now won&amp;#39;t recognize in Windows. When trying to initialize the drive I get the Hardware malfunction error.   &lt;/p&gt;\n\n&lt;p&gt;Is this drive completely dead? Or is it worth shucking? Would it make a difference if it weren&amp;#39;t in the WD enclosure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141j7d1", "is_robot_indexable": true, "report_reasons": null, "author": "dndoldhead", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141j7d1/wd_elements_not_showing_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141j7d1/wd_elements_not_showing_up/", "subreddit_subscribers": 686258, "created_utc": 1685980942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently my Seagate Exos 16 TB started having problems reading some old files so I installed HD Sentinel and it showed 9% Health and 2000+ bad sectors.\n\nI tried taking backup of these files but the speed would go down to 0 KB/sec and just get stuck there. So I formatted the entire drive (lost data was not crucial) and tried HD Sentinel's re-initializing disk surface which is showing 0 bad/damaged sectors.\n\nSo is it actually failing?\n\n&amp;#x200B;\n\n[HD Sentinel Status](https://preview.redd.it/2ww41mdyc74b1.jpg?width=3127&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ae8affa932c691ca1426d3cb484362ea700bca33)\n\n&amp;#x200B;\n\n[Re-initialization in progress](https://preview.redd.it/gpbggke1d74b1.jpg?width=2284&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=520720085dff4ccddaf0a04de81fcd80e154c886)", "author_fullname": "t2_2vxfphjm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my drive actually failing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2ww41mdyc74b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f77e8479ab682056891f94306ed02005ec03dc9"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ae603ebfe88272fac8cf729686dcfb87282f58d"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b180b1419b2d50fe2914d2705044fb5798258419"}, {"y": 378, "x": 640, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b139b14a792324bb0018e3f287929d42b9cfa7fa"}, {"y": 568, "x": 960, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=210178e02809616c8d02ce0943af0ff052958f24"}, {"y": 639, "x": 1080, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92e04a1267567f6b482178244a9d980970286a6e"}], "s": {"y": 1851, "x": 3127, "u": "https://preview.redd.it/2ww41mdyc74b1.jpg?width=3127&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=ae8affa932c691ca1426d3cb484362ea700bca33"}, "id": "2ww41mdyc74b1"}, "gpbggke1d74b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ac3678994a2df21dceeb547ded8ea82598f4cfb"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4041f31faa5fe04e4281bb59f59ba229ab45763"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c1ae2a0b340ae1ea35690e52971f21ca4634051"}, {"y": 390, "x": 640, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c208dfc0d0bcfc1760632d38cd2c3eba02aea8e2"}, {"y": 586, "x": 960, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bacb138d7ef242fa9972c8c73bd8387c7140c60e"}, {"y": 659, "x": 1080, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60f3050053240f9e2388dd28bd22cc5348a099de"}], "s": {"y": 1395, "x": 2284, "u": "https://preview.redd.it/gpbggke1d74b1.jpg?width=2284&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=520720085dff4ccddaf0a04de81fcd80e154c886"}, "id": "gpbggke1d74b1"}}, "name": "t3_141eswz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b3uS7Lq82UGQx_xcqRSa7H5Qe5RFi1kSRqI4FhKgFUc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685972153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently my Seagate Exos 16 TB started having problems reading some old files so I installed HD Sentinel and it showed 9% Health and 2000+ bad sectors.&lt;/p&gt;\n\n&lt;p&gt;I tried taking backup of these files but the speed would go down to 0 KB/sec and just get stuck there. So I formatted the entire drive (lost data was not crucial) and tried HD Sentinel&amp;#39;s re-initializing disk surface which is showing 0 bad/damaged sectors.&lt;/p&gt;\n\n&lt;p&gt;So is it actually failing?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2ww41mdyc74b1.jpg?width=3127&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ae8affa932c691ca1426d3cb484362ea700bca33\"&gt;HD Sentinel Status&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gpbggke1d74b1.jpg?width=2284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=520720085dff4ccddaf0a04de81fcd80e154c886\"&gt;Re-initialization in progress&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141eswz", "is_robot_indexable": true, "report_reasons": null, "author": "ravenous24", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141eswz/is_my_drive_actually_failing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141eswz/is_my_drive_actually_failing/", "subreddit_subscribers": 686258, "created_utc": 1685972153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never RMA'ed anything before since I never had any bad drives until now and I recently learned about the WD site hack and I'm wondering if it's safe to send my Drive-In for replacement or wait a month or two until they stop experiencing processing delays? \n\n \nA little backstory about the drive with some more information.\n\nLast February I purchased a brand new WD 5TB Elements portable HDD from Amazon and when I got it the drive was loose in the plastic drive caddy case. When you pick it up you can hear it slide to one side a little like it was a manufacturing error. I should have returned it but since it turned on, read, &amp; write just fine with only a slightly more vibration than compared to my other WD 5TB element HDDs that I have I decided against it. I used it all the way up to December until I got another 5TB drive. When I was copying data to the new drive so I could reformat it and copy the data back fresh some of the files weren't being read and when I fired up Crystal disk info and turned out I had a couple of pending sectors and the square was yellow. After I copied everything that I could I had 37 pending sectors but luckily most of the files I couldn't retrieve we're able to be redownloaded so it wasn't a total loss. When I went to check the warranty status the WD website said it wasn't good and it was beyond the Amazon return date or any of their refund dates. I decided I was going to toss it but beforehand just in case I decide to do a couple of triple pass erase formats to see how those worked. After all the formatting was done I decided to check the Crystal Disk health one last time and to my surprise all the pending sector errors were gone so I decided to not throw it away and continue to use it since it was 5TB of free space. Over time I ended up filling it up with random junk stuff since I didn't want to use it for important stuff just in case. Eventually I got another hard drive butt bigger so I was going to organize all my files properly and yet again once I started to copy everything I ran into a few files not copying that resulted in 5 pending sector errors. I decided not to trust and use the hard drive again after I was done copying all the stuff off it and it sat on my desk for a while. A few days ago I randomly decided to register the drive to the WD site and check the warranty and it said it was still good till March of next year which is great but I then learned a day later I learned about the WD website hack and the RMA status page says they are experiencing processing delays which led me to make this post. So do I send it in now for a replacement or wait till the delays slow down?", "author_fullname": "t2_iqj3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I wait another month or two to RMA my WD Elements 5TB portable HDD or do it right now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142b3cv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686046180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never RMA&amp;#39;ed anything before since I never had any bad drives until now and I recently learned about the WD site hack and I&amp;#39;m wondering if it&amp;#39;s safe to send my Drive-In for replacement or wait a month or two until they stop experiencing processing delays? &lt;/p&gt;\n\n&lt;p&gt;A little backstory about the drive with some more information.&lt;/p&gt;\n\n&lt;p&gt;Last February I purchased a brand new WD 5TB Elements portable HDD from Amazon and when I got it the drive was loose in the plastic drive caddy case. When you pick it up you can hear it slide to one side a little like it was a manufacturing error. I should have returned it but since it turned on, read, &amp;amp; write just fine with only a slightly more vibration than compared to my other WD 5TB element HDDs that I have I decided against it. I used it all the way up to December until I got another 5TB drive. When I was copying data to the new drive so I could reformat it and copy the data back fresh some of the files weren&amp;#39;t being read and when I fired up Crystal disk info and turned out I had a couple of pending sectors and the square was yellow. After I copied everything that I could I had 37 pending sectors but luckily most of the files I couldn&amp;#39;t retrieve we&amp;#39;re able to be redownloaded so it wasn&amp;#39;t a total loss. When I went to check the warranty status the WD website said it wasn&amp;#39;t good and it was beyond the Amazon return date or any of their refund dates. I decided I was going to toss it but beforehand just in case I decide to do a couple of triple pass erase formats to see how those worked. After all the formatting was done I decided to check the Crystal Disk health one last time and to my surprise all the pending sector errors were gone so I decided to not throw it away and continue to use it since it was 5TB of free space. Over time I ended up filling it up with random junk stuff since I didn&amp;#39;t want to use it for important stuff just in case. Eventually I got another hard drive butt bigger so I was going to organize all my files properly and yet again once I started to copy everything I ran into a few files not copying that resulted in 5 pending sector errors. I decided not to trust and use the hard drive again after I was done copying all the stuff off it and it sat on my desk for a while. A few days ago I randomly decided to register the drive to the WD site and check the warranty and it said it was still good till March of next year which is great but I then learned a day later I learned about the WD website hack and the RMA status page says they are experiencing processing delays which led me to make this post. So do I send it in now for a replacement or wait till the delays slow down?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142b3cv", "is_robot_indexable": true, "report_reasons": null, "author": "extremebs", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142b3cv/should_i_wait_another_month_or_two_to_rma_my_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142b3cv/should_i_wait_another_month_or_two_to_rma_my_wd/", "subreddit_subscribers": 686258, "created_utc": 1686046180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if anyone did manage to archive Synology's old DSM and package versions?\n\n[web.archive.org](https://web.archive.org) has archived a lot of it, but it didn't archive the larger files. [https://web.archive.org/web/20210416132208/https://archive.synology.com/download](https://web.archive.org/web/20210416132208/https://archive.synology.com/download)\n\nThere were 2 threads about Synology deleting all it's old  DSM and package versions:\n\n1. [https://www.reddit.com/r/DataHoarder/comments/10da7a9/official\\_synology\\_download\\_site\\_closing\\_legacy/](https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/)\n2. [https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder\\_synology\\_legacy\\_dsm\\_firmware\\_package/](https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/)\n\nWhile there were people who started downloading all the old DSM and package versions it seemed like everyone gave up before they had downloaded everything (myself included).", "author_fullname": "t2_33srm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did anyone manage to archive Synology's old DSM and package versions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424mqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone did manage to archive Synology&amp;#39;s old DSM and package versions?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org\"&gt;web.archive.org&lt;/a&gt; has archived a lot of it, but it didn&amp;#39;t archive the larger files. &lt;a href=\"https://web.archive.org/web/20210416132208/https://archive.synology.com/download\"&gt;https://web.archive.org/web/20210416132208/https://archive.synology.com/download&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There were 2 threads about Synology deleting all it&amp;#39;s old  DSM and package versions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;While there were people who started downloading all the old DSM and package versions it seemed like everyone gave up before they had downloaded everything (myself included).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "186TB local", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424mqq", "is_robot_indexable": true, "report_reasons": null, "author": "DaveR007", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1424mqq/did_anyone_manage_to_archive_synologys_old_dsm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424mqq/did_anyone_manage_to_archive_synologys_old_dsm/", "subreddit_subscribers": 686258, "created_utc": 1686028490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for something like [https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC](https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC) but with USB C.  \nSomething like [https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S](https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S) but a memory stick.  \n\n\nHas anyone came across anything similar? Im failing to find anything.  \nP.S. if there is more appropriate channel, let me know.", "author_fullname": "t2_173zc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Physically smallest USB C memory stick", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141kvwp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685984292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something like &lt;a href=\"https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC\"&gt;https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC&lt;/a&gt; but with USB C.&lt;br/&gt;\nSomething like &lt;a href=\"https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S\"&gt;https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S&lt;/a&gt; but a memory stick.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone came across anything similar? Im failing to find anything.&lt;br/&gt;\nP.S. if there is more appropriate channel, let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141kvwp", "is_robot_indexable": true, "report_reasons": null, "author": "grannyno", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141kvwp/physically_smallest_usb_c_memory_stick/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141kvwp/physically_smallest_usb_c_memory_stick/", "subreddit_subscribers": 686258, "created_utc": 1685984292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a piece of software out there that can inspect a bunch of video files, then output the results to some sort of sortable table (like Excel)?\n\nI have tried MediaInfo but it seems a bit too \"cody\" for me - I have tried for over 20 minutes to get it to export something meaningful but the templates aren't much help and I don't even understand when I edit a template, it outputs something completely different to what I edited.\n\nThere is something called \"media center master\" which is great but a little too overpowered for what I need I think.\n\nAny others to consider?", "author_fullname": "t2_mre85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software to Inspect a Bunch of Video Files, Output to Excel", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141hviu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685978313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a piece of software out there that can inspect a bunch of video files, then output the results to some sort of sortable table (like Excel)?&lt;/p&gt;\n\n&lt;p&gt;I have tried MediaInfo but it seems a bit too &amp;quot;cody&amp;quot; for me - I have tried for over 20 minutes to get it to export something meaningful but the templates aren&amp;#39;t much help and I don&amp;#39;t even understand when I edit a template, it outputs something completely different to what I edited.&lt;/p&gt;\n\n&lt;p&gt;There is something called &amp;quot;media center master&amp;quot; which is great but a little too overpowered for what I need I think.&lt;/p&gt;\n\n&lt;p&gt;Any others to consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141hviu", "is_robot_indexable": true, "report_reasons": null, "author": "banisheduser", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141hviu/software_to_inspect_a_bunch_of_video_files_output/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141hviu/software_to_inspect_a_bunch_of_video_files_output/", "subreddit_subscribers": 686258, "created_utc": 1685978313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title speaks for itself. I want to have a redundancy but unfortunately I have 400 gigs left of storage on my 1TB iPhone and my photo library is 800 gigs so I can't just install the Amazon photo app and press sync becausae then it will upload the super bad quality preview picture on my phone.   I do have all originals in the Mac so I was wondering if there is any way to upload them from there to Amazon but also preserving things such as my photo folders and orc the date the picture was taken and the GPS coordinates of each picture.", "author_fullname": "t2_1vvn658m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I export my iCloud photo library to Amazon Photos on Mac OS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141e1x7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685970511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title speaks for itself. I want to have a redundancy but unfortunately I have 400 gigs left of storage on my 1TB iPhone and my photo library is 800 gigs so I can&amp;#39;t just install the Amazon photo app and press sync becausae then it will upload the super bad quality preview picture on my phone.   I do have all originals in the Mac so I was wondering if there is any way to upload them from there to Amazon but also preserving things such as my photo folders and orc the date the picture was taken and the GPS coordinates of each picture.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141e1x7", "is_robot_indexable": true, "report_reasons": null, "author": "EthanColeK", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141e1x7/how_can_i_export_my_icloud_photo_library_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141e1x7/how_can_i_export_my_icloud_photo_library_to/", "subreddit_subscribers": 686258, "created_utc": 1685970511.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}